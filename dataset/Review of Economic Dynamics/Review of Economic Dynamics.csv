name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Bredemeier Christian,Gravert Jan,Juessen Falko","University of Wuppertal, Germany,IZA, Germany","Received 26 July 2021, Revised 5 June 2023, Available online 13 June 2023.",https://doi.org/10.1016/j.red.2023.06.002,Cited by (0),"The Frisch elasticity of labor supply can be estimated by regressing hours worked on the hourly wage rate, controlling for consumption of the individual worker. However, most household panel surveys contain consumption information only at the household level. We show that proxying individual consumption by household consumption biases estimated Frisch elasticities downward as limited commitment in the household induces individual consumption to behave differently from household consumption. We develop an improved estimation approach that eliminates this bias by exploiting information on the ==== of household consumption to infer its ====. Using PSID data, we estimate Frisch elasticities of about 0.65 for men and 0.8 for women.","The Frisch elasticity of labor supply is an important concept in both labor economics and macroeconomics. It measures how willingly individuals substitute hours worked intertemporally, which is decisive for labor-supply responses to temporary changes in wage rates and to life-cycle patterns in wage rates. It is also a key determinant of the cost of business cycles and the fiscal multiplier. Despite its importance, there is no consensus about a range of values for the Frisch elasticity with estimates from microeconometric studies diverging substantially from the results of macroeconomic studies, see, e.g., Keane and Rogerson (2015).====The Frisch elasticity is defined as the reaction of labor supply to changes in wage rates holding marginal utility of wealth constant. Hence, it can be estimated in a regression of hours worked on the wage rate when controlling for consumption, the latter being closely tied to the marginal utility of wealth, see, e.g., Altonji (1986). Building on recent advances in family economics emphasizing that commitment within the household is limited (Voena, 2015; Chiappori and Mazzocco, 2017; Ábrahám and Laczó, 2018), we show that the relevant consumption variable would be consumption of the individual worker, which is, however, usually not observed in household panel surveys, and that using household consumption instead of individual consumption biases the estimated Frisch elasticity.==== Due to limited commitment, individual household members' shares in total household consumption vary over time and reflect members' varying bargaining positions for which wage rates are important determinants. Omitting information on how consumption is distributed between household members thus leads to a bias. Using simulated data from a calibrated life-cycle model of household labor supply with limited commitment between spouses, we find that a standard labor-supply regression yields an estimated Frisch elasticity that is about 20% below its true value when household consumption rather than individual consumption is controlled for.====The intuition behind the estimation bias is as follows. When commitment between spouses is limited, a rise in an individual's wage rate exerts three effects on the individual's labor supply. First, labor supply increases due to a conventional substitution effect that is governed by the Frisch elasticity and can be used to recover this elasticity. Second, household consumption increases which induces labor supply to fall due to a wealth effect. Third, the individual's bargaining position in the household may improve, reflecting the improvement of the individual's outside option. This leads the household to grant more leisure to the individual thereby reducing the individual's labor supply through a bargaining effect. Both the wealth effect and the bargaining effect could be accounted for with data on individual consumption, but data on household consumption can only account for the wealth effect but not the bargaining effect. Yet, to assess the labor-supply impact of variations in the general wage level, such as business-cycle fluctuations or broad tax cuts, it is important to isolate the substitution effect from the bargaining effect.====We use our model to develop an improved estimation approach that eliminates the bias due to limited commitment and can be applied when data on individual consumption is not available. The key idea of our approach is that information on ==== consumes how much of total household consumption can be inferred from ==== the household purchases. Information on the latter is provided in household panel surveys such as the Panel Study of Income Dynamics (PSID) which covers a variety of different consumption items. The limited-commitment paradigm implies that relative bargaining weights determine the distribution of consumption between household members and, when preferences over consumption goods differ across household members, also how much a household spends on different goods. The (observable) composition of household consumption is thus informative about its (unobservable) distribution. We show analytically that in a labor-supply regression which also controls for the composition of household consumption, i.e., households' expenditure shares on different goods and services, and not just total household consumption expenditures, the coefficient on the hourly wage rate is an unbiased estimate of the Frisch elasticity.==== Monte-Carlo experiments using synthetic data from a calibrated model version demonstrate that our approach successfully eliminates the bias.====We then apply our approach to household panel data from the PSID. The results are strongly supportive of our theoretical results and our improved estimation approach yields a significantly larger estimate for the Frisch elasticity than the standard approach. In our specifications that correct for the bias due to limited commitment, the estimated Frisch elasticity for men is around 0.65. Our results are robust across a wide range of specifications of our estimation approach. Specifically, we consider alternative ways to incorporate information on the composition of household consumption, e.g., using as control variables expenditure shares on aggregated categories of consumption, or using principal components of detailed expenditure shares. Further, we use different kinds of wage variation for the empirical analysis. We also present estimation results for women, taking into account particularities discussed in the literature when estimating labor-supply regressions for women, such as selection and home production. For women, we estimate Frisch elasticities between 0.8 and 0.85.====Our paper contributes to both, the literature on estimating labor-supply elasticities and the literature on limited commitment between spouses in marriages. The latter has been surveyed by Chiappori and Mazzocco (2017).==== Keane (2011) provides a comprehensive survey of the literature on estimating labor-supply elasticities. The micro/macro puzzle on the elasticity of labor supply is a central question in this literature, see Keane and Rogerson (2015) for an overview. Several studies have contributed to reconciling micro and macro estimates by pointing out a number of downward biases in microeconometric estimates, see, e.g., Blomquist (1985, 1988), Alogoskoufis (1987), Heckman (1993), Rupert et al. (2000), Imai and Keane (2004), and Domeij and Floden (2006). Our empirical analysis will take these biases into account and we contribute to this literature by showing that limited commitment between spouses – if not appropriately corrected for – biases estimates of the Frisch elasticity downward. We also contribute to the part of the labor-supply literature that exploits quasi-experimental wage variation, see Bartik (1991), and Blanchard et al. (1992), as well as Blundell et al. (1998) for seminal papers on shift-share instruments and tax reforms, respectively. When the instruments also impact on one's partner's wage, an instrumental-variable approach can be biased since the estimated labor-supply elasticities will also include the bargaining effect of wage changes. We illustrate this point by estimating shift-share type regressions using as instruments industry-specific and region-specific wage variation.====While correcting for bargaining effects is a novel contribution of our paper, there are different approaches to deal with the basic challenge that labor supply depends on the marginal utility of wealth. Broadly speaking, these approaches can be categorized as proxy-variable strategies and instrumental-variable strategies, respectively. Our approach builds on Altonji (1986) who proposed to use consumption as a proxy for marginal utility and we add to this the consumption composition as a proxy for relative household bargaining power. Altonji (1986) also proposed an alternative approach that estimates the labor supply condition in growth rates and identifies the Frisch elasticity as the coefficient on expected wage growth. In the spirit of an instrument, the latter is uncorrelated with changes in marginal utility when households have sufficient access to insurance or borrowing possibilities. For their empirical analysis of insurance effects in the household, Blundell et al. (2016) develop an instrument-type GMM approach that can be used to identify the Frisch elasticity. While abstracting from commitment problems and their main focus being on the empirical pass-through of permanent shocks, their approach is also informative about Frisch elasticities, because their identification critically distinguishes between the reactions to permanent and transitory shocks, with the labor-supply reaction to identified transitory shocks being a potentially unbiased estimate of the Frisch elasticity. The intuition is similar to the one for expected wage growth. When households do not face binding borrowing constraints, purely transitory changes in wages induce no change in marginal utility as they affect lifetime income only to a negligible degree. We use our life-cycle model with limited commitment to quantify how well the Blundell et al. (2016) GMM approach performs when household decisions are characterized by limited commitment. We document that the Blundell et al. (2016) approach produces smaller biases than methods proposed by previous papers, but the estimated Frisch elasticity is still biased downward in the presence of limited commitment. To obtain an unbiased estimate of the Frisch elasticity using the Blundell et al. (2016) approach, this approach would need to be extended accordingly – just like the levels approach of Altonji (1986), the limited-commitment extension of which our paper provides. Our bias-corrected estimate for the Frisch elasticity is at the upper bound of estimates presented in previous studies and close to the values reported in previous studies that have relied on expected wage growth or transitory wage changes, see Pistaferri (2003), Domeij and Floden (2006), and Bredemeier et al. (2019) for the former and Blundell et al. (2016) for the latter.====Classifying approaches in terms of proxies and instruments helps identify their respective advantages and disadvantages. On the one hand, a bias is induced into our approach if the proxy error is correlated with the wage rate. For example, inferiority or superiority of certain goods, or preference heterogeneity within gender can induce expenditure shares to depend on wealth or age, respectively, both of which are correlated with wages, which implies potential limitations of our approach. On the other hand, the instrument-type approaches are subject to problems due to a potential correlation of the instruments with marginal utility or bargaining positions, or problems associated with the identification of the instruments in the data. Regarding the first challenge, consumption insurance possibilities are limited in the real world and a substantial share of households has close to no wealth (see Kuhn and Ríos-Rull, 2016) reducing their ability to self-insure.==== For this reason, Blundell et al. (2016) consider an age-restricted sample excluding young households, which are most strongly affected by borrowing constraints. Regarding the second challenge, wage growth – in contrast to wage levels which are used in our paper – is hard to predict and hence instruments tend to be weak, see Keane (2011) for a discussion, and transitory wage fluctuations can be hard to disentangle from measurement error in the data. Our own regression-based approach is not subject to this challenge, as it is much simpler to implement, requiring only an additional control variable that is directly observable. Given their methodological differences and respective strengths and weaknesses, the different approaches to estimating the Frisch elasticity should be seen as complements to each other.====The remainder of this paper is organized as follows. In Section 2, we present the model and analyze labor-supply regressions analytically. In Section, 3, we perform the Monte-Carlo analysis. In Section 4, we apply our estimation approach to PSID data. Section 5 concludes.",Accounting for Limited Commitment between Spouses when Estimating Labor-Supply Elasticities,https://www.sciencedirect.com/science/article/pii/S1094202523000212,Available online 13 June 2023,2023,Research Article,0.0
Tryphonides Andreas,"University of Cyprus, Panepistimiou 1, 2109, Nicosia, Cyprus","Received 3 September 2022, Revised 28 May 2023, Available online 8 June 2023.",https://doi.org/10.1016/j.red.2023.06.001,Cited by (0),This paper shows that utilizing information on the extensive margin of financially constrained households can narrow down the set of admissible preferences in a large class of ,"Identifying agent preferences from aggregate data has a long tradition in macroeconomics and consumption based asset pricing. One of the key challenges in recovering preferences by using the standard representative agent Euler equation has been the fact that households are often subject to some form of liquidity constraint. The presence of occasionally binding constraints at the household level, as well as other market imperfections, also implies that agents' intertemporal marginal rates of substitution are not equalized in equilibrium, leading to non-negligible consumption dispersion that varies over time. Both of these facts render the frictionless representative agent restriction invalid.====There is a growing literature on estimating fully specified heterogeneous agent models with specific mechanisms for restrictions on borrowing and limited risk sharing.==== However, a natural question to ask is whether we can still obtain reasonable estimates of preferences using aggregate restrictions akin to the standard Euler equation that are consistent with the implications of financial constraints at the household level.==== This paper illustrates that in comparison to the standard representative agent framework, accounting for the presence of constraints generates inequalities in the aggregate Euler equation. The latter also features an aggregation wedge due to imperfect risk sharing.====The presence of weak moment inequalities leads to set identification for preferences. The set of parameter estimates that cannot be rejected directly corresponds to the set of models that generate aggregate frictions that are compatible with financial constraints.====The key result of this paper is that information on the extensive margin of adjustment, i.e. the proportion of agents whose behavior is distorted over time due to financial frictions, can effectively narrow down the set of admissible preferences. The moment inequality approach is theoretically shown to be robust to under-predicting this margin in the data. This is important as defining which group of consumers is constrained or not is subject to interpretation and data availability. The paper provides simulation evidence that the informativeness of this margin is not compromised by measurement error, nor by the fact that cross-sectional information about consumption may be available at a lower frequency than macroeconomic aggregates.====The empirical application provides further support to these results by estimating preferences using Spanish macro and aggregated micro data. The proportion of constrained consumers is measured by combining information from the Survey of Household Finances (SHF) and the Business and Consumer Survey (BCS).====The paper's results on the importance of the extensive margin for sharper identification have several implications. First, it contributes to our understanding that since these models are highly non-linear, certain moments can provide information about preferences without necessarily having a first order predictive power for the level of macroeconomic time series. This is indeed the case for the borrowing constrained, as these individuals might represent a small proportion of the population, so their consumption choices may not substantially affect the ==== of aggregate consumption, yet fluctuations in this proportion are informative for preferences. Second, had we not accounted for the extensive margin and the growth rate of cross sectional dispersion in real consumption, preference parameters would absorb the neglected variation. This can in fact help reconcile the gap between micro and macro estimates of elasticities, contributing to the literature that studies the possible determinants of this gap. The result speaks to both the equity premium puzzle (Mehra and Prescott, 1985), in the sense that a lower degree of relative risk aversion is needed to rationalize the co-movement of consumption growth and returns, and the “Frisch elasticity puzzle” (Chetty et al., 2011; Keane and Rogerson, 2015, Keane and Rogerson, 2012), which concerns the gap between labor supply elasticities estimated using individual and aggregate data.====Finally, the paper argues that more credible identification is not only important from an econometric point of view, but it has implications for consumption based asset pricing as well. More particularly, a useful byproduct of the employed methodology is an estimate of the implied distortions to the Euler equation. In a counterfactual experiment, I show that shutting down the estimated distortions renders the representative agent model unable to rationalize the observed equity premium since 2001, highlighting the importance of borrowing constraints and limited risk sharing. Moreover, an accounting exercise in the spirit of Chari et al. (2007) shows that in most of the set valued predictions transaction costs seem to dominate trading frictions (e.g. restrictions on short selling) in distorting capital markets. The sharper estimates obtained using the extensive margin contribute to the precision of this type of inference as well, as not incorporating this information would instead produce evidence in favor of trading frictions.",Identifying Preferences when households are financially constrained,https://www.sciencedirect.com/science/article/pii/S1094202523000200,Available online 8 June 2023,2023,Research Article,1.0
"Gemma Yasufumi,Kurozumi Takushi,Shintani Mototsugu","International Department, Bank of Japan, 2-1-1 Nihonbashi-Hongokucho, Chuo-ku, Tokyo, 103-8660, Japan,Monetary Affairs Department, Bank of Japan, 2-1-1 Nihonbashi-Hongokucho, Chuo-ku, Tokyo, 103-8660, Japan,Faculty of Economics, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033, Japan,Institute for Monetary and Economic Studies, Bank of Japan, 2-1-1 Nihonbashi-Hongokucho, Chuo-ku, Tokyo, 103-8660, Japan","Received 3 June 2022, Revised 31 March 2023, Available online 5 June 2023.",https://doi.org/10.1016/j.red.2023.05.003,Cited by (0),"Inflation dynamics are investigated by estimating a generalized version of the New Keynesian Phillips curve (NKPC) of Galí and Gertler (1999) using Bayesian GMM. US macroeconomic data suggests that the generalized NKPC (GNKPC) performs best in terms of quasi-marginal likelihood among those considered both during and after the Great Inflation period. The estimated GNKPC indicates that when trend inflation fell after the Great Inflation period, the probability of price change decreased and the GNKPC flattened, which is in line with findings by previous studies.","The dynamics of inflation have long been the subject of intense investigation in macroeconomics. To describe inflation dynamics, the New Keynesian Phillips curve (NKPC) is often derived by assuming either zero trend inflation or price indexation to trend and lagged inflation.==== However, these assumptions in the canonical NKPC are at odds with empirical observations. Recent studies thus examine the effect of nonzero trend inflation on the NKPC, particularly without the indexation.==== The studies have shown that such a generalized NKPC (GNKPC) has substantially distinct features from the canonical NKPC, thereby generating important implications for policy and welfare. This finding raises the question as to which is a more plausible description of inflation dynamics, the GNKPC or the canonical NKPC.====This paper estimates and evaluates the GNKPC using a novel model selection procedure under the framework of limited-information Bayesian estimation. In the empirical literature on NKPCs, two main approaches have been adopted: limited-information (or single-equation) methods and full-information (or system) methods. For example, the GMM estimation of the NKPC in Galí and Gertler (1999) and the minimum distance estimation of the GNKPC in Cogley and Sbordone (2008) can be classified as limited-information methods. On the other hand, NKPCs in the estimated dynamic stochastic general equilibrium (DSGE) models of Christiano et al. (2005) and Smets and Wouters (2007) can be categorized as full-information methods. In a recent paper, Hirose et al. (2020) conduct a full-information Bayesian analysis to compare a GNKPC and an NKPC in an otherwise identical DSGE model, and show that the model with the GNKPC outperforms that with the NKPC in terms of marginal likelihood. However, as Mavroeidis et al. (2014) emphasize in their review of the empirical literature on NKPCs, “By imposing a theoretical model on all the variables in the system, full-information methods have the potential to improve estimator precision, but they also introduce the risk of misspecification in other equations, inducing bias or inconsistency of the NKPC parameters of interest (p. 125).” In contrast, the limited-information methods, including the single-equation GMM used by Galí and Gertler (1999), are much less subject to such a misspecification issue. Yet the GMM estimation of NKPCs has been known to suffer from a weak-identification problem, especially when NKPCs are nearly flat, or when the variation of inflation expectations is small as a result of successful monetary policy in anchoring the expectations (Mavroeidis et al., 2014, p. 139). To circumvent the weak-identification issue, our paper estimates the GNKPC with Bayesian GMM and evaluates its empirical performance using the quasi-marginal likelihood (QML) proposed by Inoue and Shintani (2018), who theoretically show that the model selection consistency of QML is robust to the presence of weakly identified parameters.====To reconcile NKPCs with the inflation data that exhibit persistence, previous studies have suggested introducing backward-looking price setting because it generates inflation inertia in NKPCs. Such an empirically plausible version of the NKPC is often called the hybrid NKPC. In a Calvo (1983) staggered price model, Galí and Gertler (1999) incorporate rule of thumb (ROT) price setters, while Woodford (2003) embeds dynamic price indexation (DPI) to lagged inflation. In the ROT specification, the hybrid NKPC is based on the presence of two types of firms, optimizing and ROT firms. In each period, both types of firms set prices using (full) indexation to trend inflation with a certain probability, while with the remaining probability, optimizing firms choose prices optimally and ROT firms adjust prices using a backward-looking ROT. In the DPI specification, the hybrid NKPC arises from the price-setting behavior of firms that adjust prices using (full) DPI to an average of lagged and trend inflation with a certain probability and optimize prices with the remaining probability in each period. Inoue and Shintani (2018) estimate these two types of the hybrid NKPC using Bayesian GMM and show that the Galí-Gertler NKPC outperforms the Woodford NKPC in terms of QML.==== In light of this empirical result, our paper employs the Galí-Gertler NKPC as the baseline NKPC to be compared with the GNKPC.====As our primary specification of the GNKPC, we consider a generalized version of the Galí-Gertler NKPC in which all types of firms with a certain probability keep prices unchanged instead of setting prices with the indexation. This small change gives rise to two notable features of the resulting GNKPC that are substantially different from those of the Galí-Gertler NKPC. First, the driving force of inflation in the GNKPC includes not only the real marginal cost, which further consists of the real unit labor cost and relative price distortion, but also the expected growth rates of future demand and the expected discount rates on future profits under nonzero trend inflation. Second, and more importantly, the GNKPC slope and inflation-inertia coefficients depend on the level of trend inflation, as well as the probability of price change and the fraction of ROT firms. These two features hold even for a Galí-Gertler GNKPC in which all types of firms with a certain probability set prices using partial indexation to trend inflation, called the Galí-Gertler GNKPC with indexation.====In addition to the Galí-Gertler GNKPC, we also consider a generalized version of the Woodford NKPC in which firms with a certain probability set prices using partial DPI to an average of lagged and trend inflation instead of the full DPI. This Woodford GNKPC also has two features similar to those of the Galí-Gertler GNKPC and includes as a special case a simple variant of the GNKPC of Cogley and Sbordone (2008) in which trend inflation is constant.==== Moreover, we consider a GNKPC that (theoretically) nests both Galí-Gertler and Woodford GNKPCs.====The main findings of the paper are twofold. First, US macroeconomic data suggests that the Galí-Gertler GNKPC outperforms the baseline (Galí-Gertler) NKPC, the Galí-Gertler GNKPC with indexation, the Woodford and the Cogley-Sbordone GNKPCs, and the GNKPC that nests both Galí-Gertler and Woodford GNKPCs, in terms of QML, both during and after the Great Inflation period. Second, the estimated Galí-Gertler GNKPC indicates that when trend inflation fell after the Great Inflation period, the probability of price change decreased and the GNKPC flattened. Therefore, the Phillips curve Alan Greenspan (and Ben Bernanke) faced is not the same as the one Paul Volcker had faced, as conjectured by Ball et al. (1988).====These findings are comparable to empirical evidence reported in previous studies. Our finding that the Galí-Gertler GNKPC empirically outperforms the Galí-Gertler NKPC coincides with the result of Hirose et al. (2020), which is based on the full-information Bayesian method.==== Since the Galí-Gertler and the Woodford GNKPCs are generalizations of their NKPC counterparts, our finding that the Galí-Gertler GNKPC outperforms the Woodford GNKPC is analogous to the result of Inoue and Shintani (2018) on the NKPC counterparts. While Cogley and Sbordone (2008) reach the conclusion that there is no need for backward-looking price setting in their GNKPC once drifting trend inflation is incorporated in it, our finding suggests that their conclusion may depend on the specification of backward-looking price setting (i.e., DPI to lagged inflation), in addition to the drifting trend inflation. In our estimated Galí-Gertler GNKPC, the probability of price change decreased after the Great Inflation period, in line with the micro evidence of Nakamura et al. (2018) that the frequency of regular price change declined after that period. The flattening of the estimated GNKPC is consistent with the empirical results of Benati (2007), Ball and Mazumder (2011), and the International Monetary Fund (2013), among others.====The remainder of the paper proceeds as follows. Section 2 presents our main specification of the GNKPC. Section 3 explains our method and data for estimating and evaluating it. Section 4 shows the results of the model selection and accounts for the estimation results of the selected model. Section 5 concludes.",Trend Inflation and Evolving Inflation Dynamics: A Bayesian GMM Analysis,https://www.sciencedirect.com/science/article/pii/S1094202523000194,Available online 5 June 2023,2023,Research Article,2.0
"Ohanian Lee E.,Orak Musa,Shen Shihan","University of California, Los Angeles, Department of Economics, 8283 Bunche Hall, Box 951477, Los Angeles, CA 90095-1477, United States of America,Board of Governors of the Federal Reserve System, 20th St. and Constitution Ave. NW, Washington, DC 20551, United States of America","Received 26 April 2021, Revised 11 March 2023, Available online 30 May 2023.",https://doi.org/10.1016/j.red.2023.05.002,Cited by (0),This paper analyzes the quantitative contribution of capital-skill complementarity in accounting for rising wage ,"Krusell et al. (2000) (KORV, henceforth) found that empirically plausible differences in substitution elasticities between skilled labor and capital equipment, and unskilled labor and capital equipment, coupled with rapid growth in the stock of quality-adjusted capital equipment, can largely account for changes in the U.S. skill premium from 1963 to 1992. Krusell et al. (2000) thus provided a theory of skill-biased technological change, showed how to measure that change, and quantified its importance in understanding wage inequality.====KORV's production technology and substitution elasticity estimates continue to be used by other researchers studying inequality and related labor market topics, and the conclusions of Krusell et al. (2000) regarding the importance of capital-skill complementarity continue to be cited in the literature. However, there have been a number of important changes since 1992 that may affect the estimated elasticities of KORV and the quantitative importance of capital-skill complementarity in accounting for wage inequality.====One key change is that information, communications, and other advanced technologies that in part motivated the conceptual basis for the KORV production function, have advanced enormously since 1992.==== To put some of these changes in perspective, we note that in 1992, the last year of the KORV estimation period, Lotus 1-2-3 was one of the most popular and sophisticated business software programs, and smart phones, online commerce, cloud computing, 3-D printing, and the portable document formatting technology (PDF), among many other technologies used today had not yet arrived.====Another important change is that labor's share of income, which is a moment condition in the Krusell et al. (2000) estimation, and which was quite stable in their 1963-1992 estimation period, has declined significantly since 1992.====Yet another change is that depreciation rates have increased and have become more volatile since 1992. Depreciation not only affects the accumulation of capital stocks, but also affects one of the moment conditions in the Krusell et al. (2000) estimation, which in turn may affect the estimated elasticities.====This paper studies the KORV framework in light of these changes. To our knowledge, this paper provides the most comprehensive assessment of this framework's empirical performance, with a focus on its ability to account for the skill premium and labor's share of income.====We update the KORV dataset through 2019, estimate the model parameters, and analyze the model fit and its implications for the skill premium.====To address changes in depreciation rates since 1992, we conduct the analysis with both time-varying and constant depreciation rates. To address the decline in the labor share, we use labor income measured using gross output and using output net of depreciation, as well as using income from the non-farm business sector. To address the remarkable technological advances in some forms of capital equipment, we specify the complementary capital stock to skilled labor as Information-Communications-Technology (ICT) capital, as well as use the KORV baseline specification of total equipment capital.====As in Krusell et al. (2000), the model's skill premium is not part of the model's estimation, but rather is an outcome of the estimated parameters and data. We find that the estimated elasticities of Krusell et al. (2000) have changed little since 1992; and are robust to changes in depreciation measurement,measurement of labor share, and the type of capital that is most complementary to skilled labor. We find that KORV framework continues to account for much of the U.S. skill premium through 2019.====However, the model's fit for the labor share deteriorates when more recent data are applied in the analysis. This arises because capital-skill complementarity, combined with even faster capital-biased technological change measured in recent data, tends to increase the model's labor share by driving up the productivity of skilled labor.====We therefore frame the three modifications to Krusell et al. (2000) that we study - ICT capital as the complementary capital stock, different measures of labor's share of income, and changes in capital depreciation rates - as potential avenues for understanding the model's labor share deviation. We also use generalized method of moments (GMM) as an alternative estimation method to KORV's full-information likelihood-based method to assess whether KORV's estimator is affecting the model's ability to account for the labor share.====We find that changing the concept of capital-skill complementarity from equipment capital, as in Krusell et al. (2000), to ICT capital captures much of the skill premium over the entire period and modestly improves the model's fit of the labor share, accounting for about half of the drop in labor's share between the early 1960s and 2019. However, the model does not capture the much more recent declines in labor's share that occurred in the last 15 years.====The rest of the paper proceeds as follows. Section 2 summarizes related literature. Section 3 summarizes the data and its construction. Section 4 presents the theoretical model, and Section 5 presents the quantitative analysis. Section 6 presents a summary and conclusion.","Revisiting Capital-Skill Complementarity, Inequality, and Labor Share",https://www.sciencedirect.com/science/article/pii/S1094202523000182,Available online 30 May 2023,2023,Research Article,3.0
"Fan Haichao,Nie Guangyu,Xu Zhiwei","Institute of World Economy, Fudan University, Shanghai, China,School of Economics, Fudan University, Shanghai, China,College of Business, Shanghai University of Finance and Economics, China,Peking University HSBC Business School, China","Received 7 December 2021, Revised 26 April 2023, Available online 12 May 2023.",https://doi.org/10.1016/j.red.2023.05.001,Cited by (0)," facing tighter financial constraints than in others. We propose a dynamic trade model to explain the facts. Greater uncertainty depresses a firm's expected value of exporting and borrowing capacity, leading to fewer exporters and smaller average size of exports. Under calibrated parameters, the uncertainty shock accounts for a sizable fraction of China's trade collapse in the 2008 financial crisis.","The global financial crisis caused substantial economic turbulence and a surge in economic uncertainty (Baker et al., 2016). Following the Great Recession, international trade experienced an unprecedented contraction known as the Great Trade Collapse.==== The contraction of trade in the two largest economies, the United States and China, reached approximately 20% in 2008-2009, significantly above the worldwide average rate of 12%. How do exporters respond to a more uncertain global economy? What is the underlying mechanism through which market uncertainty affects firm-level exporting decisions? Is market uncertainty quantitatively important for understanding the Great Trade Collapse? The aim of this paper is to address the above questions, taking the Chinese economy as a laboratory. The reasons we focus on China are twofold. First, China, the second-largest economy in the world, experienced a much more severe trade collapse than other countries. Second, the financial crisis struck the Chinese economy directly and extensively through foreign demand (i.e., international trade) instead of its own financial system. Thereby, the Chinese economy serves as an ideal context to study the consequences of foreign market uncertainty for trade dynamics.====We first document some stylized facts regarding firms' exporting decisions and market uncertainty. The data we use are from China's customs database, which is comprehensive and contains detailed information for firm-level transactions between China and its trading partners. We analyze the impacts of foreign market uncertainty on the number of exporters (extensive margin) and the average value of trade (intensive margin). The results suggest that an increase in market uncertainty in the destination country significantly reduces both the extensive and intensive margin components. The adverse effects are more pronounced in the industries facing tighter external financing conditions than in other industries. This fact suggests that financial constraints serve as an important channel for amplifying the uncertainty shocks affecting international trade. Our empirical findings are robust to various econometric specifications and different measures of market uncertainty and financial constraint.====We then propose a dynamic trade theory to account for the facts. We start with a simple two-period model to illustrate the intuition. The model features heterogeneous firms that face idiosyncratic foreign demand shocks and financial constraints. The standard deviation of idiosyncratic foreign demand captures foreign market uncertainty. Each firm has two options: exporting in the first period or waiting until the next period. Entering the foreign market incurs a fixed cost. An increase in market uncertainty raises the option value of waiting, leading to a standard wait-and-see effect on the firm's entry decision. In addition, in our paper, there is a novel financial channel to transmit the uncertainty shock. Specifically, the borrowing constraint distorts the production decisions of those firms with high foreign demand. This is because the firms cannot expand their production to the desired level to meet the high foreign demand due to their limited borrowing capacity. Greater market uncertainty (mean-preserved) increases both the upper and bottom tails of the demand distribution. The firms cannot benefit from an increase in the upper tail risks but suffer more from bottom tail risks. As a consequence, greater market uncertainty dampens the expected value of entering the foreign market, leading to a negative effect on the extensive margin of exports. In addition, greater uncertainty depresses the firm's collateral value, which is proportional to the expected firm value. This further dampens the scale of production for exporters, leading to a negative effect on the intensive margin of exports. Note that the standard wait-and-see channel fails to generate a negative response of the intensive margin to an uncertainty shock because of the selection effect.==== Therefore, the response of the intensive margin of exports to the uncertainty shock provides a key identification for the financial constraint channel. In this sense, our model proposes a novel channel that propagates uncertainty shocks to international trade.====We generalize the two-period model to a fully fledged dynamic general equilibrium framework. We calibrate the model's deep parameters by matching the model-implied moments with those in the Chinese data. The dynamic analysis shows that a positive uncertainty shock reduces the aggregate export on both the extensive and intensive margins. Moreover, the uncertainty shock also causes an economic recession by reducing the aggregate output, and the aggregate export presents a larger decline relative to the aggregate output. Thereby, our dynamic model of trade fits the observed facts quite well. In the counterfactual analysis, the model with the wait-and-see channel alone fails to generate a negative intensive margin of exports in response to an uncertainty shock because of the selection effect. When the borrowing constraint is sufficiently relaxed, the adverse impacts of uncertainty shocks on export dynamics are largely mitigated.====To evaluate the importance of market uncertainty for China's trade collapse in the 2008 financial crisis, we simulate the export dynamics by feeding into the model a sequence of uncertainty shocks constructed from the data. The simulation shows that the model-generated export sequence tracks the dynamics in the data counterpart quite well. In terms of magnitude, market uncertainty ==== accounts for 25 percent of China's trade collapse in the financial crisis observed in the data. We also study the consequences of market uncertainty for the Chinese economy. The simulation shows that market uncertainty causes a sizable decline in China's aggregate output. Therefore, economic uncertainty is an important force that drives fluctuations in international trade and the economy overall.====  Our paper contributes to several strands of the literature. First, our paper is closely related to the literature on the causes of the Great Trade Collapse. Eaton et al. (2016) investigate the forces driving global trade collapse in a dynamic multi-country general equilibrium model. They find that the shift of demand away from tradable goods due to a contraction in durable investment efficiency accounts for most of the trade collapse. Bems et al. (2011) focus on the global input-output channel and conclude that the cross-border vertical linkages played a key role in the 2008-2009 collapse of global trade. Alessandria et al. (2010) propose an inventory adjustment theory to explain the Great Trade Collapse. Their quantitative results suggest that the inventory dynamics for imported goods can account for the observed magnitude of the trade collapse during the recent Great Recession. As the recent financial crisis extensively affects the banking sector, some research, including Amiti and Weinstein (2011), Chor and Manova (2012) and Jiao and Wen (2012), attributes the collapse in trade to the deterioration in trade credit and external financial conditions. Bems et al. (2013) provide a comprehensive survey of the above literature. Our contributions relative to this literature are twofold. First, our approach emphasizes the interaction of the market uncertainty shock and the financial constraint channel, and we demonstrate that, both theoretically and quantitatively, this interaction is the key in understanding the dynamics of both the extensive and intensive margin components of exports during the Great Trade Collapse. Second, as a laboratory we focus on the trade collapse of China, the second-largest economy in the world using microdata, while the literature mainly studies the trade collapse in developed countries.====Our paper is most closely related to the recent work on uncertainty and trade dynamics. Our departure from the literature is that we focus on the role of the financial constraint in amplifying uncertainty shocks, and we show how their interaction can generate different response combinations in the extensive and intensive components of exports. Novy and Taylor (2020) introduce lumpy inventory investment into a dynamic trade model and study the impact of uncertainty shocks on trade dynamics through the wait-and-see channel. They do not consider an extensive margin response in their model, and acknowledge that allowing for such responses should be important. Alessandria et al. (2015) study the effect of microeconomic uncertainty shocks in a two-country DSGE model with heterogeneous producers and endogenous export participation. Contrary to empirical trade patterns, they find that trade rises in response to a second-moment shock. This is because in their model, exporters are at the upper tail of the productivity distribution, and a rise in the productivity dispersion tends to benefit exporters more than nonexporters. We show that with limited borrowing capacity, exporters facing high demand shock are more likely to be financially constrained, and this financial constraint channel overturns the effect of uncertainty on exports in their paper. Caldara et al. (2019) construct a news based index for trade policy uncertainty and introduces this uncertainty into a New Keynesian two-country model. They find that news and increased uncertainty about higher future tariffs reduce investment and real activities. However, their model generates an increase in export participation with higher trade policy uncertainty, which is counterfactual to the empirical patterns especially during the Great Recession.====Handley (2014) and Handley and Limao (2015) study the impact of trade policy uncertainty on the firm-level exporting decisions, and find that trade policy uncertainty reduces entry into export markets. Carballo et al. (2018) further examine the interaction of economic and policy uncertainties and their impacts on trade volumes in a dynamic heterogeneous-firms model. They find these two types of uncertainty and their interaction are the main contributors to the U.S. trade collapse during the 2008 financial crisis, and trade agreements can mitigate the adverse consequences caused by increased uncertainty. The above studies mainly focus on the extensive margin of international trade. Their underlying mechanism relies on the standard wait-and-see channel, which as we show theoretically, tends to increase the intensive margin of trade. Complementary to these papers, we emphasize the financial channel in the transmission of uncertainty shocks to international trade. We argue that the intensive margin of exports is crucial for identifying the role of financial constraints in the propagation of uncertainty shocks.====As this paper bridges international trade and macroeconomic fluctuations, our work is also related to the research on the macroeconomic consequences of economic uncertainty. Bloom et al. (2018) investigate aggregate implications of firm-level uncertainty shocks in a heterogeneous-firm model with non-convex factor adjustment costs. They show that the wait-and-see effect provides a major channel for propagating uncertainty shocks. Arellano et al. (2019) introduce demand uncertainty into a DSGE model with heterogeneous firms. Financial frictions make the idiosyncratic demand shock uninsurable, resulting in a negative effect of uncertainty shocks on firm-level production and the aggregate economy. Christiano et al. (2014) identify risk shocks through the cyclicality of the credit spread in a standard financial accelerator model. They find that risk shocks are the most important driving force for business cycles. Gilchrist et al. (2014) construct a quantitative model with both investment irreversibility and financial frictions. Their results suggest that the credit channel is more important than the wait-and-see channel for understanding the aggregate impact of uncertainty shocks. Schaal (2017) studies the impact of risk shocks on the unemployment dynamics in a heterogeneous-firm model with labor market frictions. The paper shows that the risk shocks are important to account for the magnitude of fluctuations in aggregate unemployment for past U.S. recessions. See Bloom (2014) for a more comprehensive survey of the recent research. The above literature focuses on the propagation mechanism in a closed economy. We incorporate international trade into an otherwise standard financial accelerator model. Our quantitative model offers an alternative channel through which economic uncertainty may affect aggregate fluctuations.====The remainder of the paper proceeds as follows. Section 2 documents some stylized facts regarding the effect of uncertainty shocks on exports on both the intensive and extensive margins. Section 3 presents a simple two-period trade model with endogenous exporting decisions to explain the facts and illustrate the main intuition. Section 4 generalizes the two-period model to a fully fledged dynamic general equilibrium framework. Section 5 calibrates the model to the Chinese economy and conducts the quantitative analysis. Section 6 documents the quantitative impact of market uncertainty on exports during the 2008 financial crisis. Section 7 concludes the paper. All the data descriptions, the proofs of propositions and the robustness analysis are in the appendices.",Market uncertainty and international trade,https://www.sciencedirect.com/science/article/pii/S1094202523000170,Available online 12 May 2023,2023,Research Article,4.0
"Gibbs Christopher G.,McClung Nigel","School of Economics, The University of Sydney, Camperdown, NSW 2006, Australia,Bank of Finland, Finland","Received 4 September 2022, Revised 17 February 2023, Available online 22 March 2023.",https://doi.org/10.1016/j.red.2023.03.001,Cited by (0),"We show how to characterize the economic forces that generate the forward guidance puzzle in a wide variety of structural ====. We accomplish this by showing that studying the predictions of forward guidance announcements is essentially the same as conducting E-stability analysis under adaptive learning. We show that the Iterative E-stability criterion identifies all of the most prominent forward guidance puzzle resolutions proposed in the literature, provides ways to evaluate their robustness, shows how new resolutions may be constructed, and is scalable to quantitatively relevant models. We show some common resolutions are robust while others are not. We also devise a novel solution to the forward guidance puzzle: sunspots.","A near ubiquitous feature of standard rational expectations (RE) structural monetary policy models is that credible promises to hold interest rates at zero for extended periods of time can generate significant jumps in output and inflation in the period the policy is announced. Moreover, the contemporaneous impact of a fixed future policy intervention can be made arbitrarily large today, when interest rates are constrained, simply by pushing the actual implementation of the policy farther into the future, a phenomenon known as the ====. Since this feature of structural monetary models was first pointed out by papers such as Del Negro et al. (2012) and Carlstrom et al. (2015), a number of authors have sought to ameliorate and explain away this puzzle using, for example, credibility (Haberis et al., 2019), imperfect information (Carlstrom et al., 2015; Kiley, 2016), bounded rationality or level-==== thinking (Gabaix, 2020; Angeletos and Lian, 2018; Farhi and Werning, 2019), life-cycle considerations (Del Negro et al., 2012; Eggertsson and Mehrotra, 2014; Eggertsson et al., 2019), heterogeneous agents with incomplete markets (McKay et al., 2016; Bilbiie, 2020), or the fiscal theory of the price level (Cochrane, 2017; McClung, 2021) to name just a few.====The source of the puzzle is the backward induction that agents (and the modeler) do to find the path that endogenous variables must take to arrive at the announced future state of the economy. Starting with the beliefs that must prevail when the announced policy terminates, one walks expectations iteratively backwards in time through the structural equations. This backward iteration implies an unstable dynamic in most New Keynesian models when monetary policy is constrained by the zero lower bound (ZLB) on nominal interest rates. Inflation and output beliefs diverge toward positive or negative infinity with each deduction, which generates larger and larger jumps in these variables today, the farther the terminal date of the announced policy. In other words, there is no smooth path from where agents believe the economy will be in the future to where the economy is today in many modeled environments.====The search for a smooth path from some arbitrary set of beliefs to a specific rational expectation solution is not a new problem. The expectations literature, typified by Adaptive Learning as in Evans and Honkapohja (2001), has studied this problem for decades. The problem is just framed with time running in the opposite direction. Instead of asking how we go from some belief about the future back to today, this literature asks how, starting from some arbitrary beliefs, we can arrive over time to a specific belief. We show that assessing the power of forward guidance and studying the Expectational stability (E-stability) properties of a Rational Expectations Equilibrium (REE) are essentially one and the same. Specifically, the notion of Iterative Expectational stability (IE-stability) first proposed by Evans (1985) provides sufficient conditions that allow one to determine if any model, or more precisely, any equilibrium of a model is likely to predict puzzling behavior in response to a forward guidance announcement.==== The conditions allow for a direct diagnosis of the economic assumptions that are driving the explosive beliefs across a wide variety of models and can be used for equilibrium selection.====We show that viewing the forward guidance puzzle through the lens of IE-stability provides a clear economic interpretation for why the puzzle exists. IE-stability is a local stability condition for the equilibrium dynamics of expectations. Stability depends on both the features of the economy at the ZLB and the economy after liftoff. An REE fails to be IE-stable when, for a given initial belief, endogenous variables of the economy respond more than one-for-one to expectations. For example, if higher inflation expectations trigger a rise in inflation over and above that implied by the expectation alone, then inflation and inflation expectations will continue to diverge when agents are learning. For a forward guidance announcement this dynamic happens in reverse. Agents contemplate some future belief about inflation in some period ==== and ask what realized inflation rate in the previous period, ====, rationalizes that belief. If it is an ==== higher inflation rate, then that implies inflation expectations in period ==== should be higher and hence realized inflation in ==== is higher again. Repeating the deduction over and over again leads to the unbounded response of inflation to the policy. This is exactly what occurs in the New Keynesian model when monetary policy is constrained by the ZLB. Higher inflation expectations lead directly to a lower real interest rate, which leads to higher consumption, and then further increases in inflation and inflation expectations. This dynamic repeats with every additional anticipated period of zero interest rates. Dampening or breaking this feedback loop is what all forward guidance puzzle resolutions must do. By connecting IE-stability and forward guidance, we identify three (not mutually exclusive) categories of resolution strategies that enhance or change the stability properties of models studied under adaptive learning and which mitigate or solve the puzzle:====This categorization and connection to IE-stability provides a road map for where to look for resolutions of the puzzle by appealing to the large body of knowledge of model features that enhance expectational stability. It also distinguishes between resolutions to the forward guidance puzzle that scale with the complexity of the model, i.e., increasing the number of state variables, and indicates a proposed resolution's potential robustness. For example, because the first category of resolutions seek only to dampen the existing general equilibrium effects, these types of resolutions are sensitive to parameter choices. In particular, we show that increasing the flexibility of prices by very small amounts in the basic Behavioral New Keynesian model may undue the dampening implied by the over-discounting of expectations and bring back the forward guidance puzzle. Consequently, solutions which rely on over-discounting in simple models may not scale to larger and more quantitatively relevant applications. The latter two resolution categories, though, are more robust. They seek to fundamentally change the dynamics of expectations and their general equilibrium effects.====Connecting the forward guidance puzzle to IE-stability also sheds light on two additional aspects of modeling zero interest rates and forward guidance announcements debated in the literature. First, there is nothing special about monetary policy forward guidance and the ZLB that generates this dynamic. Any equilibrium of a model that fails to meet our IE-stability criteria may result in puzzling behavior for a forward guidance announcement of any kind. For example, our IE-stability conditions are predictive for the forward fiscal guidance puzzle explored by Canzoneri et al. (2018) or for when large positive/negative impacts of announced disinflation arise such as in Ball (1994) and Gibbs and Kulish (2017). Second, IE-stability and our framework show that the forward guidance puzzle is not a by-product of indeterminacy as suggested by Carlstrom et al. (2015). It is well-known that E-stability and determinacy are distinct concepts.==== In fact, in the class of models we consider the existence of multiple stationary equilibria implies the existence of sunspot solutions that satisfy the IE-stability condition and therefore do not exhibit the forward guidance puzzle. In these puzzle-free equilibria, extraneous sunspots predetermine expectations in a manner that counteracts the effects of the policy announcements.====We are not the first to point out that indeterminacy can be exploited to resolve the forward guidance puzzle. Cochrane (2017) shows the existence of multiple equilibria at the ZLB in a standard New Keynesian environment of which many do not exhibit the features of the forward guidance puzzle. What we provide in addition to this insight is a framework for equilibrium selection that scales to a broad class of models. Our method can isolate the individual economic assumptions that contribute to the puzzle allowing for puzzle resolutions that are derived from the micro-foundations of the model rather than by selecting for the macro predictions one finds most palatable as in Cochrane's framework.====We provide a simple example of the connection between the forward guidance puzzle and IE-stability in the next section. Section 3 defines the forward guidance puzzle in a general way and shows how IE-stability can characterize its existence. Section 4 applies IE-stability to a general New Keynesian model that nests the models of Gabaix (2020) and Bilbiie (2020) to explore puzzle resolutions that rely on over-discounting of expectations and the robustness of these modeling strategies to resolve the forward guidance puzzle.====We then explore two extensions. In section 5, we provide an extension of the indeterminate solution techniques of Bianchi and Nicolò (2021) to anticipated structural change. We show how these solutions may be analyzed using IE-stability in the same way as determinate solutions. We use these techniques to characterize when the forward guidance puzzle exists in the standard three equation New Keynesian model for both unique RE solutions and non-unique sunspot RE solutions. We show that IE-stability is a necessary and sufficient condition in the model for ruling out the forward guidance puzzle in the determinate case, and that for a class of sunspot solutions, IE-stability is always satisfied, which rules out the puzzle. Lastly, we show that the sunspot resolution of the forward guidance puzzle scales to the medium-scale model of Smets and Wouters (2007) estimated on data from the United States.====Section 6 extends the IE-stability analysis to models with Markov-switching policy and explores an application to history dependent monetary and fiscal policies. Cochrane (2017) and McClung (2021) show that expectations of active fiscal policy and passive monetary policy resolves the forward guidance puzzle. This policy introduces history dependence at the ZLB because a promise to fix the path of the nominal interest rate affects the real value of debt, which in turn influences inflation and inflation expectations beyond the forward guidance horizon. Expectations of policy choices after liftoff then restrain inflation when the economy is at the ZLB, eliminating the puzzle. We illustrate this in a standard modeling environment with exogenous Markov-switching between active and passive fiscal policy regimes upon liftoff from the ZLB. Importantly, IE-stability and determinacy of an REE at the ZLB in this model do not overlap for a wide range of the plausible parameterizations, which illustrates that IE-stability is the relevant criteria to study when assessing the power of forward guidance.====Finally, leveraging the Markov-switching machinery, we study forward guidance for a central bank that pursues price level targeting upon liftoff. We show that price level targeting on its own mitigates but does not eliminate all aspects of the forward guidance puzzle. We introduce an exogenous possibility that policymakers might renege on the promised forward guidance policy in this scenario. We show that expectations of reneging on policy, or alternatively imperfect central bank credibility, resolves the puzzle.==== There are many other papers that solve or at least mitigate the forward guidance puzzle using one of the three resolutions mechanisms that IE-stability identifies. Additional examples of resolutions that rely on history dependent policy to resolve the puzzle are proposed by Bilbiie (2021) and Diba and Loisel (2021) who employ price level targeting and endogenous money growth rules, respectively. Additional examples of resolutions that rely on predetermining expectations are proposed by Kiley (2016) and Gorodnichenko and Sergeyev (2021) who use sticky information and an exogenous zero lower bound on inflation expectations, respectively; and Eggertsson and Mehrotra (2014) and Gibbs (2018) who each use a downward rigidity in nominal wages. Additional papers that rely on bounded rationality and information frictions are Angeletos and Sastry (2021), Eusepi et al. (2022), and Evans et al. (2022). The former paper relies on either imperfect information or myopia/bounded rationality - Angeletos and Huo (2021) show an equivalence result for the two deviations from full information rational expectations - while the latter two papers rely on adaptive learning and forms of level-==== reasoning.====The novel sunspot resolution to the forward guidance puzzle that we derive relies heavily on the solution methods of Cagliarini and Kulish (2013) and Kulish and Pagan (2017) combined with the methods of Bianchi and Nicolò (2021) for solving for sunspot solutions. We generalize the Bianchi and Nicolò (2021) method to capture the zero lower bound and show how to adapt it to the reduced form of Binder and Pesaran (1997).",Does my model predict a forward guidance puzzle?,https://www.sciencedirect.com/science/article/pii/S1094202523000157,Available online 22 March 2023,2023,Research Article,5.0
"Cho Daeha,Kim Kwang Hwan,Kim Suk Joon","College of Economics and Finance, Hanyang University, Republic of Korea,School of Economics, Yonsei University, Republic of Korea","Received 14 September 2021, Revised 20 January 2023, Available online 6 March 2023.",https://doi.org/10.1016/j.red.2023.02.002,Cited by (0),"Empirical evidence suggests that the degree of ==== for all countries, which we call the open-economy paradox of flexibility. We show that the severity of the paradox increases with the degree of trade and capital market openness, when home and foreign goods are Edgeworth substitutes and when the more affected home country runs a trade deficit. This implies that highly open trade and financial linkages are not desirable in terms of world welfare. We show that the inefficiencies generated by heterogeneous price stickiness can be reduced through two types of international policy coordination: i) an arrangement in which the country with relatively sticky prices raises the ==== or ii) a monetary union.","Should central banks in the world cooperate on monetary policy during a global recession? The study of international cooperation has become a cornerstone of research in international macroeconomics. Most existing studies of optimal monetary policy in open economies assume an identical degree of price stickiness across countries. In a two-country model with one sector, following a large common shock such as the COVID-19 pandemic, all countries respond in the same manner, so there is no room for countries to cooperate on monetary policy in a global recession.====In this paper, we present a novel perspective on how central banks should cooperate when the world economy is buffeted by a common large negative demand shock that pushes both countries into a zero lower bound. The critical feature that rationalizes international cooperation is a cross-country heterogeneity in the degree of nominal price rigidity, a feature strongly supported by extensive empirical evidence. Either using rich micro-data or aggregate data, researchers show that the degree of price rigidity is substantially different across countries. For example, Klenow and Malin (2010) confirm the significant heterogeneity in the degree of price stickiness across countries by summarizing estimates of the mean frequency of price changes in the literature. Dhyne et al. (2006) find that the average duration of a price spell in the Euro area is about twice that of the U.S. and that even within the Eurozone, the frequency of price changes in a given month ranges from 13% to 23% across countries. Galí et al. (2001) paint a similar picture that the prices in the Euro area change less frequently than those in the U.S., based on structural estimation.====In a two-country New Keynesian model calibrated to the empirical differences in price rigidity between Europe and the U.S., we find that, following a common adverse demand shock, heterogenous price stickiness greatly increases cross-country divergence in economic activities at the zero lower bound on interest rates, which is an outcome that is detrimental to global welfare. Specifically, the country with relatively flexible prices (the home country) experiences a more severe downturn than the country with relatively sticky prices (the foreign country). This result echoes the theoretical finding of the paradox of flexibility studied by Eggertsson and Krugman (2012), who show that, for a given demand shock, greater price flexibility is more contractionary in a closed economy setting when central banks lack commitment. However, when home and foreign goods are Edgeworth substitutes and when the more affected home country runs a trade deficit, we find that the paradox becomes more aggravated in an open economy than in a closed economy: the trade and financial linkages between two countries worsen the recession in the flexible-price home country more than in cases where these linkages are absent, increasing the cross-country divergence further. In this paper, we term this amplified cross-country divergence the ====. We then present two types of international monetary cooperation that can attenuate such a welfare-costly divergence.====To clarify the logic of the paradox in an open economy, consider a large global negative demand shock that pushes both countries into a zero lower bound. Price flexibility accelerates deflationary expectations in the home country, elevating the home real interest rate above that of the foreign country. A higher real interest rate implies that the home country suffers from a larger contraction than the foreign country. But to make matters worse, it leads to an appreciation of the home nominal exchange rate and, therefore, the terms of trade. This unfavorable movement in the home terms of trade causes an expenditure switching away from the home country, which exacerbates the recession in the home country but ameliorates it in the foreign country. This perverse movement in terms of trade is the key ingredient that generates the paradox of flexibility in an open economy. In contrast, in a closed economy, there is no such reinforcing effect of the terms of trade, so the paradox is less severe. According to the closed economy version of our model, the difference between home and foreign deflation is 3.1 percentage points, while the difference between home and foreign output gaps is 2.3 percentage points. Strikingly, in an open economy, these figures rise to 4.1 and 7.7 percentage points, respectively.====Importantly, we find that the cross-country divergence in economic activity increases in the degree of trade and capital market openness between the countries. This implies that, in a global liquidity trap, highly open trade and financial linkages, paradoxically, may have a perverse effect on world welfare. The reason is that, in our baseline model, the home and foreign goods are Edgeworth substitutes, and the more affected home country runs a trade deficit and borrows from the foreign country. In response to an appreciation of the home terms of trade, the home households can purchase more units of cheaper foreign goods that are good substitutes for the goods produced domestically. This leads to a fall in domestic production as the home households can work less without significantly reducing consumption. In this case, weakening the trade linkages increases the home country's willingness to produce, alleviating the cross-country differences in economic activity. When the home country can borrow, it consumes more than it would when it cannot. The borrowing by the home country aggravates its recession because an income effect from the high level of consumption reduces the home households' incentive to work and produce. In this case, less integrated financial markets dampen the fall in home production, thereby reducing the cross-country differences in economic activity.====The perverse terms of trade movement that arises from the heterogeneous price stickiness is a rationale for considering policy coordination between central banks. We suggest two types of policy coordination that can reduce the inefficiencies caused by the perverse terms of trade response, when central banks lack commitment. The first type is the cooperative policy that allows a supranational institution to choose the interest rates of the two countries that maximize world welfare. Under the cooperative policy, it is optimal for the foreign country to raise its nominal interest to alleviate the appreciation of the home nominal exchange rate and terms of trade, thereby limiting the degree of world expenditure-switching away from the home economy. Thus, the higher the trade and financial openness, the larger the increase in the optimal foreign interest rates. The second type is the establishment of a monetary union. This eliminates the possibility of perverse exchange rate adjustment, thereby limiting the appreciation of home terms of trade. Accordingly, the more integrated the goods and capital markets are, the greater the benefit of a monetary union.====This paper is related to a body of literature on the desirability of financial openness in a liquidity trap. Devereux and Yetman (2014) argue that, when both countries are in a liquidity trap in response to a negative home demand shock, less integrated financial markets between countries are not desirable. Similarly, Acharya and Bengui (2018) argue that the same result holds, even when only the home country is in a liquidity trap. This is because, in Devereux and Yetman (2014) and Acharya and Bengui (2018), relatively impatient foreign households borrow from the home country and consume the home goods, alleviating the recession in the home country. In contrast to these papers, in our setting, less integrated financial markets are desirable, implying that financial risk-sharing has a detrimental effect on welfare. As borrowing from the foreign country allows the more affected home country to consume without producing, weakening the financial linkages encourages home production and thereby reduces the drop in home output.====Another recent body of literature addresses the international dependence of optimal policy in a liquidity trap under floating exchange rates. Fujiwara et al. (2013) describe the optimal monetary policy under commitment and find that the direction of the policy dependence depends on whether goods produced in the two countries are Edgeworth complements or substitutes, assuming complete financial markets and no home bias. Instead, we focus on discretionary policy and show that the policy dependence depends on the degree of home bias and the degree of capital market imperfection. Cook and Devereux (2013) analyze the optimal monetary policy under discretion as we do and argue that the less affected country should raise the interest rate in order to ameliorate the recession in the more affected country. Cook and Devereux (2013) assume equal price stickiness between the countries, and thus the international policy dependence arises due to the asymmetric demand shocks. The policy dependence in their environment decreases with the degree of trade openness. In contrast, we focus on symmetric demand shocks, and thus the international policy dependence in our paper arises due to the heterogeneous price stickiness. In our case, the policy dependence increases in the degree of trade openness.====Our paper is also related to the literature on the desirability of a monetary union, which traces back to the optimal currency area theory (Mundell, 1961). Recently, Cook and Devereux (2016) and Groll and Monacelli (2020) argue that a monetary union can be beneficial over the floating exchange rate regime due to the inertia in the terms of trade that results from the combination of nominal rigidity and lack of exchange rate flexibility.==== They find that, when the monetary authority lacks commitment, the sluggish adjustment of terms of trade acts like a commitment device, tempering the recession in the more affected country. The inertia of the terms of trade is why we find the superiority of a monetary union over the floating exchange rate regime in our setting. However, the source of the inertial terms of trade in their paper is the country-specific shock, while the source in our paper is the heterogeneous price stickiness.====The rest of the paper is organized as follows. In Section 2, we develop the basic model and describe the parameter values. In Section 3, we analyze the impact of global demand shocks at the zero lower bound and discuss the parameter conditions for the open-economy paradox to occur. Section 4 examines the effect of international monetary cooperation. Section 5 studies whether the source of asymmetric responses between the countries matters in a normative sense. Section 6 concludes.",The paradox of price flexibility in an open economy,https://www.sciencedirect.com/science/article/pii/S1094202523000078,Available online 6 March 2023,2023,Research Article,6.0
"Cao Qingqing,Giordani Paolo,Minetti Raoul,Murro Pierluigi","Michigan State University, United States of America,Luiss University, Italy","Received 27 July 2022, Revised 3 February 2023, Available online 23 February 2023.",https://doi.org/10.1016/j.red.2023.02.001,Cited by (0),"We study the impact of credit relationships on firm entry, and the implications for aggregate investment and output. Exploiting Italian data, we find that relationship-oriented local credit markets feature fewer, larger entrants, and relatively more spinoff entrants. Relationship lending discourages de novo entry when banks' knowledge is incumbent-specific but promotes knowledge transfers to spinoffs. We explain these patterns in a dynamic general equilibrium model where banks accumulate information in credit relationships and can reuse information when financing entrants. Relationship lending raises output, as the larger investments and the credit reallocation from de novos to spinoffs outweigh the entry slowdown.","Firm entry is a driving force of entrepreneurship, investment and output. The credit sector is reputed to be important in determining the ease with which new firms enter markets. An aspect that has instead received little attention is the way banks' business models and lending technologies, such as relationship lending, shape intensity and modes of firm entry. In many countries, credit mainly flows to firms through long-term credit relationships over the course of which banks accumulate knowledge about firms' assets, workforce and human capital (Ongena and Smith, 2001).==== Fundamental questions arise from these observations: does the diffusion of relationship banking in the credit market promote or slow down the dynamics of firm entry? And does it favor the entry of new entrepreneurs or the creation of spinoff firms by managers or employees of incumbent firms?==== What implications do the effects of relationship banking on entry intensity and modes yield for aggregate investment and for the design of policies aimed at promoting business dynamism and aggregate investment? This paper takes a step towards addressing these questions theoretically and empirically. The analysis uncovers a critical role of banks' continuous accumulation of information in credit relationships and reuse of information in entrants' financing in the credit market.====We motivate the analysis empirically using the Italian local credit markets (provinces) as a testing ground and examining how the strength of credit relationships in the local markets affects the dynamics (intensity and modes) of firm entry. To this end, we employ granular survey data on the length of credit relationships in the provinces and on their degree of exclusivity (number of banks of a firm).==== We complement these data with business registry data on the intensity of manufacturing firms' entry and with information on entry modes, whether firms are created by new entrepreneurs (de novo entry) or by managers or employees of incumbent firms (spinoff entry). Further, we have access to unique survey data on the type of information generally accumulated by banks in credit relationships, whether information specific to incumbents and their assets or information that can instead be reused by banks when financing de novo and spinoff entrants, including knowledge about managers and about incumbents' sector of activity and local economy.====We find that local credit markets characterized by stronger (e.g., longer) relationships between banks and incumbent businesses feature a lower entry rate of firms and a larger size of firms at entry. They also feature relatively more spinoffs than de novo entrants. We detect a role of banks' information in these effects. Relationship-oriented local credit markets slow down firm entry, especially de novo entry, when banks' information is specific to incumbents and, hence, not reusable in the financing of entrants. By contrast, a stronger local intensity of credit relationships promotes spinoff entry when banks accumulate information that is transferable from incumbents to spinoffs. These findings are robust across estimation methods, including instrumental variables (IV) estimations. When performing IV estimations, we exploit indicators of the constrictiveness of the historical regulation of the Italian banking sector to assuage possible concerns of endogeneity of the importance of relationship banking in local credit markets. The 1936 Italian banking regulation imposed strict limits on the ability of different types of banks to open new branches. While its prescriptions were uniform across Italy, its constrictiveness varied across provinces and depended on the relative importance of different types of banks in the provinces in 1936. The constrictiveness of the 1936 regulation has been demonstrated to have significantly affected firms' tendency to engage in stable, long-term credit relationships through its impact on the fluidity of local credit markets (firms' opportunities to switch banks) and its profound effects on the types of banks operating in the local markets (e.g., local banks or banks with a national scope) (Herrera and Minetti, 2007; Guiso et al., 2003, Guiso et al., 2004).====The effects estimated in the empirical tests are economically sizeable. Looking for example at the entry rate, the instrumented regressions suggest that credit relationships longer by 1 year (about 5% of the average credit relationship length) imply a ratio of entrants to incumbents about 0.6 percentage points lower. And the non-instrumented regressions suggest a lower but still sizeable effect of the local intensity of credit relationships on firms' entry rate.====We rationalize the empirical patterns qualitatively and quantitatively through a dynamic general equilibrium model with credit relationships and firm entry, and use the model to evaluate the implications for aggregate investment and output. As in a broad class of models (e.g., Kiyotaki and Moore, 1997; Perri and Quadrini, 2018; Bassetto et al., 2015; Iacoviello, 2015), firms' access to credit is hindered by limited pledgeability of investment returns. Banks' information reduces firms' ability to divert investment returns, raising the pledgeability of returns and, hence, incumbents' and entrants' access to credit.==== Motivated by the evidence, the characterizing features of the economy are the distinction between de novo and spinoff entrants and the accumulation of banks' information in credit relationships and reuse of this information in the credit market. The first information channel in the credit market - “embedded information flows” - consists of relationship banks' accumulation of information on employees (managers) of incumbent firms. This favors spinoff entry: managers of incumbent firms can start a business by pledging investment returns to banks with which they interacted in parent companies. The second channel - “non-embedded information spillovers” - captures in reduced form mechanisms through which relationship banks' information accumulation on incumbents' technology, sector or local economy affects entry. On the positive side, such information can be reused by banks when entrants pledge investment returns. On the negative side, relationship banks' accumulation of knowledge on incumbents crowds out their information acquisition on entrants (e.g., due to banks' limited information capacity), especially when banks' information is incumbent-specific.====We calibrate the model to the Italian data. We choose the parameters for the investment technology and for the pledgeability of incumbents' and entrants' returns by targeting the following data moments: the ratios of entrants to incumbents and of spinoffs to total entrants; the leverage of incumbents, de novos and spinoffs; the aggregate turnover of entrants over that of incumbents; and the effect of the average credit relationship length on the stock of relationship loans. In the calibrated model, through the accumulation of information in credit relationships, stronger relationship banking (i.e., longer relationships) crowds out the production of information and loans for de novo entrants, but spurs the entry of spinoffs from incumbents, to which banks can transfer information accumulated in credit relationships. Spinoff entry, however, takes a hit from the slowdown in de novo entry and its adverse impact on the number of incumbents, from which spinoffs originate. The interaction between the information channels and firms' compositional dynamics thus exerts conflicting effects on the overall pace of entry.====The calibrated model can replicate the negative impact of a relationship-oriented banking structure on the overall entry rate. It can also match its positive association with the spinoff rate (spinoffs to total entrants) and with the size of firms at entry. The effects are in line with the estimates on the Italian local credit markets and economically sizeable. A permanent increase of 1 year in the average credit relationship length (about 5% of the average relationship length) reduces entry by 0.23 percentage points (about 5% of the average entry rate), hinting at a 1-to-1 negative relation between relationship intensity and entry intensity. This effect on the entry rate is in between the non-instrumented estimates and the IV estimates on the Italian local credit markets. Among the fewer entrants, the reallocation of credit from de novos to spinoffs is significant: a permanent increase of 5% of the average relationship length raises the spinoff rate by 0.7 percentage points (about 2% of the average spinoff rate), which is again consistent with the estimates. The average investment size of firms at entry rises by 6%.====We then evaluate implications for aggregate output. In the quantitative analysis stronger relationship lending leads to larger total output====: a permanent increase of 5% of the average credit relationship length raises output by 4.8%. First, there is a larger investment scale of entrants, which contributes to an even larger investment scale of incumbents. Second, among entrants, the reallocation of credit from de novo to spinoff entrants enhances aggregate investment and output. Spinoffs, in fact, can use higher leverage when investing due to the higher pledgeability of their investments. Together these effects outweigh the output effect of the overall slowdown in entry and of the resulting long-run contraction in the number of active firms.====Before we proceed, a caveat is in order. In the analysis we will refer to the effects of policies, such as banking regulations, on entry dynamics and aggregate investment through relationship lending. As we further elaborate below, it is important to bear in mind that these effects do not constitute the only channels of possible influence of the policies and regulations on the aggregate outcomes of interest. The paper unfolds as follows. In the next section we relate the paper to prior literature. Section 3 presents the motivating empirical evidence. In Section 4, we lay out and solve the model. Section 5 presents calibration and simulations. Section 6 concludes. Details on the data, technical derivations and additional results are relegated to the online Appendices.","Credit markets, relationship lending, and the dynamics of firm entry",https://www.sciencedirect.com/science/article/pii/S1094202523000066,Available online 23 February 2023,2023,Research Article,7.0
Vereshchagina Galina,"Arizona State University, United States of America","Received 10 June 2020, Revised 16 December 2022, Available online 8 February 2023.",https://doi.org/10.1016/j.red.2023.01.005,Cited by (0),"This paper demonstrates that accounting for firms' endogenous productivity growth over lifecycle plays an important role in understanding the link between financial and economic development. It incorporates firm productivity investment into a span-of-control model, and compares the effects of firm financing constraints arising in this model to the effects of the same constraints in the model in which firm productivity growth is assumed to be exogenous. It finds that, depending on the severity of firm financing constraints, endogenizing firm productivity growth increases the adverse effects of the constraints on steady state output by 1.5-3 times, both due to a large decrease in average productivity and due to a bigger equilibrium effect on capital used in production.","It is well understood that financial and economic development are intrinsically related. At the macro level, various measures of financial development are associated with higher output, productivity, and economic growth.==== At the micro level, identifying the channels through which the adverse effects of firm financing constraints manifest, has been one of the central questions in the literature.====Previous studies==== have successfully used span-of-control models with rich firm dynamics features==== to quantitatively assess the impacts of firm financing constraints on output via intensive margin (namely, the misallocation of factors of production across existing firms) and via extensive margin (namely, the change in the number of active firms and their productivity via selection channels). These models are particularly suitable for studying the effects of firm financing constraints because they generate endogenous distribution of wealth (which impacts how constrained the entering firms are), as well as endogenous asset accumulation by firm owners (who face the consumption / saving tradeoff). The consensus that emerges from the literature is that the intensive margin channel has rather modest effects because productive firm owners are able to quickly accumulate assets and eventually overcome the firm financing constraints, while the effects along the extensive margin may vary with the features of the economic environment, such as assumptions about the distribution of entrepreneurial skills in the population, presence of other production sectors, or the ability to choose the production technology or sector at the moment of firm origination.====One common feature in most of these studies is that the incumbent firms' productivity follows an exogenously given process, as in Hopenhayn (1992). At the same time, a number of empirical studies have documented that limited excess to external financing may not only restrict firms' usage of physical capital, but also have an adverse impact on firm productivity growth.==== This raises a natural question – ==== The objective of this paper is to provide an answer to this question.====For this purpose, I incorporate firm productivity investment into a span-of-control model, calibrate this model to match the relevant features of the US firm dynamics and income data, and ==== the steady state effects of firm financing constraints in this model (referred to as the (NP) model, for ‘endogenous productivity’) to the effects of the same firm financing constraints in the model with exogenous evolution of firm productivity. The latter model (referred to as the (XP) model, for ‘exogenous productivity’) is nested in the (NP) model and calibrated to match exactly the same moments as the (NP) model.====The analysis in the paper shows that, depending on the severity of firm financing constraints, the adverse effects of the constraints on steady state output are 1.5-3 times bigger in the (NP) model than in the comparable (XP) model. Most of the additional reduction in output in the (NP) model compared to the (XP) model is due to a reduction in productivity investment resulting in lower firm productivity. This productivity decline, on its own, results in a bigger drop in output in the (NP) model than does the reduction in capital used in production due to financing constraints in the (XP) model. In addition, it generates a bigger reduction in aggregate capital stock than in the (XP) model because reduced firm productivity results in lower profit, which diminishes firm owners' ability to accumulate assets over time to overcome financing constraints. Further, a drop in productivity investment due to firm financing constraints in the (NP) model results in slower firm growth over lifecycle and a bigger reduction in the average firm size than in the (XP) model. Thus, endogenizing firm productivity growth over lifecycle not only magnifies the impact of firm financing constraints on aggregate productivity and output, but also may contribute to explaining why firms in poorer countries are smaller and grow slower over lifecycle, as has been documented in previous studies.====The key feature of productivity investment that leads to amplification of the impact of firm financing constraints is its persistency. Because productivity is complementary to capital, limited capital usage also reduces productivity investment. Because productivity investment have persistent effects (the firm's stock of ‘productivity’ or ‘intangible capital’ is built within the firm over time), even temporary restrictions on firms' access to external financing propagate onto the stream of future productivities. An illustrative example in Section 3 demonstrates that explicitly modeling productivity investment would have no impact on the effects of firm financing constraints predicted by the model if such investment had no persistent effects, i.e., the firm's total productivity was determined solely by current productivity investment and did not depend on the stock of productivity accumulated in the past.====An important step in being able to compare the quantitative impacts of firm financing constraints in the (NP) and the (XP) model is ensuring that the two models are calibrated to match the same set of relevant moments. The effects of firm financing constraints depend on (i) how constrained the firms are at the moment of their origination and (ii) how fast their owners are able to accumulate assets in order to overcome the financing constraints. Thus, the calibration procedure must ensure that the (NP) and (XP) models generate similar assets distribution and the flow of incomes of the firm owners in the benchmark unconstrained setting. This implies that the calibrated values of some key parameters must differ across the two models. In particular, the span-of-control parameter – which impacts the firm owners' profit share – must be smaller in the (NP) model than in the (XP) model, because in the (NP) model the firm owners also incur productivity investment which reduces their income (net of expenses on capital and labor) that is available for consumption and savings. The last quantitative experiment in Section 5.4 demonstrates that the effects of the firm financing constraints would be greatly exaggerated had the same value of the span-of-control parameter been used in both (NP) and (XP) models.====The paper is organized as follows. Section 2 describes the contribution of this paper to the existing literature. Section 3 develops a simple analytical example to demonstrate the key mechanism and outline the calibration strategy. Section 4 sets up the full quantitative (NP) model and explains how the (XP) model is nested in it. Section 5 describes the calibration procedure and the results of the main counterfactual experiments, including some relevant sensitivity analysis. Section 6 provides a brief discussion and outlines a few potentially interesting questions for future work.",Financial constraints and economic development: The role of firm productivity investment,https://www.sciencedirect.com/science/article/pii/S1094202523000054,Available online 8 February 2023,2023,Research Article,8.0
"Bar-On Yinon,Baron Tatiana,Cornfeld Ofer,Yashiv Eran","Department of Plant Sciences, Weizmann Institute of Science, Rehovot, Israel,Department of Economics, Ben Gurion University of the Negev, Beer-Sheva, Israel,BFI, Sderot Shaul Hamelech 39, Tel Aviv, Israel,The Eitan Berglas School of Economics, Tel Aviv University, Tel Aviv, Israel,CfM (LSE), UK,CEPR, UK","Received 23 September 2021, Revised 6 January 2023, Available online 26 January 2023.",https://doi.org/10.1016/j.red.2023.01.004,Cited by (0),"We present novel policy tools to manage epidemics. Rather than using prevalent population restrictions, these policy strategies are based on cyclical time restrictions. Key findings on the outcomes of such policy strategies include: a significant improvement of social welfare, substantially lessening the trade-offs between economic activity and health outcomes; optimally-derived timings of interventions are shown to suppress the disease, while maintaining reasonable economic activity; and outcomes are superior to the actual experience of New York State and Florida over the course of 2020.","The COVID19 pandemic has created a global health and economic crisis of a magnitude not experienced since the Great Influenza Pandemic of 1918-1919. After 33 months, about 630 million people have become infected, about 6.6 million people worldwide have died, and estimates of excess deaths are more than three times as high====; IMF (2022) reported a 3.1% drop in world GDP in 2020, and a 4.5% drop in the advanced economies. The death toll in the U.S. is almost 1.1 million; the declines in U.S. GDP and consumer expenditures for 2020 have been ==== and ====, respectively.====We address the issue of policy responses to the pandemic, providing an analysis of new time-based tools to manage epidemics. These policy strategies were proposed in epidemiologically-grounded work by Karin et al. (2020). The contribution of our paper is the economic analysis of these new policy tools. The proposed policy consists of alternating periods of work and lockdown, at pre-defined frequencies, for the entire population. We present both normative and positive analyses. The former applies to future pandemics or epidemics while the latter evaluates policy against real world benchmarks in the U.S. using data.====The proposed policy tools are particularly relevant in light of the difficulties experienced by policymakers in finding a policy strategy that lessens the trade-offs involved. In theory, targeted population lockdowns could constitute “fine tuning” of lockdown measures, which would serve to lessen any economic cost. In practice, however, it turned out to be challenging to identify sub populations to be allowed unrestricted economic activity, while imposing restrictions on other population groups. Political and moral issues, as well as practical implementation issues, have come into play. The time-based public health management policy avoids these difficulties, taking time, rather than population, as the medium of restrictions.====Our model lies within a line of COVID19 research in Economics, which posits a planner problem. The planner formulates an optimal policy of Non-Pharmaceutical Interventions (NPIs), and in particular, lockdowns, subject to a model of disease dynamics, taking into account vaccine arrival. The planner trades off the costs of public health outcomes, such as breach of ICU capacity and death, with the economic costs of suppression policy, including declines in production. Prominent contributions in this line of literature include Acemoglu et al. (2021), Alvarez et al. (2021), Brotherhood et al. (2021), Brotherhood and Jerbashian (2020), Farboodi et al. (2021), and Jones et al. (2021). Our analysis pertains to new policy strategies, which were not considered in the literature.====We highlight five dimensions related to time:====(i) From the normative perspective, we analyze policy that relies on ====. Such policy is an alternative to policy based on restrictions of sectors, age groups, regions, or other targeted population groups. The success of the policy hinges on two mechanisms that operate over time. In lockdown days, the disease is suppressed through exogenous government-imposed restrictions. In work days, the disease is suppressed through the endogenous rational behavior of the population. This combination allows the effective reproduction number to be kept at around 1 on average. Notably, a full, prolonged lockdown is not needed to keep death rates low. All that is needed is to have the open periods short enough, backed up by rational precaution of the population, and followed by lockdown periods of reasonable length ahead of the next re-opening.====(ii) The epidemiological rationale for the proposed policy is, inter alia but not exclusively, based on the ====. The idea is that for every 14 day period, there will be ==== days of work and ==== days of lockdown. This number, ====, uses the timescales of the virus against itself, taking into account a latent period after exposure, whereby the infected person does not infect others. In other words, the epidemiological cost of open days is mitigated by the fact that individuals do not become infectious immediately. The epidemiological benefit of lockdown days is enhanced because the schedule locks down the economy when individuals are at their most infectious.====(iii) Using an optimizing social planner model, the control variables for this policy are ====–initial lockdown, the cyclical policy phase, and release. Hatchett et al. (2007) highlight the idea that imposing NPIs early in an epidemic can significantly reduce mortality. In the current paper, the exploration of timing issues, both start time and duration, are at the heart of the analysis.====(iv) This policy is compared to a prevalent policy path which sets lockdown and release as ====. Specifically, the latter uses trigger thresholds, such as the number of persons hospitalized in ICU in a given period of time, and gives rise to the pattern of recurrent lockdown and release observed in the U.S. and other countries since the start of COVID19.====(v) The proposed policy is subsequently compared to the actual experience of New York State (NYS) and Florida. The outcomes observed for NYS turn out to depend crucially on ==== those for Florida reveal particular policy preferences.====We explicitly model the dynamic path for the reproduction parameter, reflecting both rational individual behavior and the effects of lockdown policy. Thus, we model the endogenous response by individuals, who adjust to the new environment and behave accordingly. Additionally, government interventions in the form of lockdowns induce behavioral change. These elements are included in a model whereby individuals maximize utility and a social planner maximizes a welfare function, which incorporates individual utility. We then take the model to the data and use the resulting estimates to calibrate the relevant parameters, which we subsequently use in simulations.====The social planner in our model uses a PDF in order to form ex-ante expectations of vaccine arrival time. This serves three key roles; first, it sets the horizon for the problem, acting as a rate of leaving the state of the pandemic. Second, it is an expression of the risk and uncertainty embodied in the planner problem. Third, relative to the interest rate, it plays the major quantitative role in discounting future streams. In our simulations vaccine arrival is realized after 540 days. Using these elements, our analysis quantifies outcomes in terms of social welfare. We simulate the optimal cyclical policies and examine their health and economic implications.====The cyclical policy is compared to four non-cyclical benchmarks: two polar cases, of no policy intervention (i.e., no lockdown) or full lockdown till vaccine arrival; a single time span lockdown policy, whereby the starting date and the duration are chosen optimally; and a theoretical path trying to mimic a real-world rationale, whereby the planner chooses thresholds for multiple lockdowns in terms of the critically ill. Subsequently, and additionally, we evaluate the time-based policy tools in relation to the actual experience of NYS and Florida in 2020.====We plot the policy plans in terms of a policy possibilities frontier. These show the outcomes of optimal planner policies, using a two-dimensional graph of the death toll per 1 million people and the value of lost output, in annual GDP terms. Movement along the frontier occurs as the policy instrument in use changes, or as the weight assigned to fatalities in the planner objective function changes, or as the lethality of the disease changes.====Our analysis yields the following key findings.====First, the policy instruments, based on time restrictions, provide for significant improvement, substantially lessening the trade-offs involved relative to the four non-cyclical benchmarks. The latter are situated in points on the graph beyond the frontier.====Second, we quantify social welfare costs. These are given in utility terms and in Present Discounted Value (PDV) consumption terms. Using the consumption loss measure, comparing actual consumption to the pre-COVID steady state (formally defined below), the different cyclical strategies place these losses at 22% to 26%, the no intervention policy results in 36%, full lockdown in 29%, optimal lockdown duration in 27%, and the thresholds strategy in 25%. The underlying rationale for the improvement is that cyclical strategies allow the planner to achieve similar death tolls with fewer lockdowns, or to reduce the death toll dramatically without a significant damage to output. These results are due to the optimally-derived timings of intervention (for example, “front loading” interventions is beneficial in specific cases, which are spelled out) and the ability of the cyclical strategies to suppress the disease while maintaining a reasonable level of economic activity.====Third, using daily data from March 2020 to early 2021, optimal cyclical policies fare much better than actual experience in the states of New York and Florida. When deriving this result, we use estimated state-specific parameters in the simulations.====Importantly, the benefits of the time-based policy tools that we find are likely to be a lower bound of their true advantage over policy strategies that have been implemented. This is so because, for tractability, we are not giving the planner full flexibility when applying the cyclical tools. Similarly, we do not quantify additional benefits, such as predictability of production, gains in non-COVID health matters, transparency, ease of communication, and fairness.====We note that the idea of a cyclical strategy, which is at the focal point of the normative analysis of our analysis, has been brought to the attention of policymakers (see Yashiv (2020), Alon and Yashiv (2020), and Alon et al., 2020) and has been considered or implemented by a host of firms and educational institutions in the U.S., in Europe, and in Latin America. Online Appendix A provides elaboration.====The paper proceeds as follows: Section 2 discusses the model, including the policies we propose, on which we further elaborate in online Appendix A. Section 3 presents the calibration and the solution methodology. Section 4 presents the results. Section 5 explores the underlying mechanism. Section 6 examines the relation between the model planner solution and actual outcomes in two U.S. states – NYS and Florida. Section 7 concludes. Online Appendices B and C provide further elaboration.","When to lock, not whom: Managing epidemics using time-based restrictions",https://www.sciencedirect.com/science/article/pii/S1094202523000042,Available online 26 January 2023,2023,Research Article,9.0
"Caliendo Frank N.,Casanova Maria,Gorry Aspen,Slavov Sita","Jon M. Huntsman School of Business, Utah State University, United States of America,College of Business and Economics, California State University, Fullerton, United States of America,John E. Walker Department of Economics, Clemson University, United States of America,Schar School of Policy and Government, George Mason University, United States of America","Received 12 November 2019, Revised 2 December 2022, Available online 24 January 2023.",https://doi.org/10.1016/j.red.2023.01.002,Cited by (0),"People often retire at a different age than expected. We construct a measure of retirement timing uncertainty and find that the ==== of the difference between retirement expectations and actual retirement dates ranges from 3 to 6 years. To understand the potential implications of this uncertainty, we develop a simple model of exogenous but risky retirement. In this environment, individuals would give up 1.0%-4.5% of total lifetime consumption to fully insure this risk and 0.8%-3.2% of lifetime consumption simply to know their actual retirement date upon entering the labor force. This is crucial for retirement planning purposes because not saving to hedge this risk would leave individuals with even larger welfare costs.","The age at which one will retire is crucial for financial planning decisions, as it determines the number of years of wage earnings, the expected length of time one will spend in retirement and, to some extent, pension benefits. The retirement decision, however, depends on multiple factors—changes in health or working conditions, the retirement of a spouse, care needs of parents, children or grandchildren, the size and timing of bequests, among others—that are difficult to predict in advance. As a result workers face substantial uncertainty regarding when retirement will take place.====Given the large impact of one's retirement age on individual finances, understanding retirement timing uncertainty is central to assessing whether Americans are saving enough for retirement.==== Additionally, this uncertainty complicates retirement planning decisions. Given the evidence that individuals may already lack necessary financial literacy to optimally make complex financial decisions (e.g., Lusardi and Mitchell (2014) and Benartzi and Thaler (2007)), individuals may not appropriately plan for this risk. While retirement timing uncertainty has received some attention in the financial planning literature, the typical financial planning approach does not consider how one's retirement planning should adjust to this risk (Blanchett (2018)).====Our paper explores the measurement and quantitative impact of retirement timing uncertainty. Specifically, we seek to accomplish three things. First, through careful use of the HRS data, we establish a set of facts concerning the extent of retirement timing uncertainty. Second, we develop a simple analytical framework for assessing the role of retirement timing uncertainty on retirement planning. We derive optimal saving decisions in the face of this risk, and we compute the welfare cost of this risk when individuals follow an optimal hedging strategy. Third, we quantify the welfare cost for individuals who do not make optimal financial planning decisions but instead fail to hedge retirement timing risk. Altogether, our findings have important implications for retirement planning in the sense that we empirically identify a key financial risk facing individuals and we provide guidance on how to hedge against it. The rest of the introduction describes our three contributions in more detail.====The first contribution of the paper is to provide an estimate of retirement timing uncertainty. To do this, we follow a reduced-form approach that takes advantage of the panel structure of the Health and Retirement Study (HRS). We observe self-reported retirement expectations of individuals in their 50's, elicited on the first wave of the survey. We follow these individuals for up to 26 years to establish the timing of their eventual retirement. The degree of retirement timing uncertainty is given by the standard deviation of the difference between the two variables. The comparison between retirement expectations and actual retirement dates is informative about the extent to which individuals optimally update their retirement date in response to the arrival of new information (shocks) between the baseline period and the time of the retirement transition. Making conservative assumptions, we estimate that the standard deviation ranges from approximately 3 to 6 years, depending on the subsample considered.====Casual observation tells us that retirement timing uncertainty may have important welfare consequences because the date of retirement is crucial for lifetime budgeting decisions. An individual who, when first interviewed in the HRS, plans to retire at age 65 but ends up retiring at age 61—approximately one standard deviation earlier than expected—loses 4 years of prime wage earnings. The loss of lifetime income may be amplified by the need to spread available assets over a longer retirement period.====The second contribution of the paper is to provide an assessment of the importance of this risk. The standard life-cycle model of retirement, such as French (2005), generates uncertainty about the retirement date endogenously by incorporating stochastic processes for variables such as income and health. However, most workers who retire earlier or later than planned do so for reasons other than the type of shocks typically considered in retirement models. Self-reports of the reasons that lead HRS respondents to retire show that less than 30% do so as a result of poor health or disability, unexpected changes in wages, or job loss (Casanova (2013), Szinovacz and Davey (2005)). Munnell et al. (2018) find that bad health, involuntary job loss and changes in financial wealth explain less than 27% of early separations from the labor force and conclude that most early retirements may be explained by “idiosyncratic, hard-to-measure factors.” Moreover, financial shocks only generate minor changes in retirement plans.==== Examples of idiosyncratic shocks that can raise the relative disutility of work include illness or retirement of a spouse (Blau (1998), Michaud and Vermeulen (2011), Szinovacz and Deviney (2000), Zimmerman et al. (2000)), job satisfaction and job characteristics (Gielen (2008), Kosloski et al. (2001)), changes in marital status (Disney and Tanner (1999)), the birth of a grandchild (Rupert and Zanella (2018)), and the receipt of an inheritance (Brown et al. (2010)), among many others.====A life-cycle model capturing the myriad sources of retirement uncertainty would quickly become intractable. Moreover, many of these non-standard shocks are difficult to measure in datasets such as the HRS. To assess the potential impact of this risk, we follow Gertler (1999) and Blau (2008) in treating the timing of retirement as an exogenous, uncertain event. This setup allows us to use the standard deviation of the difference between actual and expected retirement from the HRS to calibrate a distribution of retirement dates to understand if this uncertainty could be an important risk. In the baseline model, individuals have full information about the distribution of this risk. They accumulate precautionary savings balances as part of an optimal plan to hedge against it.====We find that hedging retirement timing uncertainty requires significantly more saving than what is optimal when the retirement date is known. For example, an individual would need to accumulate roughly 10% more for retirement by their late 50's than would an individual who knows their retirement age. The substantial effect on optimal saving plans suggests that individuals would value the ability to insure this risk. Indeed, according to our model, individuals would give up 1.0%-4.5% of total lifetime consumption to fully insure this risk and 0.8%-3.2% of lifetime consumption simply to know their actual retirement date upon entering the labor force.====In the baseline model, individuals can only self-insure by accumulating retirement savings. To illustrate the impact of social insurance programs on the cost of retirement uncertainty, we extend our baseline analysis to include a Social Security retirement program as well as disability risk and Social Security disability insurance.==== In each case, the welfare cost of retirement uncertainty remains close to the baseline values.====A limitation of our approach of treating retirement as a stochastic event, rather than modeling all of the factors that could influence retirement, is that it constrains individuals' ability to respond to each factor. Because the retirement shock is modeled as a financial risk to individuals, the threat to this approach would be if individuals adjust their retirement date to insure themselves against other financial shocks. As noted above, empirical evidence suggests that individuals do not substantially modify their retirement plans in response to typical shocks to their wealth. Beyond this evidence, we address this shortcoming in a number of ways. First, we empirically assess how individuals' absolute mistake (defined as the difference between the expected and actual retirement age) evolves as they near their actual retirement age. We show that although uncertainty does decrease as the individual approaches retirement, the absolute mistake remains substantial even for workers who are one year away from retirement. The fact that uncertainty never resolves itself as retirement nears suggests that individuals are not simply modifying their retirement date to offset financial or other shocks, but are often surprised by their eventual retirement date. Second, to validate the calibration of retirement uncertainty, we report how uncertainty naturally declines in the model as workers update their retirement expectations based on having not yet received a retirement shock. This is a simple learning process whereby workers mechanically update their retirement expectations based on conditional probabilities. The standard deviation in our baseline calibration remains at 4.0 years at age 50 and declines to 3.0 years at age 60. The magnitude of declines from learning in the baseline model are similar to what we estimate from fixed effects regressions of the absolute value of the retirement mistake on the distance to retirement, suggesting that the speed of learning in the model is similar to the data. Third, we extend the model to allow individuals to learn the date of their retirement prior to when it actually occurs, giving them some flexibility to re-optimize consumption plans. Our results indicate that allowing individuals to learn their retirement date at age 50—well in advance of the mean retirement age—reduces but does not eliminate the welfare cost of retirement timing uncertainty. Most of the welfare cost comes from distortions to the saving profile of young individuals, and hence it can only be partially mitigated by changes in saving behavior in the years right before retirement.====Our results so far have considered optimal saving behavior with full knowledge of the distribution of retirement timing uncertainty. A large literature documents that people struggle with the financial literacy required to make complex financial decisions, and the optimal hedging of retirement timing uncertainty may fall into this category (for a review, see Lusardi and Mitchell (2014), Lusardi and Mitchell (2007), Lusardi and Mitchell (2008), van Rooij et al. (2012), Lusardi et al. (2017), Ameriks et al. (2003), Campbell (2006)). Therefore, the third contribution of the paper is to assess the welfare consequences of retirement timing uncertainty when individuals make retirement saving decisions with a target retirement date in mind, ignoring the set of possible retirement ages and associated probabilities. We show that not properly hedging this risk causes a significant increase in the welfare cost to the individual as it leaves them with inadequate retirement assets after an early retirement shock.====In related work, Gertler (1999), Blau (2008) and Grochulski and Zhang (2013) study consumption/saving decisions over the life cycle with uncertainty about the timing of retirement. Like our setting, uncertain retirement leads to precautionary savings and consumption drops discretely when individuals retire.==== We extend their analysis by providing empirical evidence on retirement timing uncertainty and by assessing the welfare cost of this uncertainty. On the technical side, previous authors assume stationarity of the timing risk (constant hazard rate of permanent job loss) and Gertler (1999) assumes individuals are risk neutral. We model a calibrated timing distribution with non-constant probability of retirement that is allowed to depend on age as in the data, and we model risk aversion to study optimal hedging behavior. Blanchett (2018) examines retirement age uncertainty from a financial planning perspective, calculating the impact of retirement uncertainty on the probability of reaching a retirement income goal. In contrast, we explicitly solve for optimal decision making under uncertainty to show how individuals should hedge this risk.",Retirement timing uncertainty: Empirical evidence and quantitative evaluation,https://www.sciencedirect.com/science/article/pii/S1094202523000029,Available online 24 January 2023,2023,Research Article,10.0
"Altermatt Lukas,Iwasaki Kohei,Wright Randall","University of Essex, United Kingdom,Osaka University, Japan,Zhejiang University, China,University of Wisconsin - Madison, FRB Minneapolis, United States of America","Received 9 May 2022, Revised 12 January 2023, Available online 20 January 2023.",https://doi.org/10.1016/j.red.2023.01.003,Cited by (0),", and eliminating currency altogether.","This project explores dynamic general equilibrium models where multiple assets convey liquidity by facilitating transactions in decentralized exchange: fiat money ====; a real fixed-supply asset ====; and reproducible capital ====. We analyze cases where assets provide direct liquidity – they can be used to acquire something, say ==== – and where they provide indirect liquidity – only ==== can be used to acquire ====, but ==== or ==== can be traded for ==== in OTC (over-the-counter) markets. Also, the theory applies whether assets serve as media of exchange or as collateral. Together these ingredients allow us to extend many previous results in monetary economics, and to apply the findings, as well as the general methods, to issues in financial economics.====It is not only for realism or generality that it is useful to have ====, ==== and ==== all in one model; it also affects substantive results. Consider, e.g., the classic Mundell-Tobin effect (the literature on this and other substantive issues mentioned in these introductory remarks is discussed in Section 2). That effect says higher inflation increases investment in ====. We prove this happens here if ==== and ==== are the only liquid assets, but with all three higher inflation can decrease investment in ====, depending on conditions relating to how assets interact in the payment process.====The conditions are made precise below, but here is the intuition: Suppose ==== and ==== but not ==== are used to make payments. Then higher inflation decreases demand for ==== and increases demand for its substitute ==== – our version of the Mundell-Tobin effect. Now let ==== also be used for these payments. Then inflation decreases demand for ==== and increases it for ==== or ====, but may increase it so much for ==== that it decreases demand for ==== if ==== and ==== are substitutes in some transactions. Symmetric results are provided for the stock market: the price of ==== goes up with inflation if ==== and ==== are the only liquid assets, but can fall if ==== is also liquid.====More generally, we describe how different types of monetary policy affect the returns to all assets. First, in steady state, it is equivalent to peg the growth rate of ====, the inflation rate, some (but not just any) interest rate, or some measure of liquidity. In this context we provide quantitative results, showing how changes in policy can move real returns, revisiting venerable questions about the cost of inflation, and considering more recent questions about eliminating cash altogether. Quantitative answers to these questions depend on having multiple assets.====We then go beyond steady state to study equilibria where endogenous variables fluctuate, deterministically or stochastically, as a self-fulfilling prophecy.==== When ==== is the only liquid asset we prove that a policy of pegging its growth rate allows many such equilibria, while pegging a certain nominal interest rate ==== implies uniqueness. Then we show this does not generalize: with multiple liquid assets, pegging ==== does not eliminate multiple dynamic equilibria. Intuitively, ==== captures the opportunity cost of holding cash, so pegging it pins that cost down, and if ==== is the only liquid asset all other endogenous variables follow immediately. But suppose, e.g., that ==== is also liquid. Its value depends on its price path, which is not pinned down by ====. Hence ===='s value can fluctuate based on beliefs, and then ===='s value will too, because although the cost of holding ==== is still pinned down by ==== the benefit varies when ==== and ==== are substitutes in some transactions.====Other results on dynamics show how over the cycle asset returns move with each other, with trading volume, with investment and with output. We also demonstrate how cyclic correlations can provide misleading predictions for the effects of policy.==== Then we demonstrate there can coexist equilibria with very different dynamic patterns: for any two assets, either may be volatile while the other is relatively stable, or both may be volatile, and they may be positively or negatively correlated. Yet another result on dynamics shows how there can be belief-based cycles in ==== if it interacts with ==== or ==== in payments, but not otherwise, which is interesting since it is not easy to get endogenous cycles in ==== in models without liquidity considerations.====Further in terms of quantitative results, first we find the welfare cost of inflation is ==== with multiple assets. This may be surprising based on partial equilibrium reasoning, because any individual is better off with access to alternative liquid assets, since when the inflation tax is high one can substitute from ==== to ==== or ====; but in equilibrium, if all agents do so, it leads to bigger distortions, consistent with the principle of public finance that it is more inefficient to tax things that are more elastic. We then consider a policy of eliminating currency altogether, as championed by some people recently. The finding is that the welfare cost of this can be very large, although to be fair we do not incorporate any of the potential benefits, like reducing criminal activity. Another result shows how welfare results can change when the acceptability of different assets is made endogenous.====Finally, consider an economy where assets provide indirect liquidity: only ==== buys ====, while agents trade ==== or ==== for ==== in an OTC market. At least under some natural conditions it is shown that many of the results go through, basically as stated, from models of direct liquidity, where ==== and ==== as well as ==== can be used to acquire ====. This seems important in the sense that, in reality, households mostly use ====, or claims on ==== deposited in their banks, to buy goods, not ==== or ==== (although one can argue that firms and financial institutions regularly use such assets to facilitate trade). In this version of the framework, agents wanting to buy ==== first need to sell other assets to get ====, and it is good to know that at least in some cases that makes them act ==== they can use other assets to get ==== directly.====The rest of the paper is organized as follows, Section 2 reviews the literature. Sections 3 and 4 present the theory and study steady state. Sections 5, 6 and 7 analyze dynamics in various settings. Sections 8 and 9 discuss policy and OTC markets. Section 10 concludes. A few technical results and variations on the model are contained in a series of Appendices.",General equilibrium with multiple liquid assets,https://www.sciencedirect.com/science/article/pii/S1094202523000030,Available online 20 January 2023,2023,Research Article,11.0
"Cao Dan,Luo Wenlan,Nie Guangyu","Georgetown University, United States of America,Tsinghua University, China,Shanghai University of Finance and Economics, China","Received 12 April 2022, Revised 9 January 2023, Available online 18 January 2023.",https://doi.org/10.1016/j.red.2023.01.001,Cited by (0),"We introduce our GDSGE framework and a novel global solution method, called ","Dynamic stochastic general equilibrium (DSGE) models are an important tool in the study of business cycles and monetary and fiscal policies. The introduction of a general framework and local solution methods in Blanchard and Kahn (1980), Uhlig (1999), and Sims (2002) among others, and the toolbox Dynare, which implements these local methods, have made it easy to solve and estimate DSGE models and have enabled a large number of important academic studies and policy applications. However, recent developments in macroeconomics highlight the importance of solving these models using global methods. These developments include studies on====In parallel, there has also been significant progress on global solution numerical methods starting with Coleman, 1990, Coleman, 1991 and Judd (1992), as surveyed recently in Maliar and Maliar (2014) and Brumm et al. (2017). However, the aforementioned DSGE models are written in very different forms and the global solution algorithms to solve them are also very diverse. Therefore, there has not been a unified framework and solution method for the global solutions of these models that can be automated for users like Dynare. This paper offers such a framework and method, accompanied by an automated Dynare-like toolbox in which users only need to write model descriptions and let the toolbox translate the model descriptions into MATLAB and C++ codes that solve the model globally.====First, we develop a general framework that encompasses many recent well-known models and their extensions. The framework consists of state variables, policy variables, and short-run equilibrium conditions, e.g., market clearing conditions and Euler equations,==== that fully describe the sequential equilibrium. A crucial difference between this framework and the standard DSGE framework is that the state variables and their ==== domain need to be specified, hence we name it G(lobal) DSGE. In the framework, a recursive equilibrium is a mapping from current state variables to current policy variables (policy function) and future state variables (transition function). We develop a general algorithm to robustly and efficiently solve for these recursive equilibria. The main idea of the algorithm is to solve jointly for policy and transition functions over the iterations. Hence, we call it ==== to differentiate it from the standard Policy Function Iteration (PFI) algorithms in the existing literature (e.g., Coleman, 1990, Coleman, 1991 and Judd (1992)), which we discuss below.==== Second, we develop a toolbox that implements the algorithm. The toolbox is similar to Dynare in that it allows users to write models using intuitive and simple scripts, despite requiring users to specify the state and policy variables and the ranges for state variables explicitly, due to the nature of global solutions.==== With such a design, the toolbox can be used to solve highly non-linear models with a finite number of agents (types), of small to medium sizes, with the number of state variables ranging from a few to a few dozens (using an adaptive sparse grid discretization method to deal with the curse of dimensionality). These models cover many of those in macroeconomics, international finance and asset pricing, as reviewed above and demonstrated in the examples described below. The toolbox is also useful in solving agents' decision problems in fully heterogeneous agent models with both idiosyncratic and aggregate shocks, as long as the optimal decisions can be characterized by first order conditions.====One of the challenges associated with the implementation of our algorithm is to solve for a large number of nonlinear equations with many unknowns, including future endogenous state variables in the STPFI algorithm and unknowns that may need to satisfy inequality constraints (e.g. the Lagrangian multiplier that enters a complementary-slackness condition). The toolbox addresses this challenge by using an efficient equation solver that is able to explicitly enforce bounds of unknowns,==== combined with an automatic differentiation method that is able to compute the Jacobian matrix up to machine precision with small cost.==== Besides, the toolbox implements all actual computations in C++ to maximize the performance, and provides a MATLAB interface that allows users to specify options and generate model output conveniently. This efficient implementation enables the toolbox to solve many classical models that are well-known for computational challenges with a few lines of toolbox codes and in a few minutes on a regular laptop (more details on the run-time is provided in the paper).====The toolbox, together with the STPFI algorithm, demonstrates its convenience and greatest power, relative to other solution methods, for models with endogenous state variables with implicit laws of motion, such as wealth shares or consumption shares. As we make clear in the applications, these endogenous state variables help reduce the number of state variables to be kept track of in models with multiple assets such as Heaton and Lucas (1996), Kubler and Schmedders (2003), and Cao (2018), or help simplify the feasible region of the endogenous state space in models with a collateral constraint such as Mendoza (2010) and Cao and Nie (2017). The endogenous state variables also help circumvent multiple equilibria issues as demonstrated in Cao et al. (2023). Our STPFI algorithm includes the vector of future realizations of endogenous state variables in the vector of unknowns to be solved at each collocation point over the iterations. The additional equations in the system of equations at each collocation point are the ==== that impose the future endogenous state variables to be consistent with current policy variables.====We provide many examples of existing seminal applications that can be solved easily using the toolbox. The examples in the paper include Heaton and Lucas (1996), Guvenen (2009), Mendoza (2010), Bianchi (2011), Barro et al. (2017), and a dynamic stochastic extension of Guerrieri et al. (2022). Each of the examples listed can be implemented within 200 lines of toolbox code and can be executed in a few minutes on regular laptops. In many cases, the toolbox algorithm is fundamentally different from the original solution methods and is either more efficient and faster or more accurate, or both. For example, in Guvenen (2009), his solution method is based on the algorithm in Krusell and Smith (1998) using fixed point iterations over the pricing and aggregate state transition functions with nested value function iterations. Our algorithm recognizes that because the agents' optimization problems are ==== concave problems, the first-order conditions are sufficient for optimality (without having to solve the agents' Bellman equation). Therefore, we can directly use STPFIs to solve jointly for agents' optimization problems and market clearing conditions. As a result, our toolbox's solution method is significantly faster and more stable than the original one in Guvenen (2009). The baseline model in Guvenen (2009) can be solved in less 3 minutes on a regular laptop (MacBook Air 2.2 GHz Intel Core i7) while it takes about 9 hours 15 minutes (on a MacPro Workstation with 8-core 3.2 GHz Xeon processors) using the original algorithm in Guvenen (2009) as reported in Appendix A in his paper.====In another example, Barro et al. (2017) solve an incomplete markets model with rare disasters and heterogeneous risk-aversion using a mixture of projection and perturbation methods developed in Fernández-Villaverde and Levintal (2018), while our toolbox's algorithm is a pure projection method using wealth share as an endogenous state variable with an implicit law of motion accommodated by STPFI. As Barro et al. (2017) discuss in their paper, their solution method is not sufficiently accurate for large values of risk-aversion coefficients (up to 10). We show that our method can tackle cases with risk-aversion coefficients as high as 100 effectively and uncover new economic insights in these cases. We provide many more examples on the toolbox's website.====Since our toolbox solves explicitly for policy functions and transition functions, we can conveniently use them to carry out numerical accuracy analyses in terms of Euler equation errors along the lines in Judd (1992), Judd et al. (2011), and Guerrieri and Iacoviello (2015). To illustrate this feature, we provide the toolbox code for these analyses for our leading example from Heaton and Lucas (1996). The toolbox can also automatically generates Monte Carlo simulations. These simulations are useful to understand the transmission mechanisms in the models using impulse response functions and to eventually estimate the models. The code to generate impulse response functions from the dynamic extension of Guerrieri et al. (2022) is available on the toolbox's website.====The examples mentioned above are highly nonlinear models which require solutions with high accuracy but are of small scale (up to three continuous state variables). The policy and transition functions in these models can be well approximated using spline approximations. Cao et al. (2023) is a New Keynesian application with five continuous state variables using piece-wise linear approximation. Cubic spline or adaptive sparse grid approximation does not work well for the model because the policy functions exhibit sharp changes in slopes in regions where the zero lower bound or collateral constraint switches from non-binding to binding and vice-versa. However, in principle, our toolbox can accommodate medium scale DSGE models of up to 40 continuous state variables such as Smets and Wouters (2007) which features around 20 continuous state variables.==== For these models with larger numbers of state variables, we provide the option of using the adaptive sparse grid method developed by Ma and Zabaras (2009) and introduced to economics by Brumm and Scheidegger (2017). This method works well when the nonlinearity in the models is not as extreme as in Cao et al. (2023). On the toolbox's website, we provide the GDSGE codes for Cao et al. (2023) and a multi-country business cycle models with 15 countries and 30 continuous state variables.====Our approach to solving models with endogenous state variables with implicit laws of motion using STPFI generalizes the approach in Cao (2010), Brumm et al., 2015a, Brumm et al., 2015b, Brumm et al., 2023, Cao and Nie (2017), Cao (2018) and Cao et al. (2023). While these papers only use wealth shares as endogenous state variables, we provide a general formulation of the approach that applies to a broad class of models, including finance and asset pricing models that use consumption shares as endogenous state variables. We show that the approach is critical in enabling a simple and automated toolbox implementation. We also provide an explicit comparison with other approaches in the existing literature in the context of the Heaton and Lucas' model and find that our approach is also numerically superior for this model. One of the other approaches is the nested-fixed point algorithm proposed by Kubler and Schmedders (2003). The algorithm is based on the existing PFI algorithm. In each iteration, the authors solve for future endogenous state variables using the consistency equations as an additional fixed point problem. The solution to the fixed-point problem is then used to formulate a system of equations and unknowns for current policy variables. This nested-fixed point procedure is not amenable to a simple and automated toolbox implementation and might be unstable.==== Another approach to tackle the implicit laws of motion is, at each policy function iteration, to use the transition function from the previous iteration and only solve for the policy function, instead of solving for both simultaneously as in our toolbox. Combining the transition function with the policy function from the previous iteration, one can obtain the forecasts of future variables. This approach is used in more recent papers such as Elenev et al. (2021). We call this method ==== (TFI).==== The advantage of our approach is that the equilibrium in each iteration corresponds to the equilibrium in a finite-horizon economy. For many models, this property guarantees the existence of solution to the short-run equilibrium systems. The convergence of the policy functions over the iterations thus reflects the convergence of equilibria in finite-horizon economies to those of the infinite-horizon economies, consistent with the theoretical proofs for equilibrium existence in infinite-horizon incomplete markets economies such as Duffie et al. (1994), Magill and Quinzii (1994), and more recently Cao (2020). The TFI approach does not always work for the examples considered in the present paper.====An earlier attempt in providing a general, unified framework for global solutions of DSGE models is Winschel and Kratzig (2010). Our framework is more general and allows for endogenous state variables with implicit laws of motion, or equivalently, implicit state transition equations. We also provide a toolbox which only requires users to provide model files, similar to Dynare. Users do not need to code their model in specific programming languages such as Java, Fortran, or MATLAB. Winschel and Kratzig (2010)'s algorithm uses PFIs based on earlier work for DSGE economies such as Coleman, 1990, Coleman, 1991 and Judd (1992).====Our framework is more readily applicable to solving, globally, DSGE models with a finite number of agents or, more precisely, a finite number of agent types.==== Cao (2020) shows that incomplete markets models with finite agent types are useful special cases of fully heterogeneous agent, incomplete markets model with both idiosyncratic and aggregate shocks à la Krusell and Smith (1998). In particular, the former corresponds to the latter in which idiosyncratic shocks are perfectly persistent. We provide an explicit comparison between the two models on the toolbox's website. In addition, the toolbox can be used to solve the agents' decision problem and to simulate in the latter given conjectured laws of motion of the aggregate state variables. Then, with an additional fixed-point iteration on these laws of motion, which can be coded simply in MATLAB, the toolbox solution can be used to solve for the DSGE in the latter. In the last section of the paper, we show how this idea can be used to solve Krusell and Smith's model in fewer than 200 lines of code.====The remainder of the paper is organized as follows. In Section 2, we present the leading example for our toolbox. In Section 3, we provide the general framework and algorithm. A wide range of examples is presented in Section 4. In Section 5 we discuss the application of our toolbox to heterogeneous agent models with both idiosyncratic and aggregate shocks. Section 6 concludes. The design and implementation of the toolbox are presented in the appendix and on the toolbox's website.",Global DSGE models,https://www.sciencedirect.com/science/article/pii/S1094202523000017,Available online 18 January 2023,2023,Research Article,12.0
"Osotimehin Sophie,Popov Latchezar","University of Québec at Montreal, Canada,Texas Tech University, United States of America","Received 15 February 2021, Revised 24 December 2022, Available online 9 January 2023.",https://doi.org/10.1016/j.red.2022.12.005,Cited by (0),"We analytically characterize the aggregate productivity loss from distortions in the presence of sectoral production linkages and show the key role of input substitutability. We analyze the various forces behind the non-monotonic effect of input substitutability on the productivity loss. We then use the second-order approximation to aggregate productivity and find that for moderate distortions, low input substitutability reduces the productivity loss and the role of intermediate-input suppliers. Moreover, when the input ==== is low, sectoral linkages do not systematically amplify the productivity loss. Using the model calibrated on industry-level data for 35 countries, we find that the insights obtained from the approximation are relevant in the context of the sectoral distortions caused by market power, even with the large distortions observed in the data. In particular, we find that using Cobb-Douglas production functions (unit elasticities) instead of accounting for low input substitutability (less-than-one elasticities) leads to overestimating the productivity loss by a factor of 1.8.","The production of goods and services involves a complex web of firms connected through supply chains. For example, food manufacturers use farm products as inputs but also other intermediate inputs such as energy, transportation, or business services, whose production in turn involves many intermediate inputs supplied by the various sectors of the economy. These production linkages across firms and sectors are key to understanding aggregate productivity, especially when the firms' input decisions may be distorted by market frictions. Frictions in one sector may ripple outward through the economy, as the frictions not only affect the distorted firms' input choice but may also indirectly affect the decisions of the other firms along the production chain. Findings from recent work, which highlight how sectoral linkages amplify and propagate various sector- or firm-level shocks (e.g., Jones, 2011; Acemoglu et al., 2012; Atalay, 2017; Bigio and La'o, 2020), may suggest that the interconnection between sectors amplifies the consequences of such distortions. Do sectoral linkages always amplify the productivity loss from allocative distortions? Do distortions in sectors supplying intermediate inputs have a larger impact than distortions in other sectors? Which types of linkages are relevant to understanding the effects of distortions on aggregate productivity?====To study these questions, we construct a multi-sector model with intersectoral linkages of production. The model builds on Long and Plosser (1983) and is similar to Jones (2011) but allows for more flexible production functions and a richer input-output structure. Each sector combines primary inputs (labor) with intermediate inputs using a nested constant-elasticity-of-substitution production function. We allow non-unitary elasticity of substitution for both nests: between different intermediate goods, and between the intermediate-goods bundle and the primary input. We consider distortions in the use of primary or intermediate inputs. These distortions could be the result of frictions in financial, input, or product markets (such as borrowing constraints, hiring and firing costs, or market power). Following Restuccia and Rogerson (2008) and Hsieh and Klenow (2009), we model the distortions in a reduced-form manner as wedges that lead firms to deviate from the frictionless input allocation. We provide analytical characterizations of the effects of distortions on aggregate total factor productivity (TFP) based on the exact solution of the model (which involves finding the fixed point of a nonlinear system) as well as on the second-order approximation to aggregate TFP around the undistorted allocation.====Our characterizations show that the consequences of distortions and the role of sectoral linkages crucially depend on how substitutable inputs are. With the exact solution for aggregate TFP, we analyze the various forces that shape the effect of input substitutability on the TFP loss from distortions. On the one hand, higher complementarity (i.e., lower substitutability) amplifies the effect of a change in the firms' inputs on their output; on the other hand, it reduces the effect of the distortions on the firms' input decision because firms respond less to the (distortion-induced) change in relative prices when the elasticity of substitution is low. The fact that distorting the use of intermediate inputs changes the quantity of output available for final consumption further complicates the analysis. These various forces make the TFP loss a non-monotonic function of the input elasticity of substitution.====Given the complexity of the interaction between the distortions, the sectoral linkages and the elasticities of substitution, we use the second-order approximation to TFP to derive additional insights. We find that the non-monotonicity is absent in the approximation formula. In fact, with the approximation, higher input complementarity leads to a smaller TFP loss. Interestingly, the low elasticities' mitigating effect (found with the approximation) stands in stark contrast with their amplification of negative productivity shocks. This opposite effect makes it important to be clear on the nature of the frictions when studying their consequences on aggregate productivity. When frictions distort relative prices (like in this paper), accounting for input complementarity reduces the TFP loss from moderate distortions, whereas when frictions entail a real resource cost (and are hence akin to lower productivity), accounting for input complementarity increases the TFP loss.====The approximation is also useful to study the role of sectoral linkages. We find that sectoral linkages do not necessarily amplify the effect of distortions; we show that they dampen the effect of all-input distortions (such as markups) when the input elasticity is low and when the covariance between the firms' distortions and their suppliers' distortions is sufficiently negative. Consider for example an economy with imperfect competition where producers with high markups purchase inputs from producers with low markups. In that economy, the sectoral linkages reduce the dispersion of prices and hence lower consumption misallocation (which is the main source of inefficiency when input substitutability is low) relative to the no-linkages case. Distinguishing distortions that are only on primary inputs from distortions on all inputs is important for the dampening conditions. In particular, we find that sectoral linkages are more likely to dampen primary-input distortions than all-input distortions.====Finally, we use the approximation to study which sectoral linkages are the most relevant to understand the effect of distortions. For this purpose, we derive the aggregate TFP impact of a distortion in one sector and look at the contribution of the sector's production linkages. Here as well, the value of the input elasticity of substitution is crucial. When the input elasticity is low, the TFP impact of distortions in sectors that supply intermediate inputs is attenuated; as a result, the relative impact of final-output suppliers becomes larger.====We complement these theoretical results by quantifying the role of input substitutability in the context of the distortions caused by market power, using industry-level data from the World Input-Output Database for 35 countries. The quantitative analysis, which is based on the exact solution of the model, allows us to verify the empirical relevance of the results obtained from the approximation.====We calibrate the production function parameters for each country separately, given the value of the markups and of the elasticities of substitution. The industry-level markups are measured using the price-cost margins and the elasticities are calibrated using estimates from the literature. Following Atalay (2017), in our benchmark calibration we set the elasticity of substitution between intermediates to 0.01 and the one between labor and the intermediate-input bundle to 0.7. Inputs are thus more complementary than in the Cobb-Doublas specification (for which the elasticity of substitution across all inputs is one). With our benchmark calibration, the TFP gain from reducing industry-level markups to zero is 1.2% for the median country and it is higher than 4% for Turkey, Indonesia, and Mexico. Note that these numbers are conservative estimates of the TFP gain from eliminating markups since they abstract from the ====-sector misallocation caused by markups.====We then use the calibrated model to study the role of the input elasticity of substitution. The numerical results (computed from the exact solution) show that the insights derived from the approximation are relevant in our context, even with the sizable markups observed in the data. First, we find that although the TFP gain is an inverted U-shape function of the input elasticity used for the calibration, the TFP gain does increase with the input elasticity within the range of elasticities empirically relevant for intersectoral substitutability. In particular, the Cobb-Douglas specification overestimates the TFP gain for the median country by a factor of 1.8 relative to the benchmark calibration (where the input elasticities are lower than Cobb-Douglas). Second, we find that in the empirically-relevant range, the strength of the amplification from sectoral linkages increases with the degree of input substitutability. The ratio of the TFP gain in the economies with and without sectoral linkages is equal to 5.3 under Cobb-Douglas and to 2.8 with our benchmark calibration. Although sectoral linkages could in theory dampen the effects of markups, we find that this is not the case empirically. However, had the distortions affected only primary inputs, there would have been a dampening effect. In that case, the TFP gain is 30 percent ==== relative to the otherwise identical economy without sectoral linkages. All these results are in line with the theoretical insights derived from the approximation.====In addition, we use the exact solution to show how the relevant linkages depend on the input elasticity. In line with the approximation results, we find that the Cobb-Douglas specification leads to the overestimation of the TFP impact of a distortion on the sectors that supply primarily intermediate inputs, such as mining, the non-metallic mineral products industry, and the renting-of-material-and-equipment industry. All in all, a central message of the paper is that the Cobb-Douglas specification, ubiquitous in the literature on sectoral linkages of production, leads to greatly overestimating the effects of industry-level markups on aggregate productivity as well as the role of sectoral linkages.====Finally, we study the validity of the approximation. As already noted, we find that the insights obtained from the approximation are relevant in the context of industry-level markups. However, in other contexts where more disaggregated inputs are considered (as is often the case with international trade models or New Keynesian models), the insights derived from the approximation may not be valid since the relevant input elasticity is higher and may be in the non-monotononicity area. Specifically, we find that the insights from the approximation are less likely to be valid in contexts where both input elasticities are higher than where only the intermediate-input elasticity is higher. In addition, we use the quantitative application to study the accuracy of the approximation. Our analysis indicates that the approximation should be used with caution for quantitative analysis because its accuracy is good only for moderate distortions and elasticities.====Our paper contributes to the literature on the consequences of allocative distortions on aggregate productivity. This literature, initiated by Restuccia and Rogerson (2008) and Hsieh and Klenow (2009), focuses on the allocation of labor and capital; usually, intermediate inputs are not explicitly modeled. By deriving analytical expressions for the effects of distortions, we complement Osotimehin (2019), who derives similar expressions in a setting without intermediate inputs. Our paper is closely related to Jones, 2011, Jones, 2013 and several related papers such as Bartelme and Gorodnichenko (2015), Grassi (2017), Leal (2017), Liu (2019), Bigio and La'o (2020), Luo (2020), Wang (2020), Hang et al. (2020), Fadinger et al. (2022), and Caliendo et al. (2022), which investigate the effects of distortions in the presence of intersectoral linkages.==== We complement these papers by showing that the amplification from sectoral linkages is weaker when input complementarity is accounted for and that sectoral linkages can actually dampen the effects of distortions.====Contrary to Jones (2011), who highlights that input complementarity ==== the effects of distortions on aggregate TFP, we show that input complementarity actually ==== the effects of distortions. Complementarity has opposite implications for productivity and distortions and jointly analyzing them, as done in Jones (2011), may mask the mitigating effect of complementarity on distortions. Contemporaneous papers by Boehm and Oberfield (2020), Miranda-Pinto and Young (2022), and Baqaee and Farhi (2020) also study the effects of distortions in the context of a production network with non-unitary elasticities, but their focus is different from ours.==== Boehm and Oberfield (2020) consider the effects of enforcement frictions in the market for inputs. Miranda-Pinto and Young (2022) study the role of the cross-sectoral heterogeneity in input substitutability in the context of financial frictions. Baqaee and Farhi (2020) focus on their non-parametric decomposition of aggregate productivity growth, but they provide also a parametric formula for the approximation of the TFP loss from distortions. We derive approximation formulas that are equivalent to Baqaee and Farhi (2020)'s and we use these formulas to perform comparative statics analyses with respect to the network structure. Our key contribution is that we derive also unapproximated expressions for the TFP loss from distortions. These expressions allow us to provide additional insights on the role of input substitutability and to study the validity of the approximation.====The paper also contributes to the literature on the macroeconomic implications of market power. The interest in the topic, which goes back at least to Harberger (1954), has recently surged with the debate on the decline in competition and the rise in firms' market power in the US and other countries.==== Several recent papers have computed the welfare costs of markups and the consequences of market power on macroeconomic outcomes in various settings (Basu, 1995; Epifani and Gancia, 2011; Dhingra and Morrow, 2019; Behrens et al., 2020; Edmond, forthcoming; Peters, 2020) and several of these papers discuss the role of intermediate inputs.==== We contribute to this literature by studying specifically how sectoral linkages and input substitutability jointly determine the TFP loss from markups and by providing estimates for 35 countries of the cost of sectoral distortions caused by market power.====The paper is organized as follows. We describe the framework in Section 2. We present the exact solution of the model and the approximation to aggregate TFP and we study the role of input substitutability in Section 3. Then, in Section 4, we use the exact solution to compute the effects of markups on aggregate productivity in 35 countries, we quantify the role of the input elasticity of substitution and we discuss the validity of the approximation in this context. We offer concluding remarks in Section 5.",Misallocation and intersectoral linkages,https://www.sciencedirect.com/science/article/pii/S1094202522000758,Available online 9 January 2023,2023,Research Article,13.0
Doniger Cynthia L.,"Federal Reserve Board, United States of America","Received 31 December 2020, Revised 8 November 2022, Available online 6 January 2023.",https://doi.org/10.1016/j.red.2022.12.003,Cited by (0),"I study a labor market in which identical workers search on- and off-the-job and heterogeneous firms employ using either an ex-ante posted wage or flexible wage contracts contingent on outside options. Firm level costs for contingent contracts generate a segmented equilibrium in which less productive firms post wages. The model with heterogeneous contracts can achieve wage dispersion, labor share, employment transitions, and flow value of unemployment that are simultaneously consistent with empirical observations while capturing information frictions and search externalities modeled by ex-ante wage posting. In contrast to well known results regarding pure wage posting models, a good fit to these data can be achieved even when the vast majority of firms post wages. Matching to moments for the U.S. economy in the 2010s implies roughly 58 percent of firms post wages and employ nearly 30 percent of workers under such contracts.","Equilibrium search theories yield a fruitful theoretical paradigm for assessing the efficiency and design of labor market policy and how these interact with wage dispersion and inequality. A key component of search models is the mechanism used to divide the output of employment between an employer and employee and set the wage. However, different wage setting mechanisms yield different—sometimes radically—theoretical implications for market efficiency, search incentives, and the distribution of output between and across workers and firms, with some of these in contradiction with observables. Discrepancies between model and data pose challenges for application to policy analysis. This paper demonstrates that a model featuring a mixture of wage setting mechanisms can simultaneously match a long list of policy-relevant observables. This result suggests that explicit modeling of wage contract heterogeneity is an important component of a policy-relevant, search-theoretic model of the relationship between wage dispersion and unemployment.====This paper characterizes the interaction and resulting wage when a firm employing under wage posting (WP) and firm employing under sequential auction (SA) compete over the same worker and, ====, the resulting impact of contract heterogeneity on the implied aggregate wage distribution and search incentives of the unemployed.==== A per-firm cost for employing under the SA contract induces firms to sort between WP and SA wage contracts such that the most productive firms select SA while all less productive firms choose WP.==== For extreme values of the cost the direct antecedents of the model are recovered: under null costs the model is identical to Bontemps et al. (2000) and as costs approach infinity the model becomes identical to Postel-Vinay and Robin (2002b).====A limited set of empirical studies have measured the incidence of wage (re)negotiability. This evidence indicates that a significant fraction of employers hire under contracts that are nonnegotiable while others are willing to renegotiate. Barron et al. (2006) find that 41.3 percent of U.S. firms surveyed in the 2001 Small Business Administration Survey reported willingness to ====negotiate the wage of the most recently hired employee. Hall and Krueger (2012) find 31 percent of U.S. workers surveyed as part of the Princeton Data Improvement Initiative in 2008 reported negotiating over pay at the time of hire. Outside of the U.S., Brenzel et al. (2013) find 38 percent of German firms surveyed in the 2011 Germany Job Vacancy Survey negotiated the wage of the most recently hired employee.==== In addition, Faberman et al. (2022) report that workers searching on the job are 12 percent more likely to have negotiated wages than the unemployed, suggesting that these workers either face a fundamentally different set of employers or that past on-the-job search has lead to positive selection and that firms higher on the job ladder are more likely to renegotiate—as posited in this paper—or both.====In addition to reflecting the reality revealed in these surveys, an advantage to explicitly modeling wage contract heterogeneity is the ability to capture analytical and policy-relevant insight regarding firms that face information frictions, as in the pure-WP model, at the same time as an improved overall fit to both micro- and macro-data.==== Meanwhile, the mixed contract model provides a solution to the Diamond (1971) paradox—since firms are fully informed about unemployed workers' outside options they extract all rents leaving these workers no strict incentive to search at all—present in the canonical pure-SA model.==== In particular, the mixed-contract model is able to produce substantial wage dispersion while providing a positive but moderate option value of search—therefore conforming with observable unemployment, job-to-job, and job-to-unemployment transition probabilities under plausible ranges for the value of non-employment—and ratio of value added to compensation of employees (labor share).====I calibrate off- and on-the-job offer and separation hazards to match job-finding, job-to-job mobility, and separation flows in the United States.==== Under this calibration, the mixed contract model is able to produce the empirically observed levels of wage dispersion and plausible flow value of non-employment even when a large majority of firms post wages.==== In the same range, my calibrated model also produces labor share comparable to that observed for the United States.====I then turn to identifying precisely the composition of contracts. The model implies that, all else equal, the greater the share of firms employing under renegotiable contracts, then the higher the probability of an on-the-job raise occurring in a given interval of time. The difficulty, of course, is to observe raises in data. A small literature documents wage change data in high frequency (Lünnemann and Wintr, 2010; Sigurdsson and Sigurdardottir, 2011; Le Bihan et al., 2012; Barattieri et al., 2014; Murray, 2020; Grigsby et al., 2021; Doniger, 2021). A theme is the predominant synchronization of pay changes.==== However, Grigsby et al. (2021) and Doniger (2021) investigate the properties of asynchronous pay changes and find that these are typically larger and more dispersed than synchronized changes. Doniger (2021) documents seven additional features of asynchronous pay changes that comport with a model of contracts in which renegotiation is triggered by on-the-job offer arrivals. In accordance with these facts, I fit the model—augmented with a probability of exogenously receiving a synchronized pay change each year—to the distribution of the number of pay changes per year recorded in Grigsby et al. (2021). This moment matching exercise suggests that 58 percent of firms and 30 percent of workers in the U.S. employ or are employed under wage posting contracts, well within the range for which the model delivers a good fit to the previously discussed auxiliary moments.====The remainder of the paper proceeds as follows. Section 2 lays out the labor market setting under consideration, including details of contracts that are available; describes worker and firm behavior; and demonstrates that, for every fixed per-firm cost for the right to SA, there is a segmented equilibrium in which low productivity firms employ under WP while higher productivity employ under SA. Section 3 calibrates the model to U.S. job flow data and documents that the model is consistent with a plausible flow of non-employment and labor share for a large range of contract compositions: notably including very large shares wage posting. Section 4 discusses recent evidence of SA contracting and pins down the composition of contracts in the U.S. Section 5 concludes. Proofs, derivations, and additional empirical evidence are found in the Appendix.",Wage dispersion with heterogeneous wage contracts,https://www.sciencedirect.com/science/article/pii/S1094202522000734,Available online 6 January 2023,2023,Research Article,14.0
Liang Yan,"Deakin University, Australia","Received 14 September 2020, Revised 25 December 2022, Available online 4 January 2023.",https://doi.org/10.1016/j.red.2022.12.004,Cited by (0),"This study examines the implications of variable markups for resource misallocation and for aggregate productivity. Using manufacturing data from India, I show that variable markups alone account for a small fraction of the dispersion in revenue productivity when allowing for other sources of misallocation. Meanwhile, variable markups are crucial to understanding the aggregate consequences of policies that reduce misallocation. When equalizing marginal products across firms in narrowly defined ====, my model generates a smaller total factor productivity gain than does a model that uses constant markups because more productive firms endogenously choose higher markups. Thus, the indirect costs of markups are higher than one might expect.","How important are variable markups for resource misallocation relative to the distortions that drive wedges into the marginal products of capital and labor across firms? To what extent does productivity increase when marginal products are equalized across firms in a setting that has variable markups? In this study, I answer these questions by extending Hsieh and Klenow's (2009) framework to incorporate endogenously variable markups. The goal is to analyze the effects of the interactions between endogenous markups and exogenous wedges on resource misallocation and aggregate total factor productivity (TFP).====My model features heterogeneous firms that compete in monopolistically competitive markets while facing preferences that do not have a constant elasticity of substitution (CES), as in the studies by Kimball (1995) and Klenow and Willis (2016). Under Kimball preferences, the price elasticity of the demand of a given variety decreases by the relative quantity that is consumed. Unlike CES, which is characterized by a constant elasticity of demand and a constantly desirable markup, under Kimball preferences, more productive firms produce more output and face less elastic demand and thus they can charge higher markups over their marginal costs. A key result that is exploited in this study is that a firm's markup is monotonically related to its market share. Given that many microdata sets include information on market share, they are used to recover a measure of firm-level markups.====In this study, I use this framework to measure the contribution of variable markups relative to capital and labor wedges to resource misallocation and aggregate productivity. I use firm-level data from India's Annual Survey of Industries (ASI) for 2001 to 2008 to measure the dispersion of markups, marginal products, and revenue productivity within each four-digit manufacturing sector. I find that variable markups account for only 3% of the total variation in revenue productivity and capital, and the output distortions that drive wedges between marginal products across firms explain the remaining 97%. The first assessment suggests that variable markups are not an important source of misallocation.====The fact that variable markups account for a small fraction of dispersion in revenue productivity highlights the importance of using theory-based approaches to estimate markups. Empirical estimations of markups that take no stand on the demand system, such as the production function approach of De Loecker and Warzynski (2012), tend to conflate markups and other distortions. India's ASI data suggest that mismeasured markups can be 30 times more variable than markups that are inferred through the lens of a structural model. However, the current approach focuses on the component of markup variation that is attributable to size variation across firms. The resulting markup estimates are consistent with the theory and can be used to perform counterfactuals.====Although variable markups are not the primary source of misallocation, they are crucial to understanding the aggregate consequences of policies that reduce misallocation. In my model, there is a crucial difference between markups and other types of distortions: a firm endogenously chooses its desired markup while taking as given its productivity and any capital and output distortions. In other words, any changes in capital or output distortions result in an endogenous markup response but not vice versa. This endogenous markup response calls for the use of model-based counterfactual experiments to assess the relative importance of markups versus exogenous distortions.====To this end, I compare the TFP gains from equalizing the marginal products across firms within an industry as predicted by my model with those that are predicted by a canonical model that is based on monopolistic competition and CES preferences. The canonical model estimates that in 2004 India could have increased its manufacturing TFP by 134% through the reallocation of capital and labor across firms to equalize marginal products. Then, I use a calibrated version of my model, hereafter referred to as the baseline model, to recalculate the output gains from liberalization. I find that in the presence of variable markups, hypothetical TFP gains are approximately 105%. The potential gains from reallocation are still substantial; nonetheless, they are less than those predicted by the canonical model. Notably, although the variable markups account for only 3% of the dispersion in the revenue productivity, they cut off more than 20% of the reallocation gains that are predicted by the canonical model of constant markups.====The reason for the disproportionate effects of markups on the aggregate productivity is twofold. First, firms that become more productive after liberalization increase their markups. This effect represents an incomplete pass-through to the final consumers of the productivity gains from the liberalization of firms. The incomplete pass-through is the reason that variable markups lead to resource misallocation. Second, under Kimball preferences, the relative demand for variety increases slower at low relative prices than it does in the Dixit–Stiglitz world of constant elasticity. Thus, the scope for reallocating resources to high-productivity, low-price firms is more limited than that under CES preferences. In other words, the benevolent planner faces diminishing returns in reallocating resources toward high-productivity firms.====To estimate the magnitude of these two effects, I calculate the TFP gains from liberalization in the efficient equilibria of the baseline and canonical models. Although the differences in the TFP gains between the efficient and the market equilibrium of the baseline model measure the effects of the variable markups on resource misallocation, those between the efficient equilibria of the canonical and baseline models measure the effects of diminishing returns that are attributable to Kimball preferences. My results show that misallocation that arises from variable markups may have reduced India's aggregate productivity by 3% in 2004 whereas the effects of diminishing returns may have reduced the TFP gains by about 18%. These numbers suggest that the effects of diminishing returns are about six times larger than the effects of markup-led misallocation.====The main insight from this study is that in a setting that has variable markups, the reallocation gains from equating marginal products are smaller than might be expected. To assess the sensitivity of the main results to the key assumptions, I perform several robustness tests. First, I extend the baseline model to allow firms to freely enter the industry, subject to a sunk entry cost. Second, I consider several alternative preferences, including the Kimball specification that is proposed by Dotsey and King (2005), the symmetric translog demand that is proposed by Feenstra (2003), and the quadratic and non-separable utility that is proposed by Melitz and Ottaviano (2008). Third, I consider an alternative model in which variable markups arise from a finite number of firms that engage in oligopolistic competition in each sector, as in the studies by Atkeson and Burstein (2008) and Edmond et al. (2015). On calibrating these models to match the aggregate moments in the data, I find that the reallocation gains from liberalization are consistently lower than those predicted by the canonical model.====Thus, this study contributes to the literature on misallocation and aggregate productivity, which was pioneered by Restuccia and Rogerson (2008) and Hsieh and Klenow (2009). Recent research on this topic has focused on explicitly modeling the sources of misallocation, including financial frictions (Buera et al., 2011; Midrigan and Xu, 2014; Moll, 2014; Gopinath et al., 2017), adjustment costs (Asker et al., 2014; Gopinath et al., 2017), managerial practices (Akcigit et al., 2016; Caselli and Gennaioli, 2013), legal institutions (Boehm and Oberfield, 2020), and variable markups (Baqaee and Farhi, 2020; Edmond et al., 2021; Peters, 2020). Although I also model variable markups as a source of misallocation, I include other types of distortions, which simultaneously drive the misallocation of resources across firms, in my model. I distinguish between markup distortions and other types of distortions in the data and quantify their relative importance for aggregate TFP gains by using model-based counterfactuals.====Moreover, this study relates to the literature that was initiated by Dixit and Stiglitz (1977) about the welfare costs of markups. In addition, Zhelobodko et al. (2012), Nocco et al. (2014), Dhingra and Morrow (2019), Bilbiie et al. (2019), and Edmond et al. (2021) study the welfare costs of markups. For example, by calibrating the model to data from the United States (US), Edmond et al. (2021) find that the welfare gains from eliminating markups are substantial (approximately 8.7%). Unlike these studies, I examine the welfare implications of removing exogenous distortions in the presence of endogenous markups. I find that variable markups have disproportionately large effects on TFP gains from equalizing marginal products across firms within narrowly defined industries. Taken together, these studies suggest that the direct and indirect welfare costs of markups can be large.====Further, this analysis is closely related to Arkolakis et al.'s (2019) study on international trade. They study the gains from trade liberalization and find that the gains that are predicted by the models based on variable markups are slightly lower than those predicted by the models based on constant markups. Moreover, my study emphasizes the differences in the gains from liberalization as predicted by the two types of models. The key difference between my study and that of Arkolakis et al. (2019) is the counterfactual experiment. They compare the gains from a uniform reduction of distortions that is predicted by models based on variable and constant markups. They find that if preferences are homothetic and there is a Pareto productivity distribution, the two models predict the same efficiency gains. However, I compare the reallocation gains from removing the idiosyncratic distortions that are predicted by the two classes of models. I show that the gains from liberalization as predicted by the models based on variable markups are lower than those predicted by the models based on constant markups, regardless of whether the preferences are homothetic.====The remainder of this study is organized as follows. Section 2 presents the model and describes the market equilibrium and the concept of efficient allocation. Section 3 describes the data and measurement issues. Section 4 presents the potential gains from reallocation. Section 5 concludes the paper.",Misallocation and markups: Evidence from Indian manufacturing,https://www.sciencedirect.com/science/article/pii/S1094202522000746,Available online 4 January 2023,2023,Research Article,15.0
Barro Robert J.,"Harvard University, United States of America","Received 23 June 2022, Revised 12 October 2022, Available online 15 October 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.10.001,Cited by (0),"In long-term data, the dynamic efficiency condition, ====, holds when ==== is the growth rate of GDP if ==== is the return on equity, ====, but not if ==== is the risk-free rate, ====. This pattern accords with a disaster-risk model that fits observed equity premia. The equilibrium may feature ====, which does not signal dynamic inefficiency. In contrast, ",None,r Minus g,https://www.sciencedirect.com/science/article/pii/S1094202522000564,15 October 2022,2022,Research Article,16.0
Gu Ran,"Department of Economics, University of Essex, CO4 3SQ, UK,Institute for Fiscal Studies, UK","Received 15 May 2020, Revised 24 June 2022, Available online 14 July 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.07.001,Cited by (0),Postgraduate degree holders experience lower cyclical variation in ,"The literature on human capital theory makes an important distinction between general and specific human capital. On job separation, general human capital does not depreciate, whereas specific human capital is lost. Becker (1962) suggests that greater specific human capital should reduce incentives of firms and workers to separate, and thus it is the key to many equilibrium search models that study the cyclical behaviour of employment (Cairó and Cajner, 2016). So far, however, few models have direct implications for the impact of specific capital on real wage variation over the business cycle in a frictional labour market. In this paper, I address this question by providing an empirical framework where firms optimally choose how aggregate shocks transmit to wages based on their workers' specific capital. I use this model to explain novel stylised facts about the cyclicality of the postgraduate-undergraduate wage premium.====Beginning with the data, can education provide shelter against wage shocks over the business cycle? Prior literature has focused on the gap between college graduates and non-college workers. Keane and Prasad (1993) find that these two groups experience the same degree of cyclical variation in real hourly wages. Hoynes (2000) finds similar results at a lower education margin (High School vs. Some College).==== Since 1980, however, the landscape of higher education has changed dramatically: the employment share held by postgraduates has doubled (Lindley and Machin, 2016). Indeed, by 2012, nearly 15% of the US adult workforce, or 40% of all college graduates, have a postgraduate degree. Given the rising number of postgraduates, it is important to understand their labour market outcomes. In this paper, I compare postgraduates to undergraduates and document a new pattern: In the US, the postgraduate wage premium is counter-cyclical. To illustrate, Fig. 1 plots the detrended real GDP and the postgraduate wage premium.==== The postgraduate wage premium increases substantially during all recent recessions, and its correlation with real GDP is -0.49. This is because postgraduate wages respond less to business cycle shocks: when real GDP goes up by 1%, the median postgraduate wage increases by 0.34%, and the median undergraduate wage increases by 0.58%.====Furthermore, I find that the difference in wage cyclicality between postgraduates and undergraduates is significantly positive for workers with high tenure, but not for new hires. Since workers' job tenure is the generally used proxy for specific human capital (Altonji and Shakotko, 1987), this phenomenon is consistent with a story of specific capital: As experienced postgraduates have accumulated more specific human capital in their jobs than their undergraduate degree-holding counterparts, the difference in their wage cyclicality is large. As new hires have not yet built any specific capital, the difference in wage cyclicality is small. This may be due to the fact that postgraduates often pursue complex and skilled jobs (Lindley and Machin, 2016), and these jobs require more specific capital (Blatter et al., 2012).====I document a variety of stylized facts to make this argument: (1) the return to the first year of tenure for postgraduates is higher than that for undergraduates; (2) postgraduates who work in the same occupation and industry before and after their job displacement have larger wage losses due to displacement than undergraduates; (3) postgraduates jobs take longer to adapt to than undergraduate jobs. Together, these facts offer prima facie evidence that postgraduates accumulate more specific capital in their jobs than undergraduates, and the type of specific capital that causes the difference between postgraduates and undergraduates is more likely to be of firm-specific nature.====My theory is that greater specific capital of postgraduates makes their outside options less attractive. Postgraduates are less mobile, which allows them to get more insurance from their employers.==== This theory is formalized in a directed search model with risk averse workers and firm commitment in the spirit of Tsuyuhara (2016) and Lamadon (2016).==== I augment it by adding specific capital accumulation and aggregate shocks to productivity. The model has three important features. First, I assume all new hires lack some specific human capital, which they obtain through a period of adaptation. Second, I assume long-term contracts between risk-neutral firms and risk-averse workers facing incomplete asset market. Because of the difference in risk aversion, firms have a risk-sharing motive to provide insurance to their workers, and thereby increase wage stability (Azariadis, 1975). Third, I assume job output depends on worker effort, which is unobserved by firms.==== As workers might shirk their effort, firms have to pay efficiency wages to incentivize their workers to exert optimal effort. With this assumption, firms have an incentivizing motive to reduce wage stability.====The model produces three key results. First, experienced workers working in jobs that require greater specific capital exert a higher effort and have a lower employment separation rate. This is because the expected value of the worker's outside options is lower in these jobs. The second result is that, under the optimal contract, wage changes track aggregate productivity shocks. When aggregate productivity increases, firms promise a higher wage to incentivize their workers, and vice versa. Finally, the third result is that, experienced workers working in jobs that require greater specific capital have more stable wages. Firms face the trade-off between the risk-sharing motive and the incentivizing motive. On the one hand, greater specific capital reduces employment separation and thus increases the effectiveness of firms' future promises in motivating workers. On the other hand, under some mild regularity conditions, greater specific capital increases firm's marginal cost of providing incentives for worker effort. As greater specific capital increases both the effectiveness and the marginal cost of providing incentives, it becomes optimal for firms to provide more insurance rather than more incentives.====I use my model to quantify the effect of specific capital on the educational gap in labour turnover and real wage cyclicality. In the model, specific capital is determined by 2 parameters: the upgrading probability from a new hire to an experienced worker and the productivity gap between them. The upgrading probability is calculated based on the time it takes for new hires to become fully productive. The productivity gap is estimated by targeting the initial wage rise due to specific capital. The model is parsimonious, and it can correctly generate the differences both in labour turnover and in real wage cyclicality across education groups, given the observed empirical differences in specific human capital. The model also shows that, as postgraduates have to accumulate more specific capital, the cut in their starting wage on a new job is larger, but their subsequent wage growth is faster.====Additionally, my paper implies that undergraduates receive less insurance within firms than postgraduates, thereby, increasing the demand for social insurance among this group. I conduct a counterfactual policy experiment to raise the unemployment insurance (UI) by 20%. I find that this policy increases wage cyclicality, indicating UI crowds out the implicit insurance provided by firms. However, the effect is less pronounced for undergraduates than for postgraduates. Furthermore, the welfare gain of undergraduates from such a policy is about 20% higher than that of postgraduates, which supports the argument for a lower UI replacement rate for postgraduates.====I also use my model to quantitatively evaluate two alternative explanations for the counter-cyclical postgraduate wage premium. The two alternative explanations are based on differences in job profitability and hiring costs. I re-estimate the model under each of these alternative hypotheses and then use empirical evidence to discriminate between them. Finally, I briefly discuss some alternatives not nested by my model: cyclical changes in relative supply, differences in risk aversion, and different cyclicality of shocks experienced by postgraduates and undergraduates. I compare them to the available empirical evidence and suggest that they can not provide a reasonable explanation.",Human capital and the business cycle effects on the postgraduate wage premium,https://www.sciencedirect.com/science/article/pii/S1094202522000382,14 July 2022,2022,Research Article,17.0
"Jones John Bailey,Li Yue","Federal Reserve Bank of Richmond, Richmond, VA 23219, USA,Department of Economics, University at Albany, SUNY, Albany, NY 12222, USA","Received 6 July 2021, Revised 8 June 2022, Available online 30 June 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.06.003,Cited by (1),"Using a heterogeneous agent, life-cycle model of Social Security claiming, labor supply and saving, we consider the implications of lifespan ","One of the most common proposals for stabilizing the Social Security system is to increase the normal retirement age (NRA). An appealing feature of this proposal is that it counters the secular trend toward longer lives, a major cause of Social Security's financial difficulties, by effectively delaying the age at which full benefits start. It is also well-known, however, that the longevity gap between rich and poor is large and growing over time (National Academies of Sciences, Engineering, and Medicine 2015; Chetty et al. 2016). This has generated concerns that raising the NRA would cut disproportionately the benefits of the poor. While these concerns can be mitigated by changing the progressivity of the benefit formula (Cremer et al. 2010), the more general question remains. If high-income workers live longer than low-income workers, should they work longer as well? Should they receive higher annual benefits?====When considering such questions, several conflicting principles come into play. (Cremer et al. 2010 provide a nice encapsulation. See also Ndiaye 2020.) Mirrlees's (1971) canonical framework highlights the tension between equalizing consumption and encouraging work by the most productive. In our setting, this implies that high-productivity workers should retire at older ages. With heterogeneous mortality, “equal consumption” must be defined as well. Solutions to social planners' problems often involve equating the weighted marginal utility of consumption across all surviving agents. This can imply equal per-period consumption flows, but larger lifetime transfers to the long-lived. Inevitably, pension design is a quantitative exercise.====To examine these issues, we develop a heterogeneous-agent, life-cycle model of Social Security claiming, labor supply and saving. In the model, individuals face uncertain health, wages, and medical spending, the distributions of which vary by education level. Individual lifespans vary, with mortality rates that themselves vary by health and education. Our framework allows us to answer two questions: first, which Social Security policies maximize welfare when lifespan heterogeneity is present; and second, how does this heterogeneity affect the optimal policies? The answer to the first question is that the welfare-maximizing policies favor redistribution across lifetime earnings groups over incentivizing work by the most skilled. The answer to the second question is that lifespan heterogeneity plays only a small role in policy design.====In our framework the government collects income, payroll and consumption taxes, and provides Social Security, Medicare, Disability Insurance and means-tested social insurance. Individuals can continue working while receiving Social Security benefits, but they may face financial disincentives to do so. They can also retire in advance of claiming Social Security, if they have sufficient wealth.====We evaluate potential reforms by solving the following planning problem. Consider a stationary cross-sectional distribution of individuals who differ along a variety of demographic and economic dimensions. Holding fixed both aggregate Social Security expenditures and revenues, and maintaining general government budget balance, what are the Social Security rules that maximize the ex-ante utility of a newborn? We consider three sets of parametric reforms:====All of the reforms that we study have appeared before as proposals, enacted changes or both. While the first two sets of reforms affect how Social Security benefits depend on wage realizations and ==== decisions, the third affects how benefits depend on ==== decisions. Because individuals can simultaneously work and receive Social Security benefits – or retire in advance of claiming – their work and claiming decisions may appear to be disconnected. This is not the case, however, because benefit receipt itself generates work disincentives. These disincentives include benefit deferrals through the Social Security earnings test and the way in which the income taxation of Social Security benefits increases the income taxation of earnings (Jones and Li 2018). When these disincentives are present, reforms that encourage later claiming also encourage work at older ages.====Each set of reforms embodies the familiar trade-off between redistribution and productive efficiency. Raising the payroll tax cap lowers the tax rate but levies taxes on a broader range of earnings. This reduces taxes for most workers but raises marginal tax rates for the most productive. Linking Social Security benefits to lifetime earnings increases the returns to work but reduces transfers from high to low earners. Raising the returns to delayed claiming can reward longer careers but punishes those with low longevity, who tend to be poor.====Our general finding is that, relative to the Social Security policies currently in place, the policies that would maximize welfare reduce work incentives in order to redistribute resources from high to low earners. Under these policies, the PIA would be independent of lifetime earnings, claiming adjustments would be smaller, and the upper bound on taxable earnings would be 50% larger. In general, the incentives generated by Social Security are small relative to the potential gains from redistribution. Collectively the reforms produce a welfare gain equal to 1.15% of lifetime consumption.====We then examine the sensitivity of our results to increased lifespan heterogeneity, by introducing a hypothetical “2050 demographics” scenario characterized by longer lifespans, lower population growth and an increased education-mortality gradient. We find that the Social Security system that would maximize welfare in this alternative environment is quite similar to the one that would maximize welfare in the baseline. Perhaps most notably, the claiming adjustments in 2050 environment would not be any larger than those in our baseline (2014) specification. Although increased longevity suggests that larger claiming adjustments are needed to promote longer careers, increased longevity also implies that the adjustments needed to induce claiming delays (and longer careers) are smaller. Moreover, our preferred package of reforms includes the elimination of the earnings test and the income taxation of Social Security benefits, as recommended by Jones and Li (2018). Once these provisions are removed, claiming decisions become largely independent of retirement decisions, and almost everyone claims benefits at age 62.====The literature on Social Security reforms is immense (see, e.g., Feldstein and Liebman 2002). In their review, Börsch-Supan et al. (2016) contrast “parametric” reforms, where the basic design of the system is unchanged, with more fundamental “systemic” reforms, such as the switch from a pay-as-you-go to a fully funded system. We restrict ourselves to parametric reforms, holding fixed Social Security's “aggregate footprint.”==== Because the absolute size of a country's Social Security system can have first-order effects on its aggregate capital stock, our approach allows us to focus on distributional concerns.==== Within the literature on parametric reforms, our contribution is to consider all the reforms simultaneously, quantitatively, and while accounting for heterogeneity in income and health. Our exercise also stands out in its breadth: among other possibilities, we consider flat benefits and what is effectively a single claiming age.====While it has been long recognized that heterogeneous mortality affects the lifetime progressivity of Social Security (Aaron 1977), and multiple studies have sought to quantify this effect (recent analyses include Goda et al. 2011; Bosworth and Burke 2014; National Academies of Sciences, Engineering, and Medicine 2015; and Sheshinski and Caliendo 2018), there has been relatively little progress in quantifying its implications for policy reform. Laun et al. (2019) and Sánchez-Romero et al. (2020) consider how to maintain fiscal sustainability in the presence of heterogeneous demographic change. (Also see Conesa et al. 2020.) Although set in Norway, the structure of Laun et al.'s (2019) model is similar to ours. However, they focus on a handful of reforms, including some that change the pension system's aggregate footprint, while we seek to maximize welfare with aggregate spending held fixed. Sánchez-Romero et al. (2020) consider systemic reforms in a general equilibrium framework with no health or wage uncertainty. Using a life-cycle model with heterogeneous mortality rates, Bagchi (2019) examines reforms to the Social Security benefit formula, but he does not consider claiming decisions, one of our key policy instruments. Huggett and Ventura (1999) and Nishiyama and Smetters (2008), who also consider reforms to the benefit formula, do not allow for claiming decisions or heterogeneous mortality risk.====Huggett and Parra (2010) find the system of life-cycle taxes that implements the social planner's solution for a cohort of individuals, holding fixed the net resources extracted from that cohort. In a separate exercise, they find that the welfare-maximizing Social Security benefit function, if considered in isolation, would decrease modestly in lifetime earnings. Ndiaye (2020) expands Huggett and Parra's (2010) framework to include a retirement choice. When considering parametric reforms, he finds that the welfare-maximizing Social Security system would tie benefits more closely to earnings and strengthen the claiming adjustments. Huggett and Parra (2010) and Ndiaye (2020) compare the welfare gains from their Social Security reforms to the gains achievable under an optimal tax system. On the other hand, to make their exercise tractable, they impose a number of simplifications. Among the most important of these are the assumptions that all agents share a common, fixed lifespan and that agents receiving Social Security benefits are unable to work. In our robustness exercises, we show that these restrictions increase the sensitivity of retirement to claiming incentives and thus favor large claiming adjustments.====The remainder of the paper is organized as follows. In Section 2, we describe our model, while in Section 3, we describe its calibration. In Section 4, we present the policies that would maximize welfare in the current demographic environment. In Section 5, we discuss the 2050 demographic environment and the policies that would maximize welfare therein. We then discuss the sensitivity of our policy recommendations to the way in which we model mortality. In Section 6, we perform a few robustness exercises. We conclude in Section 7.",Social Security reform with heterogeneous mortality,https://www.sciencedirect.com/science/article/pii/S1094202522000370,30 June 2022,2022,Research Article,18.0
Shi Shouyong,"Department of Economics, Queen's University, Kingston, Ontario, K7L 3N6, Canada","Received 30 July 2020, Revised 10 June 2022, Available online 17 June 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.06.002,Cited by (0),"This paper studies the equilibrium and the social optimum in an economy where knowledge diffusion interacts with disease transmission. Knowledge increases productivity and is diffused through learning. A learner chooses the intensities in normal learning, isolated learning and production. Normal learning is more effective than isolated learning but requires a learner to contact a teacher. A higher intensity in normal learning increases a learner's contact rate with a teacher, thereby speeding up both knowledge diffusion and the transmission of an infectious pathogen. An infection reduces productivity and possibly results in death. Calibrating the pathogen to Covid-19, the model shows that the unexpected arrival of the pathogen induces a susceptible learner to adjust the normal learning intensity in a V-shaped pattern over time. Aggregate output also follows V-shaped adjustments. Switching from the equilibrium to the social optimum reduces infections and deaths substantially and increases social welfare. I also examine temporary lockdowns in the equilibrium.","Knowledge diffusion and disease transmission have been intertwined throughout human history. As ==== argued convincingly, knowledge of agriculture (farmer power) spread from early civilizations together with infectious pathogens in domesticated animals. In the present time, the primary use of knowledge is for industrial production and services instead of agriculture. This may change the sources of knowledge and infectious pathogens, but it does not change two fundamental features of their diffusion/transmission. First, knowledge and infectious pathogens are both non-rivalrous. One is a public good and the other a public bad. Second, individuals ignore the externalities generated by their actions in the transmission of knowledge and diseases. These features put the interactions between knowledge diffusion and disease transmission squarely in the domain of economic analysis. However, the economic literature on epidemiology has largely ignored knowledge diffusion (see a partial review later). I construct a model to focus on the dynamic tradeoff between knowledge acquisition and disease infections.====The model economy has continuous time, a unit measure of risk-neutral individuals, two knowledge levels and one pathogen. The frontier knowledge has higher productivity than the baseline knowledge. An infection of the pathogen reduces the effectiveness in learning and production, and possibly results in death. An individual can divide a unit flow of time (intensity) among normal learning, isolated learning and production. Production does not require a contact between individuals but a contact can occur nevertheless. Isolated learning prevents a contact but is less efficient than normal learning. Normal learning requires a contact, where a learner randomly contacts a teacher who has the frontier knowledge. The contact rate increases in the learner's effective intensity. In such a contact, a learner successfully acquires the frontier knowledge with a positive probability. Independently of the learning outcome, an infected individual can transmit the pathogen to a susceptible individual in the meeting. A recovery from an infection can come with or without immunity. An infection increases the death rate. When an individual dies, a newborn enters the economy with the baseline knowledge and draws the disease status as either susceptible or infected.====The key mechanism of the model is that increasing the intensity of normal learning increases knowledge diffusion and disease transmission simultaneously by increasing the contact rate between learners and teachers. Thus, optimal choices of learning intensities involve a tradeoff between knowledge acquisition and the risk of infections. This tradeoff is dynamic and depends on the endogenous distribution of individuals over the levels of knowledge and disease statuses. A higher intensity in normal learning increases the fraction of infected individuals in the population, thereby increasing the risk of infection for an uninfected and reducing the benefit of normal learning in the future. This forward-looking consideration affects how much and how long a learner wants to postpone normal learning or to substitute it into isolated learning in the time of a pandemic.====To illustrate the dynamic tradeoff concretely, I calibrate the pathogen to Covid-19 and start the economy with a moderate ratio of learners to teachers. After the pathogen arrives unexpectedly, aggregate output drops significantly in a short time and the death toll from infections increases quickly. After being infected, a learner increases the normal learning intensity. In contrast, a susceptible learner adjusts the normal learning intensity in a V-shaped pattern over time. The learner shifts the intensity first from normal learning to production, then from normal learning and production to isolated learning, and finally from isolated learning back to normal learning and production. These adjustments in intensities may slow down the pathogen transmission, but increase the decline in aggregate output. Active cases of infections reach the peak quickly and then decline. Aggregate output also follows V-shaped adjustments over time.====The transmission of the pathogen feeds on knowledge diffusion. To illustrate this interaction, I compare two alternative economies with the baseline. One alternative economy starts in the steady state, where the ratio of learners to teachers is low. This economy does not grow, and learning activities are low. As a result, active cases of infections are much lower than in the baseline, and the pathogen does not affect the economy much. In another alternative, the economy starts with a higher ratio of learners to teachers than in the baseline. In the absence of the pathogen, this economy has faster diffusion of knowledge and faster growth in the short run and the medium run than the baseline economy. After the pathogen arrives in this economy, active cases of infections have a higher peak and the peak is reached earlier than in the baseline economy. The pathogen induces a substantially larger fall in aggregate output than in the baseline.====The baseline model abstracts from the policy controls implemented in the Covid-19 pandemic, such as testing, social distancing, mandatory wearing of masks and enforcing quarantines. A purpose of this abstraction is to show how the pathogen can be transmitted and interact with learning in the absence of such controls. This helps evaluate the welfare gain from the policy controls. I conduct two analyses related to the controls. The first is to characterize the social optimum under the assumption that the planner can costlessly distinguish different types of individuals. The second analysis is to examine the equilibrium with temporary lockdowns that are uniformly enforced in the entire population. In both analyses, I measure social welfare by subtracting from aggregate output the value of each life lost to the infection, which is calibrated in section ====.====The equilibrium is socially efficient without the pathogen but inefficient with the pathogen. With the pathogen, the social optimum requires an infected learner to devote all intensity to isolated learning, and none to normal learning or production, until infections become small. Since the infection rate is low, this allocation requires a susceptible learner to devote almost the same intensity to normal learning as without the pathogen. These socially efficient choices reduce infection-induced deaths to a miniscule level, with little reduction in output. Relative to the equilibrium, the welfare gain from the social optimum is 2.1% of permanent output or, equivalently, 44% of the first-year aggregate output.====I analyze two lockdowns each lasting for 19 weeks. The early lockdown starts before the peak of new infections in the baseline equilibrium, and the late lockdown starts after the peak. The early lockdown delays the peak of new infections and significantly reduces the number of deaths from infections. Output falls by less after the pathogen arrives but the reduction in output lasts for a longer time than without the lockdown. The late lockdown does not delay the peak of infections significantly, but reduces cumulative deaths by a similar amount to the early lockdown. Both lockdowns improve social welfare by about a half of the gain from the social optimum.====One might have asked: since knowledge diffusion is relatively slow, how can it play an important role in a fast spreading pathogen like Covid-19? The answer is that the infectivity of a pathogen interacts with knowledge diffusion. When a pathogen is highly infectious, even a small change in learning intensities can have a large effect on how fast the pathogen spreads by affecting the contact rate. This is why the number of infections changes significantly with the initial distribution of knowledge in the population.====It is important to emphasize that knowledge includes all non-physical elements that increase productivity but need time to acquire. Evidently, these elements are not limited to knowledge acquired in formal settings such as schools and universities. Even though I use the terms “teachers” and “learners” for the lack of concise alternative phrases, one should not interpret learning in this paper as activities limited to formal settings. For example, learning on the job is an example of learning in this paper. Section ====The paper is organized as follows. The next subsection reviews the literature. Section ==== constructs the baseline model, and section ==== analyzes the equilibrium. Section ==== presents the results of the calibrated model. Section ==== examines the social optimum and the equilibrium with lockdowns. Section ==== concludes the main text. The appendices provide proofs and sensitivity analyses. The supplementary appendix describes the procedures of calibration and computation, and provides additional sensitivity analyses.====For the proof of ====, examine ==== first. The right-hand of ==== is a function of only ==== and the disease status. By the envelope condition on the optimal choices of ====, the derivative of the right-hand side of ==== with respect to ==== is equal to ====. Thus, ==== is an ordinary differential equation of ==== with a strictly positive eigenvalue. Because ==== is a jump variable, the solution for ==== under rational expectations is that ==== jumps immediately to the long-run level, ====, that solves:====Because the right-hand side of ==== is strictly increasing in ====, the solution for ==== is unique if it exists. It is clear that the right-hand side of ==== is negative at ====, and positive at ====. Thus, a unique solution for ==== exists and satisfies ====. Because ====, the assumption ==== for all ==== implies that the net return on ==== is strictly less than the net return on ==== for all ====. Thus, ==== for all ==== if ====. If ====, the feasibility constraint on the intensities again implies ====.====To prove ====, note that an immune learner's net marginal gain from normal learning is ====. Suppose ====, contrary to the proposition. Since ====, ==== yields ====, where ==== is defined in ====. The net gain from increasing ==== is ====, which is strictly positive at ==== under the condition ==== in ====. This contradicts the supposition ====. Thus, ====. Similarly, suppose ====, contrary to the proposition. Then, ==== solves ====, where ==== is defined in ====. Under ==== in ====, the marginal net gain from ==== is negative at ====, which contradicts the optimality of ====. Thus, ====. With the result ====, ==== implies:====This inequality is equivalent to ====. Moreover, because ==== lies in the interior of ====, it satisfies the first-order condition, ====. That is, ====.====Setting ==== in ====, the equation becomes an ordinary differential equation of only ==== with a positive eigenvalue. By a similar argument to the above, ==== for all ====. Using this result and ====, I can calculate ==== as in ====. Then, the definition of ==== and the result ==== imply ==== for all ====. This completes the proof of ====.====Now turn to ====. Setting ==== in ==== and ==== shows that the net marginal gain is ==== for ==== and ==== for ====. If ====, the net marginal gain from ==== is strictly negative, which yields ====. Suppose ====. Because ====, the assumption ==== implies ==== for all ====. For all such ====, if ====, a learner 0==== can gain by reducing ==== to 0 and increasing ==== by the same amount. On the other hand, if ====, the feasibility constraint on learning intensities implies ==== again.====Set ==== as in the remainder of the proposition. ==== becomes====Similar to the proof of ====, one can show that ==== jumps immediately to the long-run level ==== solving ==== and that ==== for all ====. Also, an infected learner's marginal gain from normal learning is ====. I will prove ==== later. If ====, contrary to the proposition, then ==== yields:====Since ==== by ====, then ====, which contradicts the optimality of ====. If ====, then ==== yields:====where the inequality follows from ====. Since ==== by ====, then ====, which contradicts the optimality of ====. Thus, ==== lies in the interior of ==== and, hence, satisfies the first-order condition that yields ====. Moreover, if ====, then ==== with ==== implies that ====, where ==== is given in ====. This implies ==== for all ====.====To prove ====, I use the expressions for ==== in ==== and ==== in ==== to derive:====The inequality follows from ====, ==== and ====. By ==== and ====, ==== if and only if ====. Substituting ====, this condition becomes====To prove that this inequality holds, subtract ==== and ==== to obtain:====Temporarily denote the right-hand side of this equation as ====. Because ==== inherits the dependence of the right-hand of ==== on ====, ==== is increasing in ====. Then, ==== if and only if ====. Compute:====I have used the fact ====. By the envelope condition of the first maximization problem, the maximum is (weakly) increasing in ====. Because ==== and because the choice set is wider in the first maximization than in the second, the first maximization achieves a (weakly) greater maximum than the second maximization. Thus,====This proves ==== and, hence, ====.====To prove ====, suppose ==== to the contrary. By the envelope condition, the maximum of the first maximization problem in ==== cannot exceed that of the second maximization. Then,====This contradicts the fact ====.====Finally, since ==== for ==== and since ==== is a strictly decreasing function, the result ==== implies ====. ","Knowledge, germs, and output",https://www.sciencedirect.com/science/article/pii/S1094202522000369,17 June 2022,2022,Research Article,19.0
"Bernstein Joshua,Kamdar Rupal","Indiana University, Bloomington, United States of America","Received 28 July 2021, Revised 4 May 2022, Available online 9 June 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.06.001,Cited by (0),This paper studies optimal ==== under rational inattention: the policy maker optimally chooses her information subject to a processing constraint. Our analytical results emphasize how the policy maker's information choices shape her expectations and the dynamics of the macroeconomy. Paying attention to demand shocks lowers output volatility and causes untracked supply shocks to drive ====. Because persistent supply shocks have a minor impact on ,"Information is a key component of monetary policy decision-making. Policy makers must choose what information to pay attention to and what to ignore. As emphasized by recent chairmen of the Federal Reserve, limited information leads to considerable uncertainty about the current state of the economy, the trajectory of the economy, and whether this trajectory is predominantly driven by demand or supply-side forces. For example, Ben Bernanke said: “Uncertainty about the current state of the economy is a chronic problem for policymakers.”==== Although important in practice, this informational choice is absent from existing analyses of optimal monetary policy which assume policy makers have access to all available information about the economy or an exogenously restricted subset. In this paper, we relax this assumption. We study the determination of policy makers' information choices, the optimal monetary policy, and the implied equilibrium dynamics. Our main contribution uses closed-form analytics to show how the policy maker's information choices shape her expectations and the equilibrium dynamics of the economy.====To accomplish our goal, we suppose that the policy maker is subject to rational inattention (Sims, 2003). We add this constraint to the optimal monetary policy problem of a canonical New Keynesian model driven by exogenous demand and supply shocks (e.g. Galí, 2015). To cleanly assess its impact on optimal monetary policy, we do not incorporate information frictions on households or firms. Under full information, the optimal monetary policy attains the first best by tracking the efficient real interest rate. This policy minimizes the welfare cost of price adjustment by discouraging firms from ever changing their nominal prices. Rational inattention constrains the amount of information that the policy maker can obtain about the efficient real rate and use in monetary policy design. Assuming that the policy maker learns about supply and demand shocks independently, she is forced to trade-off her attention between demand factors and supply factors: paying more attention to one necessitates paying less attention to the other. To build intuition for the effects of this trade-off and to derive analytical results, we first consider the case in which demand and supply shocks are independent and identically distributed (i.i.d.). We obtain expressions for the policy maker's expectations of these shocks, the paths of nominal interest rates, output, and inflation, and the optimal attention allocation over the shocks.====When information processing capacity is limited, the policy maker must divide her attention between knowing about prevailing supply-side and demand-side conditions. This trade-off affects expectation formation through two channels. First, limited information about current shocks causes the policy maker to place positive weight on her prior beliefs. In the i.i.d. case, this attenuates her posterior expectations of the shocks towards zero. The informational trade-off implies that dampening the expectation attenuation of one shock comes at the cost of strengthening the attenuation of the other shock. Second, the noisy information acquired under rational inattention creates endogenous and stochastic variation in the policy maker's optimal expectations over time. This noise is subject to the same informational trade-off: forming more precise expectations of one shock necessitates forming noisier expectations of the other.====Output dynamics are determined by the informational focus of the policy maker as well as the usual intertemporal substitution channel. In equilibrium, the policy maker adjusts interest rates as a function of her expectations of demand and supply shocks: she lowers rates to accommodate increases in supply, and hikes rates to offset inefficient increases in demand. Optimality implies that monetary policy responds more strongly and more precisely to the shock that the policy maker pays more attention to. As a result, output fluctuations are larger when the policy maker focuses more on supply shocks because interest rate policy is more accommodating of changes in supply, and does not fully offset changes in demand due to a lack of information. Conversely, an increased focus on demand shocks dampens output fluctuations as policy aggressively offsets changes in demand but does not accommodate changes in supply as much.====Deviations in output from its efficient path create output gaps which cause inflation responses that are absent in the complete information economy. Inflation responds positively to demand shocks and negatively to supply shocks. The sizes of these responses depend on the informational trade-off. In particular, inflation responds more to the shock that the policy maker devotes less attention to. Intuitively, if the policy maker focuses mainly on supply shocks, then output gaps and hence inflation are mainly driven by changes in demand that are not offset by monetary policy. The informational focus of policy makers also determines the sign and strength of the co-movement between inflation and real activity. A stronger focus on supply shocks generates more volatile output and a strong positive co-movement between inflation and output driven by demand shocks. In contrast, if the policy maker more carefully tracks changes in demand, then output will be less volatile and will exhibit a weaker and negative co-movement with inflation driven by supply shocks. Thus, the disappearance of the empirical Phillips curve in recent data (Hall, 2013) may be partially attributable to increased attention on demand factors by policy makers.====Noisy expectations create a stochastic component of monetary policy that is orthogonal to the exogenous demand and supply shocks. In equilibrium, these shocks cause a positive co-movement between output and inflation, and so operate like traditional monetary policy shocks (Christiano et al., 2005). However, rather than being exogenous, our monetary policy shocks are an optimal and endogenous response to the policy maker's information constraint, and have a natural origin: limited information leads to noisy monetary policy.====We show that the optimal monetary policy can be implemented using a feedback rule that depends on noisy observations of output and inflation. Unlike the existing literature, we do not assume that these observations are available to the policy maker ex-ante (e.g. Aoki, 2003). Instead, the policy maker endogenously chooses to include these observations in her information set to best inform her beliefs about the economy, thus providing a justification for why policy makers should focus on the dynamics of output and inflation when designing monetary policy. Furthermore, our implementation shows that a focus on demand shocks by the policy maker is associated with the precise measurement and stabilization of output, while a focus on supply shocks is implemented via the more precise measurement and stabilization of inflation. Hence, the modern “price stabilization” goal of central banks around the world can be interpreted as an implicit focus on tracking supply shocks more than demand shocks.==== Intuitively, stabilizing output reduces both inefficient demand-driven output volatility and also efficient supply-driven volatility, which is optimal for a demand-focused policy maker. In contrast, traditional inflation stabilization mostly offsets demand-driven output gaps, and so is consistent with a policy maker focused on accommodating supply changes. Finally, under such a rule, we show that a sufficiently limited information capacity causes equilibrium indeterminacy because the policy maker's optimal responses to shocks become too weak to rule out the existence of sunspot equilibria.====The optimal informational focus of the policy maker depends crucially on the persistence of exogenous shocks. In the i.i.d. case, we show analytically that the information processing capacity allocated to a shock is intuitively increasing in that's shock's variance. However, when we calibrate the shock persistences to plausible values, the optimal information allocation becomes biased towards demand shocks. This asymmetric allocation of processing capacity reflects how each shock affects the efficient real interest rate that the policy maker would like to imitate through monetary policy. Intuitively, very persistent supply shocks do not have large effects on expected growth rates of output and consumption, which are the main determinants of interest rates in equilibrium. Hence, monetary policy is best served by focusing more on demand-side factors which have a stronger impact on efficient interest rates that policy makers wish to replicate.====Finally, we explore how increasing the policy maker's information capacity affects model outcomes, and compare them to macroeconomic trends in post-WW2 data. As the capacity increases, equilibrium dynamics converge to their efficient paths, volatility declines, and the co-movement of inflation with output growth falls. These patterns are consistent with the increasing accuracy of Federal Reserve beliefs as stated in Greenbooks (Tulip, 2009), the decline in empirical macroeconomic volatility (the “Great Moderation”), and offers a new interpretation of the disappearance of the Phillips curve linking inflation and real activity. Through the lens of our model, higher information capacity leads to less price volatility and a weaker correlation between inflation and output growth, even though the structural Phillips curve is stable over time.====  To the best of our knowledge, we are the first to study optimal monetary policy with endogenous information acquisition by the central bank. Aoki (2003) studies optimal monetary policy when the policy maker has complete knowledge of the economy's past, but only observes exogenously noisy measures of current output and inflation. We instead use rational inattention to fully endogenize the policy maker's information set. Our approach yields new insights that exogenous information settings obscure. First, we can analyze the important interactions between the policy maker's information choices, her expectations, and the equilibrium dynamics. Second, we find that monetary policy shocks arise endogenously and are an optimal response to limited information rather than a purely exogenous disturbance. Third, we show how the optimal policy can be implemented using noisy measures of output and inflation, where the coefficients on output and inflation, and the noise in the observations themselves are endogenous functions of the optimal information allocation. This dependence allows us to derive the conditions under which the policy maker should track inflation more accurately than output, and when the policy maker should respond more strongly to inflation than output. This analysis is impossible in an exogenous information setting. The advantages of our approach also apply to Boehm and House (2019) who study optimal monetary policy with exogenously restricted information and the additional assumption that policy must be conducted using a Taylor rule.====Although our contribution is applied in nature, we also connect our findings to the theoretical results of Svensson and Woodford, 2003, Svensson and Woodford, 2004, who study linear-quadratic optimal monetary policy problems when policy makers only observe exogenously noisy indicators of economic variables. Like them, we find that uncertainty does not affect the conduct of optimal policy when it uses optimal estimates of the variables (“certainty equivalence”), and that expectation formation cannot be separated from equilibrium dynamics when policy makers have less information than the private sector. However, our inclusion of endogenous information acquisition highlights the importance of equilibrium dynamics when assessing the optimal information allocation. For example, the high persistence of supply shocks implies that they have little impact on the efficient real interest rate so that policy makers need not pay much attention to them. Hence, it is actually optimal for the policy maker to receive very noisy indicators of supply shocks.====Our analysis also relates to the recent literature that merges the New Keynesian framework with models of incomplete information. For instance, Woodford (2010) and Adam and Woodford (2012) characterize robust, in the sense of Hansen and Sargent (2005), monetary policy under uncertainty about private sector beliefs. Alternatively, Paciello and Wiederholt (2014) and Angeletos and La'O (2020) study optimal monetary policy when firms have limited information, while Maćkowiak and Wiederholt (2009), and Afrouzi and Yang (2020) study firm pricing behavior under rational inattention. Finally, Maćkowiak and Wiederholt (2015) study business cycle dynamics when both firms and households face inattention constraints. We instead focus on the effects of limited information for the policy maker, and stress how her informational choices shape and are shaped by her expectations and the equilibrium dynamics.====Finally, our results contribute to the discussion around the flattening of the empirical Phillips curve. Closest to us, McLeay and Tenreyro (2020) show how optimal monetary policy under full information can impart a negative correlation between inflation and output in the presence of cost-push shocks. In contrast, we show how limited information can influence the empirical correlation by affecting whether demand or supply shocks drive business cycle fluctuations. At low information, demand shocks dominate and induce a positive correlation between inflation and real activity. As information improves, supply shocks play a larger role and weaken the correlation towards zero. Our approach complements others who have emphasized the role of private sector inflation expectations in estimations of the structural Phillips curve. Hazell et al. (2020) and Jørgensen and Lansing (2021) highlight the importance of anchored inflation expectations over the past two decades, while Coibion et al. (2018) demonstrate the stability of the Phillips curve when estimated using survey-based inflation expectations of households.====The paper proceeds as follows. Section 2 describes the economic environment, information frictions, and sets up the optimal monetary policy problem. We present our main analytical results in Section 3, and the extension to persistent shocks in Section 4. We discuss the link between information and macroeconomic trends in Section 5, and show that our insights hold under various extensions in Section 6. Section 7 concludes.",Rationally inattentive monetary policy,https://www.sciencedirect.com/science/article/pii/S1094202522000357,9 June 2022,2022,Research Article,20.0
"Baughman Garth,Flemming Jean","Federal Reserve Board, United States of America","Received 16 March 2020, Revised 24 May 2022, Available online 3 June 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.05.003,Cited by (0),"We construct a model where a basket-backed stablecoin – a currency backed by and pegged to a combination of sovereign currencies, such as Mark Carney's “synthetic hegemonic currency” or Facebook's proposal for Libra – is demanded for transaction purposes. In the model, demand for the basket derives from trade shocks which affect demand for the underlying sovereign currencies. Despite providing a justification for the basket, our model, in numerical simulations, predicts that overall demand for the basket will be low. This derives from a general-equilibrium effect of the basket currency: Demand for the basket creates pass-through demand for the underlying currencies that back it. This pass-through demand stabilizes the value of the currency for which the basket was meant to substitute, limiting demand for the basket. We calculate that this low demand from buyers would likely limit adoption by sellers. Further, we show that agents in the economy disagree about the optimal basket composition, but its welfare impacts are small.","Since the second world war, the US dollar has dominated international trade and financial markets (Eichengreen et al., 2018). As we march into a new decade, some have begun to call for an end to that dominance. On the one hand, a multipolar world may be better served by a multipolar international monetary system, with a multipolar reserve currency. On the other hand, the extant dollar payment system faces new potential competition in the form of publicly or privately issued digital currencies based on emerging technologies, such as blockchain, which promise ease of access and lower transactions costs. The most attractive new competitors – dubbed “stablecoins” – employ various mechanisms to maintain stable values relative to some peg. While most stablecoins are pegged to the US dollar or other single sovereign currency, two proposals in the summer of 2019 called for the creation of new international currencies comprising a basket of sovereign currencies: Facebook announced plans for its new currency, the Libra stablecoin (Libra Association, 2019); and Mark Carney, Governor of the Bank of England, proposed the creation of a “synthetic hegemonic currency” (Carney, 2019).==== In both cases, a stated goal of building on a basket of underlying currencies is to increase global acceptance by limiting fluctuations in the value of the basket relative to any one currency. However, the term stablecoin may be a misnomer for such an asset, since the basket's value fluctuates relative to each currency.====Regulators and elected officials from around the world have questions and concerns ranging from privacy and fraud prevention to broader effects for financial stability and monetary policy.==== But a simpler question arises: would a basket currency actually provide substantial value relative to the current system? Under what conditions is there a transaction role for a basket-backed currency? This paper constructs a micro-founded, international monetary model to investigate these questions.====We model a two-country, two-currency economy where agents demand currency to facilitate decentralized exchange subject to search and matching frictions. Trade shocks – fluctuations in the probability of international meetings – affect money demand, leading to variations in the value of each currency. These fluctuations detrimentally affect risk averse consumers' welfare. We introduce a basket currency that is a convex combination of the two countries' currencies, which may be preferable to the constituent currencies in certain states of trade, and analyze demand for this basket currency and the resulting welfare implications.====A large literature studies the potential for new currencies. These often focus on the network externalities inherent to the two-sided nature of payment systems – sellers' incentive to accept a currency depends on consumers' currency holdings and vice versa. In order to focus on ====, we abstract from this externality for most of our analysis, considering various exogenous scenarios for sellers' acceptance decisions. Our motivation for this abstraction is twofold. First, many technology companies already possess large user networks that make the question of acceptance less pressing. Second, as in all models of money, one equilibrium features zero adoption of the currency. Thus, we focus on obtaining an upper bound on demand, and so make generous assumptions about its acceptability.==== In the same spirit, we make other generous assumptions: the basket is perfectly safe as it is fully backed by the underlying currencies, is costless to create and fully redeemable each period, and faces no threat of theft or other drawbacks.====The model focuses closely on the basket component of the proposed currency. Hence, the model is stylized, limited to the most parsimonious general equilibrium microfounded model of money that can allow for meaningful consideration of a basket currency. The model divides the motive to hold a currency into two components: how often a buyer can use the currency in trade, its “spendability,” and its rate of return i.e. its inflation. Careful treatment of both the microfoundations of demand for currency and its general equilibrium effects are key to our ultimate conclusion, which is that, under a generous calibration, there is minimal demand for a basket currency. We show that this result is due to the fact that the basket is demanded in states of the world in which one sovereign currency's purchasing power is strictly dominated by the other currency. However, because demand for the basket implies demand for its underlying currencies, the introduction of the basket improves the purchasing power of the dominated currency and reduces demand for the basket itself. That is, the benefit of obtaining a higher rate of return than one currency is reduced as the basket itself affects this rate of return. Overall, our model shows that the introduction of the basket attenuates fluctuations in the most volatile currencies underlying the basket, reducing the welfare gains from holding the basket relative to holding the underlying currencies and making it infeasible for the basket to become the globally dominant currency. Also, because the basket currency will never dominate the sovereign currencies it comprises, we find that there are unlikely to be substantial gains in world welfare as a result of its introduction.====Finally, we use the model to compute the optimal composition of the basket. Comparing the welfare implications in partial and general equilibrium, we find that conclusions about the optimal basket composition vary drastically when considering the cases in which one country's sellers accept the basket or both countries' sellers do. When only sellers from the country with a volatile currency accept the basket, the optimal basket composition is approximately 80-20 in favor of that country's currency, implying that the general equilibrium effects of the basket, which reduce volatility of the more volatile currency, outweigh the demand channel which favors pegging to the more stable currency. Instead, when the basket is accepted worldwide, the optimal basket composition trades off the reduction in volatility of the more volatile currency with the increase in volatility of the less volatile currency, resulting in an optimal basket composition that is roughly equal between the two underlying currencies. In either case, there is disagreement between buyers and sellers in different countries as to the optimal basket composition, which we discuss in detail below.====Even under optimal basket weights, buyers' demand for the basket currency is only a small fraction of global currency holdings. Because holdings are small, so are purchases financed with the basket. This, in turn, means that sellers' profits from basket sales are low, and their willingness to pay to accept the basket is small, far less than the cost of many points of sale terminals capable of processing current electronic payments. Finally, we conduct a series of robustness exercises to test the sensitivity of our results to changes in parameters and the number and size of countries making up the basket currency, and find that the qualitative results still hold in each of these exercises.",Global demand for basket-backed stablecoins,https://www.sciencedirect.com/science/article/pii/S1094202522000345,3 June 2022,2022,Research Article,21.0
"Klein Michael A.,Şener Fuat","Rensselaer Polytechnic Institute, Troy, NY, USA,Union College, Schenectady, NY, USA","Received 28 June 2021, Revised 20 April 2022, Available online 10 May 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.05.001,Cited by (1),"We develop a model of Schumpeterian growth featuring a stochastic diffusion process where the rate of commercial success of product innovations is endogenously determined by advertising intensity. We consider both informative advertising, which young technological leaders use to increase the probability of diffusion, and defensive advertising, which incumbents use to prevent the diffusion of competing products. Economic growth depends positively on the arrival rate of product innovations and the diffusion rate of innovations into the mainstream market. We show that R&D subsidies shift relative investment incentives towards innovation and away from diffusion. This creates an inverted U-shaped relationship between R&D subsidies and both economic growth and welfare as innovations arrive more frequently, but fewer commercialize successfully. We find that advertising subsidies increase diffusion, growth, and welfare when advertising is purely informative. In the presence of defensive advertising, advertising subsidies lead to socially wasteful increases in resources devoted to advertising without large increases in diffusion, reducing growth and welfare.","Schumpeterian endogenous growth models routinely assume that successful innovators costlessly and instantaneously capture their entire potential market share. This implies that all consumers immediately adopt newly improved products and that incumbent firms are fully displaced as soon as innovation occurs. Although analytically convenient, this assumption is clearly counterfactual. Research on the marketing and diffusion of innovations has long emphasized that technical and functional superiority does not guarantee a new product's commercial success.==== Overall, empirical estimates suggest that 40-50% of new product launches fail within their first four years.==== Even in cases where commercialization is ultimately successful, it is typically a slow, uncertain process. Most successful new products experience an initial period of low penetration and slow growth followed eventually by a sharp sales increase or “takeoff” to its market share as a mature product (Agarwal and Bayus, 2002; Golder and Tellis, 2004). Furthermore, “the time to sales takeoff can vary considerably across product innovations; some quickly achieve sales takeoff after commercialization, whereas others languish for years with low sales” (Agarwal and Bayus, 2002).====To be sure, firms expend substantial resources to build and maintain market share. Marketing expenditures as a whole are estimated to comprise as much as 8% of GDP, with advertising alone accounting for over 2% (Gourio and Rudanko, 2014; Cavenaile and Roldan-Blanco, 2021). Especially when launching a new product, advertising is often necessary to establish product awareness and inform consumers of a new product's advantages (Goeree, 2008; Eliaz and Spiegler, 2011). New product advertising also plays a fundamental role in persuading hesitant consumers to change their status quo consumption behavior by weakening incumbent brand loyalty and reducing perceived switching costs (Shum, 2004; Gourville, 2006; Bagwell, 2007). As argued by Yohn (2019), “innovation alone may be enough to initiate the adoption life cycle, but marketing remains the bridge necessary to cross the chasm between early adopters to the wider group of people who will form a viable, valuable customer base.”====In this paper, we develop a novel theoretical framework to examine the dynamic interaction between product innovation, commercialization, and economic growth. As in standard Schumpeterian quality ladder models, entrepreneurial firms invest in R&D to innovate higher quality products across a fixed (measure one) set of industries and new innovations arrive according to a stochastic Poisson process. However, we introduce an endogenous commercialization process that takes place in two phases. In the first phase, successful innovators instantaneously capture a small share of the market comprised of consumers who immediately recognize the superiority of innovative products. Borrowing marketing terminology, we refer to this subset of consumers as ====. In the second phase, innovators must invest in costly advertising in order to convince the remaining ==== to recognize their innovative product's quality advantage.====To capture the uncertain nature of sales takeoff, we model this diffusion of innovations into the mainstream market as a stochastic Poisson process whose arrival rate depends upon advertising intensity. We consider two distinct formulations for the relationship between advertising and the probability of diffusion. First, we assume advertising is purely ====; only young innovators invest in advertising to expand their market share by communicating their product's advantages to potential consumers. Second, we allow for advertising to be ====; incumbent firms also invest in defensive advertising to protect their existing market share against new entrants. In this formulation, the rate of new product diffusion depends upon the advertising contest between young and old firms endogenously battling for consumers through advertising expenditure.====In our framework, firms endogenously cycle through distinct life stages of stochastic length as new innovations arrive and either commercialize successfully or fail. Each new innovator begins life as a young technology leader, serves only early adopters, and invests in advertising to increase its chances of diffusing its product into the mainstream. If a subsequent innovation arrives in the industry before a young firm captures the mainstream market, its product fails. If the young firm instead successfully diffuses into the mainstream prior to the arrival of the next competing innovation, it fully replaces the existing incumbent and begins its tenure as an adult technological leader. Once the next innovation occurs, the now incumbent adult firm transitions to its final stage where it is no longer the technological leader, and early adopters abandon the old product for the newest iteration. However, these old firms still retain a sizable market share until they are fully displaced by the next young firm that diffuses its product successfully. In our combative advertising formulation, old firms also endogenously invest in defensive advertising to protect their market share and prolong their final stage of life.====As in traditional models, economic growth is driven by the incorporation of higher quality products into households' consumption bundles. This implies that the growth rate depends positively on both the rate of innovation and the rate of product diffusion since only innovations that commercialize successfully are adopted by mainstream consumers. We show that this relationship provides novel insights into the role of R&D subsidies in promoting economic growth. Unlike the traditional models in which R&D subsidies ==== promote growth, we find that R&D subsidies can have a non-monotonic effect on growth. This is because young firms' incentives to invest in advertising in order to diffuse their product depend upon the expected length of their reign of market dominance as an adult firm. The more frequently new innovations arrive, the faster technology leaders transition to their old firm stage, and the smaller the incentive to invest in advertising. Thus, although the traditional growth-promoting effect of R&D subsidies of stimulating innovation is present in our model, we identify a novel, competing growth-reducing effect as a smaller proportion of innovations commercialize successfully. We show that this fundamental relationship holds in both the informative and combative advertising versions of the model.====Using numerical simulations, we find that R&D subsidies exhibit an inverted U-shaped relationship with both economic growth and welfare. In our benchmark case of a 25% product failure rate in the initial equilibrium, an R&D subsidy rate of 15.5% maximizes growth and a subsidy rate of 9.2% maximizes welfare. Furthermore, as the initial product failure rate increases (i.e. less frequent successful diffusion in the baseline equilibrium), the case for R&D subsidies becomes weaker. Specifically, the scope for using R&D subsidies to improve welfare diminishes with the failure rate because the social benefit of stimulating innovation is lower when a smaller proportion of new innovations succeed. Indeed, we find that the optimal R&D policy shifts to a tax when the initial failure rate is high but still within an empirically plausible range. Hence, our results suggest that standard endogenous growth models that assume instantaneous innovation diffusion may overstate the case for large R&D subsidies.====Finally, we use the model to examine the economic impact of advertising policy. When advertising is informative, advertising subsidies lead to faster product diffusion, which increases economic growth and welfare. However, when advertising is combative, advertising subsidies also stimulate defensive advertising by existing incumbents. Since advertising is characterized by reciprocal cancellation in this formulation, the primary effect is a socially wasteful increase in the resources devoted to advertising, without the dynamic benefit of a substantial increase in product diffusion. In this case, economic growth and welfare both fall when advertising is subsidized. Thus, our analysis suggests that the welfare impact of advertising policy depends critically on whether advertising is informative or combative; a finding that echoes conclusions from the long-standing literature on the economic effects of advertising (Butters, 1977; Dixit and Norman, 1978; Stegeman, 1991; Grossman and Shapiro, 1984; Dinlersoz and Yorukoglu, 2012; Cavenaile et al., 2021). We show that this pattern of results remains present in a fully dynamic setting where advertising and R&D investment decisions interact and together influence the process of economic growth.","Product innovation, diffusion and endogenous growth",https://www.sciencedirect.com/science/article/pii/S1094202522000266,10 May 2022,2022,Research Article,22.0
"Rothert Jacek,Short Jacob","U.S. Naval Academy, United States of America,FAME | GRAPE, Poland,Bank of Canada,, Canada","Received 11 May 2020, Revised 29 March 2022, Available online 28 April 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.04.003,Cited by (0),"The canonical one-sector model over predicts ==== by a factor of ten. We show that introducing a non-traded goods sector can reconcile the differences between the theoretical predictions and the observed flows. We analyze the quantitative impact of the non-traded sector using a calibrated model of a ====, in which non-traded goods are used in consumption and investment, and need capital and labor to be produced. The model features international frictions directly affecting international borrowing and lending, as well as domestic frictions that limit the scope of inter-sectoral reallocation of capital and labor. We find that: (1) the impact of domestic frictions on the size of ==== is similar to the impact of international frictions, and (2) the median elasticity of capital flows with respect to international frictions in the two-sector model with costly inter-sectoral reallocation is about 50-60% lower than that same elasticity in the one-sector model.","The international flows of capital have long been a focal point of open economy macroeconomics. However, the canonical one-sector neoclassical growth model, often used to study growth and dynamics of a closed economy, when applied to open economies produced several irregularities with observations on international capital flows in the post war period. Contrary to the one-sector model's predictions, domestic savings and investment rates in the data are highly correlated (Feldstein and Horioka, 1980), capital has not flown to countries where it is scarce (Lucas, 1990), and the correlation between total net capital inflows and productivity growth has been negative (Gourinchas and Jeanne, 2013).==== Finally, the overall size of net capital flows in the data is 10 times smaller than the one-sector model predicts (Gourinchas and Jeanne (2013), Rothert (2016), and Fig. 1 on page 4).====The first three puzzles have been extensively studied in the literature. Our focus in this paper is on the fourth one — the discrepancy between the size of net flows observed in the data and the size predicted by the one-sector growth model. The very nature of the discrepancy is quantitative and as such is our paper. More specifically, we want to measure the relative importance of domestic vs. international frictions in accounting for the size discrepancy. In order to do so, we add two features into the standard neo-classical one-sector model. First, we add a non-traded sector with frictions to inter-sectoral reallocation of capital and labor. Second, we add a distortion into the international borrowing and lending, in the form of a debt price wedge, similar to Gourinchas and Jeanne (2013), but only operating in the international markets. We calibrate the country-specific parameters of the model using a sample of 54 developing and developed economies so that the model captures the size of capital flows exactly. We then remove the domestic and international distortions, and evaluate how their absence impacts the predicted capital flows.====We find that domestic reallocation frictions matter a lot. While international frictions are essential in accounting for a low (or negative) correlation between capital inflows and the real gross domestic product (GDP) growth, the domestic reallocation frictions have a very large impact on the magnitude of net flows. First, for a median country in our sample, the removal of domestic reallocation frictions has almost the same impact on that magnitude (measured as the squared error between the data and the model generated long-run net inflows over the period of 30+ years) as does the removal of international frictions. Second, removing international frictions in the two-sector model with costly inter-sectoral reallocation has a much smaller impact on capital flows than does the same experiment in the one-sector model - the median squared error is forty percent lower. Effectively, the elasticity of capital flows with respect to financial frictions is much smaller in the two-sector model with domestic frictions (about one third of that same elasticity in the one-sector model). We find that the degree of complementarity between traded and non-traded goods in consumption is the key parameter affecting that elasticity. However, even with fairly substitutable tradables and non-tradables (when the elasticity of substitution between them equals two), capital flows in the two-sector model are still only half as sensitive to international frictions as they are in the one-sector model.====The intuition behind our results relies on two empirical features of the non-traded sector that we include in the model. First, some goods that are used for either consumption or investment purposes are non-tradable and cannot be imported. Second, the reallocation of both capital and labor between different sectors takes time. The first feature is quite apparent - certain services must be consumed domestically (a hotel stay) and construction accounts for a large part of investment expenditures.==== The second feature is an empirical phenomenon that we document in this paper.====How do these two features combined affect the size of capital flows? In the one-sector model, there are two main forces that generate large capital inflows to a fast growing economy: consumption smoothing motive and rising marginal product of capital. In the two-sector model these forces may not be as strong. With under-developed non-traded sector, the marginal utility from tradables is smaller, and hence the incentive to import them is smaller. Similarly, the return from buying tradable capital goods is not as large, if investment also requires a substantial non-tradable component. However, factor mobility implies that a fast-growing economy could simply put all the resources into a non-tradable sector and import everything else. Reallocation frictions prevent that from happening, effectively reducing the incentive to finance higher consumption and investment expenditures via borrowing. Our main contribution is the quantitative evaluation of this channel, and we find that for many countries it is just as important as international frictions in shaping capital flows.","Non-traded goods, factor market frictions, and international capital flows",https://www.sciencedirect.com/science/article/pii/S1094202522000254,28 April 2022,2022,Research Article,23.0
"Chatterjee Satyajit,Eyigungor Burcu","Federal Reserve Bank of Philadelphia, United States of America","Received 1 February 2021, Revised 19 April 2022, Available online 22 April 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.04.002,Cited by (0),"Larger firms (by sales or employment) have higher leverage. This pattern is explained using a model in which firms produce multiple varieties, acquire new varieties from their inventors, and borrow against the future cash flow of the firm with the option to default. A variety can die with a constant probability, implying that firms with more varieties (bigger firms) have a lower variance of sales growth and, in equilibrium, higher leverage. In this setup, a drop in the risk-free rate increases the value of an acquisition more for bigger firms because of their higher leverage: they can (and do) borrow a larger fraction of their future cash flow. The drop causes existing firms to buy more of the new varieties arriving into the economy, resulting in a lower startup rate and greater concentration of sales.","Across a range of advanced economies, firm leverage is increasing in firm size. Rajan and Zingales (1995) documented this positive relationship for publicly traded firms for several OECD countries, including the U.S. Recently, Dinlersoz et al. (2019) have shown that the positive relationship between leverage and firm size also extends to private U.S. firms. Extant models of firm leverage, however, do not explain this fact. In macroeconomics, the most well-known model of firm leverage, Cooley and Quadrini (2001), predicts a negative relationship between size and leverage.==== In finance, canonical models of firm capital structure (Leland (1994), Leland and Toft (1996)) are solved under assumptions that imply that optimal leverage is constant and independent of firm size.====The main goal of this paper is to explain the positive relationship between leverage and firm size when growth of existing firms and the entry of new ones is endogenous. To endogenize firm growth and firm entry, we assume a steady arrival of ideas for new product varieties. Each new idea is initially owned by someone and the owner can either sell the idea to an existing firm, leading to the growth in output of that firm, or use the idea in a startup, leading to the entry of a new firm.====To get a positive relationship between firm size and leverage, we assume that existing products can go extinct with a constant probability. This implies that a firm that manages more varieties has a less volatile growth rate. Firms can borrow with the option to default on their debts. Since output growth of larger firms is less volatile, their probability of default on any given level of debt is lower and, under certain conditions, this implies that larger firms will choose to be more leveraged.====A striking feature of our explanation of the leverage and firm-size relationship is its implications for how interest rates affect the startup rate and business concentration. We show, both analytically (in a stripped-down version of the model) and numerically, that in our model, a decrease in the risk-free rate increases the ==== from a firm's acquisition of a new variety, and does so ==== for larger firms because they are more leveraged. Thus, a decline in the risk-free rate results in more new varieties being bought by larger firms, which leads simultaneously to a decline in the startup rate and to a rise in business concentration (share of sales accounted for by larger firms). Since the risk-free rate has in fact been declining for at least two decades, our model raises the possibility that the concomitant rise in business concentration and declines in startup rates — phenomena that have received a great deal of separate attention — may have a common cause in falling interest rates.====Our model is deliberately bare bones, but its key elements are grounded in well-established facts. The model relies on the volatility of output or employment growth being lower for larger firms, for which there is very good evidence. Stanley et al. (1996, Figure 2, p. 805) document that among publicly traded manufacturing firms that survive from one period to the next (so exit is ignored), the standard deviation of the growth rate of sales falls with firm size (see also Buldyrev et al. (2020, Figure 2.8, p. 21)). In section 5.3.2 we use the Census Bureau's Business Dynamics Statistics database to show that volatility of employment growth is declining in firm size. Importantly, our measure of employment growth volatility takes exit into account. Additionally, Davis et al. (2007, Figure 12, p. 41) document that a closely-related measure of revenue growth volatility (also based on the BDS) declines with firm size.====The fact that larger businesses have less volatile growth rates does not automatically imply that they will have higher leverage. For this, it has to be easier for larger firms to borrow. We model this in two ways. In the main text, we assume that when firms borrow, they must obey a default probability constraint. The motivation for this constraint is the well-established fact that risky borrowers are simply denied credit.==== In Appendix B, we explore a case where there is no constraint on default probability but default imposes a fixed cost on creditors. Since volatility of the growth rate of cash flow falls with firm size, either approach generates a positive association between leverage and firm size.====Our model embeds a theory of firm entry that recognizes that ideas for new products occur to ==== and they get to choose the organizational form in which to implement them (Chatterjee and Rossi-Hansberg (2012), Zábojník (2019)).==== The theory applies when a person is contemplating setting up a pizza store and chooses between proceeding independently or as a franchise of a nationally-known brand. Similarly, the founders of spinoffs — new ventures that originate out of an existing one and are in the same line of business as the parent — often make a such a choice.==== The other side of the coin, of course, are the many instances in which employees with new ideas implement their ideas in the firm in which they work. One indication that this occurs is the common practice of patent assignment, which transfers patent (or patent application) rights from inventor-employees to their employers. Another indication is the fact that, as assumed in this paper, individuals with valuable knowledge/ideas are compensated in the form of equity claims to future cash flows (Eisfeldt et al. (2021)).====Our model is abstract in that firm growth is modeled as occurring via the addition of new product lines, rather than through productivity growth of existing product lines.==== While it is clear that stability of sales growth improves with size, not much is known about why this is the case. But large firms are generally viewed as having more diversified sources of revenue and “growth by new product lines” is a simple way to model this view. In addition, this approach establishes a bridge to theories of endogenous growth (Romer (1990), Grossman and Helpman (1991), Aghion and Howitt (1992)) in which growth occurs via the arrival and absorption of new goods (varieties). While we don't model the production of new ideas, we consider a different aspect: Once the idea for a new good comes about, is it implemented in a startup or in an existing business and how is this choice affected by the risk-free rate? The choice has implications for the distribution of output across ==== (business concentration), an important aspect of growth.====Turning to the decline in the risk-free rate, the view that emerges from the many studies that have examined its possible causes (Caballero et al. (2008), Mendoza et al. (2009), Eichengreen (2015), Del Negro et al. (2017), Farhi and Gourio (2018), among others) is that it has resulted largely from a rise in the premium placed on safety and liquidity. Consistent with this, our model treats the decline in the risk-free rate as occurring due to a change in the preferences of lenders. Since we assume that lenders are risk-neutral, the decline is modeled simply as a decline in their degree of impatience.==== Regarding consequences of low interest rates, Gopinath et al. (2017) argue that, due to financial frictions, the decline in interest rates led to an increase in the misallocation of capital and lower productivity in South Europe; Caggese and Perez-Orive (2019) argue that low interest rates put firms with intangible capital at a disadvantage; Liu et al. (2022) explore the role of low interest rates in generating greater business concentration through a “strategic competition effect,” while Kroen et al. (2021) present causal evidence that declines in interest rates benefit large firms, which increase leverage and also conduct more cash acquisitions — findings that complement the mechanisms emphasized in our paper. However, unlike ours, these studies do not connect low interest rates to lower entry rates.====The causes of the decline in the startup rate — often described as a “decline in business dynamism” — and the rise in business concentration are active areas of research. Regarding the decline in the startup rate, Hathaway and Litan (2014) list several factors, including slowing population growth, increasing business consolidation, and the rising burden of regulation and taxes as potential causes. The role of slowing of labor force growth has been stressed in Karahan et al. (2019) and in Hopenhayn et al. (2018), and that of changes in corporate tax rates in Neira and Singhania (2017). Studies that attempt to explain the rise in business concentration have examined increases in market power (De Loecker and Eeckhout (2020)) and the entry of large firms into new geographic markets (Hsieh and Rossi-Hansberg (2021), Rossi-Hansberg et al. (2020)).====In recent work, Aghion et al. (2019) and Akcigit and Ates (2019) explore, like us, a common cause for the decline in entry rates and the rise in concentration. The former focus on technological change that is increasingly benefiting larger firms and the latter on a decline in knowledge diffusion from leading to lagging firms. Although the conceptual frameworks of these studies are quite different from ours, they all share the key commonality that new ideas/products are increasingly appearing within larger firms.====The paper is organized as follows. Section 2 briefly documents the leverage-firm-size relationship that motivates the paper. Section 3 lays out our model of firm dynamics with borrowing, default, entry and exit. In Section 4, a simple stripped-down version of this model is analyzed to explain the key idea of this paper: the market value of debt can affect the entry rate of new firms and the growth rate of existing firms. Section 5 analyzes the full model quantitatively and establishes that the key results derived in the stripped-down model carry over. This section also assesses the degree to which the quantitative model is in consonance with untargeted facts. Section 6 analyzes the impact of a decline in the risk-free rate on firm dynamics and shows that the model is capable of accounting for most of the decline in the entry rate since the late 1990s and also sheds some light on related trends over this period. Section 7 concludes. Appendixes A and B contain additional materials on facts and theory. In particular, Appendix B describes a variant of the model in which creditors incur a fixed default cost in the event of bankruptcy and shows that this alternative setup has properties similar to the main model. Finally, the Online Appendixes C and D contain additional theoretical results.",The firm size-leverage relationship and its implications for entry and business concentration,https://www.sciencedirect.com/science/article/pii/S1094202522000242,22 April 2022,2022,Research Article,24.0
"Han Zhao,Ma Xiaohan,Mao Ruoyun","Department of Economics, William & Mary, United States of America,Department of Economics, Texas Tech University, United States of America,Department of Economics, Grinnell College, United States of America","Received 29 September 2020, Revised 11 April 2022, Available online 14 April 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.04.001,Cited by (0),We solve a rational expectations model of price formation with nominal rigidity and information frictions ,"Understanding inflation dynamics is at the core of monetary economics. A useful vehicle that plays an essential role in delivering inflation dynamics is the New Keynesian Phillips Curve (NKPC) of rational expectations. It is often coupled with a strong assumption that expectations are formed under full information. Despite its broad applicability in academic analysis and policy guidance, the full information rational expectations NKPC has a hard time explaining several stylized facts, including (i) highly persistent inflation, i.e., inflation inertia, (ii) heterogeneous inflation expectations, and (iii) systematic predictability of forecast errors (Coibion and Gorodnichenko, 2015).====The literature has proposed several modified Phillips curves to bypass inflation inertia, including the hybrid NKPC by Gali and Gertler (1999), and the NKPC with indexation by Christiano et al. (2005). However, these alternatives have often been criticized for the lack of micro-foundations. They also ignore patterns of surveyed inflation expectations. Other variants of Phillips curves, who aim to take advantage of available survey data, often need to relax the disciplines of rational expectations (see Adam and Padula (2011) and Fuhrer (2017)).====A promising candidate that can reconcile the stylized facts under rational expectations is the dispersed information NKPC, which embeds a type of imperfect information à la Woodford (2001). Built upon the supply side's price-setting problem, it explicitly models individual firms' expectation formation. Information is dispersed because firms cannot observe the economy-wide marginal cost shocks and only receive signals with idiosyncratic noises. Nimark (2008) utilizes the dispersed information NKPC to explain inflation inertia and bridge the gap between macro and micro-level pricing discrepancies. The last decade has witnessed a growing number of applications of the dispersed information NKPC (see Berkelmans (2011) and Melosi (2017)).====However, the original form of the dispersed information NKPC requires researchers to deal with infinite higher-order expectations (HOEs) explicitly. HOEs can arise under dispersed information as one agent's action depends on other agents' actions and beliefs.==== Angeletos and La'O (2009) emphasizes the degree of price inertia depends on data on the dynamics of HOEs. General equilibrium channels further complicate how HOEs affect aggregate outcomes (see Angeletos and Lian (2016)). In general, there is no direct measurement of HOEs in data.==== In the current context, the complexity of HOEs hinders understanding the role of dispersed information in inflation and inflation expectations.====This paper tackles this problem by considering the individual firm's price-setting problem without introducing HOEs in the first place. The insight is that firms do not need to formulate HOEs at the micro-level. HOEs only arise due to the aggregation of the NKPC. We follow Nimark (2008) and introduce an idiosyncratic component to each firm's marginal cost. Firms use the histories of individual marginal costs and the lagged aggregate price levels to form expectations and choose the optimal price. We impose a non-invertibility condition on the inflation process to prevent past prices from revealing too much information to firms. Since individual firms cannot distinguish idiosyncratic marginal cost shocks from aggregate shocks perfectly, they adjust prices gradually compared with the full information case, resulting in inflation inertia and predictable average nowcast and forecast errors. We utilize the frequency domain techniques developed in Kasa et al. (2013) and Rondina and Walker (2017) and derive analytical solutions of both individual and aggregate price levels. The frequency-domain method allows us to consider a wide variety of marginal cost processes without imposing unnecessary structural assumptions ====.====The closed-form solution sheds new insights into the structure of inflation dynamics and is amenable to empirical tests. Compared with the full information case, the effect of dispersed information on inflation can be summarized by an ARMA(1,1) term (i.e., an information wedge) that exhibits additional persistence. We derive an ARMA(1,1) nowcast error and an ARMA(1,2) forecast error of inflation under dispersed information. In contrast, the full information rational expectations NKPC implies an identically zero nowcast error and an ==== forecast error. It is precisely the AR and MA structures, which are byproducts of dispersed information, that yield the systematic predictability of nowcast and forecast errors. When applying to the Survey of Professional Forecasters (SPF) data, we find all AR and MA coefficients are significant at the 1% significance level. Furthermore, our model-implied nowcast and forecast errors outperform alternative ARMA(p,q) specifications in terms of both the Akaike and the Bayesian information criteria (i.e., AIC and BIC).====The analytical solution provides a convenient way of reverse-engineering the NKPC. We derive our version of the dispersed information NKPC, which cleanly separates the average inflation forecast (i.e., a first-order expectation) from the net effect of HOEs. The ==== effect of HOEs is a single term that summarizes the effects of all higher-order expectations on the ====.==== We then utilize the cross-equation restrictions of nowcast and forecast errors to perform a likelihood-based Bayesian estimation and quantify the net effect of HOEs on NKPC. Interestingly, while HOEs are indispensable parts of dispersed information, their net effect on the Phillips curve is empirically small. This result is consistent with a series of studies (see Roberts (1997), Adam and Padula (2011), Fuhrer (2017), and Coibion et al. (2018a)), which documents the good empirical performance of the NKPC that only involves the average survey expectation. While the result provides a rationale for the type of NKPC used by Adam and Padula (2011), the small net effect of HOEs does not suggest dispersed information is quantitatively unimportant for explaining inflation dynamics in the US. Quite the contrary, the estimated effect from dispersed information (i.e., the information wedge) is large and explains a significant inflation variation.====This paper belongs to a broad literature that considers how information frictions shape the Phillips curve (Lucas (1972), Woodford (2001), Mankiw and Reis (2002), Nimark (2008), Angeletos and Huo (2021), and Huo and Pedroni (2020)). Our contribution is twofold. (i). Theoretically, we solve the firm's price-setting problem analytically and can dissect the effect of dispersed information. While the dispersed information NKPC is the focal point of several macroeconomic papers (e.g., Nimark (2008) and Melosi (2017)), these papers have used general equilibrium models that also impose additional structures on household and policy behaviors. Therefore, it is meaningful to study the micro-founded firm's pricing problem in isolation without imposing the cross-equation restrictions implied by a particular aggregate demand scheme. The missing deflation in the Great Recession and the missing inflation after 2010 raised concerns about the inverse relationship between unemployment and inflation. Our approach allows us to consider a general marginal cost process without linking it tightly to the unemployment rate or the output gap. (ii). Empirically, we maintain the same spirit of not specifying a restrictive marginal cost process. Our model-implied marginal cost series display significant high-frequency variations, and the correlations between the implied marginal cost and the candidate measures are small.====This paper also connects to the strand of literature that focuses on the role of HOEs (Allen et al., 2006; Angeletos and La'O, 2009; Banerjee et al., 2009; Kasa et al., 2013). Much of the literature emphasizes the dynamics of ====-th order HOEs and their consequences on aggregate variables. Allen et al. (2006) studies the role of HOEs in asset prices and finds that due to HOEs, prices are overly sensitive to public information, and traders underweight their private information. Angeletos and La'O (2009) shows that the dynamics of HOEs are critical to quantify the degree of price inertia. Our focus is not on any particular HOEs ====. Instead, we focus on the ==== effect from dispersed information on inflation and the ==== effect of HOEs on NKPC. Our approach is most similar to Bacchetta and Van Wincoop (2008), which considers a classic portfolio selection problem and shows that HOEs can drive a “wedge” between the asset price and the average expectation of the asset's fundamental value. We characterize the effect from dispersed information as an information wedge and show that it explains a large variation of inflation quantitatively. The net effect of HOEs on NKPC provides a novel micro-founded interpretation of the markup shocks (see Smets and Wouters (2007)) commonly used in the DSGE literature. Firms rationally choose disparate price-cost markup ratios because they have different opinions on the aggregate marginal cost shocks.====The paper proceeds as follows. Section 2 provides analytical solutions to the full information and the dispersed information models after describing the firm's price-setting problem. We highlight the different structures of nowcast and forecast errors and establish an NKPC that separates the average inflation forecast (i.e., a first-order expectation) from the net effect of HOEs. Section 3 distinguishes the professional forecaster's expectation formation from the firm's signal extraction and tests the analytical results using the SPF data. We first demonstrate the good empirical performance of our model-implied nowcast and forecast errors. We then draw inferences on the effect from dispersed information, the net effect of HOEs on NKPC, and the unobserved aggregate marginal cost process. We take stock in Section 4.",The role of dispersed information in inflation and inflation expectations,https://www.sciencedirect.com/science/article/pii/S1094202522000230,14 April 2022,2022,Research Article,25.0
"Brumm Johannes,Grill Michael,Kubler Felix,Schmedders Karl","Karlsruhe Institute of Technology, Germany,European Central Bank (ECB), Germany,University of Zurich and Swiss Finance Institute, Switzerland,IMD Lausanne, Switzerland","Received 22 November 2020, Revised 4 February 2022, Available online 4 April 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.03.003,Cited by (1),"We assess the implications of collateral re-use on leverage, volatility, and welfare within a calibrated infinite-horizon asset-pricing model with heterogeneous agents and disaster shocks. In our model, the ability of agents to reuse frees up collateral that can be used to back more transactions. Re-use thus contributes to the buildup of leverage and significantly increases volatility in financial markets. When introducing limits on re-use, we find that volatility is strictly decreasing as these limits become tighter, yet the impact on welfare is non-monotone. In the model, allowing for some re-use can improve welfare as it enables agents to share risk more effectively. Allowing re-use beyond intermediate levels, however, can lead to excessive leverage and lower welfare. So the analysis in this paper provides a rationale for limiting, yet not banning, re-use in financial markets.","Re-use and re-hypothecation of collateral have become a major activity in financial markets.==== These activities refer to the practice of financial institutions to reuse collateral received in one transaction for another transaction.==== While market participants have stressed the importance of re-use of collateral as a source of funding and market liquidity more generally, regulators and supervisors have raised various concerns about this market practice. For example, the Financial Stability Board (FSB) published work that analyzes the financial stability implications of collateral re-use, in particular highlighting the contribution of “collateral re-use to the buildup of leverage”; see FSB (2017).==== Several regulatory frameworks already foresee specific rules such as limits and transparency requirements for re-use of (non-cash) collateral. European retail investment funds (UCITs), for example, are banned from reusing non-cash collateral. Moreover, the EU framework for margin requirements for non-centrally cleared derivatives foresees a ban on re-use of non-cash collateral that is posted as an initial margin. Given the importance of collateral re-use in financial markets and the need to inform ongoing regulatory initiatives, it is important to develop a model framework to understand the implications of collateral re-use on financial market outcomes and welfare.====In this paper, we present an asset-pricing framework with heterogeneous agents in which financial securities are only traded if the promised payments associated with selling these securities are backed by collateral. To generate collateralized borrowing in equilibrium, we assume that there are two types of agents who differ in risk aversion and in their beliefs about the likelihood of bad shocks to the economy. The agent with the low risk aversion and optimistic beliefs (agent 1) is the natural buyer of risky assets and takes up leverage to finance these investments. The agent with the high risk aversion and pessimistic beliefs (agent 2) has a strong desire to insure against bad shocks and is thus willing to buy bonds, thereby providing financing to the other agent. We introduce re-use of collateral in this setting by allowing agents who receive securities as collateral to sell these securities to other agents. As a consequence, the security can again be used to collateralize transactions, allowing agents to further build up their leveraged position in the risky security.====We first illustrate the leverage effect of collateral re-use qualitatively in a simple two-period version of our model. In this setting, we also analyze the effects of restricting re-use on collateralized borrowing, leverage, and welfare. Subsequently, we present an assessment of re-use within an infinite-horizon model in which agents have Epstein–Zin utility with identical time discount factors, identical inter-temporal elasticity of substitution (IES) parameters, yet differing risk aversion parameters, agent 1 being less risk averse than agent 2. Moreover, agents (agree to) disagree about the likelihood of disaster shocks. We calibrate these low-probability events based on Barro and Ursúa (2008) and assume that agents' beliefs about their likelihood deviate from the objective probabilities in opposite directions, agent 1 being optimistic and agent 2 being pessimistic.====In the dynamic model, the amount of collateral needed to back a transaction is determined in equilibrium. When the economy is hit by a bad shock, the leveraged agent, agent 1, loses financial wealth. As a result, the collateral constraint forces him to reduce consumption and to sell risky assets to the risk-averse agent. These actions trigger an additional decrease in asset prices, which further reduces the wealth of agent 1—thereby reinforcing the impact of the bad shock on the wealth distribution and on asset prices.====Allowing for re-use of collateral in the economy increases the available collateral in financial markets, thus agents can build up leverage far beyond what is permissible in economies in which re-use is prohibited. These highly leveraged endogenous asset portfolios lead to large movements in the wealth distribution when good or bad shocks hit. For this reason, our calibrated model with unlimited re-use can generate first and second moments of risk-free and risky returns as in US financial market data even for values of the IES below 1 and for risk-aversion parameters far less than 10. It is an important finding in itself that allowing for short sales (through re-use) makes it much easier for a heterogeneous-agent model to match asset-pricing moments, and in particular the high volatility of risky returns. We regard this benchmark calibration with unlimited re-use as reflecting the current financial market environment. It is a reasonable approximation for the general treatment of re-use in real-world regulatory frameworks where constraints exist only for some forms of re-use in some jurisdictions, as we report in Section 2.====In the next step of our analysis we introduce re-use limits to constrain the buildup of leverage in agents' asset portfolios. Compared to the benchmark economy with free re-use, these limits restrict agents' portfolio holdings in equilibrium. In particular, the optimistic and less risk-averse agent in the economy, agent 1, cannot build up as much leverage as in the benchmark economy. Therefore, the impact of negative shocks on this agent's wealth share is greatly reduced. As a consequence, the volatility of the wealth distribution shrinks considerably and so does the volatility of financial asset returns. This reduction in volatility is much stronger than the accompanying decrease in the equity premium. As a result, both the realized average Sharpe ratio in the economy as well as agents' anticipated Sharpe ratios under their subjective beliefs increase strongly in response to limiting re-use.====We complete the analysis of the effects of re-use limits on financial-markets equilibria with an examination of welfare implications. In our welfare analysis, we consider unanticipated changes in regulation and find that intermediate levels of re-use limits are welfare optimal. Compared to very loose or very strict regulation, the welfare of one agent is increased when transfers are chosen such that the other agent's welfare is kept constant; in some cases we observe Pareto improvements even without transfers. The relation between re-use limits and welfare is non-monotone because two counteracting forces are at play: First, the ability to reuse allows for more risk sharing in the economy. This is generally beneficial for welfare given the agents' heterogeneity in risk aversion. Second, the heterogeneity in agents' beliefs triggers agents to build up leveraged positions beyond what is needed to optimally share risks. As the ability to reuse collateral allows agents to build up this leverage, limiting re-use has the potential to steer agents' choices toward what is socially optimal within the model. Note, however, that such welfare implications of the model only capture the impact of re-use regulation on reducing leverage and volatility, yet cannot capture its impact on reducing interconnectedness in the financial system.====In our model, with two assets and two types of agents, the only collateral that can be reused is equity and the only way in which it is reused in equilibrium is for short sales. We acknowledge that collateral re-use in repo contracts plays a significant role in financial markets, with important implications that are substantially different from those of short-selling. Nevertheless, short sales also constitute a significant part of financial intermediaries' re-use activities. Moreover, some of the other re-use activities have effects similar to short sales when it comes to increasing exposure to risk, overall leverage, and volatility of returns. For instance, the collateral asset might be reused as margin in derivative trades, which may generate a negative exposure to the market, as short selling does. More generally, re-use of collateral leads to an increase in the availability of collateral in the financial system, ultimately allowing entities to enter more transactions and more leveraged positions than without re-use. We therefore view our model as a “reduced form” of a more detailed model with a variety of assets and a variety of possibilities for re-use. In Section 2 we discuss the relevance of the various re-use activities and the risks related to re-use of collateral.====To the best of our knowledge, the current paper presents the first attempt to quantitatively assess the implications of collateral re-use for leverage, volatility, and welfare. It does so within an infinite-horizon asset-pricing model with heterogeneous beliefs and disaster shocks. Thus this paper bridges several different strands of literature. An active theoretical and empirical literature explores re-use and re-hypothecation in financial markets. For more than two decades, a continuously growing theoretical and quantitative literature has analyzed the effects of collateral constraints on asset prices and welfare. Furthermore, our paper relates to the literature on the effects of trading restrictions, including collateral constraints, when agents have heterogeneous beliefs. Finally, there is a separate strand of literature that examines the role of collateral constraints for financial intermediaries.====There is a growing literature on the role of re-use and re-hypothecation in financial markets. A large part of this literature concentrates on repo markets, which are not the focus of this paper but which are obviously of related interest. Bottazzi et al. (2012), Bottazzi et al. (2017), Gottardi et al. (2019) and Park and Kahn (2019) present theoretical models of repo markets, which examine existence and optimality properties of these markets. Infante and Vardoulakis (2018) and Eren (2014) present models that consider the funding role of re-use for dealer banks. Eren (2014) shows how re-use may expose a hedge fund to a dealer's default, whereas Infante and Vardoulakis (2018) and Infante (2019) consider how collateral runs may arise due to re-use. However, none of these papers provide a quantitative analysis of the implications of re-use for aggregate financial market outcomes. Furthermore, with the exception of Andolfatto et al. (2017), who discuss limits on re-use in a monetary model and find that limits need to be stricter in economies with lower inflation, none of these papers focuses on the implications of regulating re-use. Singh and Aitken (2010), Singh (2011), Kirk et al. (2014), and Fuhrer et al. (2016) use publicly available data to estimate the amount of collateral re-use in financial markets. Infante et al. (2018) use confidential supervisory data to document how primary dealers use and re-use collateral in the United States. Baklanova et al. (2019) estimate the amount of reuse in repo markets.====Our paper builds on research on the effects of collateralized borrowing on asset prices and asset-market volatility—a literature that has been very active and continues to grow; see the pioneering work of Geanakoplos (1997) and Aiyagari and Gertler (1999) as well as, among many other papers, Coen-Pirani (2005), Fostel and Geanakoplos (2008), Brunnermeier and Pedersen (2009), Garleanu and Pedersen (2011), and Fostel and Geanakoplos (2013). More recently, Rytchkov (2014), Chabakauri (2013), and Chabakauri (2015) analyze the volatility implications of collateral constraints in theoretical studies of continuous-time general equilibrium models with two assets. Rytchkov (2014) shows how endogenous time-varying margin requirements affect equilibrium outcomes. The author finds that margin requirements reduce asset-price volatility and increase the market price of risk. Chabakauri (2013) examines the effects of margin and leverage constraints on volatilities in an economy with two stocks and two CRRA investors with heterogeneous risk aversions but homogeneous beliefs. The model also implies that tighter constraints reduce stock return volatility. Brumm et al. (2015a) show in an infinite-horizon general equilibrium model with heterogeneous agents that borrowing against collateral substantially increases the return volatility of long-lived assets. Otherwise identical assets with different degrees of collateralizability exhibit substantially different return dynamics, because their prices contain a sizable collateral premium that varies over time. Brumm et al. (2015b) explain why adjustments in stock market margins under US Regulation T had an economically insignificant impact on market volatility—raising the margin requirement for one asset class may barely affect its volatility if investors have access to another, unregulated class of collateralizable assets.====Our paper also adds to the literature on exploring the effects of collateral constraints when agents have heterogeneous beliefs. Simsek (2013) theoretically analyzes the effect of belief disagreements on asset prices and collateralized borrowing in a two-period model. Optimistic agents borrow by selling collateralized contracts to lenders who do not share the same beliefs, which leads to an endogenous borrowing constraint. The tightness of this constraint and how it affects borrowers and lenders depends on the degree of belief disagreements. Chabakauri (2015) extends the model in Chabakauri (2013) to one with heterogeneous beliefs and shows the strong effects of general trading constraints. Both borrowing and short-sale constraints again decrease the stock return volatility and generate rich non-monotone patterns in equilibrium processes. Chabakauri and Han (2020) presents a model with two CRRA investors who disagree on the probability of an economic crisis. The qualitative, theoretical analysis shows that collateral constraints dampen volatilities in bad times, when the aggregate consumption is low, and amplify them in good times, when the aggregate consumption is high. Unlike the theoretical analyses with closed-form solutions in the cited papers with heterogeneous beliefs, we need to revert to numerical solutions. The inclusion of re-use in a discrete-time infinite-horizon model does not allow for analytical solutions. In accordance with this literature, we find that tighter re-use constraints lead to a reduction of asset volatility.====For our welfare analysis of re-use limits, we cannot employ classical Pareto optimality as a welfare criterion since agents have heterogeneous beliefs. Blume et al. (2018) provide an instructive discussion on welfare economics with heterogeneous beliefs. In our analysis in the present paper, we employ the concept of belief-neutral Pareto optimality introduced by Brunnermeier et al. (2014).====The remainder of this paper is organized as follows. Section 2 provides an overview of industry practices for collateral re-use. Section 3 presents a simple two-period model in which we can observe some of the qualitative features of re-use and its regulation. In Section 4 we describe our infinite-horizon model and its benchmark calibration. Section 5 presents numerical results for the impact of re-use on leverage and volatility in the benchmark economy and a discussion of the economic mechanism. In Section 6 we examine the welfare implications of re-use limits. Section 7 concludes. The Appendix contains additional results.","Re-use of collateral: Leverage, volatility, and welfare",https://www.sciencedirect.com/science/article/pii/S1094202522000205,4 April 2022,2022,Research Article,27.0
Stangebye Zachary R.,"Department of Economics, University of Notre Dame, 3015 Nanovic Hall, Notre Dame, IN 46556, United States","Received 16 May 2018, Revised 2 March 2022, Available online 31 March 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.03.002,Cited by (0),"A model of long-term sovereign debt with a fiscal rule and endogenous default is explored in which debt maturity governs the number of risky steady states. Generally, there is a good steady state with high investment and low indebtedness and default frequencies, and a bad steady state with the reverse. The multiplicity arises when maturity tames an aggressive feedback loop that emerges in the sovereign budget set between debt service and required issuance as expected default frequencies rise. This feedback loop delivers a unique risky steady state at shorter maturities. Local dynamics and policy consequences are explored quantitatively.","Markets for sovereign debt are fertile ground for sentiments. During many crisis episodes, and particularly during the recent crisis in the Peripheral Eurozone, many have spoken seriously if abstractly about the role beliefs play in generating or perpetuating crises. Summers (2000), Lane (2012), De Grauwe and Ji (2013), and Wolf (2014) are just a few examples.====Most discussion of belief-driven crises centers around liquidity issues or debt of relatively short maturity (Cole and Kehoe (1996), Rodrik and Velasco (1999), Bocola and Dovis (2019), or Aguiar et al. (2022)). More recent work (Lorenzoni and Werning (2019), Stangebye (2020), or Aguiar and Amador (2020)) has explored the unique role played by long-term debt in belief-driven crises in endowment economies. This paper expands these discussions by quantitatively exploring a new type of belief-driven crisis that can arise in economies with long-maturity debt.====In particular, I construct a model of long-maturity sovereign debt that features a fiscal rule for the primary surplus but endogenous default. If the sovereign chooses to default, the country faces exogenous default costs in line with the literature (Aguiar and Gopinath (2006), Arellano (2008), or Gordon and Guerron-Quintana (2018)). The principal finding is that there are generally two stable steady states with strictly positive default risk. However, as we shorten the maturity one eventually disappears, leaving us with a unique “risky” steady state for shorter maturity bonds.====The result follows from the role maturity plays in the sovereign budget set and how it interacts with default expectations. To see this, consider an economy that expects an i.i.d. probability of default, ====, that persists forever. The general strategy will be to explore conditions under which such expectations are justified, i.e., when realized default probabilities match the expected ones.====Consider what happens to the sovereign budget set as ==== increases from a very low value. The first impact of this change will be a drop in the debt price. This will require additional debt issuance to meet fixed revenue goals. In steady state, this additional debt issuance has a secondary impact: It will imply greater debt service requirements, which will imply both greater primary surpluses and even more debt issuance. As ==== increases, the debt levels eventually make the primary surplus unpalatable high, which implies that default becomes more tempting and is realized more often. Thus, ==== default risk is increasing in ==== default risk.====The curvature of this relationship is important and depends on the debt maturity. In the short-term case, this feedback effect from debt issuance to debt service and primary surpluses is so aggressive that realized default probabilities are quite steep in expected default probabilities, implying a unique fixed point, i.e., steady state. But maturity tames this feedback loop, eventually flattening out the response of realized default probabilities to expected ones and delivering an additional fixed point. We will refer to these two fixed points as ====.====There are two ways in which maturity does this. First, the marginal impact of ==== on the price falls with maturity. An increase in the frequency of default naturally depresses prices. But this effect is mitigated as ==== grows larger because default in distant periods is less likely to be realized since a default is already likely in earlier periods. This implies that the steady state debt price, ====, is both decreasing and ==== in ====. Consequently, the marginal impact of higher ===='s on debt prices, and thus the budget set, is smaller than the marginal impact of lower ones. This tames the translation from expected default to greater required debt issuance through lower prices as the economy gets riskier.====The second way maturity tames the feedback loop is by reducing the impact of debt prices and service on the sovereign budget set. Since only a fraction of debt needs to be rolled over in the long-term case, a diminished bond price, which comes from greater default expectations, has a smaller impact on the sovereign budget set. This is easy to see in the limiting case of a perpetuity, in which the sovereign can maintain steady state levels paying zero costs regardless of the price. Not only does maturity reduce the need to issue new debt in response to price drops, but it also reduces debt service per unit of face value, which stymies the expansion of revenue needs as debt levels rise.====The ability of long-term debt to reduce budget stress by attenuating the impact of negative future expectations on current consumption/primary surpluses in this way is well-known (see Chatterjee and Eyigungor (2012)). What is interesting and novel here is that this same basic force can actually make the country ==== for some beliefs by allowing for a stable, higher-debt, higher-default steady state that would not exist in the absence of these attenuating forces, i.e., in the case of short-maturity debt. Thus, while elongating the maturity of debt can be a blessing, it can also be a curse. While it works to eliminate rollover risk, it can also introduce new forms of risk to the country, namely the new, worse steady state, ====.====To enflesh this skeletal intuition and assess the quantitative implications of the model, I incorporate production, an endogenous labor supply, capital accumulation with realistic frictions, and a distinction between private and public consumption. The multiplicity of interest survives all of these embellishments and carries with it the additional feature that the worse steady state implies lower output since investment is lower. I calibrate the model's risk-free moments to Italy during the 2000s. I then estimate the degree of fundamental volatility in the economy by ensuring that ==== equilibrium exists in which annual default risk is 3.0%, which is the empirical average computed by Tomz and Wright (2013) conditional on defaulting at least once.====It turns out that only the ==== steady state can accommodate a 3.0% default rate, which implies that there is a worse steady state, ====, that is even riskier. In this case, the bad steady state yields an annual default risk of 22.1%. Both admit a continuum of stable local trajectories, which I refer to as local Markov equilibria. Viewed through the lens of Lubik and Schorfeide (2004) among others, this suggests that short-run belief shocks could also influence local dynamics even while limiting beliefs are fixed.====The indeterminacy in each of these risky steady states is driven by the ====: If the expected convergence rate from above to the high-debt steady state is fast, then default risk is less of an issue in the medium term and prices are higher. These higher prices facilitate faster convergence, thus justifying expectations. The opposite is true if a slow convergence rate is expected.====To identify the potential influence of beliefs of this nature during the Italian debt crisis, I add fundamental shocks to the model and estimate their processes in the context of the linearized model on Italian data from 2010-2012. I find that the ==== equilibrium implies the highest likelihood at its estimated parameters, so I use it to identify the sequence of fundamental shocks. Feeding these shocks into both the risk-free and ==== counterfactual equilibria reveals that, conditional on having a belief shift, Italy seems to have gotten ‘lucky.’ The implied risk-free counterfactual trajectory is only mildly better than Italy's actual experience. However, had beliefs shifted to ==== rather than ====, output levels would have been substantially lower and default risk and bond yields substantially higher.====The policy consequences of the model are twofold. First, it suggests that, in contrast to the related model of Lorenzoni and Werning (2019), one cannot rid the model of equilibrium multiplicity by increasing the responsiveness of the primary surplus function. Rather, a stronger primary surplus response works to flatten the response of realized default probabilities to expected ones, which preserves the multiplicity.====Second, if there is a large, deep-pocketed, third-party agent with the ability to coordinate expectations, then there is clearly a role for them if there are multiple equilibria. Consider as examples the US Treasury during the Tequila crisis or the ECB during the Eurozone crisis. Such a third-party would want to coordinate the economy away from the risky equilibrium, be it ==== or ====, and onto the risk-free equilibrium. In the benchmark calibration, the third-party could do so regardless of the maturity of the debt. At the benchmark maturity, though, if the sovereign becomes too myopic or the economy too volatile, then the multiplicity disappears entirely and this tool is lost to the third-party as ==== becomes the unique steady state.",Long-term sovereign debt: A steady state analysis,https://www.sciencedirect.com/science/article/pii/S1094202522000199,31 March 2022,2022,Research Article,28.0
Dong Feng,"School of Economics and Management, Tsinghua University, China","Received 13 January 2020, Revised 9 January 2022, Available online 31 March 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.03.005,Cited by (0),"The ==== has witnessed not only a marked disruption in the credit markets and a sharp increase of capital underutilization, but also a severe deterioration of the matching efficiency in the labor market, as evidenced by the historically high unemployment rate and a significant outward shift of the Beveridge curve. Motivated by these facts, I develop a tractable dynamic general equilibrium model to show that financial frictions can manifest themselves as changes in the endogenous matching efficiency in the labor markets. Furthermore, I come up with sufficient statistics to capture the selection and congestion effects of financial frictions on unemployment. The analytical characterization shows that the competing effects crucially depend on interaction between credit imperfections and labor supply elasticity. I then use a calibrated version of the model to show quantitatively that the credit crunch during the financial crisis was a key driving force behind the outward shift of the Beveridge curve, which can explain up to 40% of the rise in the unemployment rate over the business cycles.","The Great Recession is the deepest recession in the post-war era. The burst of the housing bubble not only generated a marked increase in unemployment, but also led to a collapse in the credit markets. On the one hand, the unemployment rate increased dramatically while the job opening rate decreased significantly during the Great Recession, but the Beveridge curve also shifted outward since the last quarter of 2008 (see Fig. 1). On the other hand, the ratio of external funding to nonfinancial assets, one of the key measurements for the healthy functioning of the credit markets, shrank significantly.====There is no lack of explanations for the joint movement of the unemployment rate and job opening rate in the literature through the lens of the Diamond-Mortensen-Pissarides (DMP) framework. According to this framework, when labor market conditions or the matching efficiency in the labor market are worsened, both unemployed workers and the number of vacancies will increase, causing a downward (south-east) movement along the Beveridge curve. However, several important questions remain unanswered: Why did the matching efficiency suddenly decrease following the financial crisis? Why did the Beveridge curve shift out? Does the decreasing matching efficiency in the labor market have something to do with a disruption in the credit market? The standard DMP model is silent on these questions.====Data show that the capital market conditions and the labor market conditions are closely linked. For example, Fig. 2 shows that the capital utilization rate (or capital unemployment rate) and the labor utilization rate (or labor unemployment rate) comove with each other over the business cycle, especially during recessions. Following the DMP framework, we can interpret the higher labor unemployment rate in recession as a consequence of a worsening matching efficiency in the labor market and, in the same spirit, the higher unemployment rate of capital in a recession as a consequence of a worsening allocative efficiency in the capital market. But can we identify a common force that drives the allocative efficiency in both markets?====Motivated by these questions, I develop a dynamic general equilibrium model of heterogeneous firms with credit market frictions and labor search frictions. In this framework, a tightening of borrowing constraints not only affects capital accumulation, and thus labor demand, but also (endogenously) decreases the measured aggregate matching efficiency in the labor market. In particular, I show that financial frictions are important for understanding the behavior of the Beveridge curve in that a financial crisis not only stretches the Beveridge curve in the north-south direction but also shifts it outward.====To the best of my knowledge, this is the first article to show that credit market frictions can manifest themselves as adverse changes in the matching efficiency of the labor market. We can use the following equation to summarize the most essential insights of the theoretical part of this paper. Denoting the borrowing constraint of a firm as ====, where ==== is the stock of capital used for production by an entrepreneur, ==== is her net worth, and ==== is the leverage ratio capturing borrowing constraints. I can obtain an analytical relationship between the matching efficiency (====) in the labor market and the borrowing-constraint parameter (====) as below:====where ==== is a positive and increasing function of the financial friction parameter ==== (derived from the endogenous distribution of firms' productivity), and ==== represents the coefficient of the matching function in the labor market. A shock to the financial market condition ==== can generate not only a financial crisis in the usual way but also a deterioration of the matching efficiency in the labor market, causing a downward movement in both along the Beveridge curve and an outward shift of the curve itself. This financial linkage between the credit market and the labor market disappears when ==== (no borrowing constraint) or there is no heterogeneity across firms, then ==== and the model reduces to a standard random search model. I also quantify the importance of this financial channel based on a calibration exercise and find that financial shocks can contribute as much as 50% to movements in the unemployment rate.====Of course, this paper is not the first to study interactions between credit markets and labor markets. For example, the seminal work of Wasmer and Weil (2004) introduce search frictions into credit markets and study their impact on the labor market. In their model, firms have to search and match with a bank to cover the entry cost. However, Wasmer and Weil (2004) is silent on how credit market frictions translate into matching efficiency in labor markets. Therefore, interactions between credit and labor markets in their model are not through the channel of endogenous matching efficiency emphasized in this paper. Consequently, their paper is silent on issues related to the Beveridge curve.====This paper is also highly related to Lagos (2006). Lagos (2006) uses labor search frictions to generate endogenous total factor productivity (TFP) in a standard DMP model with both capital and labor. Likewise, I use credit market frictions to generate endogenous matching efficiency (analogous to TFP) in the labor market.====The key transmission mechanism in my model proceeds as follows. Although workers are homogeneous, the marginal value of being matched with labor increases with an entrepreneur's productivity. Therefore, entrepreneurs with heterogeneous productivity have an incentive to post different wage offers. Motivated by Moen (1997), I use competitive search to formalize this idea. Entrepreneurs with higher productivity tend to post higher wage positions with more workers queuing for jobs. Thus, the job-filling rate will be higher for more productive entrepreneurs. In equilibrium, wage dispersion for homogeneous workers emerges with an endogenous set of segmented labor markets.==== The recent financial crisis has spawned a large volume of research on the role that financial shocks play in output fluctuation, following the works of Williamson (1987), Gertler and Bernanke (1989), Kiyotaki and Moore (1997), Carlstrom and Fuerst (1997), Bernanke et al. (1999) and Jermann and Quadrini (2012). However, very few papers connect financial frictions and unemployment. Merz (1995) and Andolfatto (1996) were among the first to introduce labor search frictions in the RBC framework, which admits capital accumulation but is not subject to any financial frictions. See Shimer (2010) for a survey on the recent development of quantitative analysis for labor search. Wasmer and Weil (2004) adopt matching functions with random search to model frictions in both credit and labor markets. A quantitative extension is done by Petrosky-Nadeau and Wasmer (2013), among others. They then use the general equilibrium interaction between these two markets to illustrate the workings of a financial accelerator. Monacelli et al. (2011) discuss the role of credit frictions in unemployment by introducing the strategic use of debt by firms with limited enforcement. They build the model to explain why firms lower labor demand after a credit contraction even though there is no shortage of funds for hiring. Liu et al. (2016) incorporate the housing market and the labor market in a DSGE model with credit and search frictions. They then conduct a structural analysis of the dynamic relationship between land prices and unemployment. All of the aforementioned papers focus on the connection between firm-side credit imperfections and unemployment, while Bethune et al. (2015) emphasize the relationship between household credit and unemployment.====The most related papers include Lagos (2006), Michaillat (2012) and Buera et al. (2015). First, both Lagos (2006) and my paper develop a tractable theory of TFP. Lagos (2006) develops a model of endogenous TFP with labor search frictions. My paper contributes to this line of literature by incorporating both credit and labor search frictions into an otherwise standard RBC model with capital accumulation. Second, both Michaillat (2012) and the current paper initiate decomposition of unemployment. Like Lagos (2006), Michaillat (2012) focuses on labor markets per se in a canonical DMP framework without financial frictions. My paper complements Michaillat (2012) by considering both labor market frictions and credit frictions in a neoclassical RBC model with capital accumulation. Finally, both Buera et al. (2015) and the current paper quantify the effect of a credit crunch on unemployment in a heterogeneous entrepreneurs model with credit frictions and employment frictions. However, my paper differs in several important dimensions. First, their analysis is largely quantitative while the linear property of the model generates tractability and makes transparent the new channel contributed by my paper. Second, we use different modeling strategies for equilibrium unemployment. They specify a Walrasian labor market with a unique and publicly displayed price. To sustain equilibrium unemployment, they assume that only a fraction of unemployed workers can enter the centralized hiring market in a given period. In contrast, I use a competitive search by following Shimer (1996) and Moen (1997). Finally, they focus on the recent credit crunch while I take into account the historical business cycles as well as the recent recession.====The rest of the paper is organized as follows. Sections 2 and 3 describes and characterizes the model respectively. Section 4 presents the key transmission mechanism through which credit markets affect labor markets. Section 5 uses the calibrated model to decompose unemployment with financial and search frictions, one for the event study of the Great Recession and the other for the unemployment decomposition over business cycles. Section 6 concludes. The Appendix consists of data description and proofs.",Aggregate implications of financial frictions for unemployment,https://www.sciencedirect.com/science/article/pii/S1094202522000229,31 March 2022,2022,Research Article,29.0
"Jiang Helu,Sohail Faisal","Shanghai University of Finance and Economics, Institute for Advanced Research, China,University of Melbourne, Department of Economics, Australia","Received 1 November 2019, Revised 13 March 2022, Available online 30 March 2022, Version of Record 2 March 2023.",https://doi.org/10.1016/j.red.2022.03.004,Cited by (0),"The U.S. is undergoing a long-term decline in entrepreneurship. We show that this slow-down in entrepreneurship has been more pronounced for skilled individuals – those with a college degree. We document new facts on the ==== decline, and in doing so, shifts the composition of entrepreneurs towards the unskilled, ==== average entrepreneurial productivity. Our findings suggest an integral role for the changing income structure of workers in driving the broader decline in business dynamism in the U.S.","The share of entrepreneurs among total employment has declined by around 25% since the early 1980s (see Fig. 1a).==== We document that this decline has been much more pronounced for those with at least a college degree – skilled individuals. Since 1983, the rate of entrepreneurship declined by 43% for skilled and only 16% for unskilled individuals (see Fig. 1b). Given the importance of entrepreneurs – particularly skilled entrepreneurs – for job creation and economic growth, understanding the forces behind these trends is important in order to address broader declines in business dynamism observed over the same period.==== In this paper, we study the role of the changing income structure of workers in explaining the overall and skill-biased trends summarized in Fig. 1.====Intuitively, there is a straightforward relationship between workers incomes and entrepreneurship: individuals pursue entrepreneurship by considering its opportunity cost – that is, earnings in employment. So, a relative increase in workers incomes discourages entrepreneurship. We focus on one specific change in the income structure of workers – the rise in worker skill premium – and show that it fully accounts for the ==== nature of declining entrepreneurship and shifts the composition of entrepreneurs so as to ==== the average productivity among entrepreneurs.====We begin by documenting new evidence that is consistent with predictions of the intuition above. First, we show that the relatively faster earnings growth of skilled workers since the 1980s (the rising skill premium) has coincided with relatively larger declines in measures of entrepreneurship for skilled individuals. Fig. 1b shows this to be the case for one such measure – the share of entrepreneurs. Next, we show that transitions from employment into entrepreneurship also feature a skill-bias – the share of skilled workers entering entrepreneurship has remained relatively stable over time while the analogous share for unskilled workers has steadily increased. On the other hand, exit out of entrepreneurship into employment has increased steadily in a skill-neutral manner over time.==== We then conduct a novel decomposition of the decline in the stock of entrepreneurs into flows in and out of entrepreneurship and find that skill-biased changes in entry from employment play a central role in driving the skill-biased decline in entrepreneurship.====While suggestive, the concurrent increase in the workers skill premium and skill-biased declines in measures of entrepreneurship does not establish a causal link between the two phenomena. Indeed, if the skill premium for entrepreneurs kept pace with that of workers, we would predict a skill neutral change in entrepreneurship. We provide novel evidence to argue that this was not the case. By estimating a measure of the entrepreneur skill premium we show that it did not grow as fast as the worker skill premium. We also analyze the evolution of entrepreneurial earnings and find that they have not kept pace with workers earnings, especially for skilled individuals. This suggests that the income structure of workers changed so as to increase the opportunity cost to entrepreneurship, particularly for skilled individuals.====Using geographic differences in the increase in workers' skill premium, we show that U.S. states which experienced larger increases in the skill premium also feature a stronger skill-bias in the decline of entrepreneurship. Similarly, by exploiting variation in the growth of workers earnings across occupations, we document that occupations which experienced the largest increase in workers earnings feature the largest declines in employees transitioning into entrepreneurship (from those occupations). Consistent with evidence of wage polarization – the observation that wage growth has been largest for high and low earnings occupations – we find that decline in entry into entrepreneurship has been largest among high and low earnings occupations. To our knowledge, this is the first paper to document such a pattern of polarization in ====.====To quantitatively assess the impact of an increasing worker skill premium on skill-biased and aggregate declines in entrepreneurship, we develop a model of occupational choice along the lines of Lucas (1978) that features worker heterogeneity. In the model economy, agents differ in their ability as workers and as entrepreneurs; workers are either skilled or unskilled, while entrepreneurs earn profits given productivity which fluctuates randomly. Agents make an occupational choice each period by comparing the returns to entrepreneurship and wage work. The model features endogenous transitions in and out of entrepreneurship in response to fluctuations in entrepreneurial productivity.====We calibrate the model economy to match key features of the U.S. data in 1983. Then, we introduce changes to this baseline economy that are commonly viewed as influencing the worker skill premium – a rising share of college graduates and technological changes. By comparing the response of a benchmark economy in which the skill premium evolves as in the data to a counterfactual economy in which the skill premium remains stable, we quantify the impact of a rising skill premium on the evolution of entrepreneurship over time.====In the model, we find that, on its own, an increase in the skill premium pushes skilled and unskilled entrepreneurship rates in opposite directions: Raising entrepreneurship among the unskilled while lowering it among the skilled. These changes are such that the rising skill premium contributes little to the aggregate decline in entrepreneurship. Instead, the majority, around 70%, of the aggregate decline in entrepreneurship is due to skill-neutral technological change a rising share of college graduates. A rising skill premium – driven by skill-biased technological change – is crucial for generating the observed ==== decline in entrepreneurship. Indeed, comparing the benchmark and counterfactual economy reveals that, had the skill premium remained stable, the overall decline in the share of entrepreneurs would have been similar but largely skill-neutral.====In generating the skill-biased decline in entrepreneurship, a rising skill premium ==== the average productivity of entrepreneurs as the composition of entrepreneurs shifts away from skilled entrepreneurs – who tend to be more productive – towards unskilled entrepreneurs – who tend to be less productive. Quantitatively, the counterfactual economy with a stable skill premium results in a 12.5% improvement in average entrepreneur productivity compared to only a 10% improvement when the skill premium increases as in the data. This is a significant difference and emphasizes the importance of understanding not only the aggregate decline in entrepreneurship but also the differential declines by skill that we document.====The skill-biased decline in the share of entrepreneurs that results from a rising skill premium is accompanied by skill-biased changes in entry rates and skill-neutral changes in exit rates – changes that are qualitatively consistent with our empirical findings. However, the model economy fails to quantitatively match the observed ==== in entry and exit rates. We argue that in addition to technological change and a rising supply of college graduates, there has also been a trend increase in the exogenous probability of exit from entrepreneurship to employment. We evaluate the model economy's response to this increase and find that increased exit is critical for quantitatively matching the evolution of flows in and out of entrepreneurship. Increasing exit also contributes modestly to the aggregate decline in entrepreneurship (around 13% of the total) and does so in a ==== manner. Notably, a trend increase in exit does not change the impact that a rising worker skill premium has on measures of entrepreneurship – which is the focus of our paper.====Overall, this paper documents new evidence on the skill-biased decline in entrepreneurship and studies the role of the rising worker skill premium in generating it. Our empirical and quantitative findings suggest a significant role for the changing income structure of workers in shaping the evolution of entrepreneurship and, more generally, contributing to the broader decline in business dynamism in the U.S.",Skill-biased entrepreneurial decline,https://www.sciencedirect.com/science/article/pii/S1094202522000217,30 March 2022,2022,Research Article,30.0
"He Qichun,Luo Yulei,Nie Jun,Zou Heng-fu","China Economics and Management Academy, Central University of Finance and Economics, Beijing, China,Faculty of Business and Economics, University of Hong Kong, Hong Kong,Research Department, Federal Reserve Bank of Kansas City, USA,Liaoning University, Shenyang, Liaoning, China","Received 1 February 2021, Revised 26 February 2022, Available online 15 March 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.03.001,Cited by (0),"According to ====, entrepreneurs are driven to innovate not just for the fruits of success but also for success itself. This description of entrepreneurship echoes ===='s (1904-05, German; ","Since the pioneering work by Aghion and Howitt (1992), Schumpeterian growth theory has developed into a key framework for understanding long-run economic growth. This theory formulates Schumpeter's (1942) notion of “creative destruction”—the process by which new innovations replace old technologies—and shows that innovations resulting from entrepreneurial investments are crucial to long-run economic growth.====But what drives entrepreneurs to innovate? Schumpeter (1934) refutes the traditional and hedonistic assumption that defines entrepreneurs' utility on consumption and instead emphasizes the “psychology of entrepreneurs”: the entrepreneur is strongly motivated by the “dream and the will to found a private kingdom, usually, though not necessarily, also a dynasty.”====This description of entrepreneurship is very similar to Max Weber's description of the “spirit of capitalism”====: ====The theoretical literature on the spirit of capitalism—enjoying the accumulation of wealth regardless of its effect on consumption smoothing — shows that it has important implications on economic growth and asset pricing in non-Schumpeterian growth frameworks (Zou, 1994; Bakshi and Chen, 1996). These findings are consistent with the empirical evidence provided by Barro (2004) and McCleary and Barro (2006, 2019) which show beliefs and religion do matter for economic activity. However, little research has explored how the spirit of capitalism would influence innovation and long-run growth in a Schumpeterian growth model.====More importantly, incorporating the spirit of capitalism could also have significant implications for the effects of monetary policy on growth and welfare. In a Schumpeterian model with money, Chu and Cozzi (2014) show that an increase in the nominal interest rate reduces both R&D and economic growth, and that the optimal monetary policy features a positive nominal interest rate (thereby violating the Friedman rule (Friedman, 1969), which says the optimal non-negative nominal interest rate is zero). The key intuition behind their results is that a higher interest rate raises the borrowing cost of entrepreneurial investment that is subject to a cash-in-advance (CIA) constraint, which helps mitigate the possible overinvestment issue in Schumpeterian models (see Aghion and Howitt, 1992, 1998).==== However, the spirit of capitalism introduces a new channel for monetary policy to influence R&D investment, labor allocation, and consumption-leisure decisions; therefore, whether these new findings on monetary policy still hold requires a careful analysis.====To fill these gaps in the literature, we formalize Schumpeter's idea of the “psychology of entrepreneurs” by introducing the spirit of capitalism into a stylized Schumpeterian model with money based on the work of Aghion et al. (2013) and Chu and Cozzi (2014). Specifically, we model the spirit of capitalism through a direct preference for wealth. See Luo et al. (2020) for a detailed review of a long line of sociological and economic literature to justify why this modeling strategy is consistent with Weber's original work on the spirit of capitalism. To the best of our knowledge, this is the first paper to provide a unified framework to investigate (both theoretically and quantitatively) the implications of the spirit of capitalism for the effects of monetary policy on long-run growth and welfare.====Our analysis delivers three main findings. First, we show that introducing the spirit of capitalism into the Schumpeterian growth framework yields novel insights into the effects of monetary policy on long-run growth. Specifically, we theoretically prove that money is not superneutral in this framework and that the effect of higher nominal interest rates on long-run growth depends on the strength of the spirit of capitalism in an economy. When the spirit of capitalism is small or absent, a higher interest rate reduces growth. However, when the spirit of capitalism is strong enough (relative to the elasticity of labor and taste for leisure), higher nominal interest rates could promote growth.====The key intuitions of this finding are as follows. Without the spirit of capitalism, an increase in the nominal interest rate reduces long-run growth through two channels. The first channel is that, due to an elastic labor supply and the CIA constraint on consumption, higher nominal interest rates encourage households to choose leisure over consumption, leading to lower labor supply in the R&D sector and therefore lower long-run growth (this is the market size effect highlighted in Aghion et al., 2013). The second channel is that, due to the CIA constraint on R&D, higher nominal interest rates also increase borrowing costs for entrepreneurs, shifting labor away from R&D toward manufacturing and reducing long-run growth (a negative labor reallocation effect).====In contrast, with the spirit of capitalism, an increase in the nominal interest rate could promote long-run growth. Specifically, in the presence of the spirit of capitalism, a higher nominal interest rate induces consumers (who have a direct preference for wealth) to increase their savings, thereby lowering the real interest rate and raising the value of innovations for entrepreneurs.==== We show that a stronger degree of the spirit of capitalism could amplify this positive effect of a higher nominal interest rate on R&D.==== Therefore, when the spirit of capitalism is strong enough, the positive effect could dominate the two negative effects described in the previous paragraph, causing long-run growth to increase with the nominal interest rate.====Second, we theoretically and quantitatively show that the optimal nominal interest rate decreases with the degree of the spirit of capitalism, suggesting the Friedman rule–that the optimal non-negative interest rate is zero–is likely to be valid. This finding is in contrast to the main finding in Chu and Cozzi (2014) which shows that the optimal nominal interest rate is positive in a similar Schumpeterian growth model. It also provides theoretical support for negative interest rates, which have been discussed intensively in recent years.====Third, we calibrate our model to the U.S. economy and quantify the contribution of the spirit of capitalism to long-run growth. Our analysis suggests that the spirit of capitalism explains one-third of long-run growth in the U.S. (0.6 percent out of 1.8 percent annually), which is substantial. To the best of our knowledge, this is the first study to quantify the growth contribution of the spirit of capitalism. In this sense, our analysis enriches Aghion and Howitt (1992) and provides a more complete view of Schumpeterian growth models in understanding long-run growth.====Our paper contributes to a large literature studying the implications of the spirit of capitalism or the quest for status on economic growth (Zou, 1994; Futagami and Shibata, 1998; Smith, 1999; Corneo and Jeanne, 2001), savings (Cole et al., 1992; Zou, 1995; Carroll, 2000; Luo et al., 2009), asset pricing (Bakshi and Chen, 1996; Smith, 2001; Gong and Zou, 2002), wealth distribution (Luo and Young, 2009; Corneo and Jeanne, 2001), business cycles (Karnizova, 2010; Michaillat and Saez, 2015, 2021), secular stagnation (Michau, 2018; Ono and Yamada, 2018), money (Gong and Zou, 2001), taxation (Saez and Stantcheva, 2018), comparison with recursive utility (Alaoui and Sandroni, 2018), expansion of variety (Hof and Prettner, 2019), and industrialization (Chu et al., 2020). Different from the existing literature, our paper investigates the wealth and long-run growth implications of the spirit of capitalism for monetary policy. In addition, in contrast with most of these studies which focus on the theory side, we conduct quantitative analysis based on a calibrated model and also provide empirical evidence to support the key mechanism.====Our paper also adds to the literature on the effect of monetary policy on long-run growth and welfare (Sidrauski, 1967; Stockman, 1981; Gomme, 1993; Akerlof et al., 1996; Dotsey and Sarte, 2000; Akyol, 2004; Benigno and Ricci, 2011; Brunnermeier and Sannikov, 2016; Arawatari et al., 2018; Chu et al., 2019; Eyster et al., 2021). Challenging the seminar work by Sidrauski (1967) which argues for superneutrality of money in the long run (i.e., changes in the inflation rate or nominal interest rate have no effects on long-run growth), most recent studies show money could influence long-run output growth or steady-state levels through various channels. For example, Jones and Manuelli (1995) adopt an endogenous growth model with human capital and Marquis and Reffett (1994), Funk and Kromen (2010), and Chu and Cozzi (2014) focus on R&D-driven growth models. A recent paper by Eyster et al. (2021) introduces customers' concerns for pricing fairness to the New Keynesian model, and shows that monetary policy is not neutral in the long run. Most of these studies show a negative relationship between the inflation rate and growth in the long run.==== The novel contribution of our paper is to show that the relationship between the inflation rate (or the nominal interest rate) and growth depends on the strength of the spirit of capitalism.====The remainder of this paper is organized as follows. Section 2 describes the monetary Schumpeterian model with the spirit of capitalism. Section 3 presents theoretical results and intuitions. Section 4 conducts quantitative analysis. Section 5 provides general discussions. Section 6 concludes.","Money, growth, and welfare in a Schumpeterian model with the spirit of capitalism",https://www.sciencedirect.com/science/article/pii/S1094202522000114,15 March 2022,2022,Research Article,31.0
"Wang Cheng,Yang Youzhi","School of Economics, Fudan University, China,Institute for Advanced Research, Shanghai University of Finance and Economics, China","Received 23 February 2020, Revised 27 January 2022, Available online 25 February 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.02.003,Cited by (0),"We study an equilibrium model of the labor market with identical firms and homogeneous workers, and with search and on-the-job search. Jobs are dynamic contracts that allow firms to match the worker's outside offers or let the job be terminated. For a non-degenerate distribution of wage offers to arise in the environment, it is necessary and sufficient that (i) there is a positive cost of ====, in terminating an existing job or posting a new one; and (ii) there is asymmetric information regarding the worker's outside offers. The model is calibrated to the U.S. labor market to match observed worker flows and the observed mean-min ratio in wages earned. The calibrated model predicts a unimodal distribution for both wages offered and wages earned.","Can search and wage posting support a non-degenerate equilibrium distribution of wages offered in a labor market with identical firms and homogeneous workers? The search for a ==== theory of wage dispersion starts from Diamond (1971), who offers a negative answer to the above question. The only equilibrium he finds, in an environment that meets the qualifications of the question, is one in which only the monopsony wage is offered.====As is well put by Rogerson et al. (2005, page 976), a pure theory of wage dispersion is of interest for two reasons. “First, the early literature suggested that search is relevant only if the distribution from which you are sampling is non-degenerate, so theorists were naturally led to study models of endogenous dispersion. Second, many people see dispersion as a fact of life, and for them the issue is empirical rather than theoretical.” A pure theory of wage dispersion may provide an explanation for the “unexplained” wage differences in the labor market. Mortensen (2003, page 1) reports that “Observable worker characteristics that are supposed to account for productivity differences typically explain no more than 30 percent of the variation in compensation.”====Burdett and Mortensen (1998) (hereafter BM) develop a first pure theory of wage dispersion. They show that adding on-the-job search to Diamond (1971) would produce a non-degenerate distribution of equilibrium wage offers. Their idea is that, relative to lower wages, higher wages, while imposing greater costs of labor compensation on the firm, also offer the benefits of lower employment turnover – the probability with which the worker quits from his current job falls in the pay of that job. This trade off between compensation and job turnover results in differential wages offered in equilibrium by identical firms to identical workers.====In BM, an employment contract is a promise of constant wage until the worker finds a better outside offer and quits his current job. Thus the dispersion in wages is essentially a dispersion in the compensation contracts offered. Burdett and Coles (2003) (hereafter BC) generalize BM, allowing the contract to optimize on two dimensions: the initial wage offer, and the profile of continuation wages as a function of the worker's tenure at the firm. Their environment produces not only a non-degenerate equilibrium distribution of initial wages offered, but also continuation wages that are monotonically increasing in the worker's tenure.====The theories of BM and BC, however, are subject, in our view, to the following limitations. First, in both BM and BC, the continuations of the initial contract the firm offers, whether they be a constant wage until the worker quits voluntarily, or a wage-tenure profile where wage increases in the worker's tenure, are not allowed to be dynamically contingent on the outside offers the worker receives ex post. Once the job starts, the firm never responds to the worker's outside offer.==== Obviously, if one stipulates that the worker's outside offers are observable and the labor contract is allowed to be contingent on the worker's history with the firm, including all offers he receives on the job, past and current, then the contracts in BM and BC may not be optimal. Second, in both BM and BC, workers and firms do not discount future payoffs. Third, neither paper is explicit about their model's informational structure with respect to the worker's outside offers, neither are they concerned about the costs that may arise in terminating an incumbent worker or hiring a new one.====What would arise from the BM model if it is set free from the above described constraints? Would a non-degenerate wage distribution still emerge as an equilibrium outcome of their labor market? What happens if the contract is optimally designed, allowing in particular the firm to offer state-contingent responses to the worker's outside offers, with discounting, with the model's information structure explicitly specified, and with or without the presence of the costs that are commonly believed to exist in labor market transactions?====To answer these questions, we use a model that is based on BM and BC but covers three scenarios that differ in how information regarding the worker's outside offers is allocated between the two sides of the contract. In Scenario 1, outside offers are publicly observed between the firm and the worker. In Scenario 2, they are entirely private to the worker. In Scenario 3, a verification device is available to the firm for producing information regarding outside offers – fake outside offers are detected imperfectly, with a probability that increases in the cost of monitoring that the firm is willing to incur. Obviously, Scenario 3 covers the cases that are between Scenarios 1 and 2. Depending on the cost of verification incurred, Scenario 3 defines a collection of cases that vary continuously in how much informational advantage or disadvantage that each party of the relationship is given against the other.====Within this framework, we explore theoretically what sorts of specific settings, in the general environment that the model describes, would give rise to a non-degenerate distribution of wages offered. We show that for any non-degenerate distribution of wages offered to arise in the model's equilibrium, it is necessary and sufficient that (i) there are positive costs of worker turnover, either in terminating an existing job or in posting a new one, and (ii) there is some asymmetric information regarding the employed worker's outside offers.====Condition (i) is needed because otherwise the trade-off of BM between compensation and turnover would not be there to support the dispersion. Condition (ii) is necessary because, without it, in equilibrium any outside offer that the employed worker receives, which must be optimal for a vacant firm to make, would also be feasible and optimal for the (identical) incumbent firm to match. This then implies that any outside offer that targets the employed worker should not be made in the first place, breaking the BM story. On the other hand, private information reduces the degree of state contingency that can be achieved in the firm's optimal response to the worker's outside offers, essentially forcing the firm to either over match the outside offer or terminate the contract over it, in at least some states of the world. Consequently, in equilibrium some offers are made to target employed workers and this, together with condition (i), holds up the tradeoff of BM and produces the non-degenerate distribution they predict.====The paper also asks, quantitatively, whether the non-degenerate wage distributions that do arise in the model is consistent, rather than in contrast, with the data. For this we calibrate the model to target observed worker flows and the mean-min ratio of the residual wages earned – wages earned by observationally similar workers. With (entirely) private outside offers, our simulation produces a distribution of wages offered that has a unimodal (not convex) density. It also produces a density of wages earned that is increasing and convex, in contrast to the data, which is well approximated by a log-normal distribution [Hornstein et al. (2007), Mortensen (2003), Postel-Vinay and Robin (2002b)]. With private outside offers, wages earned, governed by a wage-tenure scheme which in each period gives the retained worker a raise independent of his outside offer, grow “too fast” to send too many workers to the right end of the distribution. To generate a unimodal wages earned distribution, there need be a sufficient amount of state-contingent counteroffering in the model's equilibrium to hold the employed worker's wage from rising too fast, while there cannot be too much counteroffering to kill the equilibrium dispersion in the wages offered. Such a balance is achieved in the third scenario of the model we study, where monitoring reduces the worker's incentives to fake a high outside offer upon which he hopes to be retained. There, costly state verification permits more but not too much state contingency of the worker's expected utility on outside offers, reducing the speed at which his expected utility rises over time, resulting in a unimodal distribution for both the wages offered and the wages earned.====Targeting the observed distributions of both the wages offered and earned has gone beyond the scope of the initial questions of Diamond (1971) and BM. As is well known, standard search-matching models have difficulties matching the observed wages earned distribution. Hornstein et al. (2011) show that standard search-matching models generate only a very small differential between the mean and the minimum wages paid in the U.S. labor market: the observed Mm ratio - the ratio between the mean and the minimum wages paid - is at least twenty times larger than what standard search models are able to generate. Our model, calibrated to the U.S. data, not only generates the right Mm ratio, but also offers a partial account for the shape of the observed wages earned distribution.",On the pure theory of wage dispersion,https://www.sciencedirect.com/science/article/pii/S1094202522000102,25 February 2022,2022,Research Article,32.0
"Kang DongIk,Usher Andrew","Korea Institute of Public Finance, Republic of Korea,Bank of Canada, Canada","Received 24 September 2021, Revised 15 February 2022, Available online 24 February 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.02.001,Cited by (0),"Using retail scanner data, we find that the probability of price adjustment increases with a product's revenue, and the average absolute size of price adjustment decreases with the product's revenue. Furthermore, the responsiveness of prices to monetary shocks increases with product revenue. These facts are consistent with menu cost models in which the menu cost increases less than one-for-one with revenue, and inconsistent with models in which the menu cost increases one-for-one with revenue. In a calibrated menu cost model, the real effect of ","Various recent studies using menu cost models have branched out from assuming simple fixed menu costs in a single product setting, as in Golosov and Lucas (2007), and have begun to focus more on the nature of the menu cost technology itself and its relation to the firm. For example, studies such as Midrigan (2011), Alvarez and Lippi (2014), and Bhattarai and Schoenle (2014), have explored the implications of multi-product firms in settings where, once the menu costs is paid, firms can simultaneously change the price of many products. Others, such as Alvarez et al. (2011), have studied the implications of an adjustment cost technology that depends on both physical menu costs and observational costs. Along these lines, we study the relationship between product revenue and price setting behavior, to learn about the nature of the menu cost technology. We focus on whether or not the menu cost increases directly proportionally with product revenue, which has become a common assumption in the literature, and provide empirical evidence suggesting that they do not. We find that the real effect of monetary policy is smaller in model economies where the menu cost scales less than one-for-one with revenue, and that this introduces a counter-cyclical effect that strengthens monetary policy effects during recessions.====In menu cost models, the relationship between product revenue and the menu cost determines, along with other factors, the size of the inaction region – that is the range of prices around the optimal price in which firms choose not to adjust their prices. The relative benefits and costs of price adjustment determine the size of the inaction region. The inaction region grows smaller as the benefits of price adjustment increase, and larger as the costs of adjustment increase. Our paper builds on the insight that the benefits of price adjustment are greater for products with higher revenue because they suffer larger losses from sub-optimal pricing. Therefore, unless the costs of price adjustment increases directly proportionally with revenue, the inaction region will grow smaller as revenue increases. The inaction region, in turn, determines the price setting behavior of firms and the pricing responses to monetary shocks. A smaller inaction region implies a greater probability of price adjustment together with a smaller absolute size of adjustment. Furthermore, smaller inaction regions also result in a larger price response to monetary shocks, and therefore more muted output responses.====Thus, this mechanism provides three testable implications for the shape of the menu cost and the relevance of product revenue for price setting and monetary transmission. If the menu cost increases proportionally with revenue, then we should observe no relationship between revenue and price adjustment. On the other hand, if the menu cost increases at a rate less than directly proportional with revenue, products with greater revenue will be more likely to adjust their prices, adjust their prices by smaller amounts when they adjust, and their prices will be more responsive to monetary shocks.====Using Nielsen retail scanner data, we test these predictions. We find that the probability of price adjustment increases with product revenue, and that the average absolute size of price adjustment decreases with revenue. In our preferred specification using an instrumental variables approach, a 10% increase in revenue increases the probability of price adjustment by 0.13 percentage points, and decreases the average absolute size of adjustment by 0.14 percentage points. These relationships are robust to various controls and are driven not only by cross-sectional differences in product or retailer characteristics but also by the variation in revenue of a given product across time.====Additionally, we show that the responsiveness of prices to monetary shocks also increases with product revenue. We group products into revenue quintiles by sorting by revenue within each market, and estimate the impulse response of prices to monetary policy shocks from 2006 to 2015. We perform separate local projections on each quintile of revenue using monetary shocks from Paul (2020) and Gorodnichenko and Weber (2016), constructed from high frequency data of current month federal fund futures on Federal Open Market Committee (FOMC) announcement dates. We find that not only do prices fall in response to a monetary tightening, but that the effect is stronger for products with higher revenue. The average total response of prices of products in the lowest quintile of revenue is 19% of that of products in the highest quintile of revenue after 12 months and 26% after 18 months.====Our empirical results show that variation in revenue, in relation to the menu cost, should play an important role in determining the price setting behaviors and monetary transmission properties of the menu cost model. However, the literature has often overlooked this relationship between revenue and price setting. Studies have chosen their menu cost technology without giving direct attention to the economic implications of their scalability. For example, many price setting models assume that menu costs scale directly proportionally with revenue, mostly to facilitate analytical tractability or computation. Gertler and Leahy (2008) assume that the menu costs scale with firm size to derive analytical expressions for a dynamic Phillips curve. Alvarez and Lippi (2014) and Alvarez et al. (2016) directly assume that firm loss from sub-optimal prices depends only on the price gap, which allows them to derive analytical solutions. Midrigan (2011) assumes that idiosyncratic demand shocks exactly offset productivity shocks such that revenue is normalized to reduce the computational burden of solving his model.====With menu costs that increase directly proportional to revenue, we find that revenue drops out of the expression determining the inaction region, implying price setting decisions do not depend on revenue.==== We use the term ==== to describe this property. Revenue neutrality implies that there should be no correlation between product revenue and neither the probability or size of price adjustment. The responsiveness of prices to monetary shocks should also be independent of revenue. However, these implications are inconsistent with our empirical findings.====Alternatively, some papers assume that menu costs are fixed (see, for example, Golosov and Lucas, 2007; Nakamura and Steinsson, 2010; Argente and Yeh, 2021). Such models are revenue ==== in principle and could be consistent with our empirical findings.==== Nevertheless, these models do not attempt to reflect the degree to which revenue varies in the data, and therefore cannot capture the full effect of revenue variation on price setting decisions and monetary transmission. In either case, there seems to be a lack of a clear benchmark for the treatment of the menu cost and product revenue in the literature, and especially not one that is consistent with our empirical evidence.====To verify our mechanism and to quantify their effects, we construct a dynamic menu cost model to explore the aggregate implications of our findings. We augment a standard menu cost model and add demand shocks and an exogenous mechanism that shifts the revenue distribution across time to match the data. We interpret this mechanism as time-varying shopping effort. We compare the economy where menu costs scale less than one-for-one with revenue (non-neutral) to the case with menu costs that scale one-for-one with revenue (neutral). We verify that the non-neutral economy replicates the empirical relationships between revenue and the probability and absolute size of adjustment very closely. It also generates large differences in the responsiveness of prices to monetary shocks across products, which differ by as much as twofold across products with different levels of revenue in our model. In contrast, the revenue neutral economy exhibits no such relationships between revenue and price setting.====We find that, as a result, the output response in our model is 24.6% larger in the revenue neutral economy than in the revenue non-neutral economy. This is due to the fact that the price response of high revenue products is greater in economies where revenue is non-neutral and therefore, the output response of high revenue products is low. As these high revenue products constitute a larger fraction of aggregate output, this leads to a smaller aggregate output response to a monetary shock.====In addition, given the non-neutrality of revenue, we find that cyclical shifts in the revenue distribution introduces a counter-cyclical force on the real effect of monetary policy. Because we find that revenue is greater during high output states, and more so for high revenue products as the variance of the revenue distribution increases, the price responses to monetary shocks are larger during expansions. The size of this effect is modest, however, as the difference in the counter-cyclicality of the cumulative effect of monetary policy between the revenue non-neutral and the revenue neutral economy is 6.1%.====We compare the results from our benchmark model described above, to those from a simpler model with no demand shocks and no shopping effort. This model closely resembles standard menu cost models in the literature. For example, the revenue non-neutral version of this model closely resembles the model in Golosov and Lucas (2007) and the revenue neutral version can be viewed as a simplified version of Midrigan's (2011) model without the multi-product firms. The steady state variance of revenue of this model is only about one-eighth of that of our benchmark model. As in the benchmark model, the non-neutral version of this model generates the empirical relationship between revenue and probability and size of adjustment, whereas the neutral version shows no such relations. However, the difference in the output response to monetary shocks is small: the output response is just 3.2% larger in the revenue neutral economy than in the non-neutral economy. This is intuitive as, without revenue variation, there will also be much less cross-sectional variation in the price response to monetary shocks.====To check the effect of the cyclical movements of the revenue distribution on the monetary transmission properties of the model, we also check the results from a model identical to our benchmark model but without the shopping effort mechanism. Without time-varying shopping effort, we cannot match the cyclical movements in the revenue distribution that we document in our sample. We find that in this model the state-dependent effects we find in the benchmark model disappear.====While our paper attempts to infer the nature of menu costs indirectly, some papers have attempted to measure and quantify them directly by interviewing firms. For example Zbaracki et al. (2004) show that a large part of the cost of adjusting prices in a U.S. industrial manufacturer is managerial costs. It seems reasonable to argue that the nature of the managerial costs they document – information gathering costs, decision-making costs, and communications costs – seem to be those that are unlikely to scale one-for-one with product sales or revenue. While their setting is different from ours, we believe the direct evidence on the managerial component of the cost of price adjustment complements our indirect evidence regarding menu costs.==== Levy et al. (1997) measure the price adjustment costs that arise from various sources within the context of a grocery store. Many of the in-store costs they find, such as the costs of ordering new price tags or supervision costs, also seem unlikely to scale one-for-one with revenue. As such, we believe that our paper's results are consistent with the direct evidence on the nature of menu costs, and complement them by indirectly exposing the importance of their relation with variation in product revenue.====Our work is part of a large literature that utilizes micro price data to study price setting behavior in the context of menu cost models. For example, Nakamura and Steinsson (2008) and Klenow and Kryvtsov (2008) show that menu costs models with idiosyncratic productivity shocks can match many features of the micro price data such as the co-movements of pricing moments with inflation. Lach and Tsiddon (1996, 2007) provide evidence showing synchronization of price changes within a given firm, and find that menu cost models are consistent with their empirical results. Bhattarai and Schoenle (2014) find that firms selling more goods adjust their prices more frequently and by smaller amounts, and that these facts are consistent with multi-product firm menu cost models.====Our results are also closely related to work that studies the real effect of monetary policy in state-dependent pricing models. For example, Golosov and Lucas (2007) and Midrigan (2011), demonstrate that the particulars of the idiosyncratic shock process can determine the magnitude of the real effect of monetary policy. Nakamura and Steinsson (2010) find that accounting for heterogeneity in the frequency and median size of price adjustment across different sectors of the economy increases the degree of monetary non-neutrality in menu cost models. Midrigan (2011) and Alvarez and Lippi (2014) show that accounting for multi-product firms increase aggregate price stickiness. Alvarez et al. (2016) demonstrate that the ratio of kurtosis to the frequency of price changes is a sufficient statistic for the real effect of monetary shocks. To the best of our knowledge, we are the first to demonstrate the effect of revenue variation in relation to the menu cost, on the effect of monetary policy.====Finally, some papers have noted a relationship between firm size and price changes, including notably, Goldberg and Hellerstein (2011). Bhattarai and Schoenle (2014) insinuate that larger firms change price more frequently, as larger firms are more likely to sell more goods. However, these results focus on the characteristics of price setting in the cross-section at the firm level. Our results suggest a direct relationship between revenue and price adjustment at the product level that holds both in the cross-section of products and across time.====The remainder of the paper is structured as follows. We begin with Section 2, where we establish the empirical relationships between product revenue, price setting behavior, and the price response to monetary shocks. In Section 3, we use a quantitative menu cost model to demonstrate the implications of our results on the real effect of monetary policy. Section 4 concludes.",Does product revenue matter for price setting and monetary policy transmission?,https://www.sciencedirect.com/science/article/pii/S1094202522000084,24 February 2022,2022,Research Article,33.0
"Fasani Stefano,Mumtaz Haroon,Rossi Lorenza","Lancaster University, United Kingdom of Great Britain and Northern Ireland,Queen Mary University, United Kingdom of Great Britain and Northern Ireland,University of Pavia, Italy","Received 2 April 2021, Revised 8 February 2022, Available online 24 February 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.02.002,Cited by (1),"This paper uses a FAVAR model with external instruments to show that monetary policy uncertainty shocks are recessionary and are associated with an increase in firms' exit and a decrease in firms' entry. At the same time, the stock price declines, while the TFP increases in the medium run. To explain this result, we build up and estimate a medium-scale ==== featuring firm heterogeneity and endogenous firm entry and exit. These features are crucial in matching the empirical responses. The baseline model outperforms an alternative model without firm dynamics in reproducing the FAVAR responses and implies a larger effect of monetary policy uncertainty shock on the real economic activity.","Global events such as the Great Recession and the ensuing slow recovery, the sovereign debt crisis in Europe, Brexit, and the Covid-19 pandemic, all contributed to the increase in macroeconomic uncertainty in developed countries. The recent experience has shown that sharp and timely interventions of policymakers may be crucial in times of economic distress. Announcements of policy strategies that aim to foster recovery have often helped to reassure financial markets and significantly reduce uncertainty. Instead, delayed and unclear responses by policymakers may fuel uncertainty and, likely, curb economic activity. Although the literature agrees on the recessionary effects of uncertainty shocks, the impact of heightened uncertainty about the action of policymakers and in particular, of the monetary authority is less clear. Some previous studies find that policy uncertainty influences capital flows, the business cycle, and the speed of economic recovery (Mumtaz and Zanetti (2013), Fernández-Villaverde et al. (2015), Baker et al. (2016), Mumtaz and Surico (2018), Bloom et al. (2018), Caggiano et al. (2020)).==== Other contributions document that monetary policy uncertainty affects a broad range of asset prices, including bonds, stocks, and exchange rates (Swanson (2006), Istrefi and Mouabbi (2018), Creal and Wu (2017), Bundick et al. (2017), Husted et al. (2019), Bauer et al. (2021)). In this paper, we revisit the question and consider the role of firm dynamics in propagating the impact of monetary policy uncertainty shocks. We refer to monetary policy uncertainty as to the perceived uncertainty that economic agents have about the possible realizations of future monetary policy. In particular, we investigate the effects of heightened uncertainty about the future short-run interest rate both in the data and in theory. To study the transmission of monetary policy uncertainty shocks, in the empirical analysis, we rely on a market-based measure of uncertainty about the future short-term interest rate. In the model, we inspect the dynamics by implementing innovations to the time-varying volatility of the monetary policy shock. We investigate the importance of firms' entry and exit decisions for the transmission of monetary uncertainty shocks. When monetary policy is uncertain, households and firms are unsure about the value of interest rates and inflation. For the productive sector, this uncertainty has also implications for the decisions of participating in the market. Firms that become more unsure about whether the discounted future cash flows will cover the cost of entry might decide to not enter the market. Firms that become more unsure whether the discounted future cash flows will guarantee the break-even and thereby, production might decide to exit from the market. Overall, the increased uncertainty about monetary policy might imply a lower entry and higher exit of firms, which ultimately, affect economic activity.====In the first part of the paper, we investigate the transmission channel of firm dynamics for monetary policy uncertainty shocks for the US economy over the period 1985:m1 to 2016:m6. We study the transmission in a FAVAR model, where the monetary volatility shocks are identified through an external instrument as in Husted et al. (2019).====Although we are interested in evaluating the conditional responses of entry and exit to increased monetary policy uncertainty, it is interesting to examine the comovement between monetary policy uncertainty and firm dynamics. We thus conduct an investigation on the relationship between the raw data of MPU index by Husted et al. (2019) and of entry and exit over the same sample of the FAVAR analysis. Fig. 1 reports the unconditional cross-correlations between the moving average of total entry/total exit and lags and leads of the MPU index. The figure shows that lags of the MPU index are negatively correlated with total entry and positively correlated with firm exit. This provides prima facie evidence that higher MPU is associated with lower firm births in the future while firm deaths are expected to increase.====The impulse responses to monetary policy uncertainty shocks obtained from the FAVAR show that shocks increasing monetary policy uncertainty are recessionary. Moreover, firms' births decrease, while firms' deaths increase.==== This evidence is robust both at the aggregate level, namely for establishments' births and deaths in the total private sector, and at the industry level.==== The stock price decreases in response to higher monetary policy uncertainty, and similarly to Bloom (2009), the utilization-adjusted TFP series reacts negatively on impact but overshoots in the medium-run.====We rationalize the empirical evidence on monetary policy uncertainty shocks in the second part of the paper. We consider a medium-scale New Keynesian model extended by adding firm heterogeneity and endogenous firm entry and exit. In the intermediate sector, firms are heterogeneous in terms of their specific productivity. Similar to Rossi (2019), firms decide to produce as long as their specific productivity is above a cut-off level, which is determined by the level of productivity that makes the present discounted value of the stream of profits equal to the firms' liquidation value. The defaulting probability for firms is endogenously determined by the cut-off level of productivity. The advantage of this framework is that firms' exit and average productivity evolve endogenously, bringing about endogenous TFP variations. During a recession, firms with specific productivity below an endogenous threshold exit the market, so that the average productivity and the TFP increase. The opposite occurs in an expansionary period. As in the seminal contribution by Bilbiie et al. (2012), firms enter the market up to the point where the expected discounted value of the future profits equals the sunk cost of entry. The investment in new firms is financed by households through the accumulation of shares in a portfolio of firms. This implies that the stock price fluctuates endogenously in response to shocks.====Using the theoretical model, we estimate a set of structural parameters and quantitatively assess the importance of entry and exit for the transmission of the shock to the economic activity. To better understand the role of firm dynamics we estimate the same model without firm dynamics. Both models are estimated using limited information impulse response functions matching techniques (in the spirit of Christiano et al. (2005), Basu and Bundick (2017), Mumtaz and Theodoridis (2019)). For both models, the data used in the estimation are the FAVAR-implied responses of the growth rate of real GDP, consumption, investment, consumer price index, and one-year government bond rate. The estimated models are used to calculate the responses of the variables to an unexpected increase in monetary uncertainty. The baseline model with firm dynamics produces an amplification of the shock with respect to the model without firm dynamics. Importantly, the estimates indicate that our baseline model requires a lower degree of price rigidity and nominal wage rigidity than the model without entry and exit. In this respect, we find our empirical result consistent with the recent theoretical result shown by Bilbiie and Melitz (2020). The two authors show that a simple NK model with endogenous entry-exit amplifies the response of economic activity to shocks when the model is approximated to an order higher than one. We show that with this amplification due to frictional entry, our baseline model requires a lower degree of nominal rigidities. Furthermore, the degree of wage rigidities estimated in our baseline model is higher than price rigidities, thus being in accordance with the empirical literature. The baseline model is more in line with the empirical evidence provided by the FAVAR model than the model without firm dynamics. By construction, the model with constant firms cannot replicate the dynamics of firm entry and exit. In that model, responses of the TFP and stock price are muted. Moreover, the fall in output, consumption, investment in physical capital is lower than in the baseline model. We take the results as an indication that both firm dynamics and firm heterogeneity are crucial in allowing the theoretical framework to replicate the evidence found in the FAVAR analysis.====This paper relates to two main strands of literature. It relates to the literature studying the macroeconomic effects of policy uncertainty shocks. Second, it adds to the literature investigating the role of firm dynamics for the business cycle analysis. After Bloom (2009), many papers discuss the macroeconomic impact of uncertainty shocks.==== Among them, some contributions focused on the consequences of policy-related uncertainty shocks over the business cycle, e.g. Fernández-Villaverde et al. (2015), Born and Pfeifer (2014), Mumtaz and Surico (2018). Overall, this literature points to the relevance of uncertainty shocks in explaining a large share of the fluctuations in the business cycle, and the contractionary effects on the main real variables, namely output, employment, consumption, and investment. Several contributions also drew on the availability of measures of policy uncertainty to evaluate the impact of these shocks on the economy. Considering monetary policy uncertainty, Baker et al. (2016) and Husted et al. (2019) construct text-based measures of uncertainty Istrefi and Mouabbi (2018) build up a measure of uncertainty stemming from the disagreement among professional forecasters on short- and long-run interest rates. Using a term structure model, Creal and Wu (2017) estimate the stochastic volatility of the monetary policy rule. Swanson (2006), Bundick et al. (2017), Bauer et al. (2021) use market-based measures of monetary policy uncertainty to document the effects on the financial market. This paper relates to this literature using a market-based measure of monetary policy uncertainty in the empirical analysis and assuming stochastic volatility in monetary policy shocks in the model. Importantly, this paper highlights the importance of the firm dynamic channel to interpret monetary policy uncertainty shocks. Most of the previous literature on the economic impact of higher monetary policy uncertainty indeed limited the analysis to the effects on the intensive margin of investment, that is on the decisions about new investments of firms already participating in the market. Surprisingly, the effects of uncertainty shocks on the extensive margin of investment concerning the firms' decisions about participating in the market have been largely ignored in the literature. To our knowledge, only Brand et al. (2019) has already studied in a macroeconomic model the effects of second-moment shocks on firm creation and destruction. Brand et al. (2019) build up and estimate a theoretical model with search and monitoring costs in the credit market to study how the higher dispersion in firm productivity affects macro-financial aggregates and firm dynamics. We differ from their contribution along at least three dimensions. First, they provide an alternative way to formalize firm dynamics based on search frictions between entrepreneurs and banks. Second, while we focus on the effects of monetary policy uncertainty shocks, they consider uncertainty in firms' idiosyncratic productivity. Third, they do not provide evidence on firm dynamics at the industry level.====The impact of firm dynamics on business cycle fluctuations has been extensively studied in papers investigating the effects of first moment shocks, that is level shocks. The seminal paper by Bilbiie et al. (2012) in the DSGE literature shows that endogenous entry generates a new and potentially important endogenous propagation mechanism for real business cycle models. Among others, Jaimovich and Floetotto (2008), Lewis and Poilly (2012), Etro and Colciago (2010), Clementi and Palazzo (2016), Lewis and Stevens (2015) provide evidence that the number of producers varies over the business cycle and that firm dynamics may play an important role in explaining business cycle statistics. Bilbiie et al. (2014) consider a DSGE model with monopolistic competition and sticky prices and find that deviations from long-run stability of product prices are optimal in the presence of endogenous producer entry and product variety, whereas price stability would be optimal in the absence of entry. Hamano and Zanetti (2014) and Casares et al. (2020) introduce endogenous firms exit in a DSGE model, but consider different timing and exiting schemes. While Hamano and Zanetti (2014) study the effects of a negative technology shock in a simple RBC model, Casares et al. (2020) consider a medium-scale model and estimates the effects of a set of level shocks on business cycle dynamics. Different from our framework, in their paper firms exit at the end of the production period, implying that the average productivity remains exogenous and constant even in the short run. This prevents the TFP from varying along the business cycle. Closer to our theoretical framework is Rossi (2019), who however considers a simple small-scale New Keynesian model with endogenous entry and exit interacting with banking frictions to study the effects of first-moment shocks to the aggregate productivity level.====Our paper then makes two clear contributions. First, it extends the literature on policy uncertainty shocks by considering the role of firm dynamics from an empirical and theoretical perspective. To the best of our knowledge, the role of firm dynamics in propagating a monetary policy uncertainty shock has not been investigated in the existing literature. We show that this feature is a crucial component in amplifying the effect of this shock in DSGE models. Second, from an econometric perspective, the paper proposes a FAVAR model that allows for mixed-frequency and missing data, allowing us to utilize series on aggregate and industry-specific firms' entry and exit which are available at a lower frequency and contain missing observations.====The remainder of the paper is organized as follows. Section 2 introduces the FAVAR model and provides empirical evidence. Section 3 spells out the DSGE model economy. Section 4 describes how we set the structural parameters of the DSGE and shows dynamics that follow the monetary policy uncertainty shock. Finally, Section 5 concludes. Technical details on the FAVAR estimation and data used, as well as empirical robustness checks, are left in the Technical Appendix.",Monetary policy uncertainty and firm dynamics,https://www.sciencedirect.com/science/article/pii/S1094202522000096,24 February 2022,2022,Research Article,34.0
Güner İlhan,"University of Kent, United Kingdom of Great Britain and Northern Ireland","Received 19 June 2019, Revised 4 August 2021, Available online 31 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2021.11.005,Cited by (0),". I calibrate the model to U.S. data and study the quantitative properties of the model. By explicitly examining the transition path after the change in subsidy, I highlight the tradeoff between the short-run level of consumption and long-run growth. I find that the optimal combination of the subsidy rates, as a fraction of firm R&D expenditures, is 84 percent in consumption sector and 88 percent in investment sector. By moving from the baseline subsidy rates (10 percent in both sectors), society can achieve a 20 percent welfare gain in consumption equivalent terms. The annual GDP growth rate increases from 1.5 percent to 3.2 percent by this change in subsidy. Finally, I show that it is always optimal to subsidize R&D spending in investment sector at a higher rate than that in the consumption sector when the government's subsidy budget is limited.","Government support for business research and development (R&D) varies across the globe. In 2011, the United States federal government's total support to business R&D was 0.26% of its GDP, compared to 0.43% in Korea and 0.01% in Mexico [OECD (2015)]. Inter-country variation in government support for R&D suggests that setting the optimal amount of support is complex, potentially varying across sectors. In this paper, I quantify the optimal amount of government subsidy to business R&D.====I build an endogenous growth model with firm dynamics to analyze the optimal R&D subsidy. I use the model and firm dynamics data to identify inefficiencies in the R&D expenditures of two sectors that have different characteristics: the consumption-goods sector and the investment-goods sector. Next, I characterize the subsidy rates required to address inefficiencies in innovation in these sectors. I base my model on the seminal work of Klette and Kortum (2004), where innovation by incumbent and entering firms generates firm dynamics and drives macroeconomic growth. Klette and Kortum show that their model can qualitatively account for various stylized facts about firm dynamics. Hence, I identify the inefficiencies in firm R&D expenditures within a framework that has a good fit on the firm dynamics data. I extend the Klette and Kortum model by introducing capital stock and by having not only a consumption-goods sector, but also an investment-goods sector. Differences in the rates of firm entry and expansion across these sectors and the sustained decline in the prices of investment goods relative to consumption goods imply that in these sectors there are different magnitudes of inefficiency and rates of technological change.==== A model that recognizes the heterogeneity of innovative activity across sectors will be more informative about the growth and welfare implications of R&D subsidies.====In this paper, an innovation is modeled as an increase in the quality of an existing good in the market along a quality ladder. The consumption-goods and investment-goods sectors consist of many differentiated products, each produced by a production line. In this setting, a firm is simply a collection of the production lines it possesses. An innovating firm captures the market of the innovated product from the existing producer and earns monopoly rents for as long as the innovator holds the blueprints of the highest quality versions of the goods it produces. The firm loses these rents following innovation on the same goods by other firms. Firms expand and shrink according to this creative-destruction process, entering the market when they successfully innovate and exiting if they lose the blueprints of all the goods they produce.====The model summarizes R&D activity in the economy through the ==== in each sector, defined as the ratio of the measure of differentiated products innovated at an instant to the measure of total products in that sector. The creative-destruction process associated with the market economy generates innovation rates that differ from the socially optimal innovation rates in each sector. Such inefficiencies arise due to differences between the innovator's objective and societal welfare. The innovator reaps monopoly rents as long as it holds the blueprints of the highest-quality version of the product she innovated. In contrast, society benefits from innovation due to the extra production from the innovation. Relatedly, in contrast to the limited lifetime of monopoly rents accruing to the innovator, the social benefit of any given innovation lasts forever. There are four differences between the innovator's objective and social welfare. On the one hand, an entrepreneur's inability to appropriate all of the consumer surplus it creates causes market economy innovation rates to fall below those of social planner levels (====). Also, the limited time of monopoly rents accruing to the innovator contributes to under investment in innovation (====). On the other hand, an entrepreneur does not account for the profit loss imposed on the current producer of the product that it takes over, and this moves market innovation rates above the socially efficient level (====. Finally, the monopoly power of the innovator causes factor prices to differ from the marginal products of the factors of production, and leads to over investment in innovation in the market economy (====).==== Depending on the sizes of these distortions, market economy innovation rates can be below or above the socially efficient levels. Therefore, a government may wish to employ tax/subsidy systems to correct the distortions in the economy, thereby increasing household welfare.====These distortions lead not only to deviations from the socially optimal levels of industry innovation rates but also to the misallocation of innovative resources across sectors. In particular, holding the total R&D labor constant at the competitive equilibrium level, the social planner can generate welfare gains by reallocating research labor across industries. We observe such misallocations even when the two sectors have identical innovation functions, suggesting that sectors vary in their exposure to these distortions based purely on their location in the supply chain. Put differently, the externalities distort the allocation of research labor in (potentially) different directions. For example, the monopoly power effect tilts innovation rates toward the consumption sector, while the inter-temporal spillover effect pushes innovation rates toward the industry with the lower socially optimal innovation rate. The former is a result of relative profits in the consumption sector exceeding the relative effectiveness—in the sense of promoting economic growth—of innovation in the consumption sector. The latter is a result of the inter-temporal spillover effect being larger in the industry with the higher innovation rate. In an environment with identical innovation functions across industries, the inter-temporal spillover effect distorts allocation of research labor toward the investment sector.====The optimal amount of subsidy for each sector depends on the elasticity of R&D with respect to the subsidy and the magnitudes of externalities in each sector. Various studies have estimated the former using data on firm level R&D expenditures and changes in government subsidy rates [Bloom et al. (2002), Hall et al. (2010), CBO (2005)]. In contrast, the magnitudes of externalities are not observable. To infer the sizes of the externalities in each sector and devise the optimal R&D subsidy system, I identify model parameters related to innovation that are obtained from the model's implications on firm dynamics. Recent literature emphasizes that the firm dynamics data contains important information on the innovation process. For example, Klette and Kortum (2004) qualitatively generates many empirical facts on the firm size distribution and firm growth rates. Lentz and Mortensen (2008), whose model is based on Klette and Kortum, estimate model parameters from Danish data, and the model quantitatively fits related firm dynamics moments.====Differences in the firm dynamics of the two sectors, as observed in the US data, are consistent with different magnitudes of externalities in the two sectors. First, the model relates the expected lifetime of monopoly rents that accrue to an innovator after successful innovation to the entry rate in the firm's sector (the inter-temporal spillover). Hence, a high observed entry rate in the consumption sector suggests a large inter-temporal spillover effect. Second, the business-stealing effect is driven by the difference between the profit an innovating firm captures and the ====, the extra production generated by the innovation, net of the profits lost by the incumbent producer. The model relates this additional innovation-induced production to the size of the quality improvement (the quality ladder step size), and relates the quality ladder step size to the GDP growth rate and the growth rate of the investment goods price relative to the consumption goods price. Hence, a high observed consumption growth rate suggests a large quality ladder step size in both industries; a high observed growth rate in the relative price of the investment goods (in absolute value) suggests a larger quality ladder step size in the investment sector than in the consumption sector; and a large quality ladder step size suggests a small business-stealing effect. The calibration exercise shows that the consumption-goods sector has lower quality ladder step size than the investment-goods sector, suggesting a larger business-stealing effect in the consumption-goods sector. In related work, Ngai and Samaniego (2011) study US data in a multi-sector endogenous growth setting, and document heterogeneity in innovation and production functions, and in consumer preferences across different investment good producing industries. In contrast, I focus on the heterogeneity between the consumption goods and investment goods sectors, and conduct optimal R&D subsidy analysis.====As explained above, market innovation rates are inefficient. I gauge the degree of under- or over-investment in innovation by solving the social planner problem, where a social planner dictates firms' R&D and production decisions subject to their innovation functions. Over the long-run, the social planner sets innovation rates that are substantially higher than the market rates in each sector: 13 percentage points higher in the consumption industry and 17 percentage points higher in the investment industry. Over the long term, the increased innovation rates correspond to a 1.7 percentage point increase in the GDP growth rate. Starting from the balanced growth path of the market economy with 1.5% GDP growth rate, the social planner immediately decreases the GDP growth rate to less than 1 percent, before the growth rate gradually increases to its new balanced growth path value of 3.2 percent. However, consumption and GDP follow different trajectories. Like the consumption growth rate, the level of consumption decreases initially as more labor is employed in research. Although the consumption growth rate increases gradually, it remains below its market economy balanced growth path level for some time. Eventually, the consumption growth rate converges with its balanced growth path level of 3.2 percent. Long-run consumption growth outweighs the short-run consumption loss, and the transition from the market economy balanced growth path to the social planner balanced path leads to an almost 21.5 percent welfare gain to households, as measured in consumption–equivalent terms. Thus, the market is under-investing in innovation, and a benevolent government can increase the household welfare by subsidizing R&D.====The amount of resources allocated to R&D at the social planner's balanced growth path is about six times the market economy resource allocation to innovation. This is larger than the levels reported in recent articles that employ related models but different methods. For example, Lentz and Mortensen (2008), estimated on Danish data, consider a setting where firms differ persistently in their ability to create higher quality products. Based on the estimates from this paper, Lentz and Mortensen (2015) show that the social planner would increase resource allocation to innovation threefold compared to market outcome, thereby generating a 21 percent welfare gain, as measured in the tax to social planner consumption. My estimated 21 percent welfare gain accounts for the transition path, a factor omitted in the Lentz and Mortensen analysis, which calculates the welfare gain by comparing the steady states of the market and the social planner economy.====I consider government intervention in a decentralized environment through subsidies to R&D activity and entry, all financed by lump-sum taxation of households. In my benchmark calibration, subsidizing 84 percent of consumption sector incumbents' R&D expenditures and 88 percent of investment sector incumbents' R&D expenditures generate a welfare gain (20.3 percent) close to that achieved by the social planner. In addition, the government employs an entry subsidy to equate the marginal social cost of entrant innovation and the marginal social cost of incumbent innovation. This result suggests that government can substantially increase the societal welfare by heavily subsidizing innovation.====Although replicating the socially optimal outcome may, in principle, require time-varying subsidy rates, my analysis reveals that a time-invariant subsidy system generates welfare gains that are comparable to those from moving to the social planner's allocation. Similarly, Grossmann et al. (2013) calculate socially optimum time-dependent R&D subsidy rate and report negligible welfare losses from setting instantaneously the R&D subsidy rate at its long-run value rather than employing a time-varying R&D subsidy rate. They also show that the optimal R&D subsidy is approximately 81.5%. Both results are in line with my findings. Akcigit et al. (2016) address optimal R&D policy in an environment with firm heterogeneity in research productivity and asymmetric information about this research productivity. They show that the optimal subsidy system depends on many factors, including the age of innovating firms, and the current and lagged product quality and R&D expenditures of the firms. Since there are no information asymmetries in my model and per-good research productivity is constant across firms within a sector, the optimal R&D subsidy according to Akcigit et al. (2016) would be constant across all firms in any given sector.====Subsidizing incumbent firms' R&D expenditures to maximize welfare gains requires taxes on the order of 16 percent of GDP. For reasons exogenous to my model, such large transfers to businesses might not be attainable. With this in mind, I calculate the optimal government R&D subsidy to industries when the government's transfer budget is constrained by some exogenous factors. If the government can increase its R&D subsidy budget as a share of GDP by 10 percent, it can achieve a 0.2 percent welfare gain by taxing the consumption sector incumbents' R&D expenditures by 1.1 percent and subsidizing the investment sector incumbents' R&D by 15.3 percent. I repeat this exercise with different transfer budget constraints and find that it is always optimal to subsidize the investment sector at a higher rate than the consumption sector. In a similar exercise, I show that if a social planner is allowed to increase R&D labor by 10 percent, she can achieve a welfare gain of 2 percent by allocating 15.6 percent innovation rate to the investment sector and 14.5 percent innovation rate to the consumption sector. Therefore, there is a non-negligible welfare gain of subsidizing R&D even if there are limits to resources allocatable to innovation.====Comparing my model with the Atkeson and Burstein (2019) (AB hereafter) model highlights the main contribution of this research, the analysis of optimal innovation subsidies in an environment with heterogeneous sectors. AB build a model that nests many endogenous and semi-endogenous growth models. They develop a method to linearly approximate output and productivity trajectories after a policy-induced change in the innovation intensity of the economy. Their methodology relies on a few critical assumptions, one of which is conditional efficiency: any given level of innovative resources in the economy is efficiently allocated across sectors. Whenever this assumption does not hold, their analysis applies only to proportional changes in innovation subsidies to different agents in the economy. As explained above, this key assumption does not hold in my model. Further, my methodology applies to non-proportional changes in subsidies to firms in different industries. To be clear, under the assumptions of the AB model, the AB method approximates output and productivity trajectories quite well with more politically feasible innovation policies (a 10% increase in policy-induced changes in R&D labor). AB also have a richer model in other aspects; to name a few, different levels of inter-temporal spillovers, technological progress with quality ladders as well as technological progress with expanding varieties, and so on.====Subsidizing only the investment sector produces a larger welfare gain than subsidizing only the consumption sector. Two opposing factors contribute to the difference in welfare gains. First, although each sector has a similar elasticity of innovation with respect to the user cost of R&D, the investment sector has a higher innovative step. Hence, any decrease in the user cost would lead to similar changes in innovation rates, but a given change in investment sector innovation leads to a higher consumption growth rate and, hence, a larger welfare gain than a similar change in consumption sector innovation. Second, an increase in the investment sector innovation rate leads to a larger reduction in the price of investment goods, which, in turn, increases the user cost of capital, resulting in a lower accumulation of capital. Consequently, consumption production grows more slowly than it would otherwise. During earlier periods, the investment-sector-subsidized economy has a lower consumption than the consumption-sector-subsidized economy. Overall, the first factor dominates and the welfare gain of subsidizing investment sector R&D is higher.====To achieve welfare-maximizing innovation rates, the government needs to subsidize innovation at roughly 85 percent. This large subsidy rate follows largely from two factors. First, there are significant distortions in the economy. As explained above, using related models, Atkeson and Burstein (2019) and Lentz and Mortensen (2015) find substantial under-investment in innovation in the market economy. Similarly, Jones and Williams (2000) find that the market economy typically under-invests in innovation. Second, R&D subsidies encourage innovation by decreasing the cost of innovation, but they also discourage incumbent innovation by reducing the expected lifetime of an innovation. A higher subsidy leads to a higher firm value, which increases the entry rate. When the entry rate increases, an incumbent firm is more likely to lose its monopoly rents by successfully innovating, and this reduces the expected time period of monopoly rents and the value of innovation (inter-temporal substitution effect increases). Thus, innovation will be discouraged. To compensate for the shortened expected lifetime of innovation, firms need to be subsidized even more.====This paper is organized as follows. Section 2 describes the model while Section 3 theoretically characterizes distortions in the economy and analyzes the impacts of the distortions on the allocation of innovative resources across sectors. Section 4 calibrates the model. Section 5 numerically compares market outcome to the social planner's equilibrium. Section 6 characterizes the subsidy system that would maximize household welfare. Section 7 compares my results with the results of Atkeson and Burstein (2019) and highlights my contributions. Section 8 concludes.",Growth and welfare implications of sector-specific innovations,https://www.sciencedirect.com/science/article/pii/S1094202522000059,31 January 2022,2022,Research Article,35.0
"Dai Min,Jiang Yipeng,Liu Hong,Xu Jing","The Hong Kong Polytechnic University, Hong Kong,National University of Singapore, Singapore,Washington University in St'Louis and CHIEF, United States of America,Renmin University of China, China","Received 23 April 2020, Revised 17 September 2021, Available online 31 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2021.11.003,Cited by (1),"Extant theories on the disposition effect are largely silent on most of the disposition-effect related trading patterns, including the V-shaped probabilities of buying and selling against unrealized profit. On the other hand, portfolio rebalancing and learning have been shown to be important, even for retail investors. We show that rational rebalancing with transaction costs and unknown expected returns can generate many disposition-effect-related trading patterns, including the V-shape results. Our paper complements the extant theories by suggesting that portfolio rebalancing may also constitute a significant driving force behind the disposition effect and the related patterns.","The disposition effect, which is the tendency of investors to sell winners while holding onto losers, has been widely documented in the empirical literature. For example, using data containing 10,000 stock investment accounts in a U.S. discount brokerage from 1987 to 1993, Odean conducts a set of tests of the disposition effect hypothesis in his seminal work of Odean (1998). He concludes that the disposition effect exists across years and investors.==== Closely related to the disposition effect, Ben-David and Hirshleifer (2012) show that the plots of the probabilities of selling and of buying more of some existing shares against unrealized profit both exhibit V-shape patterns, i.e., as the magnitudes of unrealized profits/losses increase, these probabilities also increase. Theories based on prospect theory, mental accounting, regret aversion, and gain/loss realization utility to explain the disposition effect have dominated the literature.==== However, it is difficult for extant theories to explain the V-shape patterns. In addition, as far as we know, there have been no theoretical models proposed to explain other well-documented disposition effect-related patterns, such as: 1) investors may sell winners that subsequently outperform losers that they hold (Odean (1998)); 2) the disposition effect is stronger for less sophisticated investors (Dhar and Zhu (2006)); 3) the disposition effect may increase with return volatility (Kumar (2009)); and 4) investors are reluctant to repurchase stocks previously sold for a loss, as well as stocks that have appreciated in price subsequent to a prior sale (Strahilevitz et al. (2011)).====Another strand of literature documents that portfolio rebalancing is an important driver behind even retail investors' trading.==== For example, Calvet et al. (2009) find strong household-level evidence of active rebalancing by retail investors who typically hold a small number of stocks. Using a sample of Japanese retail investors from 2013 to 2016, Komai et al. (2018) find that investors tend to conduct contrarian trades, as predicted by standard portfolio rebalancing models. In addition, trading patterns consistent with rational learning by investors have been widely documented. For example, Grinblatt and Keloharju (2001) report that past returns and historical price patterns affect trading decisions in ways that are consistent with rational learning. Kandel et al. (1993) and Banerjee (2011) provide evidence that investors learn about information contained in asset prices and revise their trading strategy accordingly. Furthermore, even though transaction costs have declined in recent years, bid-ask spreads and other trading costs (e.g., brokerage fees and time costs) remain significant, especially for retail investors. As a result, investors do not trade continuously (e.g., Davis and Norman (1990), Liu (2004)).====As extant literature has shown (e.g., Odean (1998)), portfolio rebalancing without any market friction cannot explain the disposition effect. Based on the aforementioned empirical evidence on the importance of portfolio rebalancing, learning, and transaction costs, we develop an optimal portfolio rebalancing model with transaction costs and incomplete information (in the form of unknown expected returns) to examine whether rational portfolio rebalancing in the presence of these frictions can help explain the disposition effect and related trading patterns of retail investors. We show that, indeed, in the presence of these frictions, portfolio rebalancing can lead to the disposition effect and many of the related patterns, including the V-shape patterns. While we believe that behavioral types of explanations are essential in understanding the disposition effect and the related trading patterns, our finding suggests that portfolio rebalancing may also constitute a significant driving force behind these results and thus complement the extant theories.====More specifically, we consider a portfolio rebalancing model in which a small retail investor (i.e., one who has no price impact) can trade a risk-free asset and multiple risky assets (“stocks”) to maximize the expected utility from the final wealth at a finite horizon.==== The stocks' expected returns are unknown and the investor updates the conditional distributions of the expected returns after observing past returns. Trading the stocks is subject to proportional transaction costs.====To make our expositions as clear as possible, we consider two sets of model parameters. We begin with a case where stocks have homogeneous return-risk profiles and uncorrelated returns. This is a clean setting to illustrate the main working mechanisms. Then, we show that these mechanisms continue to work with calibrated parameter values with heterogeneous risk-return profiles and correlated returns.====We show that the optimal rebalancing strategy implied by our model can exhibit a disposition effect. For example, for a reasonable set of parameter values, the ratio of the number of realized gains to the number of all gains (realized gains plus paper gains), i.e., ====, is 0.331, while the ratio of the number of realized losses to the number of all losses (realized losses plus paper losses), i.e., ====, is 0.168. These results indicate that the investor exhibits greater propensity of realizing gains than losses.====The main driving force for the disposition effect displayed in our model is the “exposure effect.” Intuitively, for a risk-averse utility maximizing investor, it is optimal to keep the exposure to a stock within an upper bound and a lower bound to trade off risks and returns. A rise in the price of a stock results in a gain and increases the investor's risk exposure to this stock. If the exposure increases above the upper bound, then it is optimal to sell and thus realize a gain. A fall in the stock price results in a loss and decreases the investor's risk exposure. If the exposure decreases below the lower bound, then it is optimal to buy, ====, additional shares. It is this asymmetry (i.e., selling with a large gain, but buying with a large loss) due to the exposure effect that makes investors realize gains more often than losses. On the other hand, if the exposure after a gain or loss is still within the bounds, the investor does not trade, due to the presence of transaction costs. Because selling a stock with a loss requires the upper bound of the risk exposure to be reached after a decline in the stock price and buying additional shares after a loss requires the lower bound to be reached, investors hold onto losers after small losses. Thus, the combination of the exposure effect and the presence of transaction costs makes investors tend to sell winners and hold onto losers, consistent with existing empirical findings. Because the exposure effect exists for any risk-averse utility maximizing investors, the above qualitative results apply to all risk-averse preferences, such as CRRA, CARA, and Epstein-Zin preferences.====It is empirically found that past returns also significantly affect investors' trading behavior. Using a data set containing all common stock trades of Finnish household investors from 1995 to 2000, Kaustia (2010) provides the first evidence that the investors' selling propensities increase in the magnitude of gains. Ben-David and Hirshleifer (2012) further demonstrate that the probability of buying more and of selling is both greater for positions with larger paper gains or larger paper losses.==== Theories based on the static prospect theory, or regret aversion, predict that the larger the loss, the less likely it is for investors to sell, and the larger the gain, the less likely it is for investors to buy, which is opposite of the V-shape pattern. Ben-David and Hirshleifer (2012) argue that the V-shape pattern can be consistent with change of perceptions and faiths (belief revision). Assuming that an investor can obtain a burst of reference-dependent utility from a sale in a dynamic prospect theory setting, Ingersoll and Jin (2013) demonstrate that the probability of selling can increase with the magnitude of losses because there is a benefit of realizing losses to reset references in this dynamic setting.==== Peng (2017) attributes the V-shaped selling pattern to irrational extrapolation of past returns.====We show that the V-shape patterns for both the purchase probability and the sale probability are consistent with the optimal trading strategy in our portfolio rebalancing model. Intuitively, two opposing forces exist in our model: the “exposure effect” and the “learning effect.” As previously explained, the exposure effect tends to make investors sell after a large gain but buy after a large loss (“buy low, sell high”), exhibiting a contrarian trading strategy. In contrast, the learning effect tends to make investors buy after a large gain and sell after a large loss (“buy high, sell low”), exhibiting a momentum trading strategy. This is because investors revise upward their estimate of expected returns after gains and do the opposite after losses. The patterns of the probability of selling increasing with the magnitude of gains and the probability of buying increasing with the magnitude of losses are driven by the exposure effect. On the other hand, because there is a greater increase (decrease) in the estimate of the expected return after observing a large gain (loss), the probability of buying more (selling) is greater for a large gain (loss) than for a small gain (loss).==== Thus, the patterns of the probability of buying more increasing with the magnitude of gains and the probability of selling increasing with the magnitude of losses are driven by the learning effect. The relative strength of the two effects determines the trading direction. In our model, it is the coexistence of the exposure effect and the learning effect that drives the V-shape patterns.====Moreover, in contrast to the existing literature, our model can generate many other disposition effect-related patterns documented in empirical studies, such as those four stated at the end of the first paragraph. As in the previous results, the driving forces behind these results are also the presence of, and the interaction between, the exposure effect and the learning effect. For example, less sophisticated investors may have less learning capability than more sophisticated investors. As a result, the learning effect may be smaller and the disposition effect may be stronger for less sophisticated investors.====For ease of reference, we summarize the main results and driving mechanisms in Table 1, and a comparison between our paper and some existing papers in Table 2.====It is well known that, with capital gains tax, realizing losses sooner and deferring capital gains can provide significant benefits (e.g., Constantinides (1983)). This force acts against the disposition effect. We demonstrate that, consistent with the empirical findings of Lakonishok and Smidt (1986), the disposition effect can still arise in an optimal portfolio rebalancing model with capital gains tax and transaction costs. Intuitively, when a stock's price appreciates sufficiently, the investor's risk exposure to this stock can become too high, and the benefit of lowering the exposure by a sale can dominate the benefit of deferring the realization of gains. In addition, with transaction costs, realizing losses immediately are no longer optimal, and deferring even large capital losses may be optimal. This is because the extra time value obtained from realizing losses sooner can be outweighed by the transaction cost payment, even when the transaction cost is small.====Our model offers some new empirically testable predictions for future studies. For example, our model predicts that: (1) conditional on return volatility, the magnitude of the disposition effect is greater for stocks for which there is more public information, because for these stocks much is already known, and thus the learning effect is smaller; (2) investors with a more diversified portfolio or a better hedged portfolio have a weaker disposition effect, because for these portfolios the exposure effect is smaller; and (3) the V-shaped trading patterns are more pronounced for stocks with less public information, because the learning effect is stronger for these stocks.====Although we consider a small investor whose trades have no price impact and thus adopt a partial equilibrium approach, the disposition effect can arise in equilibrium (e.g., Basak (2005), Dorn and Strobl (2009)). For example, Dorn and Strobl (2009) demonstrate that, in the presence of information asymmetry, the less informed become contrarians while the more informed become momentum traders in equilibrium. The less informed investors in their model trade in the same way as the investor in our model, and thus displays the disposition effect in equilibrium. The fact that in equilibrium for each investor who sells there must be a counterparty who buys does not imply that there is no disposition effect on average. This is because it is possible that a greater number of retail investors with a stronger disposition effect trade with a small number of institutional investors, for example, and most of the studies of the disposition effect and the related trading patterns focus on retail investors.====The remainder of the paper proceeds as follows. In the next section, we present the main model and theoretical analysis. In Section 3, we numerically solve the model and conduct simulations to illustrate that our model can generate most of the disposition-effect related patterns. We also show that the disposition effect can exist even with capital gains tax. We conclude with Section 4, and all proofs are provided in the Appendix.",A rational theory for disposition effects,https://www.sciencedirect.com/science/article/pii/S1094202522000072,31 January 2022,2022,Research Article,36.0
"Dossche Maarten,Gazzani Andrea,Lewis Vivien","Directorate General Economics, European Central Bank, Sonnemannstraße 20, 60314 Frankfurt am Main, Germany,Directorate General for Economics, Statistics and Research, Bank of Italy, Via nazionale 91, 00184, Rome, Italy,Research Centre, Deutsche Bundesbank, Wilhelm-Epstein-Straße 14, 60431 Frankfurt am Main, Germany","Received 4 May 2020, Revised 24 September 2021, Available online 31 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2021.11.006,Cited by (0),"Labor productivity is more procyclical in OECD countries with lower employment volatility. To capture this new stylized fact, we propose a business cycle model with employment adjustment costs, variable hours and labor effort. We show that, in our model with variable effort, greater labor market frictions are associated with procyclical labor productivity as well as stable employment. In contrast, the constant-effort model fails to replicate the observed cross-country pattern in the data. By implication, ==== has a greater effect on the cyclicality of labor productivity and on the relative volatility of employment when effort can vary.","This paper argues that the cyclicality of labor productivity – output per hour – is related to the different nature of labor adjustment across countries. Fig. 1 shows that countries with lower employment volatility are characterized by more procyclical labor productivity, that is, by a higher correlation between the cyclical components of output and labor productivity. The aim of our investigation is to explain this new stylized fact.====The sample period here is 1984q1 to 2019q4. The key message from Fig. 1 is robust to alternative filtering methods including the HP filter (Hodrick and Prescott, 1997), band pass filter (Christiano and Fitzgerald, 2003), Hamilton filter (Hamilton, 2018), or fourth difference filter.==== It is also robust to excluding the US from the sample, and to restricting the sample to the Great Recession, defined in Christiano et al. (2015) as the period from 2008q3 until 2013q2. The latter finding is interesting for two reasons.====First, a natural candidate explanation for the pattern in Fig. 1 is that technology shocks are the dominant source of business cycle fluctuations in countries with highly procyclical productivity, while demand shocks are more important in countries where the cyclicality of labor productivity is low. This is consistent with employment being rather stable in the former group of countries, and more variable in the latter. However, the Great Recession was a large shock that hit several countries simultaneously and, arguably, in similar ways. This suggests that the large cross-country variation in business cycle moments shown in Fig. 1 can be traced to structural differences across economies, which in turn led to differences in shock transmission, rather than the shock mix itself being idiosyncratic.====Second, the Great Recession is widely believed to have been driven by deficient demand, see for instance Christiano et al. (2015). In a demand-driven recession, a drop in labor productivity is difficult to explain with standard business cycle models in the absence of variable factor utilization. With unchanged technology, we expect firms to cut back their labor input as demand for their products declines. Labor productivity goes down only if labor falls by less than output. With capital fixed in the short run, this means that labor is utilized less intensively – i.e. effort falls – during the downturn. Variable capital utilization as in Christiano et al. (2005) could, as an alternative model feature, generate procyclical labor productivity without the necessity to endogenize labor effort. However, Lewis et al. (2019) show that effort clearly outperforms capital utilization in terms of explaining the Euro Area business cycle.====In this paper, our aim is thus to develop a model that can replicate the evidence in Fig. 1 without relying on cross-country differences in the relative importance of technology versus demand shocks. Rather, we focus on differences in labor market adjustment, coupled with variable labor utilization, as the key candidate explanation. In particular, we attribute the procyclicality of labor productivity to variations in effort, which in turn result from a reluctance of firms to adjust the workforce. This idea of labor hoarding dates back to Okun (1963) and Oi (1962); for an overview article, see Biddle (2014).====Employment protection remains restrictive in many countries, especially in the Euro Area (Deutsche Bundesbank, 2019). The OECD's employment protection legislation (EPL) index from 2019 ranges from 0.09 in the US to 3.61 in the Netherlands.==== Spain is a special case (Bentolila et al., 2012; Sala et al., 2012). While workers on permanent contracts enjoy a high degree of employment protection, temporary contracts with low firing costs are widespread. This dual labor market gives rise to US-style labor market flexibility, reflected in high employment volatility (see Fig. 1). There is also large variation in redundancy pay across countries; Lazear (1990) reports severance pay between zero and over 15 months of a worker's wage, with an overall value of 3.5. More recent data from the World Bank's Doing Business Report 2017 range from zero in the US to 27 weeks of salary in South Korea, and yet higher numbers for non-OECD countries.====High costs of laying off employees in times of low demand discourage labor adjustment along the extensive margin. Already Nickell (1979) found that hours fluctuations were higher and employment fluctuations lower after the 1966 Redundancy Payments Act increased the cost of dismissal in the UK. In a sample of 20 OECD countries over the period 1975-1997, Nunziata (2003) shows that stricter employment protection and looser working time regulations were associated with a lower variability of employment over the cycle. This finding is confirmed in more recent data by Gnocchi et al. (2015).====Today, around one half of the adjustment in total hours worked in the Euro Area is through changes in hours per employee rather than changes in employment (Dossche et al., 2019). Short-time work (STW) schemes and working time accounts, used extensively e.g. in Germany, make hours worked more flexible.==== Lydon et al. (2019) show that the STW take-up rate among firms is positively related to greater firing costs and more stringent employment protection.====Our final crucial model ingredient is variable labor utilization, or effort. Labor effort cannot be observed directly. However, several indirect measures suggest that it is procyclical: workplace accidents (Fairris, 1998; Boone and van Ours, 2002), sick leave (Leigh, 1985; Schön, 2015), indicators of bad health outcomes (Ruhm, 2000). Firms report that they pay more for labor in recessions than is strictly necessary (Fay and Medoff, 1985). The American Time Use Survey indicates that ‘non-work at work’ is, on the whole, countercyclical (Burda et al., 2020). Finally, self-reported work effort appears to be procyclical (Lewis and van Dijcke, 2020).====This paper develops a business cycle model with capital and three labor margins: employment, hours per worker and effort per hour.==== Importantly, firms face employment adjustment costs, which use up part of their output. Workers are expected to provide a certain amount of effective labor; they choose the combination of hours and effort per hour that minimizes their disutility from working (Bils and Cho, 1994). We show that, in a model with labor effort, greater employment adjustment frictions imply more procyclical labor productivity along with more stable employment, consistent with the observed cross-country heterogeneity. The constant-effort model fails to replicate the pattern in the data. As a consequence, labor market deregulation – a reduction in firms' employment adjustment costs – reduces the cyclicality of labor productivity by more when effort can vary than in the case where effort is constant. Variable effort is thus relevant for evaluating the effect of such a reform. More fundamentally, we argue that the cyclicality of labor productivity is not a reliable indicator of which type of shock – technology or demand – drives the business cycle, see also Shea (1999).====  Consistent with our cross-country evidence presented above, Mitra (2020) shows that the cyclicality of labor productivity declined more in US states and industries that experienced a larger drop in union density.====Ohanian (2010) shows that during the Great Recession, labor input fell strongly in the US, whereas labor productivity declined only by a small amount. Meanwhile, many European countries saw large drops in labor productivity, but not in employment. These findings are in line with the evidence in our Fig. 1. Viewed through the lens of a real business cycle (RBC) model, the European experience is consistent with adverse technology shocks, while the US experience is not. In this paper, we present empirical evidence for more countries and for a longer time span. More importantly, though, we seek a model that can generate the pattern of Fig. 1, keeping the underlying shock structure the same. This is the key difference between our paper and Ohanian (2010), who instead takes the model as given.====Perri and Quadrini (2018) explore the role of a financial shock as a driver of the Great Recession. They show that endogenous utilization and cross-country differences in labor adjustment costs are required to replicate the heterogeneous responses of labor productivity and employment in European countries versus the US. We would go further and argue that these two features are important in order to account adequately for business cycle fluctuations more generally, not only during that particular episode.====Llosa et al. (2014) show that a model with firing costs can explain cross-country differences in the relative importance of the intensive labor margin, i.e. changes in hours per worker, in total hours adjustment. The intensive margin of labor adjustment is more important for countries with greater firing costs. We show that introducing variable labor utilization (endogenous effort) helps to also match the procyclical nature of labor productivity observed in most OECD countries.====  The change in US labor productivity from pro- to countercyclical after 1984 has received a fair amount of attention.==== Different explanations for this change have been put forward. Barnichon (2010) proposes a search model with sticky prices and effort, where labor productivity responds in a procyclical fashion to demand shocks. The lower procyclicality of labor productivity in the more recent period is interpreted as the result of two events: smaller demand shocks and labor market deregulation that reduced the cost of adjusting employment and hours. McGrattan and Prescott (2012) extend the standard RBC model with intangible capital, i.e. accumulated know-how from investing in research and development, brands, and organizations. Insofar as it counts as an expense rather than being capitalized, it is not included in GDP. This leads to an underestimation of output movements. If labor input is correctly measured, the correlation of output and labor productivity falls as a consequence; productivity becomes less procyclical. Garin et al. (2018) propose a model of labor reallocation across sectors and show that a decline in the importance of aggregate shocks relative to reallocative shocks can account for the change in the cyclicality of US labor productivity. Following a reallocative shock, employment declines in adversely affected sectors as workers move to sectors with improved productivity, causing aggregate labor productivity to rise. Sectoral heterogeneity also plays a role in vom Lehn and Winberry (2019), who suggest that shocks to investment hubs have become more important after 1984, resulting in less sectoral comovement and less cyclical labor productivity. Schaal (2017) introduces idiosyncratic volatility shocks in a search-and-matching model. Greater volatility drives unproductive firms out of the market so that employment decreases, whereas aggregate productivity increases.====Galí and van Rens (2020) and Mitra (2020) instead propose a factor utilization margin, more specifically: variable labor effort. They argue that lower hiring and firing costs since 1984 in the US can explain why firms use the effort margin less to adjust effective labor input, but instead hire and fire workers more quickly, so that measured productivity becomes less procyclical. van Zandweghe (2010) also favors an explanation of the US productivity cyclicality sign switch based on structural changes in the labor market.====Arguably, most of the proposed structural changes to explain the fall in the cyclicality of US labor productivity (intangible capital, allocative shocks, investment hubs, idiosyncratic volatility shocks) have also occurred in other industrialized countries. This raises the question why labor productivity has remained procyclical in other large industrialized economies, such as the Euro Area or Japan. Moreover, as discussed above, labor markets function very differently across countries due to institutional differences (such as firing costs and working-time regulations). This suggests that an explanation based on differences in the functioning of labor markets may be more promising to understand the cross-country differences we observe in the cyclicality of labor productivity.====  The rest of the paper is structured as follows. Section 2 presents the model and explains our calibration strategy. Section 3 presents the dynamic effects of technology and demand shocks, and discusses the implications of allowing for variable labor effort. We also show how the size of the effort margin affects the cyclicality of labor productivity. We then consider the effects of reducing labor market rigidities when effort can vary, and contrast them with those of the constant-effort model. In Section 4, we introduce sticky prices in the model and investigate to what extent our results change. Other model modifications are discussed in a robustness analysis in Section 5. Section 6 concludes.",Labor adjustment and productivity in the OECD,https://www.sciencedirect.com/science/article/pii/S1094202522000060,31 January 2022,2022,Research Article,37.0
Bonchi Jacopo,"LUISS Guido Carli, Department of Economics and Finance and School of European Political Economy, Viale di Villa Emiliani 14, 00197, Rome, Italy","Received 18 May 2020, Revised 2 October 2021, Available online 26 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2021.11.004,Cited by (2), bubbles have a greater impact than ==== bubbles.,"Macroeconomic literature has studied the relationship between monetary policy and asset bubbles, focusing on the impact of the policy rate on bubbles (e.g., Bernanke and Gertler, 2001; Galí, 2014, Galí, 2021; Dong et al., 2020). This paper overturns the perspective, showing that bubbles can affect monetary policy by relaxing the zero lower bound constraint in a low interest rates environment.====The historical decline in the “natural” interest rate, which has reached extraordinary low/negative levels,==== has caused persistently low risk-free real and nominal interest rates, exposing the advanced economies to more frequent zero lower bound (ZLB) episodes. Against this backdrop, one aspect of the relationship between monetary policy and asset price bubbles becomes relevant: the potential impact of bubbles on the nominal interest rate.====The reason is straightforward. Wealth gains from asset bubbles are unevenly distributed across generations due to the life-cycle pattern of assets and debt. The older cohorts with large asset holdings experience a massive increase in net worth that discourages saving. On the other hand, younger cohorts are more prone to borrow and can take on higher debt via appreciated collaterals. As asset price bubbles redistribute wealth across generations by serving as a store of value and collateral, they can reduce the supply of saving and foster the demand for borrowing, absorbing the excess of saving underlying a low/negative natural interest rate. The relevance of this theoretical mechanism at the ZLB is the main object of the present paper, which aims to show that ==== bubbles can affect output in a low interest rates environment, unlike in normal times.====To that aim, I develop a three-period overlapping generations (OLG) model that features bubbles and non-neutral monetary policy. The benchmark model accounts for the life-cycle behavior of saving and net worth, and thus the natural interest rate is endogenously determined in the credit/risk-free assets market. Bubbles, formalized as intrinsically worthless assets, emerge rationally and play the twofold role of a store of value and (“bubbly”) collateral through which they can influence the natural interest rate. The effect of bubbles going through the natural rate has radically different implications for macroeconomic outcomes and welfare, whether the ZLB is binding or not.====When the natural interest rate is not negative, the economy lies in a steady state equilibrium with output at the potential and inflation at the target because the ====. The emergence of bubbles raises the natural interest rate along the transition to the new bubbly equilibrium, but this results in higher interest rates without altering output. On the other hand, a higher natural interest rate underlies a redistribution from young to old age without affecting the middle generation. Despite the bubbly collateral, bubbles crowd out credit, reducing the resources available for the borrowing-constrained young households. Equally, they raise the wealth in old age, boosting the consumption of the elderly. According to the model calibrations, this redistribution is welfare-reducing, given the greater weight to the young-age consumption losses in the lifetime utility.====In contrast, when the natural interest rate falls deeply in negative territory, the ====, and output and inflation gaps are negative in a bubbleless economy. If the upward pressure of bubbles drives the natural rate into non-negative/positive territory, the central bank can escape from the ZLB with resulting output gains. In this case, the model calibrations point to a welfare-enhancing bubble because the output increase positively affects the consumption/income in middle age, though bubbles still redistribute resources from the young to the old generation.====While the benchmark model points to the effect of redistributive bubbles on aggregate demand working through the natural interest rate, amplification mechanisms could interact with this channel. For example, the introduction of capital and endogenous fundamental collateral in the model shows that the “financial accelerator” can amplify the impact of bubbles on output greatly, not only closing the initial negative output gap but also fostering the potential output through capital accumulation. Indeed, the “financial accelerator” enlarges the initial output gains from the bubble emergence. This, in turn, determines an even higher income for the middle generation that can now both consume and save more, providing additional credit to the borrowing-constrained young households that accumulate capital and consume by pledging bubbly and fundamental collaterals. Hence, according to the extended model calibration, the stock of capital rises, along with the young-age consumption, and all generations are now better off in the final bubbly equilibrium.====Finally, I augment my benchmark model with an exogenous probability of bubble bursting and an incomplete credit market, accounting for the difference between leveraged and unleveraged bubbles. The distinction between the two bubble types helps to clarify the bubble's role of primarily influencing the natural interest rate. An unleveraged bubble serves only as a store of value, unlike a leveraged one used as collateral. While an ==== bubble raises the natural rate of interest, although it does not drive it into positive territory leaving the central bank stuck at the ZLB, a ==== bubble delivers a non-negative/positive natural interest rate, and the monetary authority can escape from the ZLB with consequently higher output gains.====The rest of the paper is structured as follows. Section 2 presents the related literature. I spell out the benchmark model with rational bubbles and monetary policy in Section 3, and I study its steady state, along with the transition to a bubbly equilibrium, in Section 4. Section 5 provides the two extensions of the benchmark model. Section 6 concludes the paper.",Asset price bubbles and monetary policy: Revisiting the nexus at the zero lower bound,https://www.sciencedirect.com/science/article/pii/S1094202522000047,26 January 2022,2022,Research Article,38.0
"Peck James,Setayesh Abolfazl","The Ohio State University, United States of America","Received 15 March 2021, Revised 8 January 2022, Available online 21 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.01.003,Cited by (0),We extend the Diamond-Dybvig model of bank runs to include a specification of ,"In the many years and many published articles following the bank runs paper of Diamond and Dybvig (1983), only a few papers have modeled the decision of ==== to deposit, much less the decision of ==== to deposit. This is peculiar, considering the vast amount of wealth that is invested by financial firms that do not provide insurance against being an “impatient” consumer with immediate consumption needs. The questions we address here are, how does the opportunity for consumers to invest outside the banking system, in investments that do not provide liquidity insurance, (1) affect the nature of the final allocation, (2) affect the nature of the optimal deposit contract, and (3) affect the fragility of the banking system?====We consider a model with ==== ex ante identical consumers, who invest some of their endowment directly (i.e., outside of the banking system) and deposit the rest of their endowment with the bank in period 0, before observing whether they will become impatient (requiring consumption in period 1) or patient (able to consume in period 2). In the model, there is a single technology available to the bank and to investments outside the banking system. We use the term “outside” to describe investments outside the banking system. A contract specifies a deposit level, ====, and period 1 withdrawals. Due to a sequential service constraint, withdrawals in period 1 depend on the consumer's place in line, but not on the number of consumers yet to withdraw in period 1.====We find that any allocation that the bank can induce (satisfying incentive compatibility and non-negativity constraints) with a given deposit level can also be induced with any higher deposit level. For the typical economy, the incentive compatibility constraint does not bind at the optimal contract when consumers deposit their full endowment. If so, we find that there is an interval of deposit levels yielding the efficient allocation in equilibrium. The intuition for this equivalence result is that the withdrawal amount augments the outside investment of impatient consumers by exactly the amount of consumption needed to achieve the optimal allocation. Since the optimal contract provides insurance against being impatient, by allowing impatient consumers more consumption than their endowment on average, the bank responds to lower deposits by magnifying the ratio of period 1 withdrawals to deposits. As we consider lower and lower deposit levels, at some point the efficient allocation cannot be achieved, due to a failure of incentive compatibility or non-negativity.====We also find that, in equilibria achieving the efficient allocation, lower deposit levels make the banking system more fragile. The lower the deposit level, the more tempted a patient consumer is to withdraw early in the no-run equilibrium, and the more tempted she is to join a run. The main reason is that, when deposits are lower, the bank must offer a larger proportion of its deposits as withdrawals in period 1 (i.e., more maturity transformation), in order to achieve the efficient allocation. A secondary reason is that withdrawals from the bank in period 1 are stored by a patient consumer until consuming in period 2, but she can keep her outside investment until period 2, receiving a higher return.====Our results have implications for the optimal size of the banking system. Under the conditions of our main result (Proposition 3), which are likely to hold especially for large economies, a limited banking system with less than full deposits is optimal. The reason is that the probability that a bank run occurs does not depend on whether deposits are full or partial; however, in the event of a run occurring, the misallocation of resources is smaller with partial deposits. Specifically, patient depositors do not have to liquidate their investments outside the banking system, so welfare is higher with partial deposits than full deposits when a run occurs.==== When the model is adjusted to allow the propensity to run to vary with the “risk factor” of the run equilibrium, a new tradeoff emerges. Under the conditions of Proposition 3, as we reduce the deposit level starting from full deposits, welfare increases for a given propensity to run. However, the propensity to run can increase, which introduces an effect that lowers welfare.====Finally, we consider the extended model in an online appendix, with the specification that outside investment held until period 2 yields an ==== higher return than bank investment held until period 2, where ==== is small but can be either positive or negative. We show that our main result is robust to the introduction of nonzero ====. That is, under the conditions of Proposition 3, the optimal contract entails partial deposits and a positive probability of a bank run on the equilibrium path.====Section 2 contains a literature review. Section 3 sets up the model, and Section 4 contains the results for the model with a zero propensity to run. Section 5 contains our most important result, for the model with a positive propensity to run. Section 6 contains an analysis of the model in which the propensity to run is a function of the risk factor of the run equilibrium. Section 7 contains a summary and discussion.",Bank runs and the optimality of limited banking,https://www.sciencedirect.com/science/article/pii/S1094202522000035,21 January 2022,2022,Research Article,39.0
"Leyva Gustavo,Urrutia Carlos","Banco de México, Research Department, Mexico,ITAM, Department of Economics, Mexico","Received 26 February 2021, Revised 8 December 2021, Available online 19 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.01.002,Cited by (2),"We document the evolution of labor markets of five Latin American countries during the COVID-19 pandemic, with emphasis on informal employment. We show, for most countries, a slump in aggregate employment, mirrored by a fall in labor participation, and a decline in the informality rate. The latter is unprecedented since informality had cushioned the decline in overall employment in previous recessions. Using a business cycle model with a rich labor market structure, we recover the shocks that rationalize the pandemic recession, showing that labor supply shocks and informal productivity shocks are essential to account for the employment and output loss and the decline in the share of informal workers.","The COVID-19 outbreak of early 2020 triggered a truly global crisis with already profound yet uncertain economic consequences. Policymakers worldwide responded by implementing immediate lockdown policies to arrest the spread of the virus at the cost of putting the global economy on hold. The Great Lockdown (Gopinath, 2020a) may already be singled out for the massive job losses and the sudden, unprecedented withdrawals from the labor market.====The so-called pandemic slump has affected the world unequally. Differences in compliance with confinement and social distancing policies, the resilience of labor markets, and the deployment of stimulus policies may all account for heterogeneous recoveries across countries. The Latin American region is a case in point.==== A unique feature that has remained entrenched in the region, accounting for half of employment, claims a decisive role across all three themes: informality.====Since informality is an enticing option for many to compensate for the loss of earnings in the formal sector, it imposes challenges in compliance to confinement policies (Loayza, 2020).==== Moreover, due to its frictionless nature, informal employment, though expected to lead the recovery in labor markets (Leyva and Urrutia, 2020a), could exert a dragging effect on output through its impact on labor productivity. Finally, precisely because informality acts outside the scope of the government, stimulus policies in the form of credits and transfers are expected to miss the targeted beneficiaries. Informality thus pervades the functioning of labor markets in Latin America, compounding the problem of managing the pandemic and steering the economy.====We start this paper by providing an overview of Latin American labor markets in the aftermath of the COVID-19 pandemic, with emphasis on informal work. We do this by exploiting our own constructed database of labor market stocks for five Latin American countries, comprising Brazil, Chile, Colombia, Mexico, and Peru (LA-5, for short, following IMF (2020)) and gross flows for the two largest economies in the region. For this, we rely on household and employment surveys publicly available. We first focus on Mexico and Brazil to document the following facts for the pandemic recession by comparing 2020.Q2 with the same quarter of 2019: (1) an unprecedented decline in employment rates, mirrored by a fall in participation rates; (2) slight increase in unemployment rates, coupled with an instant decline in the average duration of unemployment; (3) falling informality rates; and (4) less informal job creation from inactivity in Brazil while more informal job destruction to inactivity in Mexico. The most recent available data (2021.Q1) points to a rapid recovery of informal employment, leading to a rebound in the informality rate.====Compared to the Great Recession of 2008-9 for Mexico and the 2014-16 recession for Brazil, we highlight stark differences in the outgoing pandemic episode, namely the magnitude of the employment collapse and the response of the informality rate. Previously, the latter acted countercyclically (for Mexico, see Leyva and Urrutia (2020a)), but now it felt significantly on impact.====We also contribute to the understanding of the pandemic slump by documenting three novel margins of adjustment: temporary layoffs, absent employees, and telework. We see the first margin absorbing and the second mitigating the loss of employment at the trough of the recession and later both contributing to a rapid recovery. The evidence for telework is mixed. We identify some gains of working from home in the current pandemic recession but also in previous non-pandemic downturns. The employment costs of being unable to work from home seem to be more related to sectoral employment changes than something intrinsically related to telework.====We extend some of these results to all LA-5 countries and, in general, confirm findings (1) through (3), with some minor exceptions. We also decompose each country's employment and informality rates by economic sector, gender, and age, noting how the burden of the pandemic recession has fallen disproportionally on services (in particular, those deemed as contact-intensive), women, and young workers. On aggregate, however, the informality rate is not driven by composition effects, as other economic sectors, male and older workers fared similarly.====We then assess the COVID-19 pandemic through the lens of a structural model of the business cycle for a small open economy with a rich labor market structure. The model features many of the margins discussed above, including an endogenous participation in the labor market and an informal sector modeled as self-employment. We calibrate the model using Mexican data for 2005-19 to recover the shocks that rationalize the pandemic recession. Building on Leyva and Urrutia (2020a), where we consider aggregate productivity and foreign interest rate shocks as the sources of business cycle fluctuations, we add two new disturbances for the pandemic period, a sector-specific shock affecting the productivity of informal workers and a labor supply shock. These shocks are essential to reproduce the initial employment and output loss ==== the drop in the informality rate. Interestingly, our approach, close in spirit to the accounting methodology introduced by Chari et al. (2007), shows that these shocks played a negligible role in the Great Recession of 2008-9.====There is a growing literature on the economic impact of the pandemic. Our contribution is twofold. On the empirical side, we document the evolution of labor market stocks for LA-5 and of gross flows for the two largest economies, with emphasis on informality, complementing the analysis of Elsby et al. (2010) and Coibion et al. (2020) for the U.S. To the best of our knowledge, there is no comparable analysis to ours, encompassing so many countries and dimensions. In this sense, we complement IMF (2020), for the same set of LA-5 countries, by working with a mixed notion of informal employment, adding gross flows to the analysis, and comparing the pandemic recession to previous downturns. We also add to the IADB's COVID-19 Labor Market Observatory (====) by providing national estimates for Peru (not only for Metropolitan Lima) and adding more dimensions to the measurement of informality.====The use of the model to recover the shocks relevant for the pandemic is another contribution. We relate to Brinca et al. (2021), who also take these disturbances as exogenous and use vector autoregressive techniques to disentangle labor supply and demand shocks at the onset of the recession in the U.S. An alternative, widely adopted approach is the use of a SIR epidemiology model, as introduced by Atkeson et al. (2020) and Eichenbaum et al. (2020), to predict the future path of the pandemic and analyze the feedback from policies; see also Álvarez et al. (2021) and Hevia et al. (2021). Though illustrative to study confinement policies, as in Acemoglu et al. (2021), Kaplan et al. (2020), and Garriga et al. (2020), there are some challenges in disciplining the parameters of such models as pointed out by Chang and Velasco (2020).====This paper proceeds as follows. Section 2 documents the labor market adjustment in Mexico and Brazil during the pandemic. In section 3, we extend the empirical analysis to all LA-5. In section 4, we describe the model, calibrate it to Mexican data, and report the results from the accounting exercise.",Informal labor markets in times of pandemic,https://www.sciencedirect.com/science/article/pii/S1094202522000023,19 January 2022,2022,Research Article,40.0
Wang Chien-Chiang,"Department of Economics and Center for Research in Econometric Theory and Applications, National Taiwan University, No. 1, Sec. 4, Roosevelt Rd., Taipei 10617, Taiwan","Received 5 November 2019, Revised 20 November 2021, Available online 11 January 2022, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2022.01.001,Cited by (0),"We study the channels through which ==== of all maturities; this enhances the reinvestment risk but weakens the liquidation risk. Through this “liquidity channel,” a negative relationship between the interest rates and the term premium is generated, and the term premium becomes negative when the interest rates are sufficiently high. Moreover, households having different characteristics such as time discount rates and asset market participation rates demand different term premia, and this generates an endogenous market segmentation. Through this “heterogeneity channel,” the large-scale asset purchases program can effectively decrease the term premium at the ====. Further, the channel is more effective when the households have a greater variance in their characteristics.","After the financial crisis of 2007–2008, owing to a series of interest rate cuts implemented by the Federal Reserve, the short-term nominal interest rate was close to its effective zero lower bound (ZLB), a situation implying that there is little scope left for conventional monetary policy. The Federal Reserve thus turned its attention to long-term yields and conducted a series of large-scale asset purchases (LSAPs) after 2008. Under LSAPs, the Federal Reserve purchases long-term securities such as long-term Treasury securities and mortgage-backed securities, and the goal of the policy is to “ease” by placing downward pressure on long-term interest rates. In contrast to conventional monetary policy, which has been extensively studied, the purchases of the long-term securities initiated a contentious debate over whether doing so can effectively decrease the term premium, and, if it can, through which mechanism it should work. Another phenomenon that has occurred in recent years is the “yield curve inversion.” Based on the calculations of Kim and Wright (2005), the term premium on a US ten-year Treasury bond has been negative since 2018 after controlling for the expected short-term interest rate and inflation risk. In actuality, although a yield curve inversion is considered to be uncommon currently, it was an ordinary occurrence in the nineteenth-century United States.====In this paper, we construct a general equilibrium model that features idiosyncratic liquidity demand shocks, asset market participation shocks, and household heterogeneity to study the impact of monetary policy on the term premium and under what circumstances the term premium tends to be negative. The fundamental theory through which a term premium is generated in this model is the liquidity theory of the term structure of interest rates. The liquidity theory underlines a phenomenon that if the term structure of an asset runs longer than its holders would desire, the holders may bear a risk by considering that they may not be able to convert the asset into cash. Thus, holding longer-term assets incurs a higher “liquidation risk” over holding shorter-term assets. However, it is also possible that an asset matures earlier than the expected time at which the asset holder wants his or her asset in cash. By considering that the asset holder may fail to repurchase the interest-bearing asset, the holder suffers some income uncertainty compared to holding an asset of the appropriately longer maturity; this is called the “reinvestment risk.”====To distinguish the underlying channels through which the purchase of securities influences the term premium, we first study a benchmark model with homogeneous households, and then we extend the model by introducing household heterogeneity. In the benchmark model, an agent occasionally has the desire to consume goods, but cash is the only asset that can be accepted as a medium of exchange in the competitive goods market. There are also Treasury bonds with different maturities in the economy. Treasury bonds cannot be used as a medium of exchange in the goods market, but these bonds do pay cash when they mature. There is an asset market consisting of multiple submarkets; each submarket facilitates the exchange of cash and bonds with a maturity specific to that submarket. An agent can participate in the asset market with a certain probability in each period, and if an agent enters the asset market successfully, he or she can engage freely in asset exchanges in all submarkets.==== For trackability, we follow Shi (1997) and assume that agents' utility function is linear to consumption, and each agent belongs to a large household that evenly allocates its members' portfolio in each period, rendering the distribution of asset holdings degenerate after idiosyncratic shocks.====We focus on the stationary monetary equilibrium and consider that the government sticks to an asset growth rate target and chooses the supply of bonds as a monetary policy tool. In the benchmark model, the interest rates for all maturities are uniquely determined by the short-term interest rate. Higher interest rates are associated with a smaller term premium in the equilibrium, and the term premium is negative if the interest rates are sufficiently high. The mechanism generating this result is as follows. Because greater interest rates result in a greater surplus of bond purchases and a smaller surplus of bond sales, this situation, first, increases the concern over missing the opportunity of purchasing bonds and therefore enhances the importance of reinvestment risk; second, it decreases the concern over missing the opportunity of selling bonds and therefore weakens the importance of liquidation risk. Because longer-term securities bear more liquidation risk and less reinvestment risk than shorter-term securities do, greater interest rates imply a smaller term premium. We name this the “liquidity channel” of monetary policy. This negative relationship between the interest rates and the term premium has been observed from data as far back as Kessel (1971), Fama (1990), and Backus and Wright (2007), and our model explains the negative relationship based on the liquidity theory of the term structure.====We further analyze how changes in bond supplies influence interest rates and the term premium. In the benchmark model, a decrease in the supply of a bond with “any” maturity decreases the interest rates if the short-term interest rate is away from the ZLB, and this situation results in an increase in the term premium through the liquidity channel. If the short-term interest rate is at the ZLB, a further reduction in the supply of either long- or short-term bonds has no impact on the interest rates, so the liquidity channel is inactive in this circumstance. Because LSAPs are conducted at the ZLB, our benchmark model with homogeneous households implies that LSAPs have no impact on the term premium.====We then generalize the benchmark model by introducing heterogeneous households with different characteristics such as time discount rates and asset market participation rates. Household heterogeneity generates flexibility between long- and short-term interest rates, meaning that the interest rates of bonds with other maturities are no longer uniquely determined by the short-term interest rate. This allows LSAPs to effectively decrease the term premium at the ZLB, as shown in empirical studies.==== The mechanism is as follows. Consider that households in the economy have various time discount rates, and the government issues bonds with two different maturities. Households that are more patient value future consumption more than households that are less patient, so patient households are more concerned with reinvestment risk and less with liquidation risk. Therefore, patient households demand smaller term premia than do impatient households, and this situation generates occasional market segmentation. In the equilibrium region wherein the submarkets for long- and short-term bonds are perfectly segmented, a decrease in the long-term bond supply decreases the long-term interest rate but has a limited effect on the short-term interest rate. This situation results in a significant decrease in the term premium, and we name this the “heterogeneity channel” of monetary policy. There is also an equilibrium region wherein the submarkets are not perfectly segmented, meaning that some households engage in both long- and short-term bond markets, and we name these households the “marginal traders.” In this circumstance, the market term premium is equal to the term premium demanded by the marginal traders. A decrease in bond supplies decreases both the long- and short-term interest rates and influences the liquidation and reinvestment risks encountered by the marginal traders, meaning that the policy affects the term premium through the liquidity channel.====When the short-term interest rate is at the ZLB, the liquidity channel is inactive, so a decrease in the long-term bond supply decreases the term premium through the heterogeneity channel, and the strength of the heterogeneity channel is greater if households in the economy have a greater variance in their characteristics. When the short-term interest rate is greater than zero, a decrease in the long-term bond supply generates upward pressure on the term premium through the liquidity channel but generates downward pressure through the heterogeneity channel. Thus, the term premium increases if the liquidity channel dominates the heterogeneity channel, and vice versa.====In actuality, the Federal Reserve sets a short-term interest rate target and achieves the target through open market operations. In the literature, Andolfatto and Williamson (2015), Andolfatto (2015), Geromichalos and Herrenbrueck (2021), and Rocheteau et al. (2018) have studied monetary policy by taking the interest rate as the main policy instrument; the quantity of bonds or the bond-to-cash ratio is then determined passively to achieve the interest rate target. In our analysis, we consider that the government applies both the short-term interest rate target and the supply of long-term bonds as policy instruments, but the supply of short-term bonds is determined passively to accommodate the short-term interest rate target. This captures that under LSAPs, the Federal Reserve announces the size of the purchases of long-term securities instead of setting a target for long-term interest rates. Our model shows that an increase in the short-term interest rate target results in a decrease in the term premium through both the liquidity and heterogeneity channels, and the term premium becomes negative if the short-term bond supply is sufficiently high. Moreover, when the short-term interest rate target is taken as given, if the heterogeneity of households is high enough, the term premium will become negative because of the heterogeneity channel when the quantity of long-term bonds is sufficiently low.====Our general equilibrium framework allows us to conduct a welfare analysis on monetary policy. In the fixed asset growth rate regime, a decrease in the long-term bond supply at the ZLB decreases the nominal interest rates, and this decrease results in a decrease in the value of cash, so households' willingness to produce decreases as does social welfare.====The remainder of this paper is organized as follows. Section 2 reviews related literature. Section 3 describes the benchmark model with homogeneous households. Section 4 analyzes the equilibrium and conducts a comparative static analysis. Section 5 incorporates heterogeneous households into the model. Section 6 discusses welfare and alternative monetary policies and policy regimes. Section 7 concludes.","Asset market frictions, household heterogeneity, and the liquidity theory of the term structure",https://www.sciencedirect.com/science/article/pii/S1094202522000011,11 January 2022,2022,Research Article,41.0
"Athreya Kartik,Ionescu Felicia,Neelakantan Urvi","Federal Reserve Bank of Richmond, P.O. Box 27622, Richmond, VA 23261, United States of America,Board of Governors of the Federal Reserve System, 2050 C St., Washington, D.C. 20551, United States of America","Received 27 July 2018, Revised 7 December 2021, Available online 28 December 2021, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2021.12.002,Cited by (0),"While human capital accumulation is significant early in life, stock market participation is limited. As individuals age, this pattern is reversed. In this paper, we show that—when disciplined to match the substantial heterogeneity observed in earnings—a life-cycle portfolio choice model augmented to allow ==== delivers stock market participation over the life-cycle consistent with the data. Key to our finding is that returns to human capital, unlike those to stocks, depend on both individual characteristics and the amount of time invested. Our results also suggest that when human capital accumulation is endogenous, it is short sales constraints on stocks, and not borrowing constraints, that limit engagement with the stock market.","Household participation in the stock market is limited, especially early in life, despite the high returns stocks offer. By contrast, human capital investment appears widespread early in life. The returns to stock market investment, being determined competitively, are invariant across investors and do not change with the amount invested. By contrast, the payoffs to human capital investment, i.e., earnings, depend on individual-level characteristics and hence vary both across individuals and for each individual with the amount (of time) invested. The objective of this paper is to quantify the implications of endogenous human capital investment, and, in particular, heterogeneity in its payoff, for stock market participation over the life cycle.====We begin with a model of financial portfolio choice and augment it in two important ways. First, we enable households to allocate time between human capital investment and labor market work as in Ben-Porath (1967). Second, we allow households to differ in their initial endowment of human capital and in their “ability” (the skill with which they convert time to human capital). We estimate the model to match heterogeneity in returns to human capital as measured by earnings statistics for US households. Our approach exploits an insight due to Huggett et al. (2006) who show that the classic Ben-Porath (1967) framework, extended to allow for uninsurable idiosyncratic risk, is capable of replicating US earnings heterogeneity.====The main message of our paper is that a standard portfolio choice setting, when augmented to allow for human capital investment and empirically-disciplined heterogeneity in its returns, leads to plausible predictions for stock market participation over the life cycle. Specifically, it predicts that participation is low among the young and increases steadily with age. In contrast, models that employ standard parametric representations of earnings suggest rapid entry into the stock market even by very young households, and almost universal participation thereafter. Our analysis suggests that heterogeneity in human capital returns is key in providing a path to understanding stock market participation over the life cycle.====What is the channel that connects human capital investment to stock market participation? Consider a young investor with no accumulated savings and high expected returns to human capital investment. This agent will spend a lot of time learning, not earning. The prospect of a steep age earnings profile will lead them to want to borrow, not save, to smooth consumption. This will also dissuade them from wanting to save (on net) by borrowing to finance stock purchases.==== Once they accumulate savings, however, this investor will participate in the stock market, as diversification dictates, just as in any standard portfolio choice model. As in any Ben-Porath setting, individuals in our model who are most skilled in converting time to human capital (hereafter, “high-ability” individuals) accumulate human capital most rapidly, and high accumulated human capital translates into high earnings. These individuals are therefore most likely to accumulate savings rapidly as they age and then participate in the stock market at high rates. Conversely, low-ability, low human capital individuals earn less, accumulate less wealth, and participate in the stock market at lower rates. Aggregating across types yields a quantitatively plausible path for overall stock market participation.====Our model not only captures the life cycle path of average stock market participation but also the patterns of participation across wealth and earnings groups, despite being calibrated only to match earnings moments, including, importantly, heterogeneity. Given the tradeoff between human capital and stock market investment, the reader may naturally wonder why those with low human capital returns do not participate in the stock market at higher rates than those with high human capital returns. Two key features in our setting underlie this result. On the one hand, there is tension between ability and initial human capital: high ability increases the return to time invested in human capital while high human capital reduces the marginal return from an additional unit of time. On the other hand, we find that the two are positively correlated. Together, these resolve themselves in favor of delivering higher participation rates among high earners.====A second message of our paper is that in a setting in which human capital is not an endowment but must be acquired, credit constraints lose their bite in limiting stock market participation. An intuitive notion is that high borrowing costs make borrowing to invest in stocks unattractive. In our setting, however, borrowing finances ==== for those engaged in human capital investment rather than stock market purchases. Lower borrowing costs and relaxed credit limits therefore have little bearing on stock market participation. In contrast, short sales constraints in our setting do limit engagement with the stock market. If relaxed, then young households with sufficiently high returns from human capital investment would choose to go short on stocks, again to finance consumption.====Lastly, our setting suggests that accommodating human capital and the heterogeneity in its yield across households, while arguably necessary for a complete understanding of stock market participation, is unlikely to be sufficient. Specifically, heterogeneity in human capital returns does not help our model explain the empirical regularity of nonparticipation among those who ====. It has long been known that standard portfolio choice settings (which to this point have abstracted from human capital investment) are unable to account for observed nonparticipation among savers, predicting essentially universal participation instead. Our results show that this puzzle survives the incorporation of heterogeneous human capital investment opportunities.",Stock market participation: The role of human capital,https://www.sciencedirect.com/science/article/pii/S109420252100079X,28 December 2021,2021,Research Article,42.0
"Ayres Joao,Raveendranathan Gajendran","Inter-American Development Bank, United States of America,McMaster University, Canada","Received 1 April 2020, Revised 3 December 2021, Available online 20 December 2021, Version of Record 22 December 2022.",https://doi.org/10.1016/j.red.2021.12.001,Cited by (1),"We analyze aggregate shocks in a ==== model of firm dynamics with entry and exit and financial frictions. Compared to the productivity shock, a shock to the collateral constraint (credit shock) generates a larger change in firm entry and exit. Calibrating the credit and productivity shocks to the ====, we find that the credit shock accounts for lower entry, higher exit, and the concentration of exit among young firms during the ====. The changes in entry and exit account for 19 and 24 percent of the fall in output and hours, respectively. Furthermore, we discuss how the modeling of potential entrants matters for the quantitative results, and perform a COVID-19 lockdown experiment.","In this paper, we ask the following question: how do aggregate shocks affect firm entry and exit, and consequently, how do responses in firm entry and exit affect the evolution of macro aggregates? To answer this question, we build a general equilibrium model of firm dynamics with financial frictions and analyze the implications of aggregate shocks that generate recessions. Our main analyses study shocks to aggregate productivity and collateral constraint (also referred to as a credit shock). First, we show that in comparison to a standard productivity shock, a credit shock generates larger changes in firm entry and exit. Second, we use the model with both credit and productivity shocks to analyze the Great Recession. Our results show that the credit shock accounts for lower entry, higher exit, and concentration of exit among young firms. In particular, lower entry and higher exit account for 19 and 24 percent of the fall in output and hours in the first two years of the Great Recession (2008 and 2009), respectively. Furthermore, we discuss how the elasticity of the supply of potential entrants matters for the quantitative results, and we expand the analysis to study a COVID-19 lockdown through shocks to firm operation and labor disutility.====Our model economy builds on Khan and Thomas, 2008, Khan and Thomas, 2013 and Clementi and Palazzo (2016). We enrich Clementi and Palazzo (2016)'s model of firm entry and exit by incorporating financial frictions and solving the model in general equilibrium, as in Khan and Thomas, 2008, Khan and Thomas, 2013. More specifically, we add debt decisions with collateral and nonnegative dividend payments constraints to the firm's problem, and savings and labor choices to the representative consumer's problem. The inclusion of debt accumulation by firms, together with collateral and nonnegative dividend payments constraints, allows us to study the implications of credit shocks. The savings and labor choices close the economy, allowing us to solve for interest and wage rates in general equilibrium.====After calibrating our model to the U.S. economy, we validate the model against various non-targeted firm dynamics statistics. Our model does remarkably well in accounting for the share of firms by age, while overstating the relative size of entering and exiting firms. We derive our main findings from two exercises. In the first exercise, we separately analyze transition dynamics following negative shocks to productivity and the collateral constraint. The temporary shock is unexpected in period one, and thereafter, agents have perfect foresight on the evolution of the economy. The magnitude and persistence of the shocks are chosen to match the average drop in U.S. GDP in the four recessions from 1978 to 2018 (1982, 1990, 2001, and 2007) as well as the average time it took for U.S. GDP to complete half of the recovery since the beginning of each recession.====Our results from the impulse responses in the baseline model show that, in comparison to a productivity shock, the credit shock leads to larger changes in firm entry and exit. The intuition for this result is as follows. Everything else constant, a negative productivity shock reduces revenues for both incumbents and potential entrants. That reduction in revenue lowers the value of operating while keeping the value of exit constant. Hence, it should lead to a fall in entry and a rise in exit. However, in general equilibrium, the negative productivity shock leads to lower wage and interest rates, which dampen the impact of the negative productivity shock on the value of operating. Consequently, general equilibrium dampens the fall in firm entry and the rise in firm exit. In contrast, a shock to the collateral constraint decreases firm entry and increases firm exit because it constrains the ability of firms to borrow and accumulate capital. This shock especially affects potential entrants and young incumbent firms by reducing their value of operation. Potential entrants choose to not enter and young incumbents choose to exit.====In our second exercise, we analyze the Great Recession. We calibrate credit shocks to match the behavior of firm debt while feeding in productivity deviations from the data. Comparing the model with both the credit and productivity shocks to a counterfactual with only a productivity shock, we show that the credit shock generates a fall in firm entry, a rise in firm exit, and a concentration of firm exit among young firms similar to that observed during the Great Recession. We view this result as additional evidence that a credit shock played a significant role during the Great Recession.====As mentioned above, our model assumes perfect foresight and abstracts from aggregate uncertainty. This might be of concern because our model might overstate the impact of the credit shock on firm exit. That is, if there were an aggregate stochastic process, firms might save more/borrow less to protect themselves, in which case the impact of the credit shock on firm exit might be dampened. However, we do not view the lack of aggregate uncertainty as a major concern for the following reasons. First, the 2007 financial crisis was a rare event. Second, even during the credit shock in our model, none of the firms exit because of accidental default (i.e., inability to repay debt). Third, even in the current setup of our model, most firms pay zero-dividends to borrow as little (or save as much) as they can.====Having shown that a credit shock rationalizes most of the observed changes in firm entry and exit during the Great Recession, a natural question that follows is whether these variations in entry and exit matter for the evolution of macro aggregates. Therefore, we perform simulations in both the model economy that is subject to credit and productivity shocks and the one that is not subject to shocks, and compute the contribution of differences in entry and exit for macro aggregates. Lower firm entry and higher firm exit generate 19 and 24 percent of the observed fall in output and hours in the first two years of the Great Recession (2008 and 2009), respectively. But given that our model overstates the relative size of entering and exiting firms, these contributions might be somewhat overstated.====Next, we expand our analysis to discuss the importance of the modeling of potential entrants to account for entry responses in the data. In our baseline model, we assume an inelastic supply of potential entrants. In contrast, if we assume a perfectly elastic supply of potential entrants, as in Hopenhayn and Rogerson (1993), the model leads to entry responses that are too volatile compared to the data.====Finally, we use the model to simulate a COVID-19 lockdown like the one observed in 2020. We analyze shocks to firm operation, which imposes a temporary shutdown on some firms, and shocks to labor disutility, which captures the inability of some workers to go to work. The model predicts a large drop in entry and a rise in exit followed by a sharp rebound. Although we do not have data from the Business Dynamics Statistics (BDS) on entry and exit in 2020, the implications for entry are qualitatively consistent with patterns observed for new business applications during 2020. Dinlersoz et al. (2021) document an initial decline and then a sharp rebound during COVID-19 in new business applications. In this exercise, lower entry and higher exit account for 17 and 8 percent of the fall in output and hours from 2020 Q2 to 2021 Q1, respectively.==== Our paper contributes to the literature that uses structural models of firm dynamics to study entry and exit over the business cycle and credit shocks during the Great Recession, and to the empirical literature on firm dynamics statistics. The two papers that are most closely related to ours are Clementi and Palazzo (2016) and Khan et al. (2016). Clementi and Palazzo (2016) extend Hopenhayn (1992) to analyze how a shock to aggregate productivity propagates to changes in output in models with and without firm entry and exit. They find that incorporating firm entry and exit leads to higher persistence and volatility of output. In contrast to Clementi and Palazzo (2016), we analyze shocks to credit, firm operation, and labor disutility. We emphasize the larger impact of these shocks on firm entry and exit, the role of a credit shock for entry and exit dynamics during the Great Recession, and the impact of entry and exit adjustments on macro aggregates. For our analysis, we also make different modeling choices from Clementi and Palazzo (2016). We assume a general equilibrium model with endogenous interest rates and financial frictions.====Khan et al. (2016) analyze the Great Recession in a model of firm dynamics where the novel features are endogenous entry and exit and financial frictions. While our studies intersect in the modeling of entry and exit, we differ in the modeling of financial frictions. Our model assumes non-defaultable debt with a collateral constraint, whereas they assume defaultable debt. In their model, a credit shock worsens firms' cash on hand by requiring larger dividend transfers to the household. This reduces the underlying value of collateral and increases the cost of borrowing. In our model, a credit shock is captured by a shock to the collateral constraint. Our studies suggest that the magnitudes of the responses in entry and exit are sensitive to the nature of the credit shock. In their analysis, a credit shock leads to larger changes in entry and exit compared to that observed during the Great Recession. In our analysis, the credit shock generates changes in entry and exit that are less volatile and more in line with the data.====In addition to Khan et al. (2016), several other papers use a model of firm dynamics to study shocks to credit and firm entry during the Great Recession. Khan and Thomas (2013) study productivity and credit shocks in a firm dynamics model with collateral constraints and partial irreversibility of investment. They argue that credit shocks are important to account for the behavior of aggregates and employment among small and large firms during the Great Recession. While the implications of a credit shock for macro aggregates such as output, hours, investment, and consumption are qualitatively similar between our framework and that of Khan and Thomas (2013), we find a weaker response in investment for a credit shock. The reason is that their simulated credit shock persists at its lowest value for 4-years whereas our simulated credit shock completes most of its recovery in 4-years. Clementi et al. (2015), Sedláček (2020), and Siemer (2016) study how the decrease in firm entry during the Great Recession affected the slow recovery, which they term the “missing generation effect.” Buera et al. (2015) analyze productivity shocks and credit shocks in a model with frictions in both the labor and financial markets. A credit shock both decreases employment among young and small firms and increases employment among old and large firms. Mehrotra and Sergeyev (2019) target changes in job flows across firm age categories to calibrate their model of firm dynamics with financial frictions. The identification for the credit shock comes from lower job creation. They find that the credit shock accounts for 15 percent of the drop in total employment during the Great Recession. These papers have primarily emphasized the effect of credit shocks on firm entry and on young and small firms. We make two contributions to this literature. First, we show that modeling potential entrants as perfectly inelastic rather than perfectly elastic leads to entry behavior that is consistent with the data. Second, we highlight the importance of a credit shock to account for firm exit during the Great Recession.====Finally, we contribute to the empirical literature on firm dynamics. In particular, Siemer (2019) compares the performance of firms with high and low external financial dependence during the Great Recession using confidential data on the universe of firms in the U.S. and finds that financial constraints affected firm employment growth of young firms primarily through firm entry and exit. Fort et al. (2013) and Decker et al. (2014) emphasize the importance of firm age in addition to firm size in their analyses. Our finding that a credit shock accounts for the increased firm exit across different age groups complements these empirical studies and emphasizes the importance of firm age in addition to firm size. This is because in the presence of financial frictions and firm entry and exit, age is one of the determinants of a firm's idiosyncratic state (capital and debt in our model).",Firm entry and exit during recessions,https://www.sciencedirect.com/science/article/pii/S1094202521000788,20 December 2021,2021,Research Article,43.0
Imura Yuko,"Bank of Canada, 234 Wellington Street, Ottawa, ON, K1A 0G9, Canada","Received 4 June 2021, Revised 30 November 2022, Available online 9 December 2022.",https://doi.org/10.1016/j.red.2022.12.001,Cited by (1),This paper develops a two-country ,"Protectionism has become an important topic of policy discussion in recent years. Following the global recession of 2008, a number of countries implemented trade-restricting measures in order to support their domestic industry (Gamberoni and Newfarmer, 2009; Bussière et al., 2011).==== More recently, with unilateral impositions of import tariffs by large economies and the negotiations over NAFTA and Brexit, policy discussion surrounding protectionist measures increased in intensity to a level that had not been seen in recent history. At the same time, the transmission of trade policy has become increasingly complex as international trade in intermediate inputs has become a key aspect of today's globalized production processes.==== With production networks spanning beyond national borders, transmission channels through which trade policies affect individual firms and the aggregate economy depend crucially on where the affected products or economic agents stand within such production linkages.====The presence of multinational production adds another dimension to the discussion of production networks and trade policy. For example, in 2017, 75 percent of Japanese-brand vehicles sold in the United States were produced in North America, primarily in the United States. This pattern is markedly different from 1986 when 88 percent of Japanese-brand vehicles in the US market were imported (Japan Automobile Manufacturers Association).==== For Japanese automakers that now serve the US market predominantly with local production, the profitability of their US sales would be more affected by tariffs on intermediate inputs such as steel and auto parts. In contrast, for those that serve the US market through exporting, their profits from US sales would be more affected by tariffs on final goods (i.e., autos). Given the rising protectionist sentiment in the recent policy debate, a close examination of trade barriers in the context of global production linkages is essential for better understanding their economic consequences at firm and aggregate levels.====This paper provides a systematic, quantitative analysis of the dynamic effects of tariffs on final goods and tariffs on intermediate inputs, in the presence of global production linkages and multinational production. I develop a two-country dynamic stochastic general equilibrium (DSGE) model with capital accumulation, cross-country input-output linkages, and state-dependent entry/exit of firms in both exporting and multinational production. The model is calibrated to match recent evidence from micro-level data on firms' transition among domestic-only production, exporting and multinational production which exhibits a high degree of persistence. Using this model economy, I examine different channels through which each tariff affects the aggregate economy and firm dynamics in exporting and multinational production. In addition to examining the long-run implications of permanent tariffs, I analyze the economy's transition paths and address intertemporal tradeoffs in the short to medium run that cannot be seen from the long-run analysis. This reveals the importance of policy persistence in influencing the responses of investment and firm dynamics, both of which involve forward-looking decisions, and their aggregate implications over time. As my model economy captures rich micro-foundations in static and dynamic dimensions, it facilitates building intuitions from both macro and trade perspectives.====Firms in my model economy may choose to serve the foreign market by either exporting or producing abroad as multinationals. I introduce separate sunk entry costs and per-period fixed costs of continuation for these activities, which together influence the time-varying sets of exporters and multinationals. Because these costs depend on firms' activity status from the previous period, their current decisions on participation in foreign activity become forward-looking. In my calibration, the costs associated with multinational production are substantially larger than those for exporting. Therefore, relatively more productive firms choose to export and only a small number of most productive firms become multinationals. As in the horizontal FDI of Helpman et al. (2004), firms produce abroad in order to sell output in the host economy, rather than shipping back to their country of origin.==== In addition, my model captures the observed input-output linkages by introducing roundabout production whereby output of individual firms is used as intermediate production inputs by other firms and also as final goods by households, both domestically and abroad. This gives rise to international trade in intermediate inputs and final goods.====Using this framework, I first examine the effects of permanent unilateral impositions of tariffs on final goods and tariffs on intermediate inputs. I show that both types of tariffs lead to an immediate and persistent recession in the policy-imposed country, with contractions in consumption, investment and employment. In the policy-imposing country, the rising import prices reduce consumption immediately and persistently, while expenditure switching toward domestically produced products induces a short-run boom in investment that turns around over time. Such expenditure switching is weaker with intermediate-input tariffs as they raise production costs, and this accelerates the disappearance of the short-run expansion, driving a long-run GDP contraction.====At the firm level, the least productive exporters exit from the policy-imposing country, while the most productive few relocate production there and avoid tariffs. These extensive margin adjustments are highly gradual, particularly for multinational production, because of large sunk entry costs. I show that, relative to an alternative model without multinational firms, this shift in firms' production location offers an additional set of more efficiently produced varieties in place of (more expensive) imports, and dampens the loss of demand and production in the policy-imposing country following tariffs.====Crucial to the above results, particularly for the policy-imposing country, is the assumption that tariff changes are permanent, and the persistence of tariffs has important quantitative and qualitative implications for firm dynamics and the aggregate economy. While the vast majority of existing studies on trade policy have focused on permanent policy changes, a recent study by Barattieri et al. (2021) documents the prevalence of temporary trade barriers and their recessionary consequences for the policy-imposing country.==== First, I show that the extensive margin adjustments among exporters and multinationals and their positive spillover effects become increasingly muted when tariffs are more temporary. When tariffs are sufficiently temporary (lasting less than 5 years), the large sunk cost of multinational production dominates the expected returns from such investment, and the mass of multinationals remains essentially unchanged following tariffs, similar to exporter hysteresis widely studied in the literature (see, for example, Alessandria and Choi (2007) and Impullitti et al. (2013)). Without the positive spillover effects of foreign multinationals, the aggregate responses to tariffs become to resemble those predicted by a model without multinational production. Second, the short-run investment boom in the policy-imposing country that we see with permanent tariffs disappears when tariffs are more temporary, and the country can instead face an immediate recession in the short to medium run. When tariffs are sufficiently temporary, expenditure switching is short-lived and investment motives are dominated by the reduced demand due to higher import prices. This results in an immediate fall in investment, and, along with the declining consumption, drives an immediate contraction in GDP. My results therefore highlight the importance of policy commitment (persistence) in attracting domestic and foreign investments.====With these insights from unilateral tariffs, I then examine the consequences of retaliatory (bilateral) tariffs. Indeed, the series of trade restrictions imposed by the US administration starting in 2018 induced retaliatory actions against US exports by a number of its major trade partners, including China, Canada, Mexico and the European Union. For this analysis, I consider asymmetries between the two economies in my model (US and Foreign), capturing the observed US trade deficit and the asymmetry in the product-type composition between its imports and exports. I show that bilateral tariffs are contractionary for both economies irrespectively of the type of tariffs and their persistence. Importantly, the main source of such contractions shifts with the persistence of tariffs. When tariffs are permanent or highly persistent, the contractions are mainly attributed to the tariffs imposed by a country's trade partner, and the effects of the country's own tariffs on its imports are dampened by the increased presence of foreign multinationals. In contrast, when bilateral tariffs are more temporary, the resulting contractions are attributed predominantly to a country's own tariffs on its imports, rather than those imposed by the trade partner.====This paper bridges the literature on the quantitative analysis of trade policy using dynamic models of export participation, with the literature on the dynamics of multinational firms and their economic significance. First, my analysis complements recent studies on trade policy using business cycle models with firm heterogeneity and endogenous export participation.==== Barattieri et al. (2021) document empirical evidence that temporary import barriers reduce output and increase inflation for the policy-imposing country, and explain these findings using a small open economy model with endogenous firm entry, capital accumulation and nominal rigidities. Alessandria and Choi (2014) show that welfare gains from permanent bilateral trade liberalization are significantly larger when export decisions are dynamic relative to static Melitz-type models (2003). They also highlight the quantitative importance of intermediate inputs and capital accumulation for the welfare analysis. Mix (2021) develops a model in which firms make destination-specific investments in exporting capacity, and shows that welfare gains from trade liberalization are larger when exporter churning varies across destinations as observed in data. Some recent papers, such as Steinberg (2019) and Caldara et al. (2020), examine the effects of trade policy uncertainty using dynamic models with endogenous export participation.====Second, my approach to modelling forward-looking entry/exit decisions in both exporting and multinational production in a general equilibrium framework contributes to recent studies that introduce sunk entry costs of multinational production in dynamic models, building upon the model of multinational firms with productivity heterogeneity by Helpman et al. (2004). While the vast majority of the literature on multinational production consider static conditions for firms' participation in foreign activities,==== more recent studies have documented that firms' participation in multinational production is highly persistent and capture such dynamics with sunk entry costs of foreign production. Using firm-level data from Indonesia, Rodrigue (2014) estimates a two-country model with sunk costs in exporting and multinational production, and shows with static steady-state comparisons that permanent openness to trade and multinational production generates sizable productivity gains. The focus of my paper departs from his analysis along two inter-related dimensions. First, I introduce capital accumulation in my model economy which plays a crucial role in altering the aggregate consequences of tariffs for the policy-imposing country. Second, as capital accumulation and firms' entry/exit in exporting and multinational production both involve forward-looking decisions, I examine the economy's transition dynamics following tariffs, and highlight their sensitivity to policy persistence in the short to medium run, quantitatively and qualitatively. More recently, Gumpert et al. (2020) document detailed life-cycle dynamics of exporters and multinationals using micro-level data from France and Norway. They build a partial equilibrium model of exporting and multinational production with sunk costs that captures the observed life-cycle patterns, and show that a permanent fall in iceberg costs induces an increase in export participation and a decrease in MNE participation. I use their empirical evidence to discipline firms' transition patterns in the steady state of my model economy, and extend their partial-equilibrium analysis to a general equilibrium setting to quantify the aggregate consequences of trade barriers. My finding that the response of firms' participation in multinational production becomes muted following temporary tariffs is related to Fillat and Garetto (2015). They develop a dynamic model of exporting and multinational production with sunk entry costs and explain the observed higher stock market returns and earning yields for multinationals relative to non-multinationals. More recently, Garetto et al. (2021) document the expansion patterns of firms' entry into foreign production and their establishing as export platforms, and study the role of affiliate exports using a dynamic multi-country model.====Finally, the importance of trade in intermediate goods has been studied extensively following a seminal paper by Yi (2003) which examined the role of vertical specialization in contributing to the growth in world trade. More recent studies, such as Caliendo and Parro (2015) and Caliendo et al. (2017), develop multi-country, multi-sector Ricardian trade models with cross-country, cross-sectoral linkages, and highlight how input-output linkages amplify and propagate the welfare effects of trade policy changes across sectors and countries.====The remainder of the paper is organized as follows. Section 2 describes my model economy, and section 3 discusses its calibration and steady-state characteristics. Results from my quantitative analysis of unilateral tariffs are presented in section 4. In section 5, I examine the consequences of bilateral tariffs. Section 6 examines the sensitivity of the main results to the productivity dispersion, the elasticity of substitution, and the exit rate of multinational firms. Section 7 concludes.",Reassessing trade barriers with global production networks,https://www.sciencedirect.com/science/article/pii/S1094202522000643,Available online 9 December 2022,2022,Research Article,45.0
Pidkuyko Myroslav,"Banco de España, Madrid, Spain","Received 26 March 2021, Revised 18 November 2022, Available online 30 November 2022.",https://doi.org/10.1016/j.red.2022.11.005,Cited by (0),We study the ==== creates extra ==== for the mortgagors via the refinancing channel.,"Residential mortgage debt in the US accounts for a majority of household borrowing.==== The US mortgage market is also distinctive since the US government actively engages in mortgage market operations via several Government-Sponsored Enterprises (GSEs) and Governmental Agencies. While not allowed any direct lending to the households, at the peak of their activity, these agencies held almost 20% of the overall mortgage debt in the economy.==== It is well-documented (see Fieldhouse et al., 2018) that portfolio purchases of the GSEs boost mortgage lending, lower mortgage rates, and influence prices on other financial markets. In this paper, we study how this economic activity affects the most significant component of GDP, household consumption. We find that portfolio purchases by the GSEs generate a significant response of households' consumption. We show that households' financial position is crucial in understanding the spillovers from these purchases to private consumption. First, using the household-level consumption data, we empirically document and quantify the response of individuals' consumption to activity in secondary mortgage markets. Second, through the lens of a life-cycle model with incomplete markets and endogenous housing choice, we explain this empirical evidence and analyze the transmission mechanism through which mortgage market activities affect consumption.====In our empirical exercise, we explore the link between an increase in agency purchases (what we call expansionary credit policy changes) and an increase in households expenditure. To do so, we proxy households' financial position with housing tenure status. We show that following an expansionary policy change to the secondary mortgage markets, homeowners with mortgage debt increase their spending substantially. In contrast, homeowners without mortgage debt do not react to policy change. Non-homeowners also increase their consumption but less than mortgagors.====Moreover, we analyze how the same policy affects households of different ages. Complementary to the results on housing tenure status, we find that young and middle-aged households are most exposed to the activity in secondary mortgage markets and increase consumption significantly as a result of the policy.====In identifying expansionary credit policy changes, we focus on changes through exogenous governmental intervention in the mortgage markets via various federal housing agencies. For the most part, credit policy changes are a reaction to business cycle conditions (the most recent QE3 being the prime example). To analyze the consumption response to any of these policy changes, it is, therefore, essential to isolate the policy changes that are orthogonal to the business cycle (such as long-term objectives of increasing homeownership). We combine the exogenous non-cyclically motivated events from Fieldhouse and Mertens (2017) with mortgage purchases of the two largest federal housing agencies (Fannie Mae and Freddie Mac). We then use the former as an instrument in regressions of households' consumption on measures of agency purchase activity. We measure consumption using household-level data from the Consumer Expenditure Survey. If credit market interventions were neutral (Meltzer, 1974; Greenspan, 2005; Lehnert et al., 2008; Fieldhouse et al., 2018), an increase in agency purchases should have little impact on private consumption. We find empirically that credit policy changes lead to an increase in private consumption, more so for mortgagors and younger households.====In our theoretical exercise, we use a structural model to identify the transmission mechanism we found in our reduced-form analysis. We present a life-cycle model with incomplete markets and endogenous housing choice in the vein of Kaplan et al. (2020); Favilukis et al. (2017); Sommer and Sullivan (2018); Wong (2021). We model the credit policy change experiment by replicating the aggregate macroeconomic effect of mortgage market interventions documented in Fieldhouse et al. (2018). In particular, we focus on the change in both interest and mortgage rates as well as on the change in the spread between the two, as documented in Fieldhouse et al. (2018).====Our first finding is that lower mortgage rates imply lower mortgage payments for the mortgagors and a rise in long-term permanent income for this group. In terms of mechanisms in the model, we identify access to refinancing as a crucial transmission mechanism for expansionary credit policy. These results are in line with both the recent microeconomic evidence (Wong (2021) emphasizes the role of refinancing for transmission of monetary policy to consumption) as well as macroeconomic one (following a credit policy shock Fieldhouse et al. (2018) show an increase of mortgage originations due to refinancing). Since the opportunity cost of saving goes down when the interest rates drop as well - mortgagors consume this extra income instead of saving. The results we find are also in line with Cloyne et al. (2020), who argue that the behavior of mortgagors resembles that of wealthy hand-to-mouth households and empirically document a similar response of individual consumption to expansionary monetary policy shock. Indeed, in the model, mortgagors hold little liquid wealth, outstanding mortgage debt, and illiquid assets in the form of the house.====We then analyze the response of other types of households: renters and outright homeowners. For outright homeowners, who are mostly older than renters and mortgagors, any changes in the interest rate will have a very small effect, given that the marginal propensity to consume for these households is already relatively low. Interestingly, in the model, renters decrease their consumption following a cut in mortgage and interest rates. We show that through the lens of the model, when solving the dynamic problem, the utility of future homeownership dominates that of a simple consumption-saving motive and prevents renters from dis-saving while facing lower interest rates. Similarly, we also analyze the behavior along the age dimension. In the model, we can replicate empirically documented increase in consumption for young and middle-aged households and very low response for older ones. The access to cheaper mortgages via refinancing choice and lower mortgage payments increases the current consumption of young and middle-aged households. Again, these results align with recent evidence of Wong (2021), which finds that young and middle-aged households have a high concentration of new and refinanced loans.====As documented by Fieldhouse et al. (2018), expansionary credit policy affects not only housing credit markets (such as mortgage rates) but also affects short-term and long-term interest rates (what authors call an “accommodative” monetary policy). Given a rather small number of mortgage market interventions in the data, it is hard to separate the effects on mortgage and interest rates empirically. Our structural model is the perfect environment for that. As such, we analyze an alternative credit policy change that affects only mortgage borrowing rates while the short-term interest rates remain constant. While we find that consumption response in the case of constant interest rates is lower, we still document a significant response of the mortgagors (as well as middle-aged households in general), especially among those who decide to refinance.====  In exploring the link between exogenous credit policy changes and individual consumption, our paper adds to both empirical and theoretical literature on housing and mortgage markets. From the empirical side, we relate to four strands of literature. Firstly, we analyze the US federal government interventions in the mortgage markets. Most of the literature focused on governments' intervention in terms of tax policies. Recent studies include Chambers et al. (2009); Hilber and Turner (2014); Floetotto et al. (2016); Sommer and Sullivan (2018), among others. Fieldhouse et al. (2018) is the most recent study that instead analyzes the interventions to the federal housing agencies rather than any tax policies. In this paper, we use exogenously identified policy interventions from Fieldhouse et al. (2018); unlike Fieldhouse et al. (2018), however, we analyze the transmission mechanisms through which the policy operates using the US household survey data.====Secondly, this paper is related to literature that analyzes the interaction between federal housing agencies and other markets. The most recent studies include Gonzalez-Rivera (2001); Naranjo and Toevs (2002); Lehnert et al. (2008); Hancock and Passmore (2011); Hancock and Wayne Passmore (2014) as well as Fieldhouse et al. (2018). We focus specifically on the effect of mortgage purchases of governmental housing agencies on the consumption of different types of households using a novel identification strategy.====Thirdly, our paper is related to the literature on the role of household balance sheet channels in the transmission of monetary and fiscal policy shocks. These include Iacoviello (2005); Eggertsson and Krugman (2012); Luetticke (2021); Greenwald (2018); Hedlund et al. (2016); Cloyne et al. (2020); Kaplan et al. (2018); Auclert (2019); Bilbiie (2020); Wong (2021), to name a few. Coibion et al. (2017) also uses US household-level data to study the effect of conventional monetary policy on income and consumption inequality. Like in Cloyne et al. (2020), we use the households' housing tenure status to proxy their asset and debt position.====From the theoretical side, our model resembles the recent literature that extends Huggett (1996) model to incorporate housing decisions and aggregate housing and mortgage markets. To name a few, we build on the models of Kaplan et al. (2020); Favilukis et al. (2017); Sommer and Sullivan (2018); Wong (2021), that analyze heterogeneous agents life-cycle economies with uninsurable income risk in which households make a housing and mortgage choice. Unlike these papers, however, we do not focus on the aggregate implications of different macroeconomic shocks but rather analyze the individual households' behavior.====  The rest of the paper is structured as follows. Section 2 sets out the empirical model and presents the impulse response analysis. Section 3 develops a life-cycle economy with endogenous housing choice and uninsurable idiosyncratic risk. Section 4 estimates the model and describes the properties of the baseline economy. Section 5 analyzes the effect of mortgage market intervention within the model framework and discusses transmission mechanisms. Finally, section 6 concludes.",Heterogeneous spillovers of housing credit policy,https://www.sciencedirect.com/science/article/pii/S109420252200062X,Available online 30 November 2022,2022,Research Article,46.0
"Celik Murat Alp,Tian Xu","229 Max Gluskin House, 150 St. George Street, University of Toronto, Toronto, Ontario, M5S 3G7, Canada,620 South Lumpkin Street, Department of Finance, Terry College of Business, University of Georgia, Athens, GA 30602, United States of America","Received 26 May 2022, Revised 9 November 2022, Available online 25 November 2022.",https://doi.org/10.1016/j.red.2022.11.004,Cited by (0),". Removing agency frictions leads to contracts richer in stock options, boosting growth by 0.51pp, and welfare by 7.3% in consumption-equivalent terms. These findings are robust to incorporating short-termism. Short-termism itself is also detrimental, the removal of which increases welfare by 1.5%. Alleviating both frictions at the same time leads to amplified gains in growth and welfare.","Innovation is the primary engine of economic growth in economies at the technological frontier, and a path to higher profits and growth at the firm level. Companies such as Apple, Alphabet, Microsoft, and Amazon which dominate the list of top US public firms by market capitalization can dispel any doubts to the contrary. A firm's manager plays a crucial role in directing and overseeing its innovation efforts. This, however, creates a tension: The interests of the shareholders and those of the manager might not be perfectly aligned with each other, opening the door to agency frictions. In turn, these frictions can result in suboptimal investment in innovation, leading to losses in firm value for the shareholders, and low economic growth and welfare for the broader economy.====Better corporate governance can help align the interests of the managers and the shareholders, and thus alleviate the negative impact of agency frictions. Recent empirical studies confirm the significance of corporate governance in the growth process (see Nicolo et al. (2006), Claessens and Yurtoglu (2012), and Bloom and Van Reenen (2007) among others). A recent report by OECD (2012) summarizes this body of evidence by arguing that “corporate governance exerts a strong influence upon innovative activity and entrepreneurship. Better corporate governance, therefore, should manifest itself in enhanced corporate performance and can lead to higher economic growth.”====Despite the substantial strands of literature on corporate governance, agency frictions, and firm innovation, little work has been done to quantify the effects of agency frictions between shareholders and managers on firm innovation, managerial compensation structure, economic growth, and social welfare. In this paper, we model the agency frictions between the manager and the shareholders of a firm regarding corporate investment, where the managerial compensation structure is endogenously determined, and affected by the quality of corporate governance. We nest this agency problem within a rich dynamic general equilibrium framework with endogenous productivity growth that can tractably accommodate firm heterogeneity. This novel framework allows us to shed light not only on the micro-level implications of agency frictions on managerial compensation structure and firm innovation, but also on the much less investigated macroeconomic implications of these micro-level frictions on economic growth and social welfare.====To achieve our purpose, we document motivating facts that shed light on the mechanisms through which corporate governance influences firm innovation. One popular method employed by US public companies is increasing the share of stock options in manager compensation. We find that firms with better governance (as proxied by high institutional ownership) tend to adopt executive compensation contracts with a higher share of stock options. The convex payoff structure of state-contingent stock options provides incentives to the managers to engage in risky innovative activities, thereby contributing to firm value and economic growth. We combine micro-data on patented inventions from the United States Patent and Trademark Office (USPTO) with data on US public firms from Compustat and executive compensation information from Execucomp databases for 1990-2004. Unlike the majority of papers using similar data, our focus is on the quality, not the quantity, of innovations. To this purpose, we employ three scale-independent measures of disruptive innovations: (i) average citations received by the patents of a firm, (ii) the fraction of a firm's patents that make it to the top 10% in terms of patent quality, and (iii) the average originality of the patents — a measure which captures the variety of distinct technology classes that the new innovation is combining and building upon. Consistent with previous literature, the stylized facts can be summarized as follows:====How much do agency frictions between managers and shareholders matter for innovation, firm growth, firm value, and economic growth? To what extent can the inefficiencies caused by agency frictions be alleviated through better corporate governance and more incentivized executive compensation contracts? Are there systematic differences in the degree of this inefficiency across time? Answering these questions in a purely empirical setting is challenging. First, agency frictions are not directly observable. To assess their impact, we must observe what would have happened in a parallel, counterfactual world in which there are no agency frictions. Evaluating this counterfactual is difficult, because it is hard to find exogenous shocks that eliminate agency frictions. Even if there were such a shock, it is likely to be limited in scope, raising concerns about external validity. Overall, it is unclear how to quantify the effect of agency frictions without a model.====We overcome these challenges by developing and structurally estimating a new dynamic general equilibrium model with firm-level innovation, and endogenously determined managerial compensation contracts. This tractable theoretical model and its quantification are our primary contributions. In the model, the firm's board determines the CEO's compensation contract. Taking the contract as given, the CEO makes the innovation decisions. The board, however, does not fully represent the preferences of the shareholders. The CEO's compensation structure is the product of a tug-of-war between the CEO and the shareholders. The CEO has some influence over the board's final decision, and his preferences enter the board's objective function along with those of the shareholders. Consequently, the agreed-upon compensation contract deviates from the shareholder-optimal contract that would maximize firm value. Better corporate governance acts to reduce the CEO's influence, enabling the board to choose contracts with a higher fraction of stock options. This in turn motivates the manager to allocate more resources to innovation.==== Better corporate governance thus leads to a higher rate of innovation and more knowledge spillovers, thereby increasing long-run productivity growth and social welfare. Despite this complex setting with rich dynamics, the model remains highly tractable and computationally feasible, where most of the relevant quantities admit closed-form solutions. Thus we can largely avoid the problem of a “black box” model with many indistinguishable moving internal parts. This facilitates a better understanding of the key mechanisms at play.====On the quantitative front, the model successfully replicates the above-mentioned stylized facts, and the measured correlations play a significant role in disciplining the quantitative implications of the estimated model. Using simulated method of moments (SMM), the model parameters are estimated to best fit a wide-ranging set of facts about US firms, such as the aggregate output growth rate, share of research and development (R&D) expenditures, ratio of CEO compensation to market capitalization, and the correlation structure between firm innovation, corporate governance, and the share of stock options in the CEO's total compensation. Using the estimated parameters, we document the model's implications for the US economy by conducting a series of counterfactual experiments. Through these experiments, we quantify the importance of the agency frictions between the CEO and the shareholders not only on micro-level observations such as firm innovation, but also their macroeconomic impact on aggregate productivity growth and social welfare. An experiment in which we remove agency frictions by shutting down CEO influence results in an increase in firm innovation by 26.6% of its value. The equilibrium output growth rate increases by 0.51% on top of its targeted value of 2.00%. This leads to a significant welfare gain of 7.3% in consumption-equivalent terms.==== Another quantitative experiment that attempts to gauge the impact of FAS 123R, a change in accounting standards introduced in December 2004 which discourages firms from using stock options in employee compensation, reveals that it might have slightly reduced long-run economic growth while concentrating R&D in firms with better corporate governance. The overall effect is a fall in social welfare by 0.84%. We also investigate whether preferential taxation of stock options can alleviate the agency frictions. However, the welfare gain from not taxing stock options is a modest 0.91%.====The general equilibrium property of our framework matters for the precise assessment of the counterfactual implications of how agency frictions affect innovation, productivity growth, and welfare. Ignoring the endogenous responses of the wages, the interest rate, and the knowledge spillovers across firms would significantly exaggerate how firms would change their innovation policies. In particular, we demonstrate that ignoring the general equilibrium effects would increase the welfare impact of shutting down agency frictions from 7.3% to 13.7%, highlighting the importance of using a general equilibrium framework to quantify the impact of agency frictions. Our model's ability to tractably cast an agency problem with endogenous compensation structure in a dynamic general equilibrium model with endogenous growth helps in this respect.====A recent paper by Terry (2017) finds that short-term pressure on CEOs to meet earnings targets can force them to decrease investment in R&D. Motivated by this finding, we enrich our analysis by incorporating short-term earnings pressure on CEOs in our baseline model. We document new evidence on how short-termism is a more relevant problem for firms with high institutional ownership. In the extended model, CEOs are punished if they miss the short-term earnings target. This leads to a reduction in R&D spending and innovation especially for innovative firms when they face a low productivity shock. We find that this extension does not change our quantitative results significantly. Removing short-term pressure also leads to gains in growth and welfare, albeit at one quarter of the magnitudes achieved through shutting down CEO influence. Finally, the model predicts amplified gains if both frictions are alleviated simultaneously.====This paper is related to the literature exploring the effects of agency frictions and executive compensation on managerial risk-taking and investment decisions.==== Glover and Levine (2015) provide evidence that managerial incentives, shaped by compensation contracts, help to explain the empirical relationship between uncertainty and investment. Glover and Levine (2017) find that the average CEO compensation contract incentivizes overinvestment in physical capital by 1.3 percentage points per year. Albuquerue and Wang (2008) develop a dynamic stochastic general equilibrium model to study asset pricing and welfare implications of imperfect investor protection. Nikolov and Whited (2014) develop and estimate a dynamic model to study the impact of agency frictions on firm cash holdings. Despite the substantial literature on corporate governance and agency frictions, what is less well understood is the joint dynamics of agency frictions, managerial compensation, and firm innovation, as well as its aggregate implications. Our paper aims to shed light on this by developing and estimating a new dynamic general equilibrium model. Different from this strand of literature, which often treats executive compensation as exogenously given, we study the joint determination of managerial compensation, firm innovation, and economic growth in a united dynamic framework. Our general equilibrium framework offers significant value-added compared to what is previously done in the literature, given that taking the effect of knowledge spillovers into account, as well as the endogenous response of the firms to their competitors' innovation choices are key to assess the true welfare cost of managerial agency frictions.====The dynamic general equilibrium model featuring endogenous productivity and output growth links this paper to the literature on endogenous growth, pioneered by Aghion and Howitt (1992), Lucas (1988), and Romer (1990).==== In terms of the particular model used in this paper, the closest two papers are Akcigit et al. (2016) and Celik (2022). The firm-level innovation decision determines the probability of a successful innovation, which results in a permanent increase in the firm's productivity. The increase in productivity benefits from Romer-type productivity spillovers, resulting in under-investment in innovation from a social planner's point of view. Firms have an incentive to invest in costly R&D to improve the innovation probability, since their profits are linearly increasing in their relative productivity compared to the average productivity in the broader economy. This model differs from these two papers in that the CEO of a firm chooses the innovation probability under an endogenous contract determined by the board, subject to CEO influence. Due to the misalignment of incentives between the firm's CEO and its shareholders, the CEO might ==== or ==== in innovation depending on how his contract is structured. Therefore, the agency frictions between the CEO and the shareholders generate an additional mechanism through which the innovation in a competitive equilibrium might be further below the value in the Pareto efficient allocation. Another closely related paper is Greenwood et al. (2022), which focuses on how the monitoring frictions between venture capitalists and the start-ups they support can influence economic growth and welfare. In their setting, the venture capitalists play a similar role to that of the institutional owners in our current model. Iacopetta and Peretto (2018) show that modest differences in corporate governance can account for large income differences across countries using a growth model with governance distortions and resource diversion. Our framework also adds to the broader endogenous growth literature with firm dynamics, such as Peters (2020), Akcigit et al. (2021a), Akcigit et al. (2022), and Ates and Saffie (2021). We contribute by embedding a complicated agency problem where the endogenous compensation contract is the result of a tug-of-war between the manager and the shareholders without abstracting away from rich firm dynamics, or losing tractability.====This paper adds to a growing strand of literature studying the impact of corporate governance on firm innovation. Francis and Smith (1995), Eng and Shackell (2001), and Aghion et al. (2013) find that greater institutional ownership is associated with more innovation. Our paper is related to Aghion et al. (2013). We show that the effect of institutional ownership is realized chiefly through the channel of managerial compensation.==== Balsmeier et al. (2016) investigate the impact of board independence on innovation. We find that their metric is positively correlated with our measures of disruptive innovation. Chemmanur and Tian (2018) find that anti-takeover provisions and institutional ownership spur corporate innovation. Iacopetta and Peretto (2018) provide evidence that better corporate governance can lead to economic growth through innovation in a cross-country setting. Although a large strand of previous literature documents the positive impact of corporate governance on firm innovation, relatively less is known about the channel through which it operates. This paper aims to provide a micro-foundation to shed light on the mechanisms that underlie these empirical facts. Moreover, the existing literature mostly focuses on the quantity rather than the quality of innovation, as discussed earlier. We use scale-independent measures, and focus on highly cited disruptive innovations instead, as these are more likely to represent new ideas that result in significant knowledge spillovers and fuel economic growth.====This paper is also related to a large strand of literature studying how institutional investors affect the executive compensation structure. Smith (1996) and Gillan and Starks (2000) report that public fund managers often voice the opinion that managerial compensation should be linked to corporate performance. Hartzell and Starks (2003) and Chidambaran and John (2008) document that institutional ownership is positively related to the pay-for-performance sensitivity of executive compensation. Consistent with the existing literature, we also find that firms with higher institutional ownership tend to adopt compensation contracts with high pay-for-performance sensitivity, owing to a high fraction of stock options. We structurally quantify the impact of agency frictions on managerial compensation. We also move one step further to study the impact of executive compensation on firm innovation, productivity growth, and social welfare in a novel dynamic general equilibrium framework with endogenous productivity growth.====The paper is organized as follows. In Section 2, we develop a general equilibrium framework with endogenous growth and managerial compensation contracts to study the joint dynamics of corporate governance, managerial compensation, and firm innovation. In Section 3, we summarize stylized facts on the relationships among institutional ownership, executive compensation contracts, and disruptive innovation. In Section 4, we estimate the model and document its implications for US firms by carrying out a series of counterfactual experiments. In Section 5, we extend the analysis by incorporating short-term earnings pressure on the CEO. Concluding remarks are offered in Section 6.","Agency frictions, managerial compensation, and disruptive innovations",https://www.sciencedirect.com/science/article/pii/S1094202522000618,Available online 25 November 2022,2022,Research Article,47.0
"Bick Alexander,Blandin Adam","Federal Reserve Bank of St. Louis, United States,Arizona State University, United States,CEPR, United Kingdom,Vanderbilt University, United States","Received 8 May 2021, Revised 8 November 2022, Available online 17 November 2022.",https://doi.org/10.1016/j.red.2022.11.002,Cited by (0),"Economists have recently begun using independent online surveys to collect national labor market data. Questions remain over the quality of such data. This paper provides an approach to address these concerns. Our case study is the Real-Time Population Survey (RPS), a novel online survey of the US built around the Current Population Survey (CPS). The RPS replicates core components of the CPS, ensuring comparable measures that allow us to weight and rigorously validate our results using a high-quality benchmark. At the same time, special questions in the RPS yield novel information regarding employer reallocation during the COVID-19 pandemic. We estimate that 26% of pre-pandemic workers were working for a new employer one year into the COVID-19 outbreak in the US, at least double the rate of any previous episode in the past quarter century. Our discussion contains practical suggestions for the design of novel labor market surveys and highlights other promising applications of our methodology.","In response to rapidly evolving data demands amid the COVID-19 pandemic, economists have recently begun using independent online surveys to collect national labor market data (====; ====; ====; ====; ====; ====). The ability of a small group of researchers to design and field a novel national labor market survey represents a potentially important addition to the toolkit of macro and labor economists which could far outlast the pandemic. The primary advantage of independent online surveys is flexibility: surveys can be tailored to address specific research questions by asking novel questions or collecting data at strategic points in time. A second advantage is speed: a single researcher can collect thousands of responses in a few days, which is a faster timeline than existing publicly available surveys. Moreover, because the costly tasks of administering surveys can be outsourced to commercial survey companies in exchange for a relatively modest fee, this approach does not require operating one's own survey infrastructure and is therefore widely accessible.====However, important questions remain over the utility of labor market data derived from independent online surveys; see ==== for a review. One set of questions concerns the ==== of such data; in particular, whether survey samples are representative of the underlying population for variables of interest and whether survey responses contain excessive measurement error. A related set of questions concerns the ==== of the data—namely, the extent to which labor market concepts (such as “unemployment” or “earnings”) are comparable to concepts in other widely used surveys.====The current paper assesses a novel national online labor market survey designed to speak to the above concerns. Our approach is to use an existing high-quality survey, the Current Population Survey (CPS), as the “scaffolding” for the novel survey, which we label the Real-Time Population Survey (RPS). By “scaffolding,” we mean that the RPS replicates key portions of the CPS, which ensures that survey concepts are comparable and allows researchers to weight results against a widely used benchmark with a large sample size.==== At the same time, the RPS omits certain portions of the CPS, leaving room for novel questions.====We begin in Section ==== with a brief overview of how online surveys work. We emphasize practical considerations related to survey design, sample selection, and sample weighting based on our experience with the RPS. Section ==== provides more concrete details for the RPS, which ran twice per month from late March through September 2020 and then monthly through June 2021. A crucial feature of the RPS is that it assigns labor market status using the same intricate sequence of questions in the CPS. The replication package includes a template questionnaire containing this portion of the RPS, which can be used as a starting point for new labor market surveys. It also contains stata codes corresponding to this template that assigns labor market status according to the CPS criteria.==== An example using real-world data demonstrates that even minor deviations from the CPS design can have large effects on key estimates.====In Section ==== we use overlapping questions in the RPS and CPS to validate outcomes in the RPS, addressing concerns about the data's reliability. While we note some discrepancies, RPS time series for labor force participation, employment, unemployment, hours worked, and earnings are similar in both level and trend to the CPS during the pandemic, but were available several weeks before the respective CPS release. In addition, retrospective questions in the RPS about labor market outcomes in February 2020 produce statistics in line with the February 2020 CPS, providing an additional point of validation just before the onset of the pandemic in the US. Together, retrospective and contemporaneous questions allow us to observe individual-level changes over time during the pandemic, which we exploit in our central application.====Bolstered by the above comparisons, Section ==== uses RPS data on employer tenure to provide novel insights on the extent of employer separations and reallocation one year into the COVID-19 pandemic. This information is not available in the CPS because the employer tenure supplement did not run during the first year of the pandemic in the US. We estimate that, among workers employed in February 2020, 37.2% had separated from their pre-pandemic employer one year later, with 11.0% not employed and 26.2% working for a new employer. While the share transitioning to non-employment was similar to some recent recessions, the share working for a new employer was at least twice as large as in any previous episode in the past quarter century, lending support to the prediction by ==== that the COVID-19 pandemic would produce large reallocation effects. We also find notable differences in labor market outcomes by pre-pandemic employer tenure. Among workers who had been with their pre-pandemic employer less than two years, 61.8% had separated one year later, compared with 15.9% of workers who had been with their pre-pandemic employer for at least a decade. At the same time, conditional on a separation occurring, high-tenure workers were less likely to be working for a new employer one year into the pandemic. Collectively, these findings are informative for the calibration and validation of quantitative models of the labor market consequences of the pandemic (====) and suggest a particularly important role of match-specific productivity for labor market outcomes during this recession (====; ====; ====).====The RPS is far from the only national online labor market survey. Several research institutions conduct long-run online panels that collect labor market information and are intended to be nationally representative. Important examples for the US include the RAND American Life Panel (ALP), the University of Southern California's Understanding America Study (UAS), and the Federal Reserve Bank of New York's Survey of Consumer Expectations; examples for other countries include the Longitudinal Internet Studies for the Social Sciences (in the Netherlands) or the German Internet Panel. Two strengths of these types of surveys are their panel component, which allows researchers to track individuals over time, and their probability-based sampling procedures. In contrast to institutional surveys, which require access to a large support infrastructure, independent online surveys like the RPS allow researchers to outsource recruitment and survey collection to a commercial survey company in exchange for a fee. While some of the above institutional surveys do offer independent researchers the opportunity to field their own survey to the institutional panel, the costs are substantially higher. For example, a 10 minute survey with 1,500 respondents costs $39,500 with UAS and $42,000 with ALP, versus $7,500 with the provider we use.==== At the same time, during the pandemic the RPS performed at least as well as other institutional online labor market surveys (as judged by comparisons with the CPS).====Even more than its relatively modest cost, the key feature of the RPS is that its design facilitates clean comparison and validation with an existing benchmark. This aspect of the RPS is similar to contemporaneous work by ====, who ran a repeated online survey from April 2020 through March 2021 that also followed core aspects of the CPS.==== Their survey was designed independently from ours, and contains some differences. For example, their survey does not ask about employer tenure, which forms the basis of our analysis in Section ====, or about live-in spouses or partners. They also used a different commercial survey provider to collect responses. However, in weeks that overlapped with the RPS both survey yield similar estimates for several labor market series, which we interpret as additional validation of the scaffolding methodology. Section ==== concludes with a discussion of several other promising applications of this methodology related to better understanding work arrangements (e.g., remote work, non-wage amenities, and “gig work”), as well as specific policy topics lacking relevant nation-wide data.====The following is the Supplementary material related to this article.",Employer reallocation during the COVID-19 pandemic: Validation and application of a do-it-yourself CPS,https://www.sciencedirect.com/science/article/pii/S1094202522000606,Available online 17 November 2022,2022,Research Article,48.0
Dal Bianco Chiara,"Department of Economics and Management, University of Padova - via del Santo 33 - 35123 Padova, Italy","Received 27 January 2020, Revised 21 September 2022, Available online 14 November 2022.",https://doi.org/10.1016/j.red.2022.11.001,Cited by (0),"I provide a quantitative assessment of the labor market and welfare effects of return-to-work policies targeted at disability insurance (DI) recipients. I do so by estimating a life-cycle model in which individuals with different health evolving over time choose consumption, labor supply, and DI application. I find that a wage subsidy incentivizing return to work is welfare improving, and the willingness to pay for such reform is increasing in sickness and decreasing in ","In most OECD countries the rules governing disability insurance (DI) programs have changed considerably in the recent decades with a remarkable shift towards policies aimed at re-integrating people experiencing a DI episode into the labor market. These return-to-work policies, such as in-work benefits, provision of employment support and vocational rehabilitation are aimed at containing DI program expansion – in OECD countries in recent years public spending on disability amounted on average to 2.1% of GDP and it has proven to be rather stable over time – and reduce its work disincentive effects (OECD, 2010).====Despite the introduction of the return-to-work policies in many institutional contexts, there is heterogeneous evidence on their effects. What is the labor supply response to their introduction? Do they affect DI program enrollments? Which individuals are more affected? Are individuals better off after their introduction? The answers to these questions are crucial to effectively design return-to-work policies. In this paper, I address these questions by estimating a model of labor supply and savings behavior, which allows me to account for the full dynamic effects of alternative policies on agents' choices and welfare. In particular, I focus on two specific policies that are illustrative of the main changes in the DI program of many countries in the last decade: a wage subsidy received when returning to work after a DI episode and a continuous eligibility reassessment of DI beneficiaries.====I focus on individuals approaching retirement age for whom DI has been shown to play an important role in their departure from the labor market (Wise, 2016); in particular I consider men living with a partner. In the model, individuals choose whether to participate in the labor market and how many hours to work. Moreover, they can apply for DI. DI applicants are accepted with probability depending on health and age. The model allows for uncertainty about wage realization, health development, and life expectancy. In developing the model, I devote special attention to the measure of health and the evolution of health over time. I construct a continuous health index using a set of objective health indicators collected in the English Longitudinal Study of Ageing (ELSA) replicating the health conditions covered by the health assessment used to determine eligibility for DI benefits. Health depends on age and on a stochastic component allowing both persistent and transitory shocks. Health status enters the deterministic component of the exogenous wage process (productivity channel) and the probability of surviving to the next period; moreover, there is a time cost of being in poor health that affects utility through leisure.====I estimate the model using ELSA data from 2002–2008, a period in which DI policies and rules were relatively stable. The model parameters are estimated in two steps. First, the parameters of the exogenous health and wage processes are estimated using standard minimum distance techniques. Second, the remaining parameters are estimated using the Method of Simulated Moments to match profiles generated by the dynamic model with data age profiles of assets, labor force participation (LFP), hours worked, and DI participation. The model is able to replicate the main patterns observed in the data and heterogeneity by age and health in decision profiles quite well.====I use the model to simulate individual responses to two alternative policy scenarios. First, I introduce financial incentives to return to work in the form of a wage subsidy received in the first year of work after a DI episode that is proportional to the DI benefit amount. Second, I implement a yearly eligibility reassessment of DI beneficiaries which occurs with a certain probability. Because I use a more detailed measure of health than in previous works, I can better investigate heterogeneity in behavioral responses.====I document a number of relevant findings that can be summarized in three broad conclusions. First, my findings suggest the presence of a significant capacity to work among relatively younger claimants with less severe disabilities that can be brought out by return-to-work policies. If DI beneficiaries were allowed to receive the benefit for an additional year when returning to work, the DI rate for those receiving DI at least once, would decrease by 5.7 percentage points, and LFP would increase by 4.6 percentage points for the same group. Reassessing the eligibility of 10% of DI recipients every year at random would force about 30% of beneficiaries to exit the program. Of these, about 54% would return to work. Second, the one-year wage subsidy increases the program's generosity and DI enrollment for a specific group of individuals, namely those approaching retirement age that are at the margin of program entry (i.e. in relative good health and relatively good economic conditions), but this effect does not prevail. Third, the welfare effects are on average small but positive for the first policy scenario, negative and approximately zero for the second. In both cases, they exhibit large heterogeneity by health level. The willingness to pay for the introduction of a wage subsidy is positive for both low- and high-health individuals and it increases with the level of sickness, reaching its maximum for those with moderate disability. On the contrary, a policy introducing a continuous health reassessment would increase the welfare only among individuals in good health and strongly reduce the welfare of those in bad health.====Results proved to be robust to alternative model specifications regarding the relative risk aversion coefficient, the variance of the temporary health shocks, the fixed cost of work and the introduction of application costs.====The remainder of the paper is structured as follows. Section 2 presents a literature review and discusses my paper's contributions. Section 3 presents the UK DI system and Section 4 the model of lifetime decision making. Section 5 introduces the data and the health measure. Section 6 presents the estimation strategy and estimation results. Section 7 shows the model's ability to replicate the main patterns observed in the data. Section 8 presents the results of the policy reforms and their robustness to alternative model specifications and Section 9 concludes.",Disability insurance and the effects of return-to-work policies,https://www.sciencedirect.com/science/article/pii/S1094202522000588,Available online 14 November 2022,2022,Research Article,49.0
"Howard Greg,Liebersohn Jack","University of Illinois, Urbana-Champaign, 1407 W Gregory Dr, Urbana, IL 61801, United States of America,University of California, Irvine, United States of America","Received 20 October 2021, Revised 20 October 2022, Available online 28 October 2022.",https://doi.org/10.1016/j.red.2022.10.002,Cited by (1),"We document a new fact: regional divergence, the rate at which rich states grow faster than poor states, explains most U.S. ","Housing is a large part of most households' portfolios, and housing is central for fiscal and monetary policy discussions, but the determinants of house price movements are still debated. We argue that regional divergence, the rate at which rich states are growing relative to poor states, has a causal impact on house prices and is able to explain much of house price movements since World War II. The mechanism we propose matches many other features of the data, including the time series of rents, the cross-section of house prices, the location of construction, and the direction of internal migration. In addition, it suggests a new role for the effects of monetary policy which clarifies the weak empirical link between interest rates and house prices (Glaeser et al., 2012).====Regional convergence and divergence are growing topics of study in economics, largely because convergence has significantly slowed down in recent decades.==== Using a common measure of divergence based on ten-year state-level average personal income growth rates, we show that house prices and regional divergence are even more correlated than previously recognized.==== Not only is the long-term trend similar, but many fluctuations in regional divergence and house prices coincide as well, dating back to World War II and including the boom-bust-boom pattern of the last twenty years. Based on the literature's findings that industry growth is a significant driver of divergence, we use an industry-based shift-share instrument to provide evidence that the relationship between regional divergence and house prices is causal. Intuitively, when industries concentrated in rich states grow faster than other industries, divergence is high. We show such movements have an effect on house prices using the identification strategy of Borusyak et al. (2022) and the local projection methods of Jordà (2005).====Why do movements in divergence cause movements in house prices? The mechanism we present treats prices as the present value of future rents (as in Poterba, 1984) and considers specifically what determines rent expectations. When the income gap between rich and poor areas is larger, relative demand to live in rich areas is higher, leading to a larger gap in rents between poor and rich areas. Since housing supply is more elastic in poor areas, rents in poor areas are stable and rents in rich areas are higher. We model beliefs about divergence as backward-looking, i.e., people expect the current level of divergence to persist. Therefore, greater divergence implies expectations of higher future regional inequality, higher future average rents, and therefore higher house prices in the present.====We develop a parsimonious model that captures this mechanism. The model has four ingredients that build on existing research on housing and rental markets: (1) house prices are the present value of future rents; (2) people are mobile across space, defining an equilibrium cross-sectional relationship between income and rent; (3) poor areas have high housing supply elasticity; and (4) expectations of future regional divergence reflect the recent past.====Of these, assumption (4) is the most novel, and we devote a portion of the paper to explaining why the recent past determines agents' expectations. First, we establish that regional divergence helps explain a significant portion of state-level income growth. While not shocking given the existing literature on divergence, it shows that there is an important common component across states that helps predict income movements. Second, we show that divergence is both fairly noisy and highly persistent in the data, justifying why agents ought to pay attention to the recent past for predicting the future, but also ought to look at several years of data. Finally, we show that expectations data from the Michigan Survey of Consumers match what we would expect based on such an assumption.====After we feed in the data on state-level income, the model succeeds in matching many facts about house prices and rents. In the time series, we show that the model generates trends and fluctuations in rents and house prices that match the data closely. The time series of house prices and of rents look quite different from one another. We believe ours is the first paper that has even attempted to match this long of a time-series between rents and house prices.====Matching the price-to-rent ratio has been a challenge in the literature. Several papers have made the assumption that the rental and owner-occupied housing markets are segmented in order to match the volatility in the price-to-rent ratio (e.g. Greenwald and Guren, 2021). A challenge to the segmentation story is that Begley et al. (2019) show that the price-rent ratio for just rental homes exhibited the same pattern as the overall market, which is hard to explain using segmented markets.==== Therefore, an additional contribution is that our theory can explain the major movements in both rents and prices—and hence the price-rent ratio—without making the segmented markets assumption. By explaining the price-rent ratio without segmented markets, this paper shows that expectations, in addition to fundamentals, play an important role in house price determination.====Our analysis focuses on divergence at the state level rather than at the city level because state data allows us to extend the analysis back to World War II. Matching the long time series is a stronger test of our theory than focusing solely on the volatile recent years. Typically, other papers that focus on recent house price movements do not try to match such a long time period. At the same time, in matching the long time series of prices and rents the paper offers insights on recent movements in house prices. In Appendix A.2, we show robustness to measuring divergence in a variety of ways, including by CBSA.====The model is designed to explain the history of house prices in the United States, but we can further evaluate it by looking at a series of non-targeted cross-sectional relationships. First, the model is able to explain why the cross-sectional relationship between house prices and income changes over time, while the cross-sectional relationship between rent and income is relatively constant. Intuitively, when divergence is high, the income premium in high-income areas is expected to last longer, which means the rent premium in these areas is expected to last longer as well. This means that the cross-sectional relationship between incomes and prices should be greater when divergence is high than when divergence is low. In contrast, divergence should not affect the relationship between rents and incomes, which is determined by current incomes rather than expected future incomes.====Second, a well-studied fact about the U.S. housing market is that house prices in less-elastic, higher-income areas move more than one-for-one when national house prices change, whereas prices in more elastic, lower-income areas move by less than one-for-one with the national index (Mian and Sufi, 2009; Glaeser et al., 2008; Guren et al., 2021).==== Our model can match the differential sensitivities of local house prices to the national aggregate.====The model provides two main insights for understanding the history of house prices. First, local demand shocks are crucial for explaining cross-sectional variation in house prices. While supply elasticity plays a role in our theory, it cannot explain local price variation alone. Second, fluctuations in the price to rent ratio can be explained using a model of expectations based on fundamental shocks. This is an alternative to assuming segmented housing markets, as in much of the prior literature.====There are two classes of theories in the house price literature that attempt to explain movements in house prices along with the large cross-sectional price differences. The first class of theories is that there are large national housing demand changes, and that heterogeneous price changes are due to difference in the elasticity of housing supply. This class of theories is prominent in the literature due to the influential work of Mian and Sufi (Mian and Sufi, 2009, Mian and Sufi, 2011; Mian et al., 2013), and it includes much of the debate on the roles of expectations versus credit in the recent boom and the bust.====The second class of theories posits that there are varying local demand shocks and that house price movements are best-explained by shifting demand for specific locations. Sometimes these theories are thought of as more “fundamentals-based,” and as such, big swings in house prices can be discrediting. One of the best examples of a paper in this literature is Himmelberg et al. (2005), which was published at the height of the housing boom and argued that prices were in line with fundamentals. Another prominent paper in this class of models, Van Nieuwerburgh and Weill (2010), considers the long-term increase in house price dispersion, but does not propose a theory of fluctuations.==== Subsequent to our paper, research by Chodorow-Reich et al. (2021) argues, much like us, that variation in the price-to-rent ratio in the 2000s can be explained with a model of expectations rather than assuming segmented markets.====Our paper argues primarily in favor of this second class of theories, while including an important role for supply elasticity as in the first class. By modeling expectations based on recent regional divergence, our theory provides a new rationale for the large swings. The goal of our model is to match the long time series of prices and rents, but it also provides a new perspective on the more recent period. For example, because the growth rate in rich states declined in the mid-2000s—largely due to the temporary slow-down of services growth—it caused house prices to fall significantly. Our paper gives a mechanism for “fundamental”-based theories to still exhibit large swings.====Another way we contribute to this second class of theories is to explain the location of construction and migration with the help of insights from the first class of theories. National demand shocks combined with differential housing supply elasticities imply more construction in low-income housing-supply-elastic areas.==== Differential demand shocks, our class of theories, imply that construction and net migration should be higher in high-income inelastic areas. We show that data on construction and migration correspond more closely to our theory.====We also contribute to the literature on expectations in housing markets, which has been shown to be central using a variety of methodologies, but for which the fundamental causes of the change in expectations are still a matter of debate. Our contribution is to point out that regional divergence matters for expectations.==== In a sense, regional divergence is the missing ingredient that explains why expectations change.====A large literature discusses the role of expectations in the mid-2000s housing boom.==== This paper's goal is to understand the long history of prices and rents, but the 2000s boom and bust is an example that illustrates our mechanisms. As Glaeser et al. (2012) put it, ====Finally, we contribute to the literature on the relationship between house prices and interest rates (Taylor, 2007; Glaeser et al., 2008; Jordà et al., 2020). Our model has a different role for interest rates than the rest of the literature. In our model, the effect of interest rates is not always the same sign. Low interest rates put a higher weight on future income, so in times of expected divergence, low rates raise house prices, but in times of expected convergence, low rates lower house prices. Moreover, relevant to the recent experience, low interest rates make house prices more sensitive to divergence, increasing volatility.====In the next section, we show that regional divergence, as commonly measured, and house prices are highly correlated, and that there is a causal channel from divergence to house prices. In Section 3, we present a theory of why this channel exists. In Section 4, we defend the assumptions of our model. In Section 5, we show that many non-targeted predictions of the model hold in the data and that the construction and migration data are more supportive of our theory than other theories in the literature. Section 6 discusses the role of interest rates and Section 7 concludes.",Regional divergence and house prices,https://www.sciencedirect.com/science/article/pii/S1094202522000576,Available online 28 October 2022,2022,Research Article,50.0
"Lubik Thomas A.,Matthes Christian,Mertens Elmar","Federal Reserve Bank of Richmond, Research Department, P.O. Box 27622, Richmond, VA 23261, United States of America,Indiana University, Department of Economics, Wylie Hall 202, Bloomington, IN 47405, United States of America,Deutsche Bundesbank, Research Centre, Wilhelm-Epstein-Str. 14, 60431 Frankfurt, Germany","Received 19 August 2020, Revised 27 September 2022, Available online 30 September 2022.",https://doi.org/10.1016/j.red.2022.09.003,Cited by (1), rule.,"Asymmetric information is a pervasive feature of economic environments. Even when agents are fully rational, their expectation formation and decision-making process are constrained by the fact that information may be imperfectly distributed in the economy. Asymmetric information is also a central issue for the conduct of monetary policy as policymakers regularly face uncertainty about the true state of the economy, for instance, because they receive data in real time that are subject to measurement error. In environments where information is perfect and symmetrically shared, the literature has shown that policy rules can cause indeterminacy, unless they respond to economic outcomes with sufficient strength. We study equilibrium determinacy in an asymmetric information setting, where policy is conducted based on policymakers' optimal estimates of economic outcomes rather than their true values.====We consider economic environments with two types of agents, one who has full information about the state of economy while the other agent has limited information. We think of the two agents as a representative private-sector agent and a less informed policymaker. While the representative agent observes aggregate outcomes without error, the imperfect information of the policymaker can, for example, take the form of aggregate data subject to measurement error.====A key assumption of our modeling framework is that both types of agents, the policymaker and the private sector, employ rational expectations, but based on different information sets. Private-sector behavior is characterized by a set of linear, expectational difference equations. On the other hand, the policymaker's behavior is characterized by an instrument rule, which responds to the policymaker's estimates of economic conditions. Specifically, we consider the conduct of monetary policy with a Taylor-type interest-rate rule that responds to the policymaker's projection of current inflation rather than its actual value. Formally, we consider linear, stochastic equilibria with time-invariant decision rules and Gaussian shocks. In this case, the rational inference efforts of the policymaker are represented by a dynamic signal extraction problem as captured by the Kalman filter. The interaction of the two expectation formation processes is the source of the new mechanism underlying equilibrium multiplicity in our environment.====The central result of our paper is that indeterminacy is widespread for a broad class of linear imperfect-information models that have unique equilibria under full information. In our imperfect information setup, optimal information processing of the less informed agent introduces stable dynamics into the equation system that can transmit self-fulling belief shocks. The interaction of the two expectation processes generates an endogenous feedback mechanism similar to strategic complementarities or the application of ad-hoc behavior in the standard indeterminacy literature.====When there is indeterminacy in the perfect-information case, there are no restrictions on the scale and direction of effects caused by belief shocks. In contrast, the potential effects of belief shocks are tightly bounded in our imperfect information environment. The bounds arise from the required consistency of expectations of the public and the policymaker and the assumption that we consider only environments that have a unique equilibrium under full information. While the rationality of expectations under both information sets places non-trivial restrictions on outcomes, they are not sufficient to rule out multiple equilibria. Moreover, the interplay of expectations based on different information sets results in equilibrium outcomes that are not certainty equivalent even though we only consider environments that are linear.====We illustrate key insights from our framework in three models of inflation determination, where monetary policy follows a Taylor-type interest-rate rule. The rule satisfies the Taylor principle, that is, it responds more than one-for-one to the policymaker's reading of current inflation, which guarantees determinacy under full information. Under imperfect information, however, the policymaker does not observe all equilibrium outcomes, and the policy rule responds only to optimal projections of current endogenous outcomes. The sensitivity of the policy rate to movements in the endogenous variables thus depends on the sensitivity of the policymaker's projection to the incoming signal. The more the central bank is successful at stabilizing inflation, for example by reacting more aggressively to projected inflation, the noisier will be the signal and the policymaker's projection will barely respond. By the same logic, non-fundamental shocks cannot become an arbitrarily large driver of inflation. Otherwise, the central bank's signal would become highly informative and the policy rate would respond with sufficient strength to actual inflation to re-establish determinacy.====Our paper touches upon three strands in the literature. First, we contribute to the burgeoning literature on imperfect information in macroeconomic models that focuses on the implications of dispersed information among different members of the public and the resulting effects on their strategic interactions and the informational value of prices. Key contributions by Nimark (2008a, 2008b, 2014), Angeletos and La'O (2013), and Acharya et al. (2021) demonstrate that imperfect information has important implications for the amplification and propagation of economic shocks. Papers in this literature often feature non-nested information sets, which leads to an infinite number of state variables. Our assumptions on the information structure allow us to derive a finite state representation of the solution to our models.====In Angeletos and La'O (2013), aggregate fluctuations are driven by non-fundamental shocks, which are modeled explicitly as exogenous shocks to agents' beliefs, and the equilibrium is unique and always exists. In contrast, the equilibrium in Benhabib et al. (2015) is unique without non-fundamental shocks. Their sentiment shocks arise endogenously and their existence and statistical properties depend on the model's primitives. Acharya et al. (2021) extend the notion of this sentiment equilibrium to allow for persistent sentiment processes. Our paper is similar to this work in the sense that the sunspot shocks arise endogenously, and that by adding imperfect information the unique equilibrium property fails to hold. We differ in that we explore forward-looking behavior and consider hierarchy of information structure instead of dispersed information. In that respect, our framework is closer to Rondina and Walker (2021). More recently, Fajgelbaum et al. (2017) have studied information externalities in the private sector, which lead to uncertainty traps, namely self-reinforcing periods of high uncertainty and low activity. Similarly, Angeletos et al. (2020) and Kozlowski et al. (2020) model endogenous learning in the private sector which can be seen as complementary to our setup of endogenous learning by policymakers.====Second, our research makes a contribution to the literature on indeterminacy in linear rational expectations models by expanding the set of plausible economic mechanisms that can lead to multiple equilibria. A key element of the indeterminacy literature is the presence of a mechanism that validates self-fulfilling expectations. These could arise from what is often termed strategic complementarities, such as increasing returns to scale in production that are not internalized, as in the seminal contributions of Benhabib and Farmer (1994), Farmer and Guo (1994), and Schmitt-Grohe (1997). An alternative mechanism is the interplay between economic agents' forward-looking behavior and the reaction function of a policymaker, which Clarida et al. (2000) and Lubik and Schorfheide (2004) show to be a key feature of macroeconomic fluctuations.====In contrast, our framework does not rely on these previously identified sources of indeterminacy but rather on the interaction of different expectation formation processes under asymmetric information. This also sets our framework apart from the broader imperfect information literature, which is largely concerned with the strategic interaction between agents in the private sector. Although our framework utilizes the root-counting formalism of the indeterminacy literature, where we build on the contributions of Lubik and Schorfheide (2003, 2004) and Farmer et al. (2015), the mechanism to get there is novel. Critically, while equilibria are described by solutions to linear difference systems, the roots of these dynamic systems depend on endogenous Kalman gains and are not invariant to the equilibrium outcomes. We show that the set of multiple equilibria, despite the pervasiveness of indeterminacy, is tightly circumscribed by internal consistency requirements for the interaction between the two expectation processes. Our paper thereby puts some caveats on the notion that sunspot shocks are unrestricted in their effects on macroeconomic outcomes.====Third, the examples in our paper speak to the monetary policy literature concerned with the effects of interest-rate rules on determinacy. A well-known result from this literature is the Taylor principle, which requires that interest rate rules respond to endogenous variables with sufficient strength, to avoid multiple equilibria and ensure determinacy. Clarida et al. (2000) and Lubik and Schorfheide (2004) have pointed to a neglect of the Taylor rule as a possible factor behind the Great Inflation. However, their evidence is based on a full-information perspective that does not account for the uncertainties faced by the Federal Reserve in assessing the state of the economy in real time, as discussed by Orphanides (2001). Our results point to the critical role played by the central bank's (in)ability to observe the output gap (and other policy-relevant variables) when seeking to design rules that conform to the Taylor principle.====Similar to our framework, Orphanides (2003) models the economic consequences of an imperfectly informed central bank that responds to estimates of economic conditions generated by optimal signal extraction efforts. But in a fundamental difference to our framework, his model is purely backward-looking so that the issue of indeterminacy does not arise. Our paper also relates to Svensson and Woodford (2004) and Aoki (2006) who derive conditions for optimal policy when the policymaker is less informed than the public in forward-looking linear rational expectations models, but take determinacy as given.====We proceed as follows. In the next section, we introduce and motivate our framework by means of simple examples drawn from the monetary policy literature, for which we can derive analytical results. Section 3 provides a general description of the framework. We present a general linear rational expectations framework with asymmetric information sets and use results from general linear systems theory to derive properties of the resulting equilibria. We present three quantitative examples in Section 4. Section 5 concludes and discusses further extensions of our framework. An online appendix provides additional results and detailed derivations.",Indeterminacy and imperfect information,https://www.sciencedirect.com/science/article/pii/S1094202522000552,Available online 30 September 2022,2022,Research Article,51.0
"Carbonero Francesco,Offermanns Christian J.,Weber Enzo","University of Turin, Italy,Institute for Employment Research, Nuremberg, Germany,University of Regensburg, Germany,Deutsche Bundesbank, Germany","Received 23 March 2020, Revised 16 April 2022, Available online 19 September 2022.",https://doi.org/10.1016/j.red.2022.09.001,Cited by (2),"Documenting an average drop of the labor share of eight percentage points for eight European countries and the US between 1980 and 2007, we analyze the role of technological progress and labor market frictions. According to our results, while capital-labor substitution in general was not crucial, Information Communication Technology (ICT) explains more than half of the decline in the labor share, given an estimated ==== with the labor input of 1.18. Considering hiring costs slightly dampens the estimated substitution effect at aggregate level. Additionally, by modeling the substitution between ICT and labor with a set of key labor market variables, we find it to be linked to both the share of routine occupations (positively) and the share of high-skill workers (negatively) with a similar strength.","The labor income share (LS) is discussed in empirical studies on income distribution and in several macroeconomic calibrations. LS constancy is one of the so-called Kaldor's facts, and a value of 2/3 is usually adopted. However, recent studies reveal that the LS has declined for most of the OECD countries since the 1980s [OECD (2012), Raurich et al. (2012), Arpaia et al. (2009)]. This decline might be only temporary; however, it appeared at the same time that the adoption of new technologies gave rise to job polarization and occupational displacement, phenomena that are considered, at least in the public debate, to be irreversible. We contribute to the literature by theoretically and empirically analyzing the substitution between Information Communication Technologies (ICT henceforth) and labor, together with labor market imperfections and institutional and structural labor market variables.====We first compute the labor share based on labor income data from the EU KLEMS database for eight European countries and the US. The aggregate LS dropped from 71 percent to 63 percent between 1980 to 2007.==== There is substantial heterogeneity in the speed and the timing of the decline, but - except for Denmark - all the countries display a persistent drop in LS after 1990. Secondly, we look at the evolution of the price index for a specific type of capital input, namely, ICT. According to a wide range of studies, indeed, the decline in the price for computers and digital equipment is the source of important new trends in the production process, such as automation and occupational displacement. EU KLEMS provides the gross fixed capital formation price index for ICT and non-ICT and we show that the decline of the capital investment price is connected mainly to the downward evolution of the ICT equipment price. Building on that, we set up a theoretical framework to rationalize the relationship between ICT price, hiring costs and the labor share. The model provides two harmful mechanisms for the labor share, a labor-ICT substitution effect and a hiring cost effect, which we quantify by estimating the elasticity of substitution between ICT capital and labor.====Karabarbounis and Neiman (2014), using EU KLEMS data, find an elasticity between aggregate (i.e., ICT and non-ICT) capital and labor of 1.17. This result is questioned by Autor et al. (2017) and Lawrence (2015) who point out that the majority of studies argue in favor of an elasticity lower than one. In this paper, we provide further insights on the substitution between production inputs. Indeed, when we estimate the elasticity of substitution between aggregate capital and labor, we find that it lies between 0.7 and 1, as in several previous studies.==== However, when we separate ICT and non-ICT capital and consider them simultaneously, the elasticity between ICT capital and labor is higher than one, namely, 1.18. Given the downward trend in the ICT price, this estimated elasticity is responsible for half of the decline of the labor share.====Besides the distinction between different types of capital, we add to the literature along a further direction. Indeed we assess the extent to which the elasticity of substitution between ICT and labor is affected by country-specific structural and institutional labor market variables. The literature on the impact of technological change on labor markets reveals that, on one hand, the adoption of ICT raises the demand for high-skill workers (the skill-biased view) and, on the other hand, shrinks the employment share of routine occupations (the job polarization view). A recent contribution on this topic comes from Eden and Gaggl (2018), who find important effects of ICT on routine labor. Regarding institutions, lower employment protection legislation and firm-level wage bargaining have been assessed as potential channels of the impact of higher international competition on the labor share (OECD, 2012). In this paper, we examine these forces as well as the role of unemployment benefit replacement rate and of union density. The main finding of our analysis is that countries with a high share of routine occupations (high-skill workers) also exhibit a larger (smaller) elasticity of substitution between labor and ICT capital. As both factors are of a similar strength, our findings suggest that both individual characteristics (qualification) and job characteristics (task structure) play an equal role in shaping the labor market impact of technological change. By the same token, the results connect the implications of ICT adoption to the job polarization phenomenon, the task composition of jobs and the drop in the LS.====The paper proceeds as follows. Section 2 documents the decline in the labor share and in the capital price index, at the aggregate and country levels. Here, we provide evidence of the different evolutions of the price of ICT and non-ICT capital. Section 4 discusses the most recent contributions on the impact of technological change on the labor market. In particular, we review the job polarization theory and the role of ICT for routine tasks, which allows us in Section 3 to derive a theoretical setting that links the labor share, the ICT price and the hiring costs. Section 5 describes the data sources and the variables we use for the empirical analysis. Finally, in Section 6 and Section 6.2, we assess the validity of the theoretical predictions and model the elasticity parameter with state space functions of country-specific labor market variables.",The fall of the labor income share: The role of technological change and hiring frictions,https://www.sciencedirect.com/science/article/pii/S1094202522000539,Available online 19 September 2022,2022,Research Article,52.0
"Altonji Joseph G.,Hynsjö Disa M.,Vidangos Ivan","Yale University, NBER, and IZA, United States of America,Yale University, United States of America,Federal Reserve Board, United States of America","Received 18 June 2021, Revised 22 May 2022, Available online 8 September 2022.",https://doi.org/10.1016/j.red.2022.08.005,Cited by (0),"We review research on the dynamics and distribution of individual earnings and family income. We start with univariate earnings models, which dominate the literature and are often used as the exogenous component of family income in structural models of saving. We present a version of the linear model that nests most of the specifications that have been used in the literature, and then discuss recent papers that stress nonnormal shocks, nonlinear and age-dependent processes, and heterogeneous model parameters. The recent work provides a much richer description of the nature of earnings volatility than the basic model. We then turn to models of individual earnings that are based on wages, employment, job mobility, and hours. These multivariate models permit measuring the sources of permanent differences in earnings and distinguishing among shocks that influence earnings through employment, job mobility, general productivity, or hours. Finally, we consider models of lifetime family income that integrate individual earnings, marriage (accounting for marital sorting), and earnings of a spouse, if present. We conclude by discussing directions for future work.","This paper reviews research on the dynamics and distribution of individual labor earnings and of total family income. It is motivated by the following questions: What drives the distribution of earnings? What drives the distribution of family income that an ==== experiences over his or her adult life? What factors determine the dynamics and distribution of an individual's earnings, family labor earnings, and nonlabor income? And what are the channels through which various factors–such as unemployment–work their influence? The answers to these questions lie in the labor market and in the marriage market. In the labor market, ability, education, on-the-job training, unemployment shocks, job search, randomness in the wage offers people receive, as well as labor supply preferences and shocks, are central. In the marriage market, time spent married versus single, and whom one marries–which determines the earnings potential of one's partner–are critical.====A vast literature–which began in the 1970s and is still very active–has studied univariate processes of earnings or income dynamics and their implications for inequality. Key early contributions include Lillard and Willis (1978), Lillard and Weiss (1979), Hause (1980), and MaCurdy (1982). Other important papers include Baker (1997), Chamberlain and Hirano (1999), Geweke and Keane (2000), Haider (2001), Baker and Solon (2003), Meghir and Pistaferri (2004), Guvenen (2009), Hryshko (2012), DeBacker et al. (2013), Karahan and Ozkan (2013) and Hoffmann (2019). This literature typically assumes that individuals receive an exogenous, but uncertain, stream of earnings.==== It seeks to identify the stochastic process that best describes various aspects of the earnings data and characterizes the earnings risk faced by individuals.====One important reason for the popularity of univariate earnings models is that they are often used as inputs to the study of consumption and savings behavior, the measurement of risk, and the measurement of uncertainty.==== They are a critical component of many quantitative partial equilibrium and general equilibrium models that are used in macroeconomics and public finance to study questions such as the distribution of wealth, the costs of business cycles, and the effects of taxes and transfers. They are useful in part because they provide a parsimonious way to summarize earnings dynamics that economizes on state variables. In the quantitative models, the exogenous earnings process–together with a specification of tax and transfers rules and the model of savings–determines the family income process.====Univariate earnings models have also been used to study earnings inequality. A series of papers, dating from the seminal contributions of Gottschalk and Moffitt (1994) and Moffitt and Gottschalk (1995), have studied the role of permanent and transitory shocks in earnings inequality and in changes in inequality over time. Important contributions include Moffitt and Gottschalk (2011), Haider (2001), Baker and Solon (2003), and DeBacker et al. (2013). Univariate models of the income process have also played an important role in measuring insurance through taxes and transfers and through the income of other family members (Blundell et al. (2008), Blundell et al. (2015)).====In Section 2, we provide an overview of recent developments in the univariate literature. To set the stage, we present the linear model with random intercepts, random slopes, a persistent autoregressive component, a transitory component, and measurement error. The model nests most of the models that have been used in the literature, including in many recent papers. We then turn to recent papers, which have stressed nonnormal shocks, nonlinear and age-dependent processes, and heterogeneous model parameters. These papers provide a much richer quantitative description of the nature of earnings volatility than the basic model.====Univariate models of the earnings process provide a valuable summary of earnings dynamics and distribution as well as a tractable building block for structural models of savings and insurance. But for some purposes, multivariate models are required. For example, with a multivariate model, one can distinguish the effects of permanent heterogeneity in wage rates and in hours. One can also distinguish the sources of shocks that work through job mobility, employment, general productivity, or hours given employment. In Section 3 we turn to multivariate models of employment, wages, job mobility, work hours, and individual earnings. Some of these studies are structural, while others are statistical but closely guided by theory. Here we discuss in detail three papers that illustrate three different approaches to modeling individual earnings. The first paper is Bagger et al. (2014), who provide an equilibrium job search model of wages, employment, and job mobility. The second paper is Low et al. (2010), who use a structural lifecycle model of consumption, labor supply, and job mobility. The third is Altonji et al. (2013), who provide a joint model of employment, job changes, wage rates, work hours, and earnings. Their model is “semi-structural” in the sense that the model equations approximate the decision rules for intertemporal optimization based upon the current values of the state variables, but the equations are not based on specification of the intertemporal objective function and constraints agents face.====We review these three papers and summarize some of their results, focusing on the sources of earnings growth over the lifecycle (job shopping and bargaining, human capital accumulation, and job tenure); how shocks work themselves out in determining earnings; and what these studies have taught us about what drives the distribution of earnings in the cross-section and over a lifetime. Key results from these papers include the following. First, earnings growth over a career is mostly explained by human capital accumulation, although job shopping also plays a role, especially early in a career. The relative contributions of the different sources of growth vary with level of education. Second, unemployment shocks lead to large drops in hours–which recover fairly quickly–and smaller but sizeable drops in wages, which recover more slowly. The wage losses reflect a reduction in general productivity, a decline in average job match quality, and the loss of tenure. Unemployment shocks also lead to a large jump in the variance of earnings changes. The jump is due to a high degree of variation in how long a worker remains unemployed, variation in the quality of the job offers that the worker receives coming out of unemployment, and the fact that state dependence in employment implies that the worker now faces higher odds of experiencing another unemployment spell. Third, firm- or match-specific components of earnings matter a lot for the cross-sectional dispersion in earnings, as many papers have found. In Bagger et al. (2014), this takes the form of a firm-level productivity component, while in Altonji et al. (2013) it takes the form of firm- or match-specific wage and hours components.====In Section 4, we move beyond individual earnings to bring the family, and family income, into the picture. We do so because consumption depends on the household's resources, and the family is the unit on which macroeconomic models are typically focused.==== This discussion is based on work by Altonji et al. (2022), who build and estimate a dynamic model of individual earnings, marriage, nonlabor income, fertility, and family income. The model is similar in spirit to Altonji et al. (2013), but less closely tied to theory. We summarize the model and review some key results from this work. These include the dynamic effects of labor market shocks on marital outcomes and the dynamic effects of marriage, labor market, and fertility shocks on labor market outcomes and family income. The results uncover very large gender differences in the effects. For example, marital status has a much larger effect on family income for women than for men, while labor market shocks to men are more important than shocks to women. These asymmetries in turn reflect differences in the sensitivity of hours and employment to children and marriage as well as the large share of male earnings in family income. The model is also used to examine the role of marital sorting in the determination of earnings and family income. One key finding is that marital sorting plays a major role in the family income return to education and permanent wages, especially for women. Finally, the model is used to decompose the variance of lifetime family income (adjusted for family size) into various sources of variation. One key finding here is that random variation in marital histories accounts for a material share of the variance in lifetime family income for women, but less so for men. Marital sorting also increases the variance of family income more for women than men.====We conclude the paper with a brief discussion of directions for future work.",Individual earnings and family income: Dynamics and distribution,https://www.sciencedirect.com/science/article/pii/S1094202522000461,Available online 8 September 2022,2022,Research Article,53.0
Bueren Jesús,"European University Institute, Italy","Received 4 March 2021, Revised 22 August 2022, Available online 30 August 2022.",https://doi.org/10.1016/j.red.2022.08.004,Cited by (0),"In this paper, I investigate to what extent heterogeneity in both long-term care (LTC) needs and informal care support affects the savings decisions of the old. For this purpose, I develop and estimate a model of retired single individuals where agents are exposed to physical and/or cognitive health deterioration that triggers demand for LTC. To cope with LTC, agents differ in the amount of informal care provided by relatives and can purchase formal care at a market price to supplement this. I find that (i) LTC is relatively more important than bequest motives in explaining the lack of dissaving of individuals with limited access to informal care, (ii) concurrent cognitive and physical limitations account for most of the ==== related to LTC, and (iii) Abstracting from informal care provision from relatives overestimates the welfare gains from expansions in government-provided means-tested care programs.","The uncertainty about the need for long-term care (LTC) during retirement and its associated cost constitutes a major financial risk faced by the old. However, not everyone is equally exposed to this. Some individuals rely on their families to partially offset the financial burden entailed by LTC expenses. Moreover, not all states of LTC entail the same level of need. As health deteriorates, cognitive and/or physical impairment require an increasing dependence on both formal (either in the form of a caregiver or nursing homes) and informal care. In this paper I first investigate how these two margins of heterogeneity (informal care access and level of LTC need) affect the savings decisions of the old. Then, I study the importance of taking into account the provision of informal care from relatives for evaluating the welfare implications of expansions in government provided LTC programs.====For this purpose, I start by documenting three novel facts about the heterogeneity in LTC needs and access to informal care in the data. First, using the Health and Retirement Study (HRS), I document that LTC needs can be parsimoniously represented by four latent health states labeled as: healthy, physically frail, mentally frail, and impaired. Healthy individuals do not need help with daily self-care activities. In contrast, physically and mentally frail individuals are in need of assistance with activities related to mobility and cognition, respectively. While these two groups do not show large differences in mortality rates, mentally frail individuals consume more formal care if they live in the community and face a larger probability of living in a nursing home. Impaired individuals are in need of assistance with both physical and cognitive tasks and are the ones consuming the most formal care and facing larger chances of moving into nursing homes. High mortality rates for impaired individuals reduce the risk of being in acute need of care for long periods of time, thus limiting the financial risk implied by LTC.====Second, I document that formal LTC expenses in the community reflect choices and hence cannot be taken at face value to measure needs. In fact, conditional on health, richer individuals spend significantly more on formal care when the patient lives outside a nursing home. An impaired individual in the top quintile of the permanent income distribution consumes three hours more of formal care at home per day than an impaired individual in the bottom quintile.====Finally, the data reveals that the provision of informal care from relatives affects the consumption of formal care when in need of LTC. Conditional on health and living in the community, individuals who have limited access to informal care consume 2.5 times more formal care than those who have strong informal care support. Moreover, the probability of nursing home entry is around 30% higher for those with limited access to informal care. Therefore, differences in access to informal care imply a differential exposure to LTC risk.====Motivated by these facts, I develop and estimate a model of single retired individuals allowing for heterogeneity in both LTC needs and family types, as well as in gender, permanent income, medical expenses, nursing home entry, and wealth. Agents in the model derive utility from regular consumption, LTC, and leaving bequests. Family types differ in the hours of informal care provided by children when the old is in need of LTC and drive the probability of nursing home entry. Families provide informal care for free but agents can, additionally buy formal care at a market price. Then, LTC is produced by combining both formal and informal care through a CES production function. The marginal utility of LTC is also allowed to differ depending on the level of LTC need. When individuals enter in a nursing home, they lose most of the informal care support and need to pay for a fixed quantity of formal care. The willingness to bestow is modeled using a warm-glow utility function. In order to capture potential differences in the willingness to bestow, I allow the marginal utility of bequest to vary across family types. Finally, agents have the option to access a government means-tested program that provides a consumption floor and LTC services if necessary.====In order to characterize family types in the model, I investigate socio-economic and demographic characteristics that predict informal care support in the data. Having children (especially a daughter), being African American, and a low education level are strong predictors of future access to informal care if in need of LTC. Based on these characteristics, I find that around one-third of individuals receive high informal care support from relatives if in need of LTC and outside a nursing home. I label them as ==== families. On the other hand, two-thirds of individuals are classified as belonging to ==== families and receive little informal care support. In order to estimate potential differences in the willingness to bestow across family types, I require the model to match the self-reported probability of leaving bequests and Medicaid recipiency rates for each family type. The model estimates that the willingness to bestow varies little across family types.====I construct counterfactual simulations to identify the relative importance of LTC needs and bequest motives across family types. I find that LTC needs are relatively more important than bequests as drivers of savings for individuals in ====. In a world without LTC needs, the median of wealth holdings at the age of 85 would be 45% lower than in the benchmark model for individuals in distant families. As we move along the wealth distribution, the importance of bequests increases. At the ==== percentile of the wealth distribution of individuals in distant families, the importance of bequest and LTC is quantitatively comparable. On the other hand, ==== families are much better insured against LTC needs. The slow dissaving of rich individuals in ==== families is almost exclusively explained by bequest motives.====Next, I use the model economy to quantify what fraction of the precautionary savings related to LTC can be attributed to physical difficulties, mental difficulties or both. At the age of 85, 80% of the precautionary savings related to LTC can be attributed to concurrent physical and cognitive deterioration. Physical care and mental care needs separately play a quantitative small role on savings.====Finally, I assess the importance of taking into account the provision of informal care from relatives for understanding the savings decision of the old. For this purpose, I estimate a model omitting informal care provision and matching the same set of moments averaged across family types. I find that a model omitting the provision of informal care would underestimate the importance of LTC as a driver of savings for individuals in distant families but would overestimate it for individuals in close families. Furthermore, I show that considering informal care is important for public policy as a model without informal care would overestimate the welfare gains of individuals in close families from expansions in government-provided LTC means-tested programs.====My paper is related to the literature that analyzes the interaction between savings and the provision of informal care during retirement. Barczyk and Kredler (2018) and Barczyk et al. (2019) develop a theoretical framework where provision of informal care is ex-ante available to everyone but arises endogenously from a bargaining process between parents and their offspring. In their work, the old have an incentive to accumulate wealth to induce their children to provide informal care and thus informal care is ex-post heterogeneous. In my paper instead, I focus on ex-ante heterogeneity based on a set of time-invariant children characteristics that predict provision of care when parents are in need of LTC. While Barczyk and Kredler (2018) also document the presence of ex-ante heterogeneity, they do not incorporate it into their model. Moreover, in contrast to Barczyk and Kredler (2018) where bequests are exchange motivated, in this paper, I model the bequest motives as a warm glow for several reasons. First, this formulation is convenient as it simplifies computations by avoiding modeling strategic interactions between parents and children. Second, it can be very flexible and thus I can allow the parameters of the bequest function to vary across family types. Finally, previous work has found substantial heterogeneity in the bequest motives depending on individual characteristics (Kopczuk and Lupton 2007; Laitner and Juster 1996). Thus, the literature has not reached a consensus on how to best model bequests.====Lockwood (2018) develops a model with exogenous LTC expenses and finds that strong bequest motives can rationalize the low take-up of LTC insurance. By both endogenizing LTC expenses and requiring the model to match consumption of formal care for different levels of the permanent income distribution and family types, I find that LTC needs play a larger role quantitatively as a determinant of the savings behavior of individuals in ==== but are of comparable magnitude for individuals in close families.====In contrast to Lockwood (2018), Ameriks et al. (2020) estimate a much smaller intensity of the willingness to bestow. Using a novel dataset with strategic survey questions, the authors document that when faced with hypothetical scenarios between leaving a bequest or consuming when in need of LTC, individuals report a large propensity to spend when in need of LTC. In my paper, I study which particular LTC needs drive savings the most. I find that while physically frail individuals hold marginal propensities to consume close to the healthy ones, impaired individuals, who represent one fourth of individuals in need of LTC, hold marginal propensities to consume versus bequeath that are in line with Ameriks et al. (2020).====Dobrescu (2015) estimates a model for a set of European countries in which individuals can produce care using a combination of formal and informal care. The author assumes that the provision of informal care is exchange motivated by making it a function of bequeathable wealth and ruling out altruistic bequests. In order to match the fact that individuals in southern Europe are more likely to receive informal care in spite of holding lower wealth, the author allows for an elasticity of care to wealth which is larger for children in southern Europe. Through the lens of my model, differences across European countries could potentially be rationalized by variation in socio-economic and demographic factors that drive the share of ==== and ==== (e.g. education of parents and children, ethnicity, divorce) as well as cultural differences in family arrangements.====In the empirical literature, there is a lack of consensus on how future bequests affect the provision of informal care. On the one hand, Brown (2006) and Groneck (2016) find that end-of-life transfers tend to favor both current and expected caregivers. On the other hand, Mukherjee (2020) finds little support for exchange motivated transfers using variation in Social Security benefits. My parameter estimates imply that the intensity of the bequest motive does not significantly vary across family types in spite of the large differences in the provision of informal care and the fact that individuals in close families tend to accumulate lower savings during their working life. These facts point towards a stronger role for altruistic bequest motives even if in my setting I cannot explicitly separate them from exchange motivated transfers.====From the policy point of view, previous literature has quantified the welfare consequence of changes in the current structure of US government insurance through means-tested programs. Kopecky and Koreshkova (2014) find large welfare gains from increases in the generosity of transfers to nursing homes residents. De Nardi et al. (2016) find that expansions of Medicaid would be valued at less than its cost. Barczyk and Kredler (2018) find welfare gains from subsidies to formal care. In this paper I show that in order to analyze the welfare benefits of expansions of means-tested LTC programs it is important to take into account heterogeneity in the provision of informal care. First, I find that for individuals in close families, the cost of expanding the generosity of formal care in means-tested programs outweighs welfare gains. Second, I show that abstracting from the provision of informal care would over-estimate the welfare gains from increases in the generosity of LTC means-tested programs.====The rest of the paper is organized as follows. In Section 2, I explain how I identify different levels of LTC needs from the data and document new facts on LTC expenditure choices. Then, I propose a model that is able to accommodate these facts in Section 3. In Section 4, I present counterfactual experiments to quantify the forces affecting the saving behavior. Section 5 concludes.",Long-term care needs and savings in retirement,https://www.sciencedirect.com/science/article/pii/S109420252200045X,Available online 30 August 2022,2022,Research Article,54.0
"Geromichalos Athanasios,Herrenbrueck Lucas,Lee Sukjoon","University of California – Davis, United States of America,Simon Fraser University, Canada,New York University Shanghai, China","Received 28 March 2022, Revised 7 July 2022, Available online 12 August 2022.",https://doi.org/10.1016/j.red.2022.08.003,Cited by (0),"We study asset liquidity in a model where financial assets can be liquidated for money in over-the-counter (OTC) secondary markets, in response to random liquidity needs. Traders choose to enter the market where they expect to find the best terms, understanding that their chances to trade depend on the entry decision of other investors. We find that small differences in OTC microstructure can induce very large differences in the relative liquidity of two assets. We use our model to rationalize, qualitatively and quantitatively, the superior liquidity of U.S. Treasuries over equally safe corporate debt.","Why do U.S. Treasuries sell at higher prices than corporate or municipal bonds with similar characteristics, even after controlling for safety?==== A popular answer is “due to their ====”. More precisely, the Treasury sells its bonds at a ==== because investors expect to be able to (re)sell these bonds easily in the secondary market and are, thus, willing to pay higher prices in the primary market.==== While this is a plausible explanation, some important questions remain. Why are the secondary markets for other types of bonds less liquid than the one for Treasuries? Is it hard(er) for sellers to find buyers due to some hardwired market friction (e.g., a poorly organized interdealer network)? Or, are there not enough buyers drawn to those markets to whom I could sell my bonds – and if so, why? Or, perhaps finding trading partners is not so hard, but there are not enough bonds to go around in the market? Finally, how do these candidate explanations (and their interaction) affect asset prices and liquidity in general equilibrium?====To answer these questions, we develop a model where liquidity depends not only on the (exogenous) characteristics of the market an asset trades in, but also on the (endogenous) decision of agents to visit that market. Our model has two main ingredients. The first is an empirically relevant concept of asset liquidity: agents can liquidate assets for money in over-the-counter (OTC) secondary markets which, as in Duffie et al. (2005), are characterized by search and bargaining. This implies that assets are imperfect substitutes for money and have, generally, positive liquidity premia. The second ingredient is an entry decision by the agents. Each asset trades in a distinct OTC market, and agents choose to visit the market where they expect to find the best terms. Additionally, we explore the endogenous determination of the supply of liquid assets. Concretely, we focus on two issuers of assets who play a differentiated Cournot game, where, crucially, the product (asset) differentiation stems from differences in the ==== of the secondary market where each asset trades.====First, we study the endogenous determination of OTC market participation, keeping asset supplies fixed. Agents receive an idiosyncratic shock that determines whether they will need, ex-post, additional liquidity in the secondary market (i.e., sell assets) or whether they will be the providers of that liquidity (i.e., buy assets). An agent who turns out to be an asset seller can only visit one OTC market at a time; since, typically, assets are costly to own due to the liquidity premium, agents choose to ‘specialize’ ex-ante in asset ==== or ====. Unlike sellers, who must take into account the cost of holding a particular asset, the asset buyers make their market choice in a more ‘elastic’ way since their money is good to buy any asset. As a result, when one of the markets, say market ====, has any kind of advantage – an exogenous matching advantage or simply offering bigger surpluses because there are more ====-assets to be traded – asset buyers rush into that market more eagerly than sellers. In turn, this implies that the trade probability in that market for sellers increases by far more than that for buyers. Crucially, it is the sell-probability that affects the issue price, because someone who buys an asset (in the primary market) cares about the ease of selling it later.====Through this channel, small differences in market microstructure can be ==== into a big endogenous liquidity advantage for one asset, even if the matching function exhibits constant returns to scale (CRS). When we consider increasing returns to scale (IRS), our channel becomes further amplified because IRS promote concentration of investors in the market with the exogenous advantage.====Thus, our model can shed some light on the superior liquidity of U.S. Treasuries over equally safe corporate or municipal bonds. One may argue that this stylized fact has an easy explanation: the secondary market for Treasuries is more well-organized (which in our model would be captured by a more efficient matching technology). However, the relative illiquidity of corporate or municipal bonds has been well-documented for many decades. If the key behind this illiquidity was just some poorly organized secondary markets, one wonders why the issuers of these bonds have not taken steps to improve the efficiency of these markets, which would lower the rate at which they can borrow. Hence, it seems unlikely that the stylized fact in question can be purely explained by differences in market efficiency. Our model can offer a deeper explanation: perhaps Treasuries have a small exogenous advantage over other types of bonds, but this is amplified into a large endogenous liquidity advantage by the fact that investors choose to concentrate their trade into the secondary market for Treasuries, rather than get exposed to the liquidity risk associated with trading other types of bonds.====To quantitatively assess the importance of this amplification mechanism, we calibrate our model to basic facts about yields in US fixed income markets, and use it to estimate how large the exogenous liquidity differences must be in order to match the difference between Treasury and high quality corporate bond yields observed in the data. We find that, even if we assume CRS, our model requires the matching technology in the corporate bonds market to be just seven percent less efficient than the one in the Treasury market to perfectly match the data. And with just a small degree of IRS, the exogenous liquidity differentials that are required to match the data virtually vanish (see Section 4.1 for details).====We also perform a counterfactual exercise. The secondary corporate bonds market is known to be particularly segmented, and practitioners (BlackRock, 2014) have argued that corporate bond liquidity would benefit from moving to a more consolidated secondary market.==== To test this proposal, we develop a version of our model with three assets trading in three distinct secondary markets – representing Treasuries, AAA, and AA corporate bonds – and ask the model what happens when we consolidate the corporate bond markets. Indeed, we find that the liquidity premia for both AAA and AA bonds increase, while the one on Treasuries decreases.====Next, we study the duopoly game between two issuers, who realize that the demand for their assets depends on the (exogenous and endogenous) liquidity characteristics of the secondary markets where their assets trade.==== When the matching technology exhibits CRS, asset supplies tend to be strategic substitutes. In this case, equilibrium issue sizes are low, and the prices of both assets include liquidity premia. When the matching technology exhibits IRS, asset supplies tend to be strategic complements. This promotes aggressive competition among issuers, in the sense that equilibrium issue sizes can be large, and that equilibria of the subgame tend to be in a corner in which only one of the two OTC markets operates, and therefore, only one asset ends up liquid.====We also study how changes in the exogenous market microstructure affect optimal issue sizes, and, consequently, asset prices and liquidity premia. More precisely, letting ====, ====, denote the matching efficiency in the OTC market, we start with ==== and study the effect of decreases in ====. The exogenous liquidity advantage of asset ==== is magnified by the entry choices of agents, which, in turn, feeds back into a rising (falling) liquidity premium on asset ==== (====). As ==== declines further, there comes a point at which issuer ==== has an incentive to boost up her supply and drive ==== out of the secondary market altogether. At that point asset ==== becomes fully illiquid. As ==== falls even further, the threat of competition by asset ==== becomes so insignificant that issuer ==== practically turns into a monopolist in the supply of liquid assets.====With a degree of IRS in the matching technology, this process is accelerated. We show that asset ==== will become completely illiquid even if the matching function in market ==== is almost equally efficient as the one in market ==== (say, ====), and there is only a tiny amount of IRS in the matching function. If one were to look at these numbers, one might infer that asset ==== cannot be much less liquid than asset ====. This conclusion would be mistaken, because it would be based only on the exogenous factors. What is more important is that agents endogenously choose to concentrate their trade in market ==== because they expect other agents will do the same – and, reinforcing this, because both issuers have an incentive to compete for this concentration by issuing large (enough) amounts.====Finally, our model delivers some important results regarding welfare. First, and most importantly, there exists no monotonic relationship between welfare and “liquidity” (for any measure of liquidity we could choose). Second, unlike output, social welfare tends to be maximized for small-to-intermediate quantities of liquid assets. This alone does not tell us whether a monopoly or a Cournot duopoly of liquid assets would be superior; each is possible, depending on parameters. However, it does tell us that aggressive competition for secondary market liquidity, where issuers issue large amounts and drive liquidity premia to zero, is suboptimal. Consequently, market segmentation and exogenous liquidity differences can be good for welfare because they tend to discourage such aggressive competition.====The present paper is related to a branch of the recent literature, often referred to as “New Monetarism” (see Lagos et al., 2017), that has highlighted the importance of asset liquidity for the determination of asset prices. See for example Geromichalos et al. (2007), Lagos and Rocheteau (2008), Lester et al. (2012), Andolfatto and Martin (2013), Nosal and Rocheteau (2013), Andolfatto et al. (2014), and Hu and Rocheteau (2015). In these papers assets are ‘liquid’ because they serve as a medium of exchange in frictional decentralized markets.==== In some other papers, liquidity properties stem from the fact that assets serve as collateral, as in Venkateswaran and Wright (2014) and Andolfatto et al. (2017).==== The majority of this literature has studied asset liquidity (and prices) under the simplifying assumption that asset supply is fixed. Exceptions include Rocheteau and Rodriguez-Lopez (2014) and Branch et al. (2016). Moreover, Bethune et al. (2019) consider an environment with asset issuance and decentralized secondary markets, but they focus on efficiency and policy rather than liquidity. Our paper is also related to Caramp (2017) who endogenizes asset creation with a focus on asset quality and asymmetric information.====A key difference of our paper with the works mentioned so far is that here asset liquidity is ====. Assets never serve as media of exchange (or as collateral) to purchase consumption. Their liquidity stems from the fact that agents can sell them for money in a secondary market. This idea is exploited in a number of recent papers, including Berentsen et al., 2014, Berentsen et al., 2016, Geromichalos and Herrenbrueck (2016), Mattesini and Nosal (2016), Herrenbrueck (2019a), and Geromichalos et al. (2023). As argued earlier, we believe that this approach is empirically relevant for a large class of financial assets. A common feature of these papers is that a secondary asset market allows agents to rebalance their liquidity after an idiosyncratic expenditure need has been revealed. This idea draws upon the work of Berentsen et al. (2007), where the channeling of liquidity takes place through a competitive banking system. Our work is also related to Lagos and Zhang (2015), but in that paper agents use money to purchase assets (rather than goods) in an OTC financial market.====Our work is also related to the literature initiated by the seminal work of Duffie et al. (2005), which studies how frictions in OTC financial markets affect asset prices and trade. A non-exhaustive list of such papers includes Vayanos and Wang (2007), Weill, 2007, Weill, 2008, Vayanos and Weill (2008), Lagos and Rocheteau (2009), Lagos et al. (2011), Afonso and Lagos (2015), Chang and Zhang (2015), and Üslü (2019). Our paper is uniquely distinguished from all these papers, starting with the very concept of liquidity: we have a monetary model where agents sell assets for cash after learning of a consumption opportunity, while in those papers, agents differ in the utility flow derived from holding an asset and pay for assets with transferable utility. Furthermore, we characterize the strategic incentives facing issuers of potentially liquid assets, and thereby endogenize the supply of such assets in addition to their liquidity.====Our paper is also related to a strand of the Industrial Organization literature that studies the effect of secondary markets for durable goods on the producers' pricing decisions. Examples include Rust, 1985, Rust, 1986. In these papers, the existence of a secondary market, where buyers could sell the durable good in the ====, affects the pricing decisions of sellers ==== through affecting the buyers' willingness to pay for the good.==== In our model, if secondary markets were shut down (so that assets have to be held to maturity), agents would be only willing to buy assets at their fundamental value. The existence of secondary markets endows assets with (indirect) liquidity properties, which, in turn, allows issuers to borrow funds at lower rates (i.e., sell bonds at a price that includes a liquidity premium).====The paper is organized as follows. Section 2 describes the model. In Section 3, we study the economy with exogenous asset supplies, and in Section 4, we calibrate our model to the data. Section 5 offers microfoundations for one of the key assumptions. In Section 6, we endogenize asset supplies by characterizing the game between asset issuers, and Section 7 concludes. Appendix A discusses empirical counterparts of our modeling choices, and Appendix B contains some technical details of the model. Finally, the Web Appendix contains several extensions of our analysis – only one asset issuer being strategic, one asset issuer being a Stackelberg leader, and one asset issuer having a higher cost of creating assets than the other – and an analytical characterization of the equilibria in our model.",The strategic determination of the supply of liquid assets,https://www.sciencedirect.com/science/article/pii/S1094202522000448,Available online 12 August 2022,2022,Research Article,55.0
"Chen William,Phelan Gregory","Department of Economics, Massachusetts Institute of Technology, United States of America,Department of Economics, Williams College, United States of America","Received 11 August 2021, Revised 4 August 2022, Available online 11 August 2022.",https://doi.org/10.1016/j.red.2022.08.002,Cited by (1)," can promote financial stability and improve household welfare. We consider a macro model with a financial sector in which banks do not actively issue equity, output and growth depend on the aggregate level of bank equity, and equilibrium is inefficient. ==== rules responding to the financial sector are ex-ante stabilizing because their effects on risk premia decrease the likelihood of crises and boost leverage during downturns. Stability gains from monetary policy increase welfare whenever macroprudential policy is poorly targeted. If macroprudential policy is sufficiently well-targeted to promote financial stability, then monetary policy should not target financial stability.","Economists increasingly debate where financial stability fits into a central bank's mandate. Core to this debate is whether the financial sector creates inefficiencies with aggregate consequences that monetary policy can adequately address. Monetary policy should consider targeting financial conditions in addition to (or independent of) conventional targets only if the financial sector creates additional externalities.==== In other words, to justify targeting financial stability in addition to inflation or output gaps, the financial sector must be more than a source of shocks to the rest of the economy; it must be an inefficient source of shocks. This raises several fundamental questions. How do monetary policy rules affect financial stability? Can monetary policy effectively correct financial-sector externalities? Do any benefits of targeting financial stability with monetary policy persist if macroprudential tools are available?====It is widely recognized that central bankers may pursue aggressive policies to stabilize the financial system during downturns—i.e., enacting a “Fed Put” to cut borrowing costs—which encourages excessive risk-taking and leverage in good times and may ex-ante increase the probability of financial crises. To counteract excessive risk-taking, some economists suggest that central banks should “lean against the wind” (“LAW”) in good times to mitigate overheating in the financial sector. Proponents argue that systematically raising the cost of intermediation in good times will decrease the probability of (extremely costly) financial crises.==== Implicitly, LAW turns the standard dual mandate into a triple mandate of price stability, full employment, and financial stability. There are prevailing doubts that the benefits of LAW outweigh the costs, and LAW may exacerbate crises if the economy enters a crisis starting from a weaker position (Svensson, 2017). Opponents of LAW prescribe macroprudential tools as the appropriate instruments for addressing financial instability and thus support continued adherence of monetary policy to a dual mandate.====Our primary contribution to this debate is a quantitative model demonstrating that when macroprudential policy is poorly targeted, monetary policy should address financial stability by implementing a Fed Put. The intuition is that both macroprudential policy and a Fed Put correct externalities due to financial frictions by recapitalizing the financial sector at a faster rate. However, if macroprudential policy is imperfect or improperly timed, then it fails to address financial-sector externalities. To the extent that macroprudential tools are inflexible and difficult to apply correctly, our analysis suggests that the central bank mandate should include financial stability.====We arrive at these conclusions using a continuous-time stochastic general equilibrium model in which financial frictions endogenously create inefficient instability and systemic risk, building on Brunnermeier and Sannikov (2014). Banks invest in productive capital, but due to frictions banks can issue only risk-free debt and not additional equity. As a result, banks invest more when they have more equity, and capital is allocated more efficiently when banks are well-capitalized. Limited equity issuance creates a distortion between the private and social values of bank equity, and so policies that improve financial stability can potentially increase household welfare. To this setting we add the model of monetary policy transmission from Drechsler et al. (2018) in which the nominal interest rate determines the liquidity premium on banks' investments. To capture constraints on monetary policy from inflation concerns, we also add inflation costs. We assess optimal monetary policy and explain the underlying mechanisms by investigating the impact of interest rate rules on the amplification of shocks, nonlinear dynamics, and systemic risk, which are central features of the framework in Brunnermeier and Sannikov (2014). We then evaluate whether macroprudential policy in the form of state-contingent leverage constraints changes the optimal monetary policy.====In our model, monetary policy can affect the return on banks' investments, the rate of bank equity growth, and banks' leverage decisions. Through these effects, monetary policy can change the frequency and duration of good and bad outcomes. We solve for the global dynamics of the economy and find that the impact of interest rate rules on bank profitability, and thus on bank equity growth, varies systematically with the state of the economy. How bank leverage varies over the financial cycle depends primarily on how monetary policy ==== over the cycle, much more than on the overall level of rates.====Optimal monetary policy implements a Fed Put because it increases financial stability ==== while minimizing inflation costs in good times. A Fed Put increases leverage ====, which improves aggregate outcomes and raises the expected rate at which banks recapitalize during crises. Surprisingly, even though banks expect accommodation during crises, bank leverage in good times does not sharply increase, so the probability of crises is ==== with a Fed Put. The key reason is that effects of monetary policy on returns, leverage, and stability are state-dependent.====Many economists have argued that macroprudential policy (MaP) should be the main tool for addressing financial crises (see Svensson, 2017). An important concern is whether monetary policy still improves financial stability and/or welfare when macroprudential policy has been applied. We find that if macroprudential policy is poorly targeted, then monetary policy should continue to address financial stability, and conversely, if macroprudential tools are well-targeted, then monetary policy should not address financial stability. When macroprudential policy does not sufficiently restrict leverage at the right times, monetary policy has scope to improve stability and welfare. If macroprudential tools properly target excessive risk taking across the financial cycle, then monetary policy is free to focus on conventional targets alone (e.g. inflation). However, our quantitative results suggest that for this condition to be met, MaP must be so effective that it almost completely prevents crises, which may be unrealistic.====  Methodologically, our paper follows the stochastic continuous-time macro literature, pioneered by Brunnermeier and Sannikov, 2014, Brunnermeier and Sannikov, 2015, Brunnermeier and Sannikov, 2016a and He and Krishnamurthy, 2012, He and Krishnamurthy, 2013, He and Krishnamurthy, 2019, who analyze the nonlinear global dynamics of economies with financial frictions, building on seminal results from Kiyotaki and Moore (1997) and Bernanke et al. (1999). Within this literature, we combine the models of Drechsler et al. (2018) and Phelan (2016) to study how monetary policy affects global dynamics. Drechsler et al. (2018) develop an asset-pricing model in which banks hold liquidity to insure deposits against funding risks. By influencing the liquidity premium on deposits, monetary policy affects the risk premium component of the cost of capital. Risk-tolerant agents issue deposits to buy assets, and lower nominal rates raise leverage, resulting in lower risk premia as well as higher asset prices and volatility.====We find that monetary policy has different effects on stability depending on the extent to which the returns on productive factors are affected. Hence, our mechanism is distinct from that in Brunnermeier and Sannikov, 2012, Brunnermeier and Sannikov, 2016a, in which cutting interest rates mechanically increases bond prices, raising bank equity via direct recapitalization. In our model, cutting rates does not necessarily increase the price of capital per se (no direct recapitalization) and the timing of rate cuts matters. Simply cutting interest rates is not enough for indirect recapitalization to occur, but rates must be cut at the appropriate time.====It is helpful to contrast our results with several closely-related papers. Svensson (2017) assumes that a weaker economy in good times leads to a weaker economy in a crisis. This generally need not be the case in reality, and in our model a Fed Put leads to a ==== severe crisis because bank leverage increases in a crisis, a feature that is shared by Caballero and Simsek, 2019, Caballero and Simsek, 2020. These latter papers study the role of monetary and macroprudential policies to mitigate the severity of demand-driven recessions. However, in these papers while the severity of a crisis is endogenous, the probability is exogenous, whereas in our model both the severity and likelihood of crises are endogenous. Van der Ghote (2021) shows that in a New Keynesian version of the model in Brunnermeier and Sannikov (2014), it is optimal for monetary policy to implement a countercyclical employment gap. In that model, monetary policy affects banks' risk-taking incentives only indirectly through the impact of the nominal rate on price dispersion. In current work, Van der Ghote (2019) includes money as a means of payments to examine the real effects of interest-rate corridor policies, which affect the cost of liquidity in financial markets.====There is a broad literature studying the relationship between monetary policy and financial stability. Cieslak and Vissing-Jorgensen (2020) find that the Federal funds rate responds to asset prices (the stock market); we consider responses to the condition of the financial sector (banks' equity levels). Several papers have cautioned against central banks intervening in financial markets (Diamond and Rajan, 2012; Farhi and Tirole, 2012). We take maturity mismatch and correlated risks as given and then consider, given these features, how monetary policy affects stability. Because of the general equilibrium effects in our model, a Fed Put increases leverage when rates are low but not before and the increase in leverage in bad times is stabilizing. Moreover, a Put does not introduce a commitment or time-consistency problem because it is ex-ante stabilizing. Bornstein and Lorenzoni (2018) develop a simple model where passive monetary policy causes overborrowing due to an aggregate demand externality. Our results suggest that moral hazard may not be a severe concern when a Fed Put addresses resource misallocation.====Our paper's insight is also distinct from other research affirming the use of monetary policy to address financial stability. Stein (2012) provides a model in which monetary policy can enhance financial stability by reducing excessive short-term debt. Gertler and Karadi (2011), Cúrdia and Woodford, 2011, Cúrdia and Woodford, 2016, Christiano et al. (2015), and Caballero and Simsek (2019) study the ability of monetary policy to address aggregate demand externalities in models with financial frictions, and show that prudential monetary policy can mitigate the severity of a demand-driven recession. In our economy, monetary policy corrects a pecuniary externality that produces dynamic resource misallocation affecting both the severity and the probability of crises. These results together show monetary policy can mitigate aggregate demand ==== aggregate supply issues caused by financial frictions. Hansen (2018) characterizes LQ-optimal monetary policy near an efficient deterministic steady state in a New Keynesian economy with a Bernanke and Gertler (1989) financial accelerator. Equilibrium in our model is inefficient with a stochastic steady state.",Should monetary policy target financial stability?,https://www.sciencedirect.com/science/article/pii/S1094202522000436,Available online 11 August 2022,2022,Research Article,56.0
Yu Zhixiu,"University of Minnesota, Department of Economics, 4-101 Hanson Hall, 1925 Fourth Street South, Minneapolis, MN, 55455, United States","Received 1 November 2021, Revised 3 August 2022, Available online 10 August 2022.",https://doi.org/10.1016/j.red.2022.08.001,Cited by (0),"This paper uses a search-theoretic model to study conditions under which ==== is valued and under which it coexists with ==== in which the stock of money is exogenously given. In fiat money economies, the inflation rate is determined by the rate of growth of the money stock. My result is also in sharp contrast with other types of private money economies, in which the inflation rate must necessarily be different from zero. In such private money economies, the cost of producing additional money does not depend on the existing nominal stock. Moreover, I show that cryptocurrency and fiat money can circulate at the same time and that the rates of return on these two assets may not be the same. Competition with cryptocurrency restricts the government's ability to over-issue fiat money and thereby might improve on pure fiat money equilibria without government commitment.","The emergence of Bitcoin has triggered a large wave of public interest in cryptocurrencies. Unlike most common forms of fiat currency such as dollars or euros, cryptocurrencies are not backed by a central bank or any government authorities. As predetermined by a computer algorithm, the new cryptocurrency is produced by computer servers (“miners”) who are willing to solve complicated computational problems using programming efforts. The predetermined program algorithm makes cryptocurrencies costly to produce, and the number of new cryptocurrencies that can be produced is decreasing in the total money stock (e.g. Bitcoin, Litecoin). This deflationary property of cryptocurrency may preclude the over-issuance problem, which happens to fiat money when government tends to raise its seigniorage by over-issuing money (see, e.g., Araujo and Camargo, 2006, Araujo and Camargo, 2008).====There has been growing interest in cryptocurrencies, and this growing interest raises several questions: Under which conditions can this currency be valued in equilibrium? Can it provide price stability? Under which conditions can it coexist with government-issued fiat money? Would this privately-issued currency be welfare-enhancing? The goal of this paper is to provide a theoretical framework to address these issues.====To that end, I first develop a search-theoretic model of an economy with a privately-produced money—cryptocurrency. My framework builds on the workhorse model of monetary exchange by Lagos and Wright (2005) and adds a new type of private agents—profit-maximizing miners—who are the sole issuers of cryptocurrency. The Lagos-Wright (LW) framework is particularly insightful for addressing currency issues because the acceptability of a medium of exchange is determined endogenously in equilibrium, and it is amenable to analysis and allows me to incorporate a miner sector while keeping the distribution of currency holdings analytically tractable.====My model highlights two key attributes of cryptocurrency: it is private money, and it is costly to produce. Its supply is endogenous and driven by the production decisions of miners, who have access to a costly mining technology that recognizes the legitimacy of cryptocurrency. The cost of producing additional cryptocurrency increases not only in the number of new units, but also in the aggregate nominal stock. This is a key feature that makes cryptocurrency different from other types of private money in the literature, e.g., bank notes, since the production cost of those private monies is independent of their existing nominal stock. These assumptions are intended to capture the deflationary property of cryptocurrency. For example, when producing Bitcoin, there are costs associated with the production, such as computer power and electricity, and the new bitcoin mining rewards halve every 210,000 blocks.==== Thus, the cost of mining the same amount of bitcoin gets more expensive as more bitcoin is minted.==== In the real world, however, there are myriad cryptocurrencies with different supply rules. For example, similar to Bitcoin, Litecoin and Handshake have a maximum attainable supply through halving new coin rewards after reaching a certain supply level. There are also some cryptocurrencies with no maximum supplies, including Ethereum and Dogecoin. Furthermore, the supply of some other cryptocurrencies is hard to quantify, for instance, Terra, which is minted and burned to maintain a 1:1 value with the U.S. dollar.==== My model applies to cryptocurrencies whose marginal production costs depend on their nominal stock of money—a property of mainstream cryptocurrencies in the real world, such as Bitcoin.==== More broadly, my model is applicable to any intrinsically worthless object that can be privately produced via a costly technology and serve as a medium of exchange.====Moreover, I model the cryptocurrency law of motion by assuming that the stock of cryptocurrency in each period is determined by both the newly produced units and the depreciation from the last period. The amount of new cryptocurrency is endogenously determined by miners' decisions, while the depreciation is modeled to capture the loss of cryptocurrency. In particular, I assume that a proportion of the cryptocurrency holdings from the last period will be lost in each period. These assumptions are intended to capture the idea that cryptocurrency is more vulnerable to being lost because people sent it to a wrong address, or lost or discarded their device, or forgot their password which has complicated strings, etc.==== Unlike most types of the medium of exchange, such as cash or commodity money like gold, which can be reused after getting lost, once a cryptocurrency is lost, it might be lost forever, and other people cannot reuse it.====In the economy with cryptocurrency only, I show that given that the marginal production cost increases in the existing nominal stock of money, the inflation rate must be zero in a stationary monetary equilibrium. My result is in sharp contrast to models with fiat money in which the stock of money is exogenously given, e.g., Lagos and Wright, 2003, Lagos and Wright, 2005. In fiat money economies, the inflation rate is determined by the rate of growth of the money stock, and it can be different from zero as long as the stock of money changes over time. My result is also in sharp contrast with other types of private money economies, in which the inflation rate must necessarily be different from zero, e.g., Fernández-Villaverde and Sanches (2018). In their private money economy, the production cost is independent of the existing money stock, and there is no currency depreciation. Profit-maximizing producers always have an incentive to create an additional unit of money and, thus, a monetary equilibrium necessarily has inflation. Their result does not hold in my cryptocurrency economy. Intuitively, in order to have an inflationary equilibrium, the aggregate nominal stock of cryptocurrency must be increasing. In my model, however, the marginal production cost increases in the aggregate nominal stock of money. It follows that, if the money stock goes up, the marginal cost increases, thereby weakening miners' incentives to mine and contradicting the supposition. Therefore, the inflationary equilibrium cannot be sustained. Likewise, a deflationary equilibrium requires that the nominal stock of cryptocurrency declines. Such an equilibrium cannot arise because it is inconsistent with the rising production incentives on the part of the miners. Thus, the price and stock of cryptocurrency must remain constant in a stationary equilibrium, and all the newly produced units replace the depreciated currency each period.====Next, to explore the coexistence of cryptocurrency and fiat money, I extend my cryptocurrency-only model by adding government-issued fiat money and multiple decentralized markets into the economy. Fiat money differs from cryptocurrency in the issuers, supply rules, production costs, and probabilities that agents visit the markets where sellers accept that currency as a payment method. Fiat money is costless to produce, and is exogenously supplied according to a deterministic growth rule. Each period agents randomly enter one of the three decentralized markets: DM1, DM2, and DM3, which differ in the currencies that can be used for transactions. Specifically, agents can only trade with fiat money in DM1, e.g., transactions that accept cash only or involve the government authorities; agents can only trade with cryptocurrency in DM2, e.g., online Bitcoin stores or black markets where fiat money is not used; and agents can trade with both currencies in DM3, e.g., AT&T, PayPal, Microsoft, etc.==== This market structure is analogous to that of the two-currency, two-country search models for studying international currencies and exchange rates, e.g., Zhang (2014) and Zhu and Wallace (2020).==== The assumption of currencies with different degrees of acceptability in markets is akin to the cash-in-advance assumptions in Lucas (1982), which constrain agents to use one type of currency in a particular trade.====Similar to what happens in multiple-fiat currency models, e.g., Camera et al. (2004) and Engineer (2000), there are currency regimes with neither, both, or only one of the currencies that are valued in the economy of cryptocurrency and fiat money, depending on the fundamentals and parameters of the model. Though the probability that agents visit each decentralized market is exogenous, the acceptability of a currency is endogenous in the following sense. There exists an equilibrium in which there are no markets where fiat money is used for transactions, including DM1; there exists an equilibrium in which there are no markets where cryptocurrency is used, including DM2; and there exists an equilibrium in which both currencies are circulating. However, different from traditional two-fiat currency models, where rates of return on two currencies cannot be different if both currencies are in circulation, e.g., Kareken and Wallace (1981), cryptocurrency and fiat money can coexist regardless of their rates of return in my model. That is because each currency is essential in some decentralized meetings, agents will choose to hold both currencies in order to smooth their consumption in all decentralized markets, so long as neither currency is too costly to carry. Thus, a low-return currency can coexist with a high-return currency.====Moreover, my model of cryptocurrency and fiat money has a novel difference, compared to other models of currency competition with payment acceptability constraints, e.g., Zhang (2014) and Zhu and Hendry (2019). In their models, the cost of carrying one currency is tied with the exogenous growth rate of the money supply; while in my model, the cost of carrying cryptocurrency depends not only on the exogenous parameters, such as currency depreciation, but also on the endogenous production decisions of miners, which rely on the cost function and further affect the price path of cryptocurrency in equilibrium. Due to the shape of the cryptocurrency cost function, a monetary equilibrium with coexistence is consistent with a zero inflation rate in cryptocurrency.====Further, when there exists a market where both currencies can be accepted, the real values of cryptocurrency and fiat money are interdependent, and the substitution between two currencies put constraints on government monetary policy. Specifically, when one currency becomes more costly to hold or less useful in decentralized markets, agents would demand less for that currency and instead substitute the other currency for transactions. As a result, the real value of that currency decreases, whereas the real value of the other one increases. In particular, cryptocurrency becomes more costly to carry if it is lost at a higher rate or its marginal production cost diminishes, whereas fiat money becomes more costly to use if it is issued at a higher growth rate. A currency becomes more useful when its acceptability degree gets larger. Even under the case that cryptocurrency, in some sense, inferior in production costs and degrees of acceptability in decentralized markets, it can coexist with fiat money, an asset that is more acceptable and costless to produce, when appropriate monetary policy is implemented. As the inflation rate of cryptocurrency is zero in a stationary monetary equilibrium, the competition with cryptocurrency restricts the government's ability to over-issue fiat money for raising its seigniorage.====To further support the point where the monetary policy is constrained when fiat money competes with a private currency that is costly to produce, my model generates the Laffer curve and captures its changes with the introduction of cryptocurrency. The Laffer curve shows that the steady-state seigniorage first rises and then falls with increasing inflation because the higher inflation lowers the purchasing power of fiat money and thus discourages agents from holding it. As a result, there exists a fiat money growth rate that maximizes seigniorage earnings. In my two-currency economy, agents have a probability of entering the market where cryptocurrency is accepted alongside with fiat money. Compared to the fiat money economy, the Laffer curve with the introduction of cryptocurrency shrinks, and the government's seigniorage decreases. In particular, the peak of the Laffer curve shifts down and to the left, and the seigniorage-maximizing level of the money growth rate falls. The larger the market size in which both currencies can be accepted, the lower the seigniorage-maximizing money growth rate and the fewer seigniorage earnings when inflation rises.====The way that cryptocurrency can affect the circulation of fiat currency raises a question: should the government ban cryptocurrency? In the real world, there are many countries with different attitudes toward cryptocurrency and its regulations. While a number of countries like China have banned crypto mining, most countries like the United States and Canada hold a more neutral attitude towards cryptocurrency.==== There also have been some countries, such as The Bahamas and Nigeria, that decided to create their own central bank-controlled digital currencies to compete with cryptocurrency.==== Since the mining process of cryptocurrency requires large amounts of energy, it leads to concerns about energy waste and its negative impact on the environment.==== On the other side, currency competition can be helpful in calming inflation and preventing the manipulation of interest rates and prices by the government.==== My model suggests that whether or not the introduction of cryptocurrency would improve welfare depends on the acceptability of cryptocurrency and the government's ability to commit to maintaining the targeted fiat money growth rule. Banning cryptocurrency can clearly save the resources used in its production, but it may worsen the total welfare of the economy. One reason for this is that, in the absence of cryptocurrency, agents would only trade using fiat money in decentralized markets, thereby missing out on the trade surplus in the market where only cryptocurrency is accepted. In that case, if the government tends to overissue fiat money, there would be additional welfare loss from consuming less output. However, if cryptocurrency is not widely accepted and the government can maintain sufficiently low inflation, then banning cryptocurrency would be welfare-enhancing. There would be welfare gains from avoiding resource waste associated with producing cryptocurrency and from consuming more output, which would outweigh the welfare loss from no trade surplus in the market where only cryptocurrency is used. As cryptocurrency is consistent with zero inflation in a stationary equilibrium, the government that tends to use the inflation tax would have a strong incentive to ban cryptocurrency.====My paper is related to two branches of a large literature on multiple currencies that are competing as media of exchange. One branch is the literature where no currency is privately produced, e.g., Kareken and Wallace (1981). The other branch, where my paper belongs, is the literature where at least one of those currencies is privately produced, e.g., Lagos and Rocheteau (2008). Different from these currency competition models in which two assets can coexist only if they have the same rate of return, my model shows that fiat money and cryptocurrency can coexist and that the rates of return on these two assets may not be the same. This is driven by the payment acceptability constraints in decentralized markets. Thus, each currency is essential in some transactions.====There is a growing literature on cryptocurrency issues. For instance, Schilling and Uhlig (2019) analyze the price dynamics of Bitcoin. They show that bitcoin prices form a martingale, and the risk-adjusted real return on Bitcoin and the dollar has to be identical when both currencies are simultaneously in use. You and Rogoff (2019) study the competition between online retailer-issued tokens and bank debit accounts, and focus on issuers' sale and issuance strategies. My emphasis is different from theirs. I focus on the coexistence of cryptocurrency and fiat money in a stationary equilibrium, and I show that the two currencies can coexist with different rates of return. In addition, my paper has it in common with Choi and Rocheteau (2020) and Zhu and Hendry (2019) that two competing currencies have different degrees of acceptability in decentralized meetings. However, one of the central points of my paper is to answer the question: If cryptocurrency is costly to produce and the aggregate new cryptocurrency is endogenously determined by miners' decisions, what sorts of equilibria are possible? In contrast, the aggregate mining rate of cryptocurrency is exogenously determined in Choi and Rocheteau (2020), and the private money is costless to produce in Zhu and Hendry (2019). Therefore, these papers cannot have an analog of the monetary equilibrium in which the price of cryptocurrency must be constant.====My paper is also related to an extensive literature on currency competition where the issuers of private money are costly operating sectors, e.g., banks. He et al. (2008) and Chari and Phelan (2014) develop an economy where fiat money and bank deposits serve as means of payment. In the former paper, cash is low-cost but subject to theft, while bank deposits are in safekeeping, but the bank is costly to operate. They show that with exogenous theft, there is no concurrent circulation of both currencies. In the latter paper, bank deposits serve a socially useful insurance role and are privately useful because the bank pays interest on deposits, but banks are costly and subject to bank runs. The authors show that there is no equilibrium in which fiat money and bank deposits coexist. My results are different from theirs. Cryptocurrency can coexist with fiat money in equilibrium, even if it is costly to produce. Moreover, unlike those papers with fractional reserve banking, there is no reserve requirement in my model, and the cryptocurrency that is produced by miners is not associated with any promise to exchange for goods or assets in the future.====The rest of this paper proceeds as follows. Section 2 lays out a monetary environment of an economy with cryptocurrency only. Section 3 studies the equilibrium of the cryptocurrency-only model. Section 4 presents an environment with cryptocurrency and fiat money in the economy. Section 5 explores the equilibrium condition of the two-currency model. Section 6 concludes.",On the coexistence of cryptocurrency and fiat money,https://www.sciencedirect.com/science/article/pii/S1094202522000424,Available online 10 August 2022,2022,Research Article,57.0
"Paluszynski Radoslaw,Yu Pei Cheng","University of Houston, United States of America,University of New South Wales, Australia","Received 16 March 2020, Revised 13 July 2022, Available online 10 August 2022.",https://doi.org/10.1016/j.red.2022.07.003,Cited by (1),"Life insurance premiums display significant rigidity in the data, on average adjusting once every 3 years by more than 10%. This contrasts with the underlying marginal cost which exhibits considerable volatility due to the movements in interest and mortality rates. We build a dynamic model where policyholders are held-up by long-term insurance contracts, resulting in a time inconsistency problem for the insurer. The optimal contract balances commitment and flexibility and takes the form of a simple cutoff rule: premiums are rigid for cost realizations smaller than the threshold, while adjustments must be large and are only possible when cost realizations exceed it. We use a calibrated version of the model to show that it matches the data and captures several aspects of premium rigidity in the cross-section and over time.","Traditional theories in finance assume that markets are efficient, so that prices of financial contracts respond to changes in the fundamentals. In contrast, this paper documents the high degree of price rigidity for a specific long-term financial contract, life insurance, the cost of which displays significant volatility. This finding has important implications for understanding the pricing of long-term financial services. We explain this unique pricing phenomenon using a quantitative model that features the commitment versus flexibility trade-off and generates price rigidity endogenously.====We show that life insurance premiums are characterized by long periods of rigidity with occasionally sizable adjustments (on average more than 10%). This is intriguing because the underlying marginal cost of life insurance is volatile over time, with a monthly coefficient of variation of 6.3% and the average absolute month-to-month change of 1.4% of the mean. In the data though, the overall probability of a monthly premium change amounts to just 2.6%. This implies an average premium duration of roughly 39 months, placing life insurance on the far-right tail of the price change frequency distribution documented by Bils and Klenow (2004). Fig. 1 presents an illustrative plot of premiums over time for the most significant and longest-observed companies in our sample. Remarkably, some products have maintained a constant premium for over 20 years! More generally, our empirical findings indicate that life insurance companies tend to maintain stable profiles of premiums with respect to age. This means that over the life cycle, young policyholders will likely pay the same amount as what the older cohorts used to.====To explain the empirical findings, we construct an OLG model where the insurer faces a commitment versus flexibility trade-off, which stems from the consumer hold-up problem. Consumers live for three periods and buy one of two types of policies from monopolistically competitive insurers, renewable or non-renewable. They incur a transaction cost before purchasing, which represents the monetary expenses and opportunity cost of research and medical examination. They may also experience adverse health shocks in the second period, which could lead to significant premium hikes if they searched for a new policy. In the second period, non-renewable policyholders face all of these costs, while renewable policyholders are guaranteed coverage at no additional expense. Therefore, renewable policyholders are locked into a long-term relationship with the company, limiting their future options. This creates an incentive for the insurer to raise renewal prices, which also lowers the consumers' ex ante willingness to sign. In essence, the insurer is time-inconsistent and values commitment. The insurer also faces stochastic cost shocks in the second period, so it values flexibility. However, policyholders do not observe the shocks, so they are unsure if premium hikes are due to being held-up or due to changes in the cost.==== Consequently, excessive flexibility in adjusting premiums could exacerbate the hold-up problem by discouraging consumers from purchasing.====To balance the need for commitment and the desire for flexibility, we show that the optimal premium as a function of cost follows a simple cutoff rule: Premiums are low and rigid for marginal costs below an endogenously determined threshold, while above this threshold premiums are high and initially rigid before full flexibility is possible. The reason is the renewal demand of locked-in policyholders is inelastic for low premiums. In the inelastic region, premiums can increase slightly without losing any consumers, rendering flexibility in adjusting premiums within this region non-credible. For marginal costs below the threshold, the unconstrained premiums are low and map to the inelastic region, so the insurer commits to a single low premium for all cost realizations sufficiently small to gain credibility. Premium adjustments have to be costly to the insurer for it to be credible. Therefore, increases from the low premium need to be significant to induce enough reduction in demand, which is optimal when marginal cost is large.====Our model explains why level-term insurance policies have a non-guaranteed premium schedule that affords them the room to be flexible, while the finalized premiums rarely deviate from it. The main result is also consistent with the numerous premium drops observed in the data which may occur when the cost in the second period decreases significantly, as well as small premium changes which can be explained by the flexible part of the optimal schedule.====Having established the general properties of an optimal premium, we proceed to solve the model numerically and calibrate it to match the quantitative features of ten-year renewable insurance. The model generates realistic premium amounts and predicts a jump in the premium of 12% when the cost shock switches between the low and high regions, in line with what we observe on average in the data. We then use the quantitative model to perform several comparative statics exercises, highlighting the subtle differences between the consumer's hold-up problem and the traditional monopoly power.====In the final part of the paper we show that the life insurance premiums data supports the main predictions of our model. First, we show that as the level-term of a renewable policy increases, which weakens the hold-up problem due to a higher probability of policy termination before the renewal date, premiums are also more likely to be adjusted and exhibit smaller jumps. Second, we find that between the 1990s and the 2000s, a period of time when the consumer's hold-up problem was likely weakened due to falling transaction costs and less adverse health shocks, the frequency of premium changes increased and the average size of such adjustments fell, bringing the pricing patterns of life insurance companies closer to those in typical consumer goods markets. Third, we demonstrate that life insurance companies tend to respond to cost shocks predominantly on the ====, by increasing the hazard of a premium change, while no apparent effect is detected on the ====, by varying the size of a premium change. This observation is in line with our model where pricing is based on a threshold rule. Fourth, we contrast life insurance premiums with prices of annuities, a related product whose buyers are not held-up by the insurer. We find that these prices adjust very frequently and by small margins, thus providing external validity to our theory. Finally, we test several alternative frictions that commonly lead to price stickiness, such as staggered contracts or menu costs, and show that their predictions are not consistent with the facts about life insurance premiums.====To summarize, our paper offers two main contributions. Empirically, we provide new evidence on the frequency and size of price changes in the life insurance market.==== On the theoretical side, we explain this phenomenon with a model where the optimal incentive compatible contract necessarily features price rigidity and a discrete jump. We calibrate our commitment versus flexibility model to the life insurance market and show that the predicted premium rigidity and jumps are quantitatively significant.====Our empirical finding provides support for a crucial assumption in the literature on life insurance contracts. In a seminal paper, Hendel and Lizzeri (2003) examine the front-loading of life insurance premiums, i.e., policyholders pay a surcharge when young to cover for expected future losses when they age. They analyze the cross-sectional data on premiums to show that when policyholders lack commitment and face health reclassification risk, the optimal insurance contracts exhibit front-loading. However, their analysis relies on the assumption that insurers keep their promises in that premiums for older cohorts are the future premiums. In essence, they use the data from a single point in time (July 1997), making an implicit yet crucial assumption that companies never deviate from the current non-guaranteed premiums. Several papers have since extended their framework.==== Therefore, the findings in this paper allow us to empirically and theoretically validate the implicit assumption in Hendel and Lizzeri (2003).====Our model contributes to the literature on optimal delegation, which analyzes a principal-agent setting with no transfers and a biased agent who is better informed (Holmstrom, 1984; Melumad and Shibano, 1991; Alonso and Matouschek, 2008; Amador and Bagwell, 2013). In these models, the principal typically has full commitment and chooses a set of actions that the agent can take.==== This is similar to our paper since the time-inconsistent insurer commits to a rule, i.e., a subset of renewal premiums that it can choose in the future.==== In particular, our characterization of the optimal renewal premium function builds on the theoretical insights of Melumad and Shibano (1991) and Alonso and Matouschek (2008). Our paper also makes three novel contributions to the delegation literature. First, in our model, the time inconsistency of the insurer is endogenous. The insurer is able to decrease or even eliminate its intertemporal conflict, but we show quantitatively that it does not under empirically relevant parameters. This differs from the literature which analyzes an exogenously biased agent. Second, the optimal renewal premium will ==== feature a discontinuous jump if the insurer has discretion in adjusting the premiums in the future. This is in contrast to the previous literature which has found conditions for interval delegation to be optimal (Amador and Bagwell, 2013). Third, our paper provides empirical support for the trade-off between commitment and flexibility, which has not been quantified or tested in this literature.====This paper also contributes to the empirical literature on life insurance. Koijen and Yogo (2015) show that life insurers have recently been posting highly negative markups which can be explained by financial frictions around the 2008 crisis. Our paper provides an alternative theory for why many of these companies were reluctant to increase premiums in the presence of large marginal cost. Ge (2022) shows that insurers often adjust life insurance premiums in response to shocks to their divisions in other markets. Her story suggests that on their own, life insurance premiums may be even more rigid than the analysis in our paper indicates.====The remainder of the paper is structured as follows. Section 2 describes the construction of our dataset and summarizes the main findings about price dynamics in the life insurance market. Section 3 develops the theoretical model. In Section 4, we present the main qualitative predictions of the model, calibrate it and perform a numerical analysis of the solution. Section 5 provides empirical support for the main predictions of the model. Section 6 concludes. The Supplementary Appendix contains the proofs of our theoretical results, a description of the numerical algorithm, and some more nuanced discussions of our data.",Commitment versus flexibility and sticky prices: Evidence from life insurance,https://www.sciencedirect.com/science/article/pii/S1094202522000412,Available online 10 August 2022,2022,Research Article,58.0
Pizzo Alessandra,"LED Université Paris 8, France","Received 21 May 2018, Revised 12 July 2022, Available online 9 August 2022.",https://doi.org/10.1016/j.red.2022.07.004,Cited by (0),"I examine the different effects of a progressive tax and transfer schedule on unemployment, labor income, an individual's labor supply, and savings. My modeling strategy follows ====, who build on the workhorse model of ====, to which I add individual labor supply. I allow for ==== heterogeneity in productivity, as well as in preferences, while the only idiosyncratic risk is the endogenous risk of unemployment. I compare my baseline model with a frictional labor market to one with a Walrasian labor market where the unemployment risk is exogenous.====Steady state comparisons, based on a utilitarian welfare criterion, show that a tax and transfer schedule with a positive degree of progressivity is optimal in both frameworks, and that the presence of labor market friction implies that the optimal level of progressivity is (slightly) higher.","Developed economies are characterized by progressive redistributive fiscal systems: the average tax rate increases with income, and the tax revenue allows the state to make transfers to poor agents. If insurance markets are incomplete and agents are heterogeneous in certain characteristics (such as skill or productivity), a progressive tax and transfer system helps provide insurance against income shocks and redistributes between different types of agents. However, it can distort the choices to supply labor or save: progressive taxation reduces the hours of work and potentially decreases total productivity (even more so when the more productive agents earn higher wages). Publicly provided insurance can induce lower savings and therefore reduce physical capital accumulation, thus decreasing total production.====The pros and cons of progressive taxation have been analyzed at length in papers featuring incomplete insurance, for example in Heathcote and Tsujiyama (2021), Heathcote et al. (2017), Bakış et al. (2015), and Conesa and Krueger (2006).====These papers consider a Walrasian labor market; however, the literature on frictional labor markets has pointed out that progressive taxation interacts with imperfections of the labor market. This interaction has been analyzed since at least the 1990s in a framework in which agents are risk-neutral.==== This strand of the literature has pointed out that, in the context of a frictional labor market, a more progressive tax schedule can decrease the rate of unemployment through its effect on wage bargaining. A more progressive tax schedule in fact reduces the worker's effective bargaining power, by decreasing the part of the match surplus that would be received, and thus contributes to downward wage pressure, which in turn can have a beneficial effect on the rate of unemployment.====In a framework in which hours of work are also endogenous, and determined through bargaining together with the hourly wage, the literature has highlighted that the overall effect of tax progressivity on labor market tightness (and therefore the unemployment rate) becomes ambiguous. Higher progressivity reduces hours of work, which implies a lower level of output (decreasing total match surplus), but also a lower disutility for which the worker has to be compensated through the wage. The overall effect of higher progressivity on labor market tightness is thus ambiguous, and depends on how hours of work affect production and disutility, even if for reasonable assumptions on the functions and parameters the overall effect is found to be positive.==== In a context in which the unemployment rate of the market economy is higher than the socially optimum level, and higher tax progressivity can reduce unemployment, efficiency could thus be restored. However, this strand of the literature does not develop fully fledged macroeconomic models: most of the contributions typically leave aside capital as a production factor, and consider risk neutral agents.====In this paper I thus analyze the efficiency losses and gains induced by a progressive tax and transfer system in a framework in which the labor market is affected by search and matching frictions, wages and hours are bargained à la Nash, there is capital accumulation, and financial markets are incomplete. Considering that the main source of revenue for a large group of agents is labor income, and the main source of risk is job loss, it is important to endogenize the unemployment process, since there is a feedback effect between the level of progressivity of the tax and transfer system and labor market performance.====In the benchmark model, agents are risk-averse and ==== (and permanently) heterogeneous in their productivity, as well as in their preferences for leisure and their levels of impatience. Agents are also subject to an endogenous unemployment process: in particular, the labor market is characterized by search and matching frictions à la Diamond–Mortensen–Pissarides (DMP), so that the level of progressivity of the tax and transfer system affects the job finding rate and unemployment rate, through its effect on wage bargaining, while job separations are exogenous. Agents can choose how much to work and to save to (partially) insure themselves against the unemployment income shock, and thus end up with different levels of assets, due to different sequences of spells of employment and unemployment.====I thus aim to encompass both the mechanisms of tax progressivity highlighted by the labor market literature, which focuses on the effects on the bargained wage and the unemployment rate, and those pointed out in the heterogeneous agents framework, in particular the effects on individual labor supply and total savings.====In order to highlight the effects of progressivity on unemployment and on welfare, I compare the results of two versions of the model: in the benchmark economy, the labor market is frictional à la DMP, so that changes in tax progressivity affect the equilibrium level of the unemployment rate. In the alternative experiment, the labor market is perfectly competitive, i.e., wages simply reflect marginal productivity, but workers are still subject to the risk of becoming unemployed: the transition probabilities between the states of employment and unemployment in this case are exogenous and constant, as in Marcet et al. (2007). In both versions, hours of work are endogenous.====The result of the interaction between progressivity and unemployment is not clear ====. First, if the extensive margin (the employment rate) increases as progressivity increases, this counteracts the decrease in the intensive margin of labor supply, so that the overall effect on total labor is at least less negative than for a case in which the unemployment rate does not react. Secondly, if at a higher level of progressivity the level of bargained wages is lower than when this reflects the marginal productivity of labor, the depressed level of wage income could tilt the balance in favor of lower progressivity. As noted by Dávila et al. (2012), when total capital accumulation decreases, the interest rate increases, and this favors those agents who rely more on interest income than those relying on wage income (i.e., the wealthy agents).====The adopted welfare function in both versions of the model is a simple utilitarian welfare criterion, as in Bakış et al. (2015) and Heathcote et al. (2017), and I focus on steady state comparisons.====I find two main sets of results: under the benchmark parametrization for the US, the optimal level of progressivity is higher than the current calibrated one, and it is only slightly higher in the case with a frictional labor market than for the Walrasian case. The gains in welfare in both models are sizeable and mainly driven by redistribution.====When I distinguish between “pure economic efficiency” and redistribution, in line with the definition by Bénabou (2002), the main welfare effects come from the redistribution motive. However, even in accordance with the pure economic efficiency criterion, optimal progressivity should be greater than the current level, and the model with a frictional labor market prescribes a higher level of optimal progressivity than does the version with no friction.====This result about optimal progressivity stems from the conflicting interests of the different categories of agents, which are characterized in terms of five levels of productivity or skill: while the bottom three groups always gain from higher progressivity, the top two (which in my calibration exercise correspond to agents with college education) always lose; moreover, while for the top two categories the unemployment rate is very low and barely changes, the unemployment rates of the bottom three categories decrease with tax progressivity. The results in terms of welfare are thus related to the fact that agents are born with different levels of skill that cannot be changed.==== In order to further clarify the importance of the different effects of progressivity on welfare, I also consider a version of the main model in which the level of heterogeneity is limited to the employment status and asset level — i.e., there is only one type of agent. In this case, the optimal level of tax progressivity is markedly lower than in the full version of the model in which agents have a certain probability of being born within a productivity category; moreover, the welfare gains or losses are much smaller. This difference in the level of optimal tax progressivity can therefore be attributed to the additional motive for insuring against the probability of being born in a certain category. When comparing the two versions of the model, the optimal level of tax progressivity is greater in the case in which the labor market is affected by search and matching frictions, and the difference between the two optimal levels of progressivity is more important than in the full model with heterogeneity in skill types.====If the imperfections of the financial market are overcome by allowing the presence of a “large household” that provides perfect unemployment risk sharing across agents, in line with Merz (1995) and Andolfatto (1996), the economy becomes a representative agent model. In this case, the welfare maximizing tax and transfer schedule is regressive, because the effect of a progressive tax system is to disincentive hours of work and savings.====Lastly, I perform some robustness checks to highlight the importance of the calibration of two specific parameters in the full version of the model: the Frisch elasticity of the labor supply and the workers' bargaining power. As expected, a higher elasticity implies stronger distortive effects on the labor supply and a lower level of optimal progressivity. The important point is that since the framework with a frictional labor market distinguishes between the extensive and the intensive margins of the labor supply, it is appropriate to calibrate the Frisch elasticity to a value in line with the evidence from microeconomic studies.==== I also show that the greater is the workers' bargaining power, the higher the optimal level of tax progressivity: this result is linked to the fact that a higher level of progressivity decreases the effective bargaining power of the workers, thus counteracting their stronger initial bargaining position.====In terms of the related literature, Heathcote et al. (2017) develop a model in which agents, who are ==== heterogeneous in terms of learning abilities and preferences for leisure, face uninsurable labor income shocks and choose how much to work and to consume. They show that the optimal level of progressivity is tightly linked to the modeling of government spending: the presence of government expenditures that enter into the utility function of the households (and that cannot be replaced by private consumption) pulls the optimal level of progressivity downward. In the full version of their model, they find that the optimal tax and transfer system for the US should be less progressive than what is observed. However, when government expenditures do not enter into the utility function of households, and the ratio of government expenditures to production is taken as given, then the optimal level of progressivity is found to be slightly higher than what is observed.====Bakış et al. (2015) develop a general equilibrium model with heterogeneous agents and precautionary savings as well as bequests. Their conclusion is that, considering steady state welfare comparisons, the optimal tax schedule for the US is slightly regressive. As Heathcote et al. (2017) note, this is due to the fact that agents have an additional self-insurance mechanism through bequests, so that the publicly provided insurance is even more distorting. However, when the transition path from one equilibrium to another is taken into consideration, the welfare costs in terms of lower consumption (which is necessary to increase savings) imply that the optimal tax schedule is in fact progressive.====The workhorse model of Krusell et al. (2010) combines the Bewley–Huggett–Aiyagari model of incomplete financial markets with the search and matching model à la DMP. Setty and Yedid-Levi (2020) build on Krusell et al. (2010) without aggregate uncertainty to study the role of unemployment benefits when agents differ in their levels of skill or productivity and are subject to unemployment, productivity, and lifespan shocks. My framework is similar to the one developed by Setty and Yedid-Levi (2020); however, while their tax and transfer system allows for progressive taxes, it redistributes wealth using lump-sum transfers, nor do those authors consider the intensive margin of the labor supply. They show that even with an optimal level of progressivity, there is space for unemployment insurance to improve welfare.====This paper is organized as follows: Section 2 presents the full version of the model, Section 3 discusses the calibration, Section 4 contains the main quantitative results, while Section 5 discusses the mechanisms behind the effects of tax progressivity on aggregate variables and on welfare. Finally, Section 6 presents some robustness checks regarding the full version of the model, and Section 7 concludes.",The welfare effects of tax progressivity in a frictional labor market,https://www.sciencedirect.com/science/article/pii/S1094202522000400,Available online 9 August 2022,2022,Research Article,59.0
"Nie He,Roulleau-Pasdeloup Jordan","School of Economics and Management, Wuhan University, China,Department of Economics, National University of Singapore, Singapore","Received 20 May 2021, Revised 8 July 2022, Available online 2 August 2022.",https://doi.org/10.1016/j.red.2022.07.002,Cited by (0),We develop a model with ,"What can central banks do when they have no choice but to set their nominal interest rate to its lower bound? Judging from recent experience, the answer seems to be: quite a lot. Indeed, central banks all over the world have been confronted to the interest rate lower bound and have responded by using quantitative easing, credit easing and forward guidance. The last one has been repeatedly used by the U.S. Federal Reserve and other central banks. Recently, Powell (2020) announced a new framework for the Federal Reserve monetary policy that shares features with a policy of forward guidance. Accordingly, forward guidance will be the focus of this paper. Loosely speaking, forward guidance involves a central bank announcing its future course of action. In the context of a binding Zero Lower Bound with below target inflation, this usually involves promising above target inflation in the medium run. Because this is done to make up for below target inflation in the past, this is usually referred to as “catch-up inflation”. Such a policy has a dynamic feature to it and is announced to economic agents that are forward looking. In turn, from a modeling perspective this means that past promises arise as endogenous state variables and one has then to solve the model numerically. Because of this and despite its widespread use, little is known about the general properties associated with forward guidance.====Suppose that the central bank is forced to set its interest rate to zero, which is associated with a recession. What kind of forward guidance is advised to dampen the aforementioned recession? It turns out that the answer heavily depends on what exactly brought about the recession that forced the central bank to reach the lower bound. If this recession was brought about by an exogenous decrease in the natural rate of interest, then we know since Eggertsson and Woodford (2003) that the central bank needs to keep its interest rate to zero even after the shock subsides. This creates anticipations of an overheated economy after the shock is over which, by anticipation, generates an increase in private demand early on. If the recession is brought about by a decrease in confidence that has nothing to do with fundamentals however, the central bank should implement an ==== in interest rates instead—see Schmitt-Grohé and Uribe (2017), Bilbiie (forthcoming) and Nakata and Schmidt (forthcoming).====In this paper, we show that these—the presence of sunspot liquidity traps and forward guidance—are tightly linked. To do so, we develop a variant of the standard New Keynesian model along the lines of Eggertsson and Pugsley (2006), Carlstrom et al. (2015) and Bilbiie (2019b), but where we allow the central bank to ==== anchor its announcements about future policy to current inflation: we call this a ==== reaction function. If inflation undershoots in the short run, then the central bank promises to overshoot inflation in the medium run. Interestingly, the framework shares important features with the recent monetary policy framework announced in Powell (2020).====To study the effects of central bank announcements, we use a simple three-state Markov chain representation to solve the model in closed form. As a result, the model can be represented graphically using standard Aggregate Supply/Demand curves that take endogenous expectations into account. This approach clarifies that control-contingent forward guidance changes the ==== of both these curves. In contrast, state-contingent forward guidance==== that anchors announcements to the natural rate of interest only ==== those curves. The key is that control-contingent forward guidance ==== ties expected inflation to realized inflation. Throughout the paper, we will show that all the relevant statistics for the model can be expressed as a function of the effective slopes (==== slopes that take into account expectations) of the Aggregate Supply/Demand curves.====Using this new framework, we show analytically that a control-contingent reaction function that promises enough catch-up inflation in the medium run can rid the model of sunspot liquidity traps. To do so, we consider a New Keynesian model that features sunspot liquidity traps in the absence of forward guidance. As has been shown in the literature (see Mertens and Ravn (2014), Wieland, 2018, Wieland, 2019 and Bilbiie (forthcoming)), for a persistent enough sunspot the Euler and Phillips curves can cross a second time at the Zero Lower Bound (ZLB). By modifying the slopes of these two curves, a suitable policy of control-contingent forward guidance makes these curves rotate apart from each other: the sunspot liquidity trap cannot be an equilibrium. This arises because this policy makes households and firms expect a swifter recovery and thus counteracts contractionary income effects in the short run that become self-fulfilling in the absence of forward guidance. In contrast, a policy of state-contingent forward guidance does not modify the slopes of these curves and thus cannot rid the model of sunspot liquidity traps.====At first glance, this result seems to be at odds with Bilbiie (forthcoming), who shows that forward guidance is contractionary in a sunspot liquidity trap. What our model makes clear is that a control-contingent policy of forward guidance has an inherent “====” feature: if the promised inflation in the medium run does not sufficiently make up for current below target inflation, then this policy actually worsens the sunspot induced recession. This arises because in such a recession households want to save instead of consume. In this context, a timid policy of control-contingent forward guidance makes the expected returns of saving less elastic to current savings through expected inflation. As a result, households rationally save more to reach their optimal expected returns on savings in the short run. These increased savings translate into lower aggregate demand and the sunspot liquidity trap is associated with a deeper recession as a consequence. Therein lies the spirit behind the title of the paper: while a timid policy of forward guidance can worsen a sunspot liquidity trap, a bold enough one can get rid of the sunspot liquidity trap altogether.====This framework also allows us to study in closed form a central bank policy that mimics Price Level Targeting (PLT). Indeed, a special case of a control-contingent reaction function is one where expected inflation/deflation in the medium run exactly offsets inflation/deflation in the short run. For example, assume that a sunspot liquidity trap generates a recession with deflation in the short run. If the central bank can credibly commit to generate enough inflation for a sufficiently long time in the medium run, then the expected cumulative sum of inflation during the simulation period can be made to be exactly zero. By construction,==== the expected sum of inflation rates between time ==== and ==== is equal to ====. Therefore, a policy that equates this quantity to zero effectively stabilizes the price level to its starting point. We show that such a policy can prevent the occurrence of sunspots if it is aggressive enough. This sheds new light on the numerical findings in Holden (forthcoming), where price level targeting is shown to almost always guarantee a unique equilibrium in models that feature an occasionally binding ZLB constraint.====In addition, the fact that we can solve the model in closed form allows us to dig deeper. Accordingly, we can show that stabilizing the expected price level is ====: the return of the price level has to be rapid enough to rule out sunspots. This result is also related to Armenter (2018), who shows that an economy with a discretionary central banker that targets the price level features multiple Markov-perfect equilibria. We show that allowing for some degree of commitment can rid the economy of sunspots, which are not considered in Armenter (2018). It follows that these features turn out to be also warranted from a normative perspective: adopting a rapid return of the price level to its starting value when the economy is subject to sunspot liquidity traps can perfectly stabilize the economy: the announcements become a simple ====.====Finally, we show that announcements of future interest rates do not matter ====, but only through their effects on expected inflation. The results described in the previous paragraphs are derived in a model where the central bank directly announces expected inflation in the medium run as a function of short run inflation. In the last part of the paper, we show that this is the defining feature that makes monetary policy effective, ====. To show this, we study two extensions where the same path for inflation in the medium run can be either attained through zero, decreased or increased interest rates. Therefore, this framework makes clear that for an effective policy of forward guidance announced interest rates are mostly a mean to an end: ====.====—The earliest formulation of what is now commonly known as forward guidance can be traced back to Krugman (1998). In this paper, the solution advanced to deal with a liquidity trap is for the central bank to commit to a ==== increase in the price level. This intuition has further been formalized in a standard New Keynesian model in Eggertsson and Woodford (2003). There, the authors present numerical simulations of the optimal policy under commitment and show that the latter can be implemented through a suitable price level targeting rule. There is by now a burgeoning literature on optimal policy in fundamental liquidity traps: see Jung et al. (2005), Sugo and Teranishi (2005), Adam and Billi (2006), Nakov (2008), Sugo and Ueda (2008). Recent contributions also include Mendes (2011), Werning (2011), Schmidt (2013), Basu and Bundick (2015), Bilbiie (2019b), Armenter (2018), Hasui et al. (2016), Cochrane (2017), Nakata and Schmidt (2019), Mertens and Williams, 2019, Mertens and Williams, 2021 and Duarte (2019). Except from Bilbiie (2019b), Mertens and Williams (2019) and Nakata and Schmidt (2019), these papers present numerical solutions for optimal monetary policy after shock sends the economy at the interest rate lower bound. We will explain in more detail later how our paper differs from Bilbiie (2019b). Our paper differs from Nakata and Schmidt (2019) in that these authors consider a central bank that cannot commit beyond the current period, which is crucial in our framework. Our paper is closely related to Mertens and Williams (2019). They focus on the ergodic distribution of interest rates/inflation in a New Keynesian model that potentially features commitment, but do not study how the latter influences the slopes of AS/AD schedules and gets rid of sunspots.====Since Benhabib et al. (2001), we know that the presence of the ZLB generates global multiplicity of equilibria in New Keynesian models featuring a Taylor rule. As a consequence, sunspots can send the economy at the ZLB without any fundamental shocks. The ensuing literature has shown that policy prescriptions differ markedly compared to a fundamental driven liquidity trap. For example, Mertens and Ravn (2014) show that the government spending multiplier is below 1 in this situation, which is in stark contrast with the results for a fundamental liquidity trap derived in Eggertsson (2010), Christiano et al. (2011) and Woodford (2011). This has been further clarified in Wieland, 2018, Wieland, 2019. Recently, both Bilbiie (forthcoming) and Nakata and Schmidt (forthcoming) have studied the effect of different policies in a sunspot driven liquidity trap.==== Finally, Aruoba et al. (2018) provide evidence for the fact that the U.S. has recently been stuck in a fundamental liquidity trap while Japan has been stuck in a sunspot driven liquidity trap.==== As a result, studying how monetary policy interacts with both types of liquidity traps should be high on the research agenda.====Compared to the existing literature, in this paper we show that taking the type of liquidity trap as given and studying the policy prescriptions is potentially misleading. Indeed, we formally show that the conduct of monetary policy in the medium run can either get rid of (the promise) or worsen (the perils) sunspot driven liquidity traps. As a result, this paper is also related to Sugo and Ueda (2008) and Schmidt (2016). The former show that under optimal monetary policy with full commitment, the second steady state described in Schmitt-Grohe et al. (2001) disappears. The latter gets rid of short run sunspots with suitable Ricardian fiscal rules. Relatedly, Ono and Yamada (2018), Michaillat and Saez (2021), Nakata and Schmidt (forthcoming), Glover (2019), Cuba-Borda and Singh (2019), Diba and Loisel (2021) and Gabaix (2020) all find ways to get rid of the sunspot liquidity trap equilibrium. In a separate literature that departs from rational expectations, Benhabib et al. (2014) show that fiscal policy can get the economy out of an expectations-driven liquidity trap. To the best of our knowledge, none of these points to forward guidance as a potential solution.====This paper is also closely related to and builds heavily on Eggertsson and Pugsley (2006), Carlstrom et al. (2015) and Bilbiie (2019b). Eggertsson and Pugsley (2006) consider a central bank that announces an ==== medium run target inflation rate and applies this framework to make sense of the “mistake of 1937” in the context of the U.S. Great Depression. Bilbiie (2019b) is the first paper to analyze in closed form a policy of Optimal forward guidance (OFG). In this paper, the welfare maximizing policy after a large decrease in the natural rate of interest is to make the ==== of zero interest rates conditional on the shock. In the standard AS/AD representation (with inflation on the ==== axis and output gap on the ====-axis) of a textbook New Keynesian model, the natural rate shock shifts the AD curve down. A suitably specified OFG as in Bilbiie (2019b) shifts it back up and shifts the AS/Phillips curve down, generating inflation in the process. The same can be said of the policy considered in Eggertsson and Pugsley (2006).====In contrast, optimal monetary with commitment is control-contingent in the sense that it is ==== anchored on current economic conditions. The same holds for the famous catch-up rule proposed in Reifschneider and Williams (2000). To capture this, we assume that announcements about medium run policy by the central bank are ==== dependent on current economic conditions. As a result, in the same AS/AD representation such a reaction function==== will modify the slopes of both AS and AD curves.==== Therein lies the intuition as to why such a policy can worsen/prevent sunspots.====Finally, this paper is closely related to a recent paper by Eggertsson and Giannoni (2020). In this paper, the authors show that monetary neutrality in the medium/long run is prone to generate non-existence of equilibria and contractionary black holes. The same result arises in the standard New Keynesian model if natural rate shocks are close to a threshold level of persistence. In a flexible price version of the model, the authors show that a price level targeting policy can restore uniqueness of equilibrium in this context. The results in our paper therefore complement these as we show formally how a policy of forward guidance (or its price level targeting special case) can get rid of sunspot ZLB equilibria.====The rest of the paper is structured as follows. We lay out the baseline model with forward guidance in Section 2 and describe the different types of equilibria that can arise. In Section 3, we describe how central bank announcements about medium run policy interact with sunspot liquidity traps. In Section 4, we present additional results and a discussion. We conclude in Section 5.",The promises (and perils) of control-contingent forward guidance,https://www.sciencedirect.com/science/article/pii/S1094202522000394,Available online 2 August 2022,2022,Research Article,60.0
"Ordoñez Guillermo,Piguillem Facundo","University of Pennsylvania and NBER, United States of America,EIEF, Italy,CEPR","Received 24 June 2020, Revised 1 November 2021, Available online 3 December 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.11.002,Cited by (0),"The flow of saving as a fraction of disposable income (====) are tightly connected. We use a standard dynamic model to show that they may move in opposite directions when financial and/or human capital change dramatically. Making this link theoretically explicit provides an internally consistent measure of savings ratios based on saving rates and other publicly available data. We implement this measure for the four largest economies: U.S., China, Germany and Japan, and identify periods in which saving rates and savings ratios have moved in opposite directions. We find that those departures are not explained by capital gains, but instead by changes in the value of human capital.","In the last 20 years there has been an intense debate about the evolution of individual and aggregate savings. While some countries, such as China, appear to be experiencing saving gluts, large western economies appear to have embarked in dearths of savings. These trends have drawn the attention of academics and policymakers alike, mostly because declining household saving rates seem puzzling when confronted to evidence of substantial contemporaneous increase in household wealth. A clear example is the unprecedented increase in U.S. household's net worth (from five to seven GDPs since the eighties), accompanied by drastic decline in saving rates (from 15% to 5% of GDP since the eighties). Are then households saving more or less? Is this pattern special to the U.S.?====In this paper we clarify that there are two concepts of savings that are closely related, but capture different phenomena and can be moving in opposite directions. One is the propensity to save out of disposable income, or ====. This rate has been measured for many years in almost all countries using standard national accounting techniques. Thus, it is easily comparable over time and across economies, becoming the main input in the discussions about savings. The other is the propensity to save out of total wealth, which we call ====. Despite being tightly linked to fundamental saving motives, this ratio have not been measured consistently over time and across countries, mostly in response to the lack of a common methodology to measure total wealth, human and non-human. We show how its measurement helps to make sense of apparently puzzling observations related to wealth and saving rates, in the U.S. and in other countries.====We use a standard dynamic consumption model to theoretically link these two concepts of savings. We clarify under which conditions they move in opposite directions, so there is neither a puzzle nor reasons to be confused about a joint reduction in saving rates and rising financial wealth. Intuitively, households target a level of financial assets that depends on their total wealth, present and future. When present wealth increases (because of ==== for instance), or when future wealth rises (because ==== improves), agents rely less on delaying current consumption to achieve their desired level of savings. We show that indeed saving rates are a good proxy for the evolution of savings ratios ==== when the price of current assets and the present value of future human capital are stable.====To see this more clearly, consider a stationary economy where agents save 10% of their income every period so to target a level of savings as a fraction of total wealth. If agents experience capital gains, they have been saving relatively too much, and will react by reducing their saving rate below 10% for some time. Similarly, if agents believe that their future human capital will increase, leading to more income than expected, saving so much from current income is not needed, reducing again their saving rate below 10%. Thus, there are situations where savings ratios remain at the same level while standard measures of saving rates decline.====The second benefit of clarifying the theoretical linkage between the two concepts of savings is to highlight how to construct measures of savings ratios that are internally consistent both with saving rates and with the key components of wealth (namely financial and human capital) that affect saving decisions. To implement this theory-based measurement method, we quantify the link for the U.S. economy, which provides rich and abundant publicly available data, and has been the subject of extensive discussions. We show that despite the much debated fall in saving rates in the U.S., Americans have been steadily increasing their savings ratio for the last 40 years. This result is indeed remarkably consistent with alternative, micro-data based, methodologies proposed in the literature.====The third benefit of our exercise is to determine which of the main wealth components, capital gains or present value of human capital, was the most relevant in explaining the joint dynamics of saving rates and savings ratios.==== The capital gain component has received some recent attention. Fagereng et al. (2019) and Robbins (2019), for instance, adjust saving rates by redefining income to include capital gains and find that this explicit change in measurement helps to adjust saving rates upwards, but not dramatically.==== We also find that capital gains help to explain the decline in saving rates and their departure from savings ratios, but not enough to capture their opposite trend. Indeed, we find that the main driver in the saving rate moving in opposite direction to the savings ratio is the sharp increase in the present value of human capital, primarily determined by the decline in interest rates. These findings are also in line with Lustig et al. (2013), who were the first to point out the sharp increase in the U.S. wealth-consumption ratio and the key role played by human wealth. They do so, however, by appealing to a rich data set of individual behavior. It is reassuring that our estimates are consistent while relying on standardized publicly available aggregate data and a methodology that can be easily extended to many economies so to perform cross-country comparisons.====To show the applicability of our methodology we also compute savings ratios for China, Japan and Germany, which together with the U.S. represent approximately 50% of the world GDP. These measures are then comparable across countries. Unlike the U.S., we find that both measures of savings move in the same direction for Japan (downwards) and Germany (upwards). Thus, although discrepancies can arise between saving rates and savings ratios, that is not necessarily the case. However, when analyzing China, a different pattern emerges: while the saving rate has remained steadily high for the last 20 years, the savings ratio has steadily declined. There is, however, a common pattern across all these countries: ====. For instance, had human capital maintain its value, U.S. and Germany would have displayed saving rates of around 35%, while China and Japan would have experienced slightly negative saving rates. Hence, in absence of changes in the present value of human capital, driven in western countries by a reduction in interest rates and in eastern countries by a decline in growth rates, the international flow of capital could have been very different, with eastern economies borrowing from western economies.==== There is an extensive literature addressing the movements of the aggregate saving rate in the U.S. The first paper noting its fall was Summers and Carroll (1987), who stressed the relevance of savings for long-term growth and urged the U.S. government to take action to prevent a stagnation.==== This observation sparked a rich literature on the causes of the declining saving rates. Given the aforementioned inconsistency with the evolution of wealth, one approach was refining the measurement of saving rates.==== In an influential paper Gale and Sabelhaus (1999) show that, among many potential adjustments to measured saving rates (such as retirement accounts, inflation and taxation) the most relevant was capital gains. Other research proposed adjusting for changes in TFP, Chen et al. (2006b), and reformulating NIPA calculations, Boskin (2009). Here we clarify that there may not be any inconsistency in how we measure saving rates, but instead that those measures capture a different aspect of the increase of savings.====Summers and Carroll (1987)' observation also sparked a rich literature on the economic consequences of declining saving rates and the implied policy responses, ranging from the impact on growth, such as Campbell (1987), Attanasio (1994), Nordhaus (1995), Gokhale et al. (1996), Attanasio (1998) and Parker (1999), to the impact on wealth inequality more recently, such as Gomez (2017), Karabarbounis and Neiman (2019), Fagereng et al. (2019) and Robbins (2019).==== While our work is silent about the macroeconomic effects of a reduction in saving rates, we clarify that savings may instead have investment implications through valuation effects.====The U.S. case was not an exception. Similar controversies took place in other countries. The high saving rates observed in Japan until 1980 and the subsequent fall also sparked a literature trying to understand the observed patters, as surveyed by Horioka (1990). Given the large differences in economic performance between Japan and the U.S. Hayashi et al. (1988) discussed how puzzling were the similarities in the evolution of saving rates. This inconsistency was finally settled by Chen et al. (2006a), who show the important role played by TFP. In this paper we also show the relevance of the pattern of grow and convergence to a steady state on shaping the observed saving rates. In China, in contrast, the debate has been about the persistently high observed saving rates. Explanations that range from demographics, Curtis et al. (2015), the one child policy, Choukhmane et al. (2013), and insurance motives, Imrohoroglu and Zhao (2018), have been proposed. This literature is in general puzzled that a fast growing economy as China saves instead of borrow. With our methodology we uncover a decreasing savings ratio in spite of the high saving rate, exactly as predicted by the theory. Moreover, we show that without the implied changes in human capital, China would have indeed borrowed instead of saving.====Reconciling apparently contradicting signals from saving rates and wealth is relevant beyond clarifying their relation. It is also relevant, for instance, to qualify the related discussion about the role of life expectancy on savings. The view that higher life expectancy in the U.S. implies an increase in savings has been usually challenged, and sometimes outright discarded, because they are at odds with declining saving rates.==== Farhi and Gourio (2018) and Eggertsson et al. (2019a), for instance, seemingly counterfactually argue that savings in the U.S. economy should have sharply increased in the last 30 years. Here we show that indeed savings have been increasing when measured as a ratio of total wealth.====There is also an evolving literature studying how the development in financial markets impact savings. Carroll et al. (2019) argue that financial liberalization helps to explain the reduction in saving rates (the easier it is to borrow, the less agents need to save). This view has been used, for instance, by Guerrieri and Lorenzoni (2017) to argue that a sudden sharp reversal on the trend of loosening credit played a large role in the recently, and relatively short-lived, saving rate rise after the Global Financial Crisis. Ordonez and Piguillem (2019) and Eggertsson et al. (2019b) combine the demographic and financial drivers of savings in the same setting, also claiming that savings out of wealth in the U.S. should have increased in recent decades. In this paper we show, indeed, that the valuation of assets and human capital closely follows movements in interest rates and affect both saving rates and savings ratios consistently with standard dynamic macroeconomic theory and with the data.",Saving rates and savings ratios,https://www.sciencedirect.com/science/article/pii/S1094202521000776,3 December 2021,2021,Research Article,61.0
"Kim Soojin,Rhee Serena","Department of Economics, Georgia State University, 55 Park Place, Atlanta, GA 30303, United States of America,School of Economics, Chung-Ang University, 84 Heukseok-ro, Dongjak-gu, Seoul 06724, Republic of Korea","Received 19 August 2018, Revised 21 November 2021, Available online 29 November 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.11.001,Cited by (0),"We study the aggregate consequences of the Social Security Disability Insurance (DI) program, focusing on the role of complementarity between heterogeneous human capital. First, we develop and estimate a wage process in which individuals' human capital comprises (pure) labor and experience, and their efficiencies are affected by disability. We find that older workers are more experience-abundant, and that disability causes a smaller loss in the efficiency of experience than it does in the efficiency of labor. Further, the estimated aggregate production technology shows that labor and experience are complementary inputs. Combining these empirical results with a structural ==== model, we analyze the labor market implications of removing the DI program. Removal of the DI program induces an increase in the relative supply of experience, thus affecting the marginal productivities of inputs and wages of all workers in the economy. Despite the increased labor market entry of disabled workers, the aggregate productivity may increase in the counterfactual economy, thanks to the complementarity between labor and experience.","In 2019, approximately 10 million people in the United States benefited from the Social Security Disability Insurance (DI) program (Social Security Administration, 2020), and its growth is accelerating with the aging of the population.==== Although DI serves as an important safety net against health risks, recent empirical studies (e.g., Maestas et al., 2013 and French and Song, 2014) have found that it suffers from considerable work disincentive effects. Given the large scale of the DI program, understanding the aggregate implications of the labor supply effects of DI is essential. This study addresses this question by evaluating the individual-level effects of disability on workers' productivities and combining the micro-level results with the structural model to measure the effects of DI on aggregate outcomes.====To assess the DI program, we need to measure the productivity of disabled workers, and how the loss of these workers impacts the labor market and aggregate production. Thus, we first estimate the productivity effects of disability on workers. Following the seminal paper of Katz and Murphy (1992) and expanding the work of Jeong et al. (2015), we assume that workers are endowed with two inputs: “(pure) labor” and “experience.” Using the detailed micro-level data, we quantify how detrimental the severity of the disability is on the efficiencies of labor and experience, thereby identifying the sources of the low productivity (wage) of disabled workers. Then, we further exploit the time-series variations in the relative price and quantity of labor and experience to measure the substitutability between the two inputs in aggregate production. The modeling of these heterogeneous inputs helps measure both the direct productivity loss and the potential aggregate efficiency consequences from losing workers due to the DI program. Lastly, we use a general equilibrium life-cycle model of workers to evaluate the aggregate labor market effects of DI within our heterogeneous input model and to measure the value of the DI program for workers.====The micro-level estimation of disability effects on productivity uses data from the Panel Study of Income Dynamics (PSID), which contains work history (years of experience) and disability status information. Using the information on the binary indicator of work limitation and the extent to which it limits work, we categorize workers into three disability types: non-disabled, moderately disabled, and severely disabled. Then, using the hourly wage rate as a measure of productivity, we estimate the amounts of efficiency units of labor and experience of workers over the life-cycle and the effects of disability on these human capital after controlling for selection bias using Heckman's two-step procedure. We find that having a moderate (severe) disability lowers the workers' efficiency units of labor by around 27% (40%) and their efficiency units of experience by 4% (17%). That is, the worker's disability is less detrimental to the efficiency of experience than that of labor. In conjunction with the fact that the experience is the primary source of human capital for older workers, this finding suggests that the amount of experience lost from the reduced labor market participation of older workers, the majority of DI recipients, might be considerable. Further, if these inputs are imperfectly substitutable in aggregate production, the changes in the relative supply of inputs can cause indirect effects through equilibrium factor prices. To capture the aggregate effect, we estimate the elasticity of substitution between the two inputs, assuming a constant elasticity of substitution (CES) production function. Exploiting the time-series variations in the relative supply and the relative price of experience, we find that labor and experience are gross complements with the elasticity of substitution of 0.40.====Next, we develop a general equilibrium model to quantify the aggregate impacts of the DI program incorporating the empirical findings. Finitely-lived individuals are subject to disability, survival, medical expenditures, and labor market risks. The individuals' disability status affects their survival probabilities, dynamics of future disability status, medical expenditures, preferences, and labor market productivities and opportunities, richly capturing its various impacts on workers. These workers make endogenous labor supply and saving decisions, and, if disabled, they are allowed to apply to the DI program. Importantly, we model the key features of the DI program, including the application processes and the risks (e.g., acceptance, reassessment, and Medicare qualification) associated with the policy and other social insurance programs (e.g., unemployment insurance) that can affect worker decisions jointly with DI. We calibrate the model to match the life-cycle statistics of worker outcomes by disability statuses, given the estimated wage processes and aggregate production technology. Our model matches the targeted moments (e.g., employment rates and DI beneficiary shares) well and can generate empirically plausible estimates of labor supply elasticities and non-employment elasticities with respect to DI generosity that are not targeted.====Finally, we use the model to evaluate the impact of DI on aggregate outcomes. In the calibrated economy, the removal of the DI program increases the work incentives of all workers, with a more pronounced rise among old and disabled (both moderate and severe) workers. Thus, the relative supply of experience increases by 0.94%, accompanied by 0.53% higher price of labor and 1.80% lower price of experience. These changes in prices are important contributors to wage effects. At the individual level, young workers with abundant labor benefit thanks to the increased price of labor. Old workers benefit from the higher amounts of experience they accumulate in the counterfactual economy despite lower prices. In the aggregate, the employment rate increases by 3.25 percentage points (====) and output by 2.88%.====To understand the role of the input complementarity, we conduct the no-DI counterfactual analysis in a recalibrated economy where labor and experience are perfect substitutes. We find that accounting for the complementarity between inputs is important for gauging the productivity effects of removing the DI program. Thanks to the complementarity between labor and experience and the relatively small detrimental effects of disability on experience, the entry of experience-abundant old workers induced by the removal of DI leads to a 0.08% increase in aggregate productivity (output per hour). This contrasts with the negative productivity effect (−0.03%) of DI removal under the assumption of perfectly substitutable inputs. This finding suggests that disabled (old) workers provide valuable human capital in the labor market, thereby also impacting young workers' productivities.====Lastly, we measure the value of DI to workers. An unexpected temporary (one-period) removal of the DI program generates an overall welfare loss equivalent to 1.14% of the consumption in the benchmark economy with DI. While the valuation of DI varies widely across disability and labor market statuses, ex-ante, workers of all disability types value the DI program.====Overall, our findings can be summarized as follows. First, we find that disability is less detrimental to the efficiency of experience than it is to the efficiency of labor, thereby limiting the productivity losses of old, disabled workers. Second, because of input complementarity, DI impacts the labor market's input composition and their efficiencies, affecting wages of all workers in the economy. Lastly, due to the interaction between inputs, a reduction in the DI program may increase the aggregate productivity (output per hour) of the workforce, whereas an abstraction from this channel would imply a decrease in aggregate productivity. These findings underscore the importance of incorporating heterogeneous human capital of the workforce and their interactions in evaluating and reforming the DI program.====  Our work is related to several strands of literature studying the role of heterogeneous inputs in production and their interactions in the labor market; the disincentive effects of DI on labor supply; and the effects of social insurance policies in structural models with heterogeneous agents.====First, we build on the literature that studies heterogeneous inputs in production. Similar to the previous literature (e.g., Card and Lemieux, 2001; Krusell et al., 2000; Karabarbounis and Neiman, 2014), we estimate the degree of substitutability across heterogeneous inputs in production using empirical data, assuming a CES production function.==== In terms of methodology, we are most closely related to Jeong et al. (2015), who extends the work of Katz and Murphy (1992) to estimate the amount of labor and experience, which are two distinct inputs (human capital) a worker is endowed with. They use work experience data along with individual-level characteristics from the PSID. We expand their wage process to estimate the impact of disability on labor and experience over the life cycle after controlling for selection bias.====Second, this paper builds on and expands the studies of the labor supply disincentive effects of DI, which has long been a topic of interest, starting with Bound (1989). Maestas et al. (2013) and French and Song (2014) use random assignments of disability examiners and judges to estimate the disincentive effects of DI on the labor supply of workers, and find substantial disincentive effects.==== Although these papers use econometric approaches to study individual behavior, Kitao (2014), Low and Pistaferri (2015), and Autor et al. (2019) are among the few who develop life-cycle models to analyze the effects of DI. In particular, Kitao (2014) focuses on the interaction between DI and unemployment insurance, whereas Low and Pistaferri (2015) focuses on the incentive and insurance trade-off that individual workers face. Meanwhile, Autor et al. (2019) evaluates welfare effects of DI by explicitly incorporating household structures and finds that spousal labor supply serves as an important insurance against disability. This paper is distinct from theirs in two dimensions. First, most analyses on disability assume that a worker's human capital is one-dimensional, whereas we explicitly model and estimate the effects of disability on heterogeneous human capital endowments of workers. Thus, our analysis provides an understanding of the sources of the productivity losses that disabled workers face, and how these effects might differ over the life cycle. Second, we further use these micro-level findings and incorporate the interactions between inputs in aggregate production to evaluate the DI program.====Finally, this paper contributes to the broad literature analyzing the effects of social insurance policies, especially concerning health or medical expense risks (e.g., Hubbard et al., 1995; Attanasio et al., 2011; Pashchenko and Porapakkarm, 2017; De Nardi et al., 2018). Some recent papers in the literature include De Nardi et al. (2016) and Braun et al. (2017). Both studies analyze the role of social insurance policies for the old and measure the welfare gains from the policies in the presence of health and medical expenditure risks. Hosseini et al. (2020) studies the role of health risks in accounting for lifetime earnings inequality and finds that DI is an important contributing factor, as it prompts the labor market exit of workers with poor health. We find that when the DI program is removed, workers with disabilities experience a larger increase in their income than non-disabled workers, consistent with their finding. Our study complements these studies by focusing on the role of DI and its aggregate labor market implications.====The organization of the paper is as follows. Section 2 outlines our empirical estimation of productivities of workers with different disability statuses and the elasticity of substitution between labor and experience. Section 3 develops a general equilibrium model with DI, which serves as a laboratory for evaluating DI. Section 4 discusses the calibration of the model and its validity. Section 5 uses the calibrated model to conduct counterfactual analyses. Finally, Section 6 concludes.",Understanding the aggregate effects of disability insurance,https://www.sciencedirect.com/science/article/pii/S1094202521000764,29 November 2021,2021,Research Article,62.0
"Fujiwara Ippei,Waki Yuichiro","Faculty of Economics, Keio University, 2-15-45 Mita, Minato-ku, Tokyo 108-8345, Japan,Crawford School of Public Policy, Australian National University, Canberra, Australia,The Asian Bureau of Finance and Economic Research (ABFER), Singapore,The Research Institute of Economy, Trade and Industry (RIETI), Tokyo, Japan,Centre for Economic Policy Research (CEPR), London, United Kingdom,Faculty of Economics, Aoyama Gakuin University, 4-4-25 Shibuya, Shibuya-ku, Tokyo 150-8366, Japan,School of Economics, the University of Queensland, Australia","Received 10 December 2019, Revised 29 September 2021, Available online 11 October 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.10.002,Cited by (1)," and the output gap. We call this phenomenon the Delphic forward guidance puzzle. In more elaborate models with endogenous state variables, a combination of Delphic forward guidance and preemptive policy actions may improve welfare. However, full information revelation is generally not optimal and what information needs to be revealed is highly model-dependent.","Central banks have been thought to possess private information about future economic conditions. Romer and Romer (2000) provide empirical evidence of asymmetric information between the central bank and private agents: “the Federal Reserve has considerable information about inflation beyond what is known to commercial forecasters.”==== The possession of such superior information by the central bank raises several questions. How should monetary policy be designed when the central bank has private information about future economic conditions? Does the central bank benefit from managing the private sector's expectations by utilizing such information?====This paper investigates whether central banks should reveal their superior information about future economic conditions, either by communicating it or by undertaking observable policy actions, in representative-agent dynamic stochastic general equilibrium (DSGE) models with nominal rigidities. DSGE models with nominal rigidities are best suited for our analysis because the central bank in the models can manage the expectations of forward-looking agents by conveying its superior information. In addition, these models are widely used in central banks to guide policy.====Central banks' communication of such information is of practical relevance. Campbell et al. (2012) distinguish between ==== forward guidance, which involves public statements about “a forecast of macroeconomic performance and likely or intended monetary policy actions based on the policymaker's potentially superior information about future macroeconomic fundamentals and its own policy goals,” and ==== forward guidance that involves the policy-maker's commitment to a future, possibly state-contingent, action plan. They found empirical evidence suggesting that the forward guidance employed by the Federal Open Market Committee has “a substantial Delphic component”. Although the importance of Odyssean forward guidance has been well established in the New Keynesian monetary policy literature, it is not yet known whether Delphic forward guidance is useful in New Keynesian models. This paper sheds light on this issue.====The present paper argues that improving social welfare through Delphic forward guidance is indeed difficult, if not impossible, in these DSGE models. Our argument is based on a theoretical result in simple New Keynesian models that Delphic forward guidance unambiguously reduces welfare and on numerical experiments in more elaborate models that the welfare implications of Delphic forward guidance are highly model-dependent.====First, we identify a key mechanism in New Keynesian models through which Delphic forward guidance generates a welfare loss. This is done by extending a textbook, purely forward-looking New Keynesian model (Woodford, 2003) to incorporate a direct communication channel from the central bank. In the model, Delphic forward guidance unambiguously decreases ex ante welfare. The benevolent central bank finds it optimal to commit to not revealing its superior information about future shocks, either directly through communication or indirectly through observable policy actions. The result holds for any shocks — a natural rate shock, a cost-push shock, or a shock to the welfare loss function — and even in the presence of an effective lower bound on the nominal interest rate.====The underlying mechanism is simple and operates through the forward-looking New Keynesian Phillips curve. Intuitively, any information provided through forward guidance that helps predict future inflation moves current expected inflation and, therefore, acts to shift the Phillips curve as does the cost-push shock. To see this point, consider a simple New Keynesian model in which the central bank has preferences for stabilizing inflation and the output gap. When the private sector becomes better informed about future shocks, its inflation expectations vary too and, from an ex ante point of view, become more volatile. Because the increased volatility of inflation expectations acts as an additional source of disturbance in the New Keynesian Phillips curve, it translates into higher variability of inflation and the output gap, and is therefore harmful to the central bank that aims to stabilize these variables.====Second, we consider two variants of New Keynesian models with endogenous state variables. This is because lack of endogenous state variables in the purely forward-looking model rules out the potential benefits of preemptive monetary policy actions to combat future shocks. In the first model, the economy can be in a severe recession in the future because of a negative natural rate (====, demand) shock and of a binding effective lower bound on the nominal interest rate. Preemptive monetary policy easing can mitigate the future recession because a rise in current inflation raises future inflation at an effective lower bound through backward price indexation. In the second model, both price and nominal wage are sticky, making the real wage a slow moving variable. The central bank can, potentially, influence the current real wage so as to reduce the effect of future price and wage mark-up shocks upon their realization.====In both models with endogenous state variables, we examine whether information revelation, either through direct communication or through information-dependent policy actions, can improve welfare. In the model with price indexation and an effective lower bound, information revelation can improve welfare, but only when the Phillips curve is sufficiently steep and the elasticity of intertemporal substitution is extremely high. With realistic parameter values, it is optimal for the central bank to reveal no information and to set the current nominal rate independently of its knowledge about the future natural rate shock. In the sticky price and wage model, information revelation improves welfare when either the price or the wage is relatively — but not perfectly — flexible, but reduces welfare when both are sufficiently sticky. Hence, although endogenous state variables open up the possibility that the gains from information revelation and preemptive policy actions exceed the costs, secrecy is still optimal for a range of realistic parameterizations of these models. We also show that, even when secrecy is suboptimal, indirect information revelation through policy actions is often sufficient in these two models. In other words, Delphic forward guidance should not reveal more information than the central bank's actions.====Moreover, the welfare implications of Delphic forward guidance are determined by some complicated interactions between shocks and frictions in the model, and are thus dependent on the details of the model at hand. The shock type by itself does not determine the sign of the welfare effect of Delphic forward guidance: forward guidance about the mark-up shocks may improve welfare in the model with sticky price and wage, whereas it unambiguously reduces welfare in the purely forward-looking New Keynesian models; and forward guidance about the future demand shock may be harmful to social welfare in the presence of the effective lower bound on the nominal interest rate. Even within the same model, the welfare effect becomes either positive or negative, depending on parameter values. Overall, the welfare implications of Delphic forward guidance are model-dependent.====Finally, we complement the previous analysis using a more elaborate, nonlinear DSGE model that is based on Christiano et al. (2005) and Smets and Wouters, 2003, Smets and Wouters, 2007. The model features multiple distortions, frictions and shocks. Both price and wage are sticky and are subject to backward indexation. The household accumulates capital subject to investment adjustment costs, and external habit formation is assumed. Monetary policy follows a Taylor rule with inertia. There are four kinds of shocks: price and wage mark-up shocks, a technology shock, and a monetary policy shock. The model is solved using a second-order approximation, assuming that Delphic forward guidance perfectly reveals future realizations of any of these four shocks. Different versions of the model are also examined, by turning on and off a subset of frictions.====The results reiterate the model-dependency of the welfare effects of Delphic forward guidance: the sign of the welfare effects differs across the shock types and can change easily when some distortions and frictions are included or excluded from the model. In the full-blown model, forward guidance that reveals the price mark-up shocks and the monetary policy shocks improves welfare, but welfare decreases when the wage mark-up shocks or the technology shocks are revealed. However, with weaker wage rigidity, forward guidance about the price mark-up shock reduces welfare; without price and wage rigidities, forward guidance about the technology shock improves welfare; and without policy inertia, forward guidance about the monetary policy shock reduces welfare. These implications are the opposite to what we find in the full-blown model. Therefore, the welfare implications are highly model-dependent and choosing which shocks and how much information to reveal ==== is not at all straightforward.====The importance of ==== has been emphasized in the New Keynesian literature (Woodford, 2003).==== However, our results overall suggest that, in the same class of models in which expectations are important, it is difficult to improve welfare using Delphic forward guidance based on the central bank's superior information about the future economic conditions. Therefore we call this property ====. The central bank should instead aim to conduct Odyssean forward guidance by communicating its state-contingent policy, ====, what it will do in response to these shocks after they materialize.====This paper is structured as follows. Section 2 extends a textbook New Keynesian model to incorporate a direct communication channel by the central bank and shows that it is optimal for the benevolent central bank to commit to secrecy. Section 3 analyzes two stylized New Keynesian models with endogenous state variables as well as a more elaborate DSGE model. Section 4 concludes.",The Delphic forward guidance puzzle in New Keynesian models,https://www.sciencedirect.com/science/article/pii/S1094202521000752,11 October 2021,2021,Research Article,63.0
"Gihleb Rania,Lifshitz Osnat","Department of Economics, University of Pittsburgh, United States of America,Tiomkin School of Economics, Reichman University, Israel","Received 21 September 2018, Revised 1 October 2021, Available online 8 October 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.10.001,Cited by (0),"The gender education gap has undergone a transition in the post-war period, from favoring men to favoring women. As a result, in 30% of young American couples, the wife is more educated than the husband. These “married down” women display substantially higher employment rates, relative to women with husbands with the same or higher level of educational attainment. We argue that the interaction between work and marital decisions can explain the higher employment rates of women who marry down. Returns to experience are key in this mechanism, since they lock in early employment choices. We formulate a dynamic ","The post-war period has witnessed a remarkable increase in women's employment as well as a rise in women's educational attainment relative to men.==== At the same time, there was a considerable increase in the proportion of married households in which the wife has more education than the husband. We will refer to these wives as “married down” (see Fig. 1). This phenomenon is primarily driven by women with at least a college degree whose proportion of all married women grew from 5% in 1980 to 16% in 2010. By that year, 59% of all married-down wives had at least a college degree.====Married-down women display significantly higher employment rates than women who married up or married equal throughout the post-war period, even after controlling for their husband's income.==== This pattern is particularly pronounced among wives with a college degree, as shown in Fig. 2.====In this paper, we study the mechanism behind the higher employment rates among married down women and quantify the degree to which the rise in the share of married down women accounts for the aggregate rise in married women's employment. The working hypothesis is that returns to experience play a critical role when the strong interaction between work and marital decisions is considered. In the case of married down women, there is typically a relatively high wife/husband wage ratio at the start of the marriage and the wife's career, which leads to a higher employment rate than for wives in households with a lower wage ratio. The returns to experience imply that married down women will have a greater incentive to work throughout their working life, leading to higher employment rates relative to married up women. The difference will be larger for highly educated women because returns to experience are more important for this group. This highlights an important source of heterogeneity that points to a differential response among women to designated policy incentives.====To explore this hypothesis, we develop a dynamic life-cycle model of endogenous marriage and divorce with labor supply decisions. Educational attainment is exogenous and determines the age of entry into both the labor and marriage market.==== In this model, an individual chooses whether to get married/divorced and whether to work.==== Decisions are made at an annual frequency. An individual's wage is determined by his/her human capital, which is composed of schooling and endogenously accumulated work experience, on which the rate of return varies with level of schooling. Experience influences both wages and the probability of a job offer, which is based on the strong persistence of labor market outcomes observed in the data. We also incorporate the tax/benefit rules that our cohort faced over time. Single agents are confronted with an exogenous probability of matching with a potential partner. Their decision to marry depends on the realization of a match-quality shock as well as on the value of remaining single, which depends on the potential wage. If they remain single, they continue searching in the marriage market. Married couples make decisions in accordance with the collective model under limited commitment with endogenous bargaining power==== (Mazzocco et al., 2013; Voena, 2015), they share risk and resources, and they enjoy economies of scale. The decision to remain married depends on the spouses' outside options. This rich setting allows us to evaluate how marital circumstances interact with labor supply and the allocation of resources within the household.====We estimate the dynamic model using the simulated method of moments for a sample of white women from the NLSY79 dataset. The estimated dynamic model provides a close fit to the data, and the parameter values appear to be sensible. In particular, the model provides a good match to the pattern of marital sorting by education and generates the variation in employment rates across the various wife-husband education levels observed in the data.====Returns to experience are a key feature of the model. Higher levels of employment early in the life cycle lead to higher future potential earnings and job offer probabilities, which generates a lock-in mechanism (that is, a mechanism characterized by persistence or even irreversibility) in which workers with higher employment rates early in the life cycle continue to have higher employment rates in later years. This mechanism interacts with the marital decision. In particular, a woman who marries down will initially choose a higher employment rate than a woman who marries up or marries equal due to the higher potential wife-to-husband wage ratio. Once that occurs at the beginning of married life, it will induce higher employment throughout the life cycle.====To gauge the quantitative impact of the mechanism, we perform a backcasting exercise. Given the estimated benchmark model, we replace the educational attainment distribution of the NLSY79 cohort (the 1965 birth cohort) with that of a cohort twenty years older (the 1945 birth cohort). As a result, the proportion of married-down women drops by 11 percentage points relative to the baseline model, while the proportion of married-equal women remains largely unchanged. These predictions are comparable to those observed historically and therefore provide validation for the model. This simulation suggests that the resulting changes in marital sorting patterns ==== can account for 11% of the observed increase in the married women's employment rates between the two cohorts (1945 and 1965). In other words, it is not only the increased proportion of women with higher educational attainment that drives the increase in married women's employment but also the endogenous increase in married women's labor supply, induced by the increase in women's ==== education, which in turn raises the share of married-down women.====In order to assess the importance of these dynamic labor supply effects, we consider a counterfactual simulation without returns to experience. In this simulation, the gap in employment between women who married up and women who married down narrows by about 45 percent, which is even greater in the case of more educated women due to the higher returns to experience they face. These findings suggest that returns to experience and higher relative education (i.e. marrying down) play important and roughly equal roles in accounting for the persistence in women's employment rates.====Finally, the model is used to simulate a scenario in which the US replaces the joint taxation of couples with individual taxation. We predict that this would increase the average employment rate of married women by 6.1%, with a larger impact on women with a high school degree (which is in line with our estimated elasticities).==== Moreover, the reform predicts a larger labor supply increase for married down women relative to the other groups. This is due to the progressive tax structure which results in larger penalties for households with two earners who have similar incomes. It would also increase the marriage rate by 7.6% and reduce the divorce rate by 8.9%. The ability to predict the effect of such a change in tax policy on not just the behavior of existing married couples, but also on marriage (and divorce) rates, as well as intrahousehold allocations, demonstrates the benefit of using such a rich setting.====The findings have implications for the gender wage gap. In light of the reversal of the gender gap in education, women's advances in the professional occupations, and the fact that married women are increasingly more likely to be more educated than their husbands, and persistently employed, we should have seen a sizable convergence in the gender wage gap, especially for college workers. Instead, the gender wage gap has in fact persisted (Blau and Kahn, 2017), suggesting that other factors are also at play.====There is an extensive literature on the dramatic increase in married women's labor supply in recent decades. Explanations of the phenomenon include: women's increased educational attainment (Eckstein and Lifshitz, 2011); technological change in the household (Greenwood et al., 2005); advances in medical/contraceptive technology (Goldin and Katz, 2002; Albanesi and Olivetti, 2009); changes in the wage structure, such as, the skill premium, in the gender wage gap, or in the returns to labor market experience (Jones et al., 2003; Olivetti, 2006; Gayle and Golan, 2011; Knowles, 2012; Heathcote et al., 2017); changes in cultural trends (Fernández et al., 2004; Fernandez and Wong, 2017); structural changes in the economy (Goldin, 1990; Galor and Weil, 1996; Rendall, 2010); and changes in child-care costs (Attanasio et al., 2008). We contribute to this literature by offering a novel perspective on explanations why married women's employment rates increased. We highlight the overlooked yet significant implications of the phenomenon of women marrying down, in light of the reversal of the gender education gap. We focus on the endogenous response of labor supply and the dynamic impact of returns to experience. Part of the rise in married women's employment has been attributed to the increase in their educational attainment. While it is generally true that higher educated women have higher employment rates, it is interesting that these women have relatively higher employment rates when they are married down. The growth in the share of educated women marrying down has amplified the role of increased educational attainment on the rise of women's employment.====The paper also contributes to a growing empirical literature that estimates dynamic models of intra-household allocations and marital behavior using the collective framework (Mazzocco et al., 2013; Gemici and Laufer, 2011; Voena, 2015; Bronson, 2015). Mazzocco et al. (2013) make an important contribution by extending the collective model with no commitment to an inter-temporal setting. It is worth mentioning that estimating such models with endogenously evolving state variables involves considerable computational complexity.==== We focus on the life-cycle aspect of work decisions and their interaction with marital choice, and in particular, we differentiate between married couples by relative education level. In particular, we use the intertemporal household model with limited commitment in order to study the drivers behind the substantial employment gap observed between women who are married down and women who are married up (or married equal).====Finally, our paper draws from the extensive literature that examines female labor force participation dynamics (see for example, Eckstein and Lifshitz, 2011; Eckstein et al., 2019). Dependencies between an individual's past and current labor supply decisions are well established. These may be generated by positive wage-based rewards for human capital accumulated via labor market experience (see for example, Eckstein and Wolpin, 1989; Altug and Miller, 1998; Imai and Keane, 2004; Olivetti, 2006), as well as by habit persistence (Altug and Miller, 1998).==== The abovementioned studies stress the importance of financial returns to work experience as a determinant of women's labor supply. We build on this literature by endogenizing both marriage and divorce, and estimating the dynamic labor supply effects produced by the marriage match characteristics, specifically, the spouse's relative education, which work through the accumulation of labor market experience. To the best of our knowledge, this paper is the first that focuses on changes in the cohort labor supply and marriage behaviors and more importantly, their interactions over the course of the life-cycle.====The paper is organized as follows. Section 2 describes the employment behavior of married white women and its association with relative spousal education.==== Section 3 develops the dynamic model. Section 4 presents the estimation methodology and section 5 presents the results. Section 6 provides counterfactual analysis and section 7 concludes.",Dynamic effects of educational assortative mating on labor supply,https://www.sciencedirect.com/science/article/pii/S1094202521000740,8 October 2021,2021,Research Article,64.0
"Biolsi Christopher,Craig Steven G.,Dhar Amrita,Sørensen Bent E.","Department of Economics, Western Kentucky University, Bowling Green, KY, United States of America,Department of Economics, University of Houston, Houston, TX, United States of America,Department of Economics, University of Mary Washington, Fredericksburg, VA, United States of America,CEPR, United Kingdom","Received 10 December 2019, Revised 28 August 2021, Available online 4 October 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.09.004,Cited by (0),"This paper takes a novel time series perspective on the financing of K-12 schooling. About half of school spending is financed by state government aid to local districts, and because state aid is generally income conditioned, it acts as a mechanism for risk sharing between school districts. We show that temporal ","Over the last forty years, U.S. state governments have attempted to reduce spending disparities between school districts in K-12 education financing. While the initial impetus for these attempts arose from the courts, starting with the ==== decision in California in 1976, most states—even those that have avoided court decisions—now use some form of income-conditioned grants. A substantial economic literature has studied inequality in school spending, but it has been less appreciated that school spending varies over time. Our goal is to empirically model the variation in school spending over time and its impact on inequality between cohorts of students. Being income-conditioned, state aid partly offsets the impact of purely local income shocks on local school taxes (“local revenue”) and serves as a mechanism for risk sharing across school districts, especially at longer horizons. State aid, however, is positively correlated with state-level income shocks which affect all districts, leaving school spending vulnerable to such shocks.====To characterize the dynamics of school system finances, we use data from 8,676 independent school districts in the United States from 1992 to 2014, focusing on intertemporal fluctuations in school spending caused by changes in income. The dynamic patterns in school spending and their dependence on state-level and local income shocks have not been studied in detail before. Local income shocks are found to be as likely to be negative (or positive) in relatively wealthy school districts as in relatively poor school districts, so a study of risk sharing between districts is quite separate from a study of redistribution between rich and poor districts. As districts change their position in the income distribution, the allocation of aid will change and have dynamic implications.====School spending is mainly financed by local revenue and state aid, and we construct a model of state and local government behavior with an objective function for each level of government.==== For the state, we model the choice between school spending and lower taxes while allowing for habit formation, and we model preferences for equality in spending across school districts. For the local school district, we model the choice between school spending and lower taxes while allowing for habit formation, and we model the preferences for offsetting state aid. We do not attempt to identify “natural experiments” and we do not model the many constraints under which school districts operate, so we do not consider this a structural model of state and local agents.====The model is designed to capture salient patterns in school spending over the business cycle and interprets the patterns in terms of declining marginal utility of state aid and local school revenue. It further incorporates habit persistence, which captures the gradual adjustment of spending found in the data. We interpret the results as showing reactions to exogenous income shocks at the governmental level for states and school districts. The model estimates do not control for potential bias due to migration of families in response to schooling decisions. Further, government decisions are made by a host of agents, including politicians, courts, and voters, and we model governments as acting as a single agent. Finally, we assume myopic decision making at the governmental level. Nonetheless, our estimates meaningfully show whether state governments prioritize school spending more when spending is low, (captured as concave utility of state and local “governments”), whether state and local governments respond to changes in the economic environment slowly, (“habit formation”), and whether school districts with fewer local resources get more state aid (“state government utility from equalization”).====The first-order conditions from the model deliver three equations that we take to panel data and obtain highly statistically significant parameter estimates. These results are important for illustrating the strengths and weaknesses of state finance systems for addressing disparities between student cohorts over time. The pooled regressions show average patterns for the U.S. states. Therefore we also examine a specific set of school finance reforms to illustrate that our model specification captures changes in state objectives through the specified and estimated preference parameters.====To further illustrate the risk sharing attributes of the school finance systems, we simulate the model using the empirical parameter estimates, assuming exogenous income shocks. The steady state distributions are similar to the empirical distributions, which indicates that estimation bias is not severe.==== We first simulate impulse response functions, which show how state aid and local revenue react to local and statewide income shocks while accounting for the interdependence of state aid and local revenue. Second, we highlight that there is significant variation in school spending between cohorts within the same school district. Third, we find the incidence of district-level income shocks on after-tax local income, school spending, and state aid (which, via state taxes, are transfers from other districts) in the short and long run. Fourth, we analyze recent changes in school finance systems and find that efforts to equalize contemporaneous school spending may exacerbate intertemporal disparities.====In order to provide intuition for our results, consider two examples which illustrate what we try to capture with our model. Youngstown, Ohio, had substantially lower income growth over 1995–1998 than Ohio as a whole (7.1 percent versus 11.0 percent). As a result, local revenue declined by 5.0 percent (of initial school spending in 1995), which was more than compensated for by state aid increasing by 8.6 percent. State aid is ultimately financed by taxes on all districts in Ohio, so other school districts in the state shared the idiosyncratic income shock to Youngstown. In the case of statewide shocks, school districts cannot share the aggregate risk. Consider the South San Antonio Independent School District in Bexar County, Texas. State income per capita fell by 1.1 percent in 2013, though income barely changed in Bexar County and, as a result, state aid per student fell by close to 1.6 percent of 2012 school spending. Local revenue increased by 0.3 percent (of initial school spending) to partly offset this loss of aid, but it was not sufficient to prevent a substantial drop in school spending even though local income changed little.====Dupor and Mehkari (2015) develop a model in which school districts behave as optimizing consumers. They focus on school districts and treat revenue as exogenous, while we model school spending and the interactions between school districts and state governments as endogenous. Our paper also relates to the work of Fernández and Rogerson (1996) and Fernández and Rogerson (1998) in that it examines the distribution of school spending across the income distribution. These authors consider the long-run dynamic effects of schooling on migration and the future income of students—issues we do not consider.====The paper proceeds as follows. Section 2 describes our data. Section 3 develops the model and the resulting estimating equations for total state aid, for its distribution across districts, and for local revenue. Section 4 reports our empirical estimates, while Section 5 shows the steady-state allocations, impulse response functions, and dynamic incidence of income shocks. Section 6 performs the analysis splitting the sample into the periods before and after school finance reforms. Section 7 concludes.",Inequality in public school spending across space and time,https://www.sciencedirect.com/science/article/pii/S1094202521000673,4 October 2021,2021,Research Article,65.0
"Ray Debraj,Mookherjee Dilip","Department of Economics, New York University, 19 West 4th Street, New York NY 10012, United States of America,Department of Economics, University of Warwick, Coventry CV4 7AL, United Kingdom,Department of Economics, Boston University, 270 Bay State Road, Boston MA 02215, United States of America","Received 14 May 2021, Revised 15 September 2021, Available online 28 September 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.09.003,Cited by (2),"We study the long run implications of workplace automation induced by capital accumulation. We describe a minimal set of sufficient conditions for sustained growth, along with a declining labor share of income in the long run: (i) a basic asymmetry between physical and human capital; (ii) the ==== in favor of automation.","This paper describes a theory of automation and its implications for growth and labor income share in the long run. Our framework is considerably more general than existing models, and generates a number of distinctive predictions. The model features a countable infinity of final goods, and three intermediate goods: machine capital, robots and education (or more accurately, services produced by each of these intermediate goods). In each of these sectors, production takes place by combining machine capital with tasks performed by a combination of human labor and robot services. We impose minimal restrictions on the technology, except that capital and tasks are both essential inputs, and that it is technologically feasible — though not necessarily economically viable — for humans to be dispensed with in every task.==== The model permits human capital accumulation that allows workers to shift occupations in response to the threat of automation.====We show that under some conditions, sustained declines in labor share can be a consequence of progressive capital-deepening resulting from capital accumulation ==== (rather than technical progress or rising markups). This result, in line with the evidence in Karabarbounis and Neiman (2014), can happen despite arbitrarily inelastic capital-labor substitution in individual sectors. At the same time, the environment that drives the decline in labor share is also closely connected with sustained economic growth per capita, so absolute wages can grow unboundedly at the same time that the labor ==== converges to zero. We describe the conditions, and explain how the limiting share of labor can be positive when any one of them is violated.====The baseline model deliberately abstracts from technical progress, though we later incorporate endogenously directed technical change. It exhibits the following phenomena:====Features (a) and (b) are the implications of a fundamental asymmetry between physical and human capital. While individual claims to physical capital in any sector can be replicated and scaled indefinitely, the same is not true for ownership of labor. Humans cannot be bought and sold the same way machines are. Instead, human capital accumulation takes the form of acquiring embodied skills for a specific task or occupation, a capacity always contained in ==== physical self. So workers can invest in human capital to an unbounded degree, but are subject to diminishing returns in the acquisition of skill within any one task, and to the possibility that new skills may be needed to switch occupations or tasks. Consequently, the returns to human capital acquisition are determined endogenously, and the pattern of household demand across goods produced by different sectors plays a central role in this determination.====In this setting, we first provide conditions for positive long run growth in per capita income: a ==== in the technology for producing “robots” or digital services, and a minimal threshold for ==== or intergenerational altruism in preferences. The self-replication property, which we discuss in detail below, implies that the production of robots will be fully automated if the capital rental price falls sufficiently relative to the unit cost of tasks. It isn't a universal condition, but does holds automatically in familiar settings such as Cobb-Douglas production. Self-replication bounds the price of robots relative to capital rentals, and we show that this condition, along with patience, generates long run growth as the economy is progressively freed from the constraint of a given endowment of human labor.====But endogenous growth is not the only implication. Because robot prices are bounded by self-replication, they must decline relative to human wages in a growing economy, for that growth must arise precisely from the accumulation of physical capital relative to human labor, inclusive of human capital accumulation. Therefore, if automation is ==== in any sector, it will also be a long run ====, as long as that sector expands with the economy. By itself, this does not erode labor share, as humans could move from sector to sector. If, however, preferences are ====, then we prove that this intersectoral movement cannot prevent a sustained decline in the labor share. Making transparent these conditions for the declining labor share is a central goal of our paper.====The share decline must perforce be gradual if there exist occupations where humans are sufficiently productive relative to robots: for then there are always sectors that are yet to be automated. But labor movement across sectors only attenuates the decline, without negating it. If preferences are asymptotically homothetic, there is just not enough demand to sustain the persistent scaling of human capital needed to ward off the decline. Section 3.8 argues that other conditions on preferences also deliver a similar result. We emphasize “preference neutrality” in particular: a condition stating that preferences do not particularly favor human-friendly sectors — nor do they necessarily disfavor them.====Thus automation is a double-edged sword. It is an engine of income growth. And yet that same engine causes the labor income share to asymptotically vanish. This dichotomy explains why a vanishing labor ==== can co-exist with sustained growth in ==== human wages. Proposition 2 formalizes this intuition by providing conditions for some human wages to grow without bound: the existence of essential sectors and occupations in which humans have sufficiently high marginal productivity relative to robots even as they are close to full displacement. If additionally the costs of occupation-switching are bounded, all human wages grow without bound. Automation can raise all boats — only not all at the same rate, with wage incomes growing slower than capital incomes.====Sections 3.5–3.8 explain why each of these conditions is required for the results, by dropping them one by one. This helps identify various pathways for the share of labor to remain positive in the long run: (a) a failure of the self-replication condition in the robot sector, (b) the impossibility of fully automating some sectors, (c) the possibility of unbounded human capital acquisition within occupations, and (d) the failure of asymptotic homotheticity in preferences.====The analysis so far abstracts entirely from technical progress, relying entirely on changes in prices of capital goods relative to human wages. Certainly, the labor share could remain positive if technical progress is biased in favor of humans. For instance, Acemoglu and Restrepo (2018) restrict technical progress to enlarge only the productive capacities of human labor. The asymmetry is then built in by assumption, rather than endogenously explained — giving rise to the question of the kind of bias that might emerge when the opportunities for technical progress are more evenly distributed between human labor, robots and capital services. To explore this question, Section 4 extends our model to permit directed technical progress in machine, human and robot productivities, but we explicitly assume no technological bias, either in favor of humans or against them. Such a model could also be reinterpreted (with a hedonic reinterpretation of the commodity space) as one in which new goods are created. In such a symmetric setting, with technical progress equally sensitive to the prices of machines, robots and humans, we show that our long run distributional implications continue to be robust. Progressive capital deepening ensures that in equilibrium the derived demand for innovations in capital productivity cannot be surpassed by those for innovations in human productivity. Hence technical progress cannot be biased in favor of humans.====While our motivation is primarily conceptual and intended to guide largely speculative thoughts about the future, our theory provides a potential explanation for the recent decline in labor shares documented by Karabarbounis and Neiman (2014). As Section 5 clarifies, such a theory can be distinguished from explanations based on capital-augmenting technical progress, human capital accumulation, rising markups and market concentration or declining bargaining power of labor unions. Its relevance lies in the evidence provided by Karabarbounis and Neiman (2014), that a substantial fraction of the decline in labor share worldwide is explained by falling capital prices, ====. Their theoretical explanation for this result is based on elastic capital-labor substitution, an assumption which runs contrary to evidence provided in industry panel studies; see, e.g., Chirinko and Mallick (2014). Our model shows that a declining labor share can result from capital deepening even in the presence of inelastic capital-labor substitution in most sectors.====It is important to clarify that we do not address the question of inequality in the ==== distribution of incomes. Nor do we argue that a growing functional divergence between capital and labor incomes ==== imply growing inequality in personal incomes. These issues require analyses of the composition of household investment between financial wealth and human capital, and in inequality of labor incomes. Suitable applications and extensions of our model are needed to study these questions, as elaborated in the concluding section.====Section 2 presents the baseline model. The main results are in Section 3, with related lines of discussion. Section 4 studies technical progress. Section 5 discusses the connections to existing literature in detail, while Section 6 concludes. Proofs are collected in Appendix A.","Growth, automation, and the long-run share of labor",https://www.sciencedirect.com/science/article/pii/S1094202521000661,28 September 2021,2021,Research Article,66.0
Wang Fan,"Department of Economics, University of Houston, United States of America","Received 25 October 2019, Revised 31 August 2021, Available online 17 September 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.09.001,Cited by (2),I develop and estimate a dynamic equilibrium model of risky entrepreneurs' borrowing and savings decisions incorporating both formal and local-informal credit markets. Households have access to an exogenous formal credit market and to an informal credit market in which the ,"In recent decades, development banks and micro-finance institutions in developing countries have funded the expansion of formal financial institutions into rural areas. Recent randomized evaluations show that the effects of expanding formal borrowing and savings opportunities are generally positive but limited (Banerjee et al., 2015). One factor that might impact the effectiveness of formal credit market expansions is the availability of local informal alternatives.====In this paper, I structurally evaluate the effects of microfinance interventions in the presence of informal credit markets. Microfinance interventions introduce formal options to households' financial choice set. But microfinance might also impact existing informal choices through equilibrium effects as demand and supply in those informal credit markets shift in response to interventions. The joint effects of microfinance could lead to different welfare consequences for heterogeneous households depending on their borrowing and savings needs. By combining the formal and informal options, my analysis spans prior structural analyses of microfinance. Kaboski and Townsend (2011) conduct a structural evaluation of microfinance at the micro level without distinguishing between formal and informal choices, while Buera et al. (2021) analyze the macro-equilibrium effects of microfinance without considering the response of local informal credit markets.====To accomplish this, I develop and estimate a dynamic equilibrium risky entrepreneur model that incorporates formal as well as informal borrowing and savings choices. In the model, households are infinitely lived, risk averse, and have varying productivity and wealth. Households choose risky capital investments and savings, and they can finance their risky investments through borrowing. The formal credit market is characterized by exogenously determined differential borrowing and savings interest rates, and formal credit is subject to a potential collateral constraint. In contrast, in the informal market considered here, full enforcement is assumed, and market clears given a locally determined borrowing and lending interest rate. The model has several key features of formal-informal credit market interactions: First, formal and informal credit market options differ in interest rates, collateral requirements, and access/transaction costs; second, formal and informal options could be substitutes or complements as households sort into seven credit market participation categories====; third, informal credit markets are localized, and exogenous changes in formal credit market conditions could shift equilibrium local informal interest rates.====Given these features, microfinance availability can have redistributive consequences through equilibrium effects on locally determined informal interest rates. The existing structural microfinance evaluation literature generally models improvements in microfinance as a shift in the borrowing collateral constraints (Buera et al., 2015). In the context of this model, subsidizing the centrally set formal borrowing and savings interest rates, reducing access transaction costs, and relaxing borrowing collateral constraints can separately and jointly improve microfinance availability. The effects of relaxing the collateral constraints could be magnified or dampened by variations in formal borrowing and savings fixed costs and interest rates.====I estimate the model using panel data from the Townsend Thai Monthly Survey, which contains extensive information about the sizes and rates of formal and informal credit market transactions for about 650 households from 16 villages between 1999 and 2009. During this period, in 2001, the Thai government, under Prime Minister Thaksin Shinawatra, implemented a wave of large-scale policies, including the Million Baht Village Fund program, aimed at broadening rural households' access to the formal credit market.==== Specifically, the policies reduced interest rates by offering a low fixed borrowing rate, reduced fixed costs by directly administering formal loans with centrally set rates via village committees, and potentially helped relax collateral constraints by increasing the number of formal loans available. As a result of these policies, the proportion of households that used formal borrowing increased by up to 32 percentage points, and the formal borrowing interest rates decreased by up to 8 percentage points. The local informal interest rate also dropped by up to 14 percentage points. I introduce the Thaksin policy shift in my model as possible changes in centrally set formal interest rates, in formal fixed costs, and in formal borrowing collateral constraints.====The model is estimated using simulated maximum likelihood. I develop solution and estimation algorithms that capture the differential effects of variations in formal and informal access costs and constraints on asset choices and distributions. The estimation allows for the identification of unobserved fixed costs and collateral constraints, and household preferences and productivities. I allow the parameters that characterize the credit market to vary over time and estimate their changes based on changes in credit choice category participation probabilities. I find that the fixed costs to access the formal credit market dropped from about 9% of average annual income to about 4.6%, and the collateral constraints were also significantly relaxed, more than doubling how much each household could borrow formally given the same amount of physical capital. Along with the lowering of the centrally set formal borrowing interest rates, these changes resulted in significant improvements in households' access to formal borrowing.====In terms of welfare comparisons, the estimated model suggests that there were winners and losers. The majority of rural households are better off under Thaksin's policies, although the estimated steady state gains are close to zero in consumption-equivalent variation for most households.==== Steady state welfare effects are heterogeneous across households. High productivity households could gain up to about 5%. These households were previously constrained by the collateral constraints or by the fixed costs from investing more in their household farms or businesses. However, low productivity households could lose up to 1% in consumption-equivalent variation. These households have relatively higher preferences for safe savings over taking out loans to finance their low expected return risky investments. Their gains from lower borrowing costs are outweighed by diminished savings opportunities due partly to the drop in locally determined equilibrium informal interest rates.====I conduct counterfactual experiments to decompose the relative contributions of each of the three formal borrowing access dimensions in explaining the effects of the Thaksin policies. The redistributive welfare effects are largely driven by the relaxation of the collateral constraint, which by itself accounts for 47% of the reduction in the informal interest rate. Reductions in formal borrowing fixed costs had a large impact on participation shares, and by themselves these reductions account for 60% of the increase in the fraction of households using formal credit options. Interestingly, the reduction in the centrally set formal borrowing interest rates had limited effects on aggregate participation and the informal market interest rates. The three measures of access have different impacts because they shift the average costs of formal loans differently for different types of households.==== The analysis in this paper contributes to several strands of the literature. First, there has been substantial research on the impacts of greater financial access on developing economies (Greenwood and Jovanovic, 1990; Lloyd-Ellis and Bernhardt, 2000; Gine and Townsend, 2004; Kaboski and Townsend, 2011; Dabla-Norris et al., 2020; Buera et al., 2021). Despite the importance of informal financial arrangements (Udry, 1994; Townsend, 1994), these dynamic models of financial deepening—formal credit market expansion—generally do not explicitly consider informal financial options. Additionally, studies that test the fit of informal risk-sharing models to data do not model formal options explicitly (Alem and Townsend, 2014; Karaivanov and Townsend, 2014; Kinnan, 2021). In this paper, I model risky entrepreneurs' choices over formal and informal credit market options in an exogenous incomplete markets setting. While there are different ways for rural households to transfer financial resources, Karaivanov and Townsend (2014) find that a model with exogenously incomplete borrowing and savings options fit consumption and investment data in rural Thai villages better than constrained efficient credit/insurance models. The model in effect augments equilibrium models of risky entrepreneurs (see review: Quadrini (2009)) with additional exogenous borrowing and savings options. Similar to Kaboski and Townsend (2011), I treat villages as small open economies where formal prices are exogenously determined, but I extend the framework to explicitly consider informal choices and equilibrium interest rates determined within a local informal credit market. My approach here focuses on the ==== effects of formal credit market expansion on village credit markets. This complements the work of Buera et al. (2021); Breza and Kinnan (2021), which focuses on the macro equilibrium effects of large microfinance roll-outs on prices, including interest rates and wages, on the aggregate economy.====Second, this paper contributes to work that studies the interaction between formal and informal credit markets. Due to the expansion of development banking, there has been significant interest in the interaction between formal and informal credit markets (Hoff and Stiglitz, 1990; Besley, 1995). Studies have analyzed the impact of differential access costs and contract enforcement on the sorting between formal and informal borrowing options (Gine, 2011; Karaivanov and Kessler, 2018); the interaction between suppliers of informal credits and formal banks (Floro et al., 1997; Madestam, 2014); and the effects of formal credit expansion on informal credit market interest rates (Mookherjee and Motta, 2016; Demont, 2016). To pin down the analytical structures, these papers generally rely on non-dynamic models and generate sorting conditional on wealth and types. This paper incorporates some core features of formal and informal credit markets interaction into an empirical dynamic equilibrium framework with endogenous asset distributions. Credit market participation now includes borrowing and savings in both markets, and the supply side of informal credit is endogenized through the dynamics of savings.====Third, there is a significant and growing empirical literature that analyzes separate dimensions of credit market policies. Studies have found that formal borrowing and savings choices are elastic to subsidies or shifts in the interest rates of borrowing (Karlan and Zinman, 2008; Dehejia et al., 2012) and saving (Schaner, 2018). The expansion of formal borrowing or savings opportunities, which partly reduces the fixed/transaction costs of access, have had modest but heterogeneous effects on households in previously under-exposed areas (Banerjee et al., 2015; Dupas et al., 2018). Offering loans to borrowers who would otherwise be ineligible due to a possible lack of collaterals (Banerjee and Duflo, 2014; Augsburg et al., 2015) has also had mixed results. To allow for the coexistence of various formal and informal credit market participation categories, I incorporate into the model formal borrowing and savings interest rates, fixed costs, and collateral constraints. As a result, this paper provides a potential framework for studying the non-separable effects of these important dimensions of formal credit market policies.====Fourth, there is a literature that studies how the provision of formal insurance could crowd-out informal insurance (Attanasio and Rios-Rull, 2000; Krueger and Perri, 2011; Chandrasekhar et al., 2011). These papers find that more formal insurance provisions might worsen informal conditions and lead to welfare losses. In this paper, I study formal and informal market interactions in the context of exogenously incomplete credit markets. The welfare gains and losses in this paper arise out of a competitive local-informal credit market's interaction with an exogenous formal credit market with centrally set rates. This contrasts with the insurance literature, where welfare losses arise in the context of an endogenously incomplete market structure that is constrained by limited commitment and other frictions.====The structure of the paper is as follows. Section 2 develops the model. In Section 3, I describe the data and background. Section 4 describes the estimation results and counterfactuals. I offer the conclusion in Section 5. Additional details for the solution and estimation methods are in Appendix Sections C.",An empirical equilibrium model of formal and informal credit markets in developing countries,https://www.sciencedirect.com/science/article/pii/S1094202521000648,17 September 2021,2021,Research Article,67.0
"Moser Christian,Yared Pierre","Columbia University, United States of America,Federal Reserve Bank of Minneapolis, United States of America,CEPR, United Kingdom of Great Britain and Northern Ireland,NBER, United States of America","Received 16 October 2020, Revised 30 July 2021, Available online 8 September 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.08.001,Cited by (11),"This paper studies lockdown policy in a dynamic economy without government commitment. Lockdown imposes a cap on labor supply, which improves health prospects at the cost of economic output and consumption. A government would like to commit to the extent of future lockdowns in order to guarantee an economic outlook that supports efficient levels of investment into ====. However, such a commitment is not credible, since investments are sunk at the time when the government chooses a lockdown. As a result, lockdown under lack of commitment deviates from the optimal policy. Rules that limit a government's lockdown discretion can improve social welfare, even in the presence of noncontractible information. Quantitatively, lack of commitment causes lockdown to be significantly more severe than is socially optimal. The output and consumption loss due to lack of commitment is greater for higher intermediate input shares, higher discount rates, higher values of life, higher disease transmission rates at and outside of work, and longer vaccine arrival times.","The COVID-19 pandemic brought about a great rise in both epidemiological and policy uncertainty.==== In response to the pandemic, governments across the world implemented lockdown policies to limit the spread of infections. In numerous cases, these policies were first scheduled to end in the near future and then were extended. For instance, New York Governor Andrew Cuomo imposed a statewide stay-at-home order on March 22, 2020, with an initial end date of April 19. This lockdown was later extended, first until April 29 and then until May 15. While several restrictions were further extended on May 15, Cuomo also presented a clear contingency plan with criteria for lifting restrictions in the future.==== Elsewhere, the discretion to extend lockdowns was limited by decree. For example, on September 25, 2020, Florida Governor Ron DeSantis announced a lower limit of 50 percent on allowed restaurant capacity, regardless of local restrictions. The stated goal of this lower limit was to reduce future lockdown policy discretion by local governments.==== Similar lockdown extensions, rules for lifting them, and restrictions on future lockdowns were implemented by many other regional and national governments.====As is evident from these examples, lockdown policies create additional uncertainty over and above that posed by epidemiological factors. Such uncertainty affects businesses that need to make forward-looking investments subject to sunk costs. Common examples of sunk costs include airlines maintaining their fleet, hotels deciding how many employees to retain on payroll, and restaurants placing inventory orders ahead of reopening. Because these investments are forward looking, lockdown policies dynamically impact current economic activity through businesses' expectations of their government's plans for reopening.====To formalize these dynamics, in this paper, we study the role of government commitment in designing lockdown policy. We consider a dynamic economy that embeds sequential government policy decision-making into a general SIRD model of pandemics (Kermack and McKendrick, 1927; Ferguson et al., 2020). Each period, firms invest in intermediate inputs before the government chooses a lockdown policy and workers supply labor. A lockdown imposes an upper bound on labor supply, limiting disease spread at the cost of economic activity. Our framework is general and subsumes key mechanics of many macroeconomic SIRD models in the literature with lockdown or disease-mitigation policies.==== A key feature of our model is that investment in intermediate inputs is determined before a lockdown policy is chosen. We think of this as capturing the kinds of investments that businesses make in maintenance, employee retention, and inventory while anticipating the ensuing trajectory of lockdown policies during a pandemic. Through the forward-looking nature of investment, current economic activity depends on firms' expectations of future lockdown policy.====Lockdowns induce both health benefits as well as output and consumption costs. In our model, lockdown reduces contemporaneous disease spread during a pandemic, which evolves according to a modified SIRD model. At the same time, through two channels, output and consumption decrease with the intensity of the lockdown. First, they decrease statically, as labor supply is directly curbed by the lockdown. Second, they decrease dynamically through lower investment in anticipation of lower future marginal returns to investment resulting from future lockdown. Under government commitment, the optimal lockdown policy equates its marginal health benefits with the output and consumption costs.====Our main result concerns the effect of a government's lack of commitment on optimal lockdown policy. A government would like to commit to limit the extent of future lockdowns in order to support more optimistic firm expectations in the present. However, such a commitment is not credible, since investment decisions are sunk when the government decides on future lockdowns. Faced with a sunk investment, a government without commitment wants to impose a more stringent lockdown relative to the optimal policy under commitment, because it does not fully internalize the associated reduction in returns to investment in intermediate inputs. Firms rationally foresee the government's lack of commitment, causing them to invest less than they would in anticipation of the policy under commitment. Through this mechanism, lack of commitment distorts the efficient levels of investment and therefore output and consumption associated with lockdown policy.====In light of this time inconsistency problem, we study how a government can improve the efficiency of lockdown policy by committing ex-ante to a contingent plan that depends on the evolving health state. We show that an ex-ante rule that imposes state-contingent limits on future lockdown severity can attain the efficient allocation.====We extend the model to a setting in which additional information arrives during a lockdown. Examples of such information include estimates of disease mortality, the state of the economy, the medical system's capacity, or progress on vaccine development. Some of this information may be relevant for the payoffs and costs of lockdown policy. If this information is a contractible part of the state space, we show that it continues to be the case that an ex-ante rule that imposes state-contingent limits on future lockdown severity can attain the efficient allocation. Moreover, even if this information is not contractible—so that policy flexibility is valuable—rules that limit lockdown severity increase social welfare. This is because it is always socially beneficial on the margin to prevent excessive future lockdowns as a means of raising investment in the present.====These results provide a theoretical justification for the social benefits of mandated limits on future lockdowns, such as those implemented by some state governments in the United States. It is important to note that our analysis does not imply that lockdowns are harmful. In fact, reducing or lifting the lockdown is detrimental if the associated health costs exceed the economic gains. However, committing to limiting future lockdowns is beneficial if the economic gains from stimulating investment toward its efficient level exceed the health costs.====In a quantitative exercise, we use a calibrated version of our model to show that lack of commitment leads to an overly severe lockdown, with significant output and consumption losses compared with those of the policy under commitment. We show that the output and consumption losses are greater for higher discount rates, higher values of life, higher disease transmission rates, higher intermediate input shares, and longer vaccine arrival times. Our findings suggest that optimal policy commitments to limit lockdown would result in a significant reduction of output and consumption losses during a pandemic.====  This paper relates to the nascent literature on optimal policy in a pandemic and, in particular, to the work of Alvarez et al. (2020), Atkeson, 2020a, Atkeson, 2020b, Berger et al. (2020), Chari et al. (2020), and Eichenbaum et al., 2020a, Eichenbaum et al., 2020b. This literature focuses on the optimal design of government policy, including the timing and intensity of lockdowns, under the assumption that the optimal policy can be enforced at all dates and under all contingencies. Our work highlights that such analyses omit an important aspect of lockdown design—namely, that the optimal policy may be hard to enforce because of issues of time inconsistency. What distinguishes our approach is the focus on the value of government commitment to lockdown policy and the optimal design of rules that limit government discretion.====That prior work on policy responses to a pandemic has ignored issues of time inconsistency is perhaps surprising, given the parallel insights from an older literature that studies government commitment in the context of capital taxation. This body of work includes the important contributions by Kydland and Prescott (1980), Chari and Kehoe (1990), Klein et al. (2008), Aguiar et al. (2009), and Chari et al. (2019). As it does in the previous work on capital taxation, in our model lack of commitment reduces economic activity by distorting investment. Relative to this literature, our work incorporates two new insights that are central to the context of pandemics. First, a lockdown distorts investment not directly via capital taxation but indirectly by lowering the marginal returns to investment through a cap on labor supply. Since lockdown distorts labor, in a way similar to how a labor income tax does, our work more broadly highlights the existence of a time consistency problem that would arise in a model of labor taxation with endogenous labor supply and capital: A government distorting labor ex-post does not internalize the ex-ante effect on decisions by investors. A second difference relative to the capital taxation literature is that investment distortions from lockdown serve not to relax the government budget constraint but instead to improve the future health state. Since this health state is not static but evolves according to an SIRD model, the tradeoff faced by the government is not static but dynamic, and the time inconsistency problem evolves over time.====Our analysis of rules for lockdown policy in the presence of noncontractible information relates to a growing literature on commitment versus flexibility in macroeconomics (Athey et al., 2005; Amador et al., 2006; Halac and Yared, 2014, Halac and Yared, 2018; Moser and Olea de Souza e Silva, 2019). Prior work in this area has focused on rules for either savings or monetary and fiscal policy. Our work adds to this literature and to a growing number of papers on the economics of pandemics—specifically, the theoretical analysis of optimal lockdown policy. Our result that rules can strictly increase social welfare, even if flexibility is valuable, is reminiscent of similar insights in the context of savings or fiscal and monetary policy. However, our results do not directly follow from the methods developed in prior work, which rely on stronger assumptions on the utility function and the information structure than the ones we require in our setting. By extending these insights and applying them to optimal lockdown design, we highlight an overlooked aspect of the debate around lockdown policy during pandemics.",Pandemic lockdown: The role of government commitment,https://www.sciencedirect.com/science/article/pii/S1094202521000594,8 September 2021,2021,Research Article,68.0
Shakhnov Kirill,"University of Surrey, United Kingdom of Great Britain and Northern Ireland","Received 30 January 2018, Revised 12 July 2021, Available online 6 September 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.08.002,Cited by (0)," have been focusing policy debate as to the function of the financial sector and on its social desirability as a whole. I propose a heterogeneous agent model with asymmetric information and matching frictions that produces a tradeoff between ==== and entrepreneurship. By becoming bankers, talented agents efficiently match investors with entrepreneurs, but extract excessive informational rents due to contract incompleteness. Thus the financial sector is inefficiently large in equilibrium, and this inefficiency increases with wealth ====. The estimated model with time variation in the banker capacity accounts for the simultaneous growth of wealth inequality and the financial sector in the US. The endogenous feedback between inequality and the size of the financial sector is quantitatively important.","The growth of the financial sector is well known and well documented. Fig. 1 shows that the financial sector's share as a percentage of GDP as well as of employment has increased substantially since the Second World War. The figure shows that finance accounts for a higher share of GDP than of employment before the Second World War and after the 1980s (Philippon and Reshef, 2012). Interestingly, while the share of finance in employment has stabilized since the 1980s, its share of GDP has continued to rise. At the same time, this rise has been accompanied by substantial, and well documented, changes in wealth distribution.====This paper explains the growth of the financial sector by linking it to the dynamics of wealth distribution through an occupational choice. While the literature on occupational choices and long-run wealth distribution is well established (Banerjee and Newman (1993); Galor and Zeira (1993), and many others), few focus on finance-related occupations despite their natural association with wealth. Gennaioli et al. (2014) partially attributes the growth of finance to the increase of wealth to income ratio. Their idea is that the more wealth there is, the more assets there are to intermediate. Quantitatively, however, growth of the wealth to income ratio alone explains only a small fraction of the increase in the size of finance. I focus not on aggregate capital accumulation but rather on increasing wealth inequality and show that the growth of wealth inequality significantly contributes to the growth of finance. This echoes Piketty and Zucman (2014)'s argument that one of the reasons for increased inequality is the fact that financial services associated with asset management generate superior returns and disproportionately affect the wealthy. According to Greenwood and Scharfstein (2013), much of the growth of the financial sector comes from asset management, which is mostly a service for wealthy individuals. Furthermore, over a cross-section of countries, there is a positive relationship between inequality and the size of the financial sector.====I build a model in which financial intermediation potentially enhances welfare but draws some talented individuals away from production. The model includes three key elements: (a) heterogeneous agents who differ in terms of capital and talent; (b) an occupational choice between being a banker or an entrepreneur; (c) financial frictions. Heterogeneity and occupational choice provide a framework to study the allocation of capital (wealth) and talent. Talent is essential for both industry and the financial sector: more talent in industry means that more output is produced per unit of capital, while more talent in finance means that capital is potentially allocated more efficiently. Financial frictions in the form of private information result in the misallocation of capital as investors cannot distinguish between talented and ordinary entrepreneurs. Since it is the role of talented bankers to be able to make this distinction, the financial sector should serve to correct this misallocation.====The model generates two important insights into the financial sector. First, the model provides a novel explanation for the growth of finance by linking it to an increase in ====. Talented bankers provide an investment opportunity with superior returns because of their informational advantage. Only wealthy individuals can afford to pay for the services of talented bankers. In the dynamic framework, this effect is self-reinforcing: small initial differences in wealth among investors cause substantial income inequality among entrepreneurs, which is translated into greater wealth inequality during the next period. Wealthy investors are willing to pay a higher premium for financial services that increase the return on their savings, and so the greater the dispersion of wealth, the higher the price of financial services. This higher price induces a larger fraction of talented agents to pursue careers in finance. Hence, finance, wealth inequality, and inefficiency grow simultaneously.====Second, I show that decentralized equilibrium exhibits a misallocation of talent: the financial sector absorbs talent beyond a socially desirable level. Bankers create net surplus through intermediated matches and extract a part of the surplus. The size of the net surplus is proportional to the degree of wealth and talent inequality. This surplus should be split between three parties: an investor, who provides capital, an entrepreneur, who provides an idea and a banker, who efficiently matches the idea and capital. Due to matching and bargaining, there is no ex-ante reason why the split should make the income of a banker and an entrepreneur equivalent when the number of bankers is efficiently constrained. Bankers extract excessive informational rent from investors Even though the equilibrium is generically ====, efficiency can be restored by taxing the financial sector.====This second insight helps to reconcile the two sides of a debate as to whether this expansion is socially desirable. On the one hand, the former chairman of the Federal Reserve, Alan Greenspan (2002) stated: “[M]any forms and layers of financial intermediation will be required if we are to capture the full benefit of our advances in technology and trade.” This idea is related to a vast literature arguing that financial development causes economic growth, because the financial sector corrects capital misallocation and consequently enhances productivity by relaxing financial constraints. On the other hand, critics of the financial sector suggest that it might have negative implications for the allocation of talent. Another former chairman of the Federal Reserve, Paul Volcker (2010) clearly stated: “[I]f the financial sector in the United States is so important that it generates 40% of all the profits in the country…What about the effect of incentives on all our best young talent, particularly of a numerical kind, in the United States?”====Many papers provide indirect empirical evidence on the misallocation of talent. Data from college graduates in the US suggests that the financial sector has become one of the most popular destinations for graduates of elite universities with high levels of raw academic talent, regardless of their major (see Goldin and Katz (2008) for Harvard graduates and Shu (2015) for MIT graduates). In addition, Kneer (2013) finds that US banking deregulation reduces labor productivity disproportionately in relatively skill-intensive industries.====This paper is related to a vast literature on misallocation, particularly to papers attributing the misallocation of capital to the financial industry. (Buera and Shin, 2013; Midrigan and Xu, 2014). Whereas most studies focus on the impact of frictions on output and the allocation of capital and abstract away its impact on the labor market (Jovanovic and Szentes (2013) is one of the exceptions), this paper argues that financial development has a significant impact on the allocation of both capital ==== talent, which cannot be neglected. This argument is in line with a recently growing body of theoretical literature, which studies occupational choice and rent-seeking within the financial industry (Philippon, 2010; Bolton et al., 2016). Underlying this concern is the view that finance is a largely rent-seeking industry and that the resources it attracts could be better employed elsewhere. On the empirical side, Cochrane (2013); Greenwood and Scharfstein (2013); Philippon (2015); Kurlat (2019); Bazot (2017) evaluate this argument.====Many studies analyze the causes of expansion in the financial sector. Several explanations have been suggested: fluctuation of trust in financial intermediaries (Gennaioli et al., 2014), increasing efficiency of the production sector (Bauer and Rodriguez Mora, 2014), and structural changes in finance itself Cooley et al. (2020). None of them provide a causal link from the increase in wealth inequality to the expansion of the financial sector. On the contrary, Greenwood and Jovanovic (1990) theoretically show that financial development might cause a reduction in inequality. Cooley et al. (2020) develop a model of human capital accumulation and increasing competition for talent in the financial sector that generates more risk-taking, greater income inequality, and higher aggregate income.====There is a vast literature that studies the relationship between capital accumulation and financial development (Levine (2005) surveys the literature). Apart from the current paper, few other papers have analyzed the efficient size of the financial sector in the context of ====. The financial sector is inefficient in all the literature discussed below, but authors find the sources of inefficiency in very different places.====Murphy et al. (1991) argue that the flow of talented individuals into law and finance might not be entirely desirable, because law and finance provide high private but low social returns (are pure rent-seeking activities). However, they give no reason for disparities between social and private returns. The study of Philippon (2010) acknowledges the meaningful role of the financial sector, as a monitoring device that helps to overcome the opportunistic behavior of entrepreneurs in the class of occupational choice models. The source of inefficiency in the model is the production externality in the forms of human capital within the industrial sector. The projects developed by entrepreneurs have higher social than private benefits. Therefore, they need to be subsidized relative to all other occupations: workers and bankers. Bolton et al. (2016) focus on financial innovations in the sense that the financial sector creates a new over-the-counter (OTC) market. Similar to my paper, informed dealers in the OTC market extract excessive informational rent and, consequently, the financial sector attracts too many individuals. However, contrary to my model, the size of the informational rent is independent of wealth inequality.====The calibrated model provides a good qualitative replication of the US data: the increase in inequality, and the growth of the financial sector as a share of both employment and value-added. While the number of talented agents limits the size of the financial sector in terms of employment, the size of the financial sector in terms of value added grows with the surplus of all matches, which each banker intermediates. An exogenous increase in the banker capacity, which can be viewed as a driver of the financial sector's productivity, is necessary to explain a large increase in the relative productivity of the financial sector. The increase in the banker capacity generates the rise not only in the size of finance, which has more than doubled since World War II but also in inequality. The endogenous feedback from inequity to the financial sector is quantitatively important, and it accounts for 20% of the variability in the size of finance.====The paper is structured as follows. Section 1 describes the static version of the model and policy results. Section 2 provides a dynamic extension of the model and a quantitative analysis. The last section concludes.",The allocation of talent: Finance versus entrepreneurship,https://www.sciencedirect.com/science/article/pii/S1094202521000612,6 September 2021,2021,Research Article,69.0
"Liu Siming,Shen Hewei","School of Finance, Shanghai University of Finance and Economics, China,Department of Economics, University of Oklahoma, United States of America","Received 3 October 2018, Revised 26 August 2021, Available online 6 September 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.08.005,Cited by (2),This paper studies the interaction between fiscal commitment and ==== risk in a model with ==== can improve debt sustainability by 53.3% and result in a significant welfare gain.,"The recent European debt crisis has ignited a debate on the effects and desirability of fiscal consolidations among countries with high sovereign default risks. Throughout these debates, it is often emphasized that governments need to be able to make commitments to their future fiscal plans. For example, OECD encourages governments to establish track records for fiscal consolidations to “further strengthen their credibility with citizens and international markets” (OECD, 2011). Similarly, in a press conference, Mario Draghi emphasized that “a key issue of fiscal consolidation is credibility” (Draghi and Constâncio, 2012).====Following the suggestion to establish credibility, many countries have adopted reforms to engage themselves in fiscal commitments. For example, Germany introduced a constitutional amendment in 2009, calling for a limit on structural deficit of no more than 0.35 percent of GDP for the federal government. Lithuania and Italy introduced similar balanced budget rules as constitutional amendments in 2012.==== The elaborate and difficult process required to change the constitutional amendment makes these balanced budget rules a natural credit device for making fiscal commitments, at least in the short run. In addition to introducing fiscal rules as domestic laws, some governments attempt to achieve fiscal credibility through their tough fiscal stances. For example, the Latvian government undertook a massive consolidation (of over 15% of GDP) in 2009, and such a drastic fiscal reform is interpreted as an attempt to strengthen the government's fiscal credibility.====A common argument for why governments should build credibility is that they can improve their fiscal outlooks through credible fiscal plans and thus achieve more favorable prices on their sovereign bonds.==== However, it remains unclear how sizable such a benefit is and how fiscal commitment affects the government's ability to repay its debt. In addition, many countries have included escape clauses in their fiscal rules to introduce flexibility to their committed fiscal plans.==== Nevertheless, there has been little discussion on why such flexibility is necessary and, if desirable, how much flexibility should be included in these fiscal plans when governments can make credible fiscal commitments. This paper addresses these questions by analyzing the interconnection between fiscal commitment and sovereign default risk in a quantitative model.====To investigate these questions, we build a sovereign default model with optimal taxation and government spending. The model economy is populated by three types of agents: domestic households, a government, and a continuum of foreign lenders. Households receive stochastic income and enjoy private and public consumption. A benevolent government levies income taxes and issues one-period non-contingent bonds to foreign lenders to finance the provision of public consumption. In the model, bond repayment is not enforceable, and the government may choose to default on its debt. If the government defaults, it is temporarily excluded from the financial market and suffers a one-time utility loss. Foreign lenders are risk-neutral and charge a risk premium on the bond to account for the default risk they face when lending to the government. As in Cuadra et al. (2010), the model predicts a procyclical tax policy====; that is, the government levies higher tax rates during economic downturns when the default risk makes it difficult to issue bonds in the international financial market.====A time-inconsistency problem, as in Gonçalves and Guimaraes (2015), arises in this environment as the government cannot commit to its future fiscal policies. During economic downturns, the government can choose to either levy a high tax rate to repay its debt or default on its debt and avoid levying an overly high tax rate. Setting a higher tax rate provides a stronger incentive for the government to repay. However, a discretionary government makes its default-taxation decision solely based on its debt balance and income realization and disregards the benefit of setting a higher tax rate on default risk. Consequently, the government is reluctant to make fiscal adjustments and defaults too frequently. In equilibrium, the excessive default incidence is priced by foreign creditors and limits the government's ability to borrow.====This time-inconsistency problem creates a role for the government to use commitment devices to alleviate default risk and improve welfare. In this paper, we use the model to assess the value of fiscal commitment that has been promoted extensively after the recent European debt crisis. Specifically, we consider two types of commitment measures: a non-contingent tax commitment and a contingent tax commitment.==== Under the non-contingent tax commitment, the government sets a single tax rate at the time the bond is issued. In the next period, the preset tax rate is implemented, regardless of the government's default decision and income realizations. In comparison, under the contingent tax commitment, the government commits to a full schedule of tax rates that is contingent on the next-period income. In that sense, the government is more flexible and can customize its tax policy to future economic conditions.====We find that a government with a non-contingent tax commitment has an incentive to commit to a tax rate higher than the one chosen by a discretionary government. The higher committed tax rate suppresses the government's incentive to default in the following period, thus reducing the risk premium that the current government has to pay when issuing bonds. However, since a government that can freely set the tax rate conducts a procyclical tax policy, the tax rigidity implied by the non-contingent commitment limits its ability to adjust the tax rate based on future income realizations. Such rigidity prevents the optimal allocation of resources between private and public consumption and leads to a welfare loss. In equilibrium, the ==== benefit of the tax commitment in reducing default risk may be partly or completely offset by its ==== cost when the commitment device is simple and inflexible. In the meantime, the non-contingency cost is reduced substantially if we allow the government to tailor a schedule of committed tax rates based on the future income states. Under a contingent commitment, we find that the government only commits to raising tax rates in high default risk states where commitment is valuable. By preserving the procyclicality of tax policy, this flexible commitment device is more effective in reducing default risk compared to the non-contingent policy, and thus enables the government to sustain a higher level of debt at a lower cost of borrowing.====We calibrate our model to the Greek economy and use it as a laboratory to evaluate the effects of the two types of tax commitments we study in this paper. We find that both types of commitment devices enable governments to suppress their default incentives. However, the tax rigidity associated with the non-contingent tax commitment makes it an ineffective device: the default probability decreases only marginally, and the level of debt that can be sustained in equilibrium shrinks. Therefore, the rigidity cost of the non-contingent tax commitment outweighs its benefit, and the policy causes a net welfare loss of 0.02% of permanent private consumption relative to the discretionary fiscal policy. In contrast, the contingent commitment allows the government to reap the benefits of its commitment power while minimizing the cost associated with tax rigidity. In our simulations, the contingent commitment effectively reduces the default risk and increases the debt level that the economy can sustain by 53% relative to the discretionary fiscal policy. On average, it delivers a net welfare gain of 0.13% of permanent private consumption.====Our findings rationalize the widespread policy efforts to establish fiscal credibility since the recent debt crisis==== and, more crucially, highlight the importance of incorporating flexibilities into fiscal commitment plans. The results indicate that a rigid fiscal plan, although relatively easy to implement and requiring smaller costs to enforce, may backfire and even worsen a government's debt sustainability. In contrast, a fully flexible commitment plan significantly improves a government's borrowing capacity and results in a sizable welfare gain. Such a comparison suggests that a superior fiscal plan should be flexible enough to accommodate changes in future economic conditions, and justifies the inclusion of escape clauses in fiscal rules.====  This paper builds on the literature that studies fiscal policy in the sovereign default model proposed by Eaton and Gersovitz (1981), Arellano (2008), Cuadra et al. (2010), and Arellano and Bai (2017). Cuadra et al. (2010) show that in a model with endogenous fiscal policy and sovereign default risk, debt is less useful to the government in smoothing consumption fluctuations, and the tax policy becomes optimally procyclical. Such a feature is also present in our baseline economy with a discretionary fiscal policy. Arellano and Bai (2017) study how a fiscal constraint makes default more likely: the default option helps free up resources available to the government to finance public consumption when tax rates cannot change. While their paper evaluates the effect of tax rigidity on sovereign default risk, our paper allows the government to choose (committed) tax rates optimally and analyzes the benefit of making a tax commitment. Fiscal rigidities are still present in our model, which may either come from the non-contingent taxation or from the one-period implementation lag.====The paper that is closest to ours is that of Gonçalves and Guimaraes (2015). They use a theoretical model to show that a time-inconsistency problem arises when the government chooses tax rates each period after the debt has been issued. In their model, a tougher fiscal policy reduces the amount of borrowing necessary to repay the maturing debt and makes the government less likely to default in the future. Since creditors always break even in expectations, the benefits of higher tax rates on bond prices accrue to the debtor. However, because the government fails to recognize this benefit and lacks incentives to adopt tough fiscal policies, the economy has too little fiscal adjustment and excessive default incidence.====Compared to Gonçalves and Guimaraes (2015), our paper is different in two ways. First, they make several simplifying assumptions in their model in order to obtain analytical characterizations of the time-inconsistency problem. In contrast, our baseline setup follows the quantitative sovereign default literature along the lines of Aguiar and Gopinath (2006), Arellano (2008), and Cuadra et al. (2010). Following these papers, we quantitatively evaluate the implications of adopting tax commitments on bond prices and welfare. Second, in addition to the acknowledgment of the time-inconsistency problem, our paper compares two types of commitment devices that are different in the degree of state-contingency. Our quantitative results highlight the advantage of having a flexible commitment measure that can accommodate the change in economic conditions. Therefore, our paper contributes to the recent debate on the design of fiscal rules during financial depression periods.====In a sovereign default model with downward nominal wage rigidity, Bianchi et al. (2019) show that the cyclicality of optimal government spending highly depends on the debt state of the economy and the associated sovereign default risk. They also study different forms of austerity programs and show that a “fiscal forward guidance” that constrains the future, rather than the current, spending can be beneficial. Both their paper and ours acknowledge the importance of introducing flexibility into fiscal rules. However, their paper focuses on the trade-off between the stimulus benefit of expansionary fiscal policies and the associated interest rate cost. In contrast, our paper investigates the benefit of enforcing tax commitment and the cost from a lack of fiscal contingency.====Arellano and Heathcote (2010) study how dollarization affects a country's ability to borrow in the international financial market. In their framework, the government faces two fiscal instruments: inflation tax (or seigniorage) and international debt issuance. Under specific shock structures in the economy, renouncing the inflation tax (dollarization regime) makes the debt instrument more valuable in smoothing consumption. Therefore, dollarization could relax the borrowing constraint and facilitate the county's financial integration into the global market. Similar to their paper, we also investigate how fiscal commitment affects the government's default incentives and, consequently, the ability to borrow. However, in their paper, the financial market features an endogenous borrowing limit enforced by foreign creditors, and default never materializes in equilibrium. In contrast, we allow the defaults to happen in equilibrium, so international creditors charge a premium on their lending. This feature makes our framework suitable to evaluate the recent policy recommendations of using credible fiscal plans to reduce the cost of borrowing.====In a model with debt maturity choice, Aguiar et al. (2019) show that if a government lacks both the commitment to repay and the commitment to the path of bond issuance, it maximizes the equilibrium budget set when it refrains from both actively issuing and repurchasing long-term bonds. Therefore, the government always finds it optimal to only engage in short-term bonds. Unlike the model in this paper, there is only one type of consumption good in their model, and the government does not face a fiscal constraint. As a result, the time-inconsistency problem and the associated commitment power studied in this paper do not emerge in their model.====Hatchondo et al. (2015) study fiscal rules in a sovereign default model with long-term bonds. In their framework, the government lacks both the commitment not to dilute the value of existing debt and the commitment to repay. They show that a carefully designed fiscal rule can mitigate this debt dilution problem and improve welfare. Moreover, under model heterogeneity and parameter uncertainty, the spread-brake rule serves as a superior and more robust fiscal planning instrument than the debt-brake rule. Our framework also features an interaction between two commitment problems, but we focus on the lack of commitment to future taxation instead of the debt dilution problem. Such an interaction exists in a sovereign default model with one-period bonds. Dovis and Kirpalani (2017) investigate the effects of fiscal rules on local governments' overborrowing problem when the central government cannot commit to not providing transfers or cannot enforce penalties if these rules are violated. They show that fiscal rules are effective in reducing debt accumulation only when the reputation of the central government is high enough.====Another paper that is related to ours is that of Pouzo and Presno (2014). They study the optimal taxation in a closed-economy framework where the government is given the option to default and restructure its debt. They show that the option to default generates an endogenous credit limit, which hinders the government's ability to smooth taxes using debt. In their paper, the government can adhere to its tax commitment until it defaults on its debt. However, in our work, the committed tax plans are kept even if the government defaults. Furthermore, they study the dynamic taxation problem in a closed economy where bondholders are the domestic households whose welfare is considered by the government, while our model features a small open economy with foreign lenders holding all the debt.====Conesa and Kehoe (2014) also investigate the fiscal policy during the Eurozone debt crisis. They develop a sovereign default model where self-fulfilling debt crises may arise. Their framework suggests that, under certain economic fundamentals, it may be optimal for the government to increase borrowing to smooth consumption rather than run a fiscal surplus and reduce borrowing to eliminate the possibilities of a debt crisis. Moreover, they also show that a bailout program implemented by a third party is not sufficient to induce the government to run down its debt to eliminate default probabilities.====The rest of the paper is organized as follows. Section 2 develops a two-period model that allows us to discuss the time-inconsistency problem and to explain how fiscal commitment affects welfare. Section 3 presents the infinite-horizon framework that we use for our quantitative analysis. Section 4 presents the quantitative results and compares the two types of commitment measures we study in this paper. Section 5 concludes the paper.",Fiscal commitment and sovereign default risk,https://www.sciencedirect.com/science/article/pii/S1094202521000636,6 September 2021,2021,Research Article,70.0
"Chien YiLi,Wen Yi","Research Division, Federal Reserve Bank of St. Louis, P.O. Box 442, St. Louis, MO 63166, United States of America,Antai School of Economics and Management, Shanghai Jiaotong University, Shanghai, China","Received 18 May 2020, Revised 17 August 2021, Available online 6 September 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.08.004,Cited by (0), redistribution rather than on the failure of the MGR due to capital overaccumulation.,"The seminal work of Aiyagari (1995) has inspired a large literature. However, despite several important revisits, such as Chamley (2001), Conesa et al. (2009), Dávila et al. (2012), and many others, many issues regarding optimal capital taxation in a heterogeneous-agent incomplete-markets (HAIM) economy (á la Aiyagari (1994)) remain unsettled.====For example, is a positive tax levied on capital in the Ramsey steady state motivated mainly by the failure of the modified golden rule (MGR) in light of capital overaccumulation, by wealth redistribution in light of inequality under borrowing constraints, or both? How does optimal capital taxation relate to the optimal level of public debt and the existence of an interior Ramsey steady state?====These issues are intertwined because government bonds not only serve as a self-insurance device for individuals, they also alleviate the overaccumulation of the aggregate capital stock and thus reduce the need for a distortionary capital tax.==== In the meantime, capital taxation (in addition to a labor tax) may be necessary in order to finance the interest payments on public debt and to redistribute wealth from the rich to the poor.====Moreover, to the best of our knowledge, the existence of a Ramsey steady state in infinite-horizon HAIM models is often ==== instead of proven in the bulk of the existing literature. Without such an assumption, the Ramsey allocation is hard to analyze because of these models' intractability; but, optimal tax policies drawn from the analyses may hinge critically on the validity of such an assumption.====The issue at stake is a trade-off under capital taxation between (i) aggregate allocative efficiency (in terms of the MGR) and (ii) individual allocative efficiency (in terms of self-insurance). A positive capital tax improves aggregate allocative efficiency by equalizing the premium-adjusted private marginal product of capital (MPK) and socially optimal MPK but worsens individual allocative efficiency by increasing the liquidity premium (or deteriorating individuals' self-insurance positions). In addition, since the interest rate lies strictly below the time discount rate—a hallmark feature of infinite-horizon HAIM models—it would seem optimal for the Ramsey planner to keep increasing the bond supply until all individuals are no longer borrowing constrained and become fully self-insured against idiosyncratic risk, at which point the MGR would be automatically restored.====Specifically, the competitive market equilibrium in a HAIM model may appear to be dynamically inefficient due to overaccumulation of capital under precautionary saving motives, which results in (i) inequality across households, (ii) a liquidity premium on the rate of return to household savings, and (iii) an excessively low aggregate MPK. Consequently, the MGR fails to hold in a competitive equilibrium. This observation provides the key intuition that the Ramsey planner should tax capital to restore aggregate allocative efficiency. However, a capital tax has not only a strong redistributional effect on individual wealth, but also a strong adverse effect on individuals' self-insurance positions. So why is a capital tax more desirable than a labor tax?====In other words, the conventional wisdom for relying solely on the MGR to justify a positive capital tax in an infinite-horizon HAIM model is counterintuitive, at least from a micro viewpoint. By taxing capital income and thus reducing each individual's optimal buffer stock of savings, the government is hampering and effectively destroying individuals' ability to self-insure against idiosyncratic risks when lump-sum transfers are not available.==== Since taxing capital ==== does not directly address the lack-of-insurance problem for households (if anything, it intensifies the problem), why would taxing capital be always optimal for the Ramsey planner, especially when government bonds are less distortionary than capital taxes and thus more effective in addressing the problem of capital overaccumulation without hindering individuals' self-insurance positions?====These questions are intriguing because the lack of full self-insurance (FSI) is the root cause of the capital overaccumulation problem in HAIM economies and should hence be the ultimate concern of a benevolent government. In other words, in the absence of any wealth-redistribution effect of a capital tax, eliminating the inefficiency of overaccumulation through capital taxation does not help to alleviate the primal friction in the model—the lack of full self-insurance under borrowing constraints; thus, why one would use only the MGR as the optimality criterion to justify a positive capital income tax regardless of its effect on individual allocative efficiency is not at all clear.====In short, intuition tells us that in the absence of any concerns for wealth redistribution, a Ramsey planner can improve welfare more likely through improving individuals' self-insurance positions (such as issuing enough bonds to substitute for capital) than through taxing individuals' buffer stock (capital) when labor taxes are available to finance the interest payments on public debt and the interest rate (cost) is itself below the time discount rate.====The capital taxation trade-off between aggregate allocative efficiency and individual allocative efficiency thus pertains intrinsically to the determination of the optimal level of public debt, which in turn depends critically on the existence of an interior Ramsey steady state. That is, the greater the supply of government bonds, the greater the self-insurance position individuals can achieve, creating less of a need to impose a distortionary capital tax. Hence, optimality seems to require the bond interest rate to equal the time discount rate. Yet the demand for government bonds in a standard infinite-horizon HAIM model approaches infinity as the interest rate approaches the time discount rate, as pointed out by Aiyagari, 1994, Aiyagari, 1995. Hence, without any borrowing limits, the assumption of the existence of an interior Ramsey steady state necessarily implies that the optimal level of the bond supply is finite. But the question is why?====Therefore, daunting challenges in the determination of an optimal capital tax in the class of infinite-horizon HAIM models lie (critically) in analyzing the trade-off problem under capital taxation and in proving the existence of an interior Ramsey steady state, which in turn dictate the determination of the optimal level of public debt and optimal labor tax. In other words, since a capital tax can always be substituted by a labor tax and the capital tax rate can always be lowered further by increasing the bond supply, the policy mix must be simultaneously determined in a Ramsey equilibrium by investigating the full set of the Ramsey planner's first-order optimal conditions, such as those for the bond supply, capital tax, and labor tax.====The goal of this paper is to design a tractable HAIM model—where the redistribution effect and the saving effect of a capital tax can be separated—to analytically investigate the trade-off between aggregate allocative efficiency and individual allocative efficiency when an interior Ramsey steady state can be proven to exist—in the absence of any wealth-redistribution effects of capital taxation.====Our infinite-horizon model follows in the spirit of Aiyagari (1994), but with two key differences: Individuals in our model face idiosyncratic shocks to the marginal utility of consumption, and their preferences are quasi-linear. This preference structure completely eliminates any wealth-redistribution effects of a capital tax and enables us to solve both the competitive equilibrium and the Ramsey allocation in closed forms.====Our analytical approach shares a similar spirit to the recent work of Heathcote and Perri (2018). In their model, households can reshuffle asset holdings at the end of each period such that the distribution of households' end-of-period wealth is degenerate. This feature allows their HAIM model to be analytically tractable despite idiosyncratic risk and precautionary saving motives. In our model, however, the degenerate distribution of wealth is an endogenous outcome of individuals' rational choices and we can characterize the distribution of consumption and savings by a single endogenous cutoff variable that is fully responsive to aggregate conditions. By eliminating the wealth redistribution effect, our contribution is to highlight what does and does not drive the results about positive long-run taxation in the Aiyagari model.====Our contributions are thus five-fold: First, we provide an analytically tractable model in which necessary and sufficient conditions for the existence of an interior Ramsey steady state can be explicitly proved and stated.==== Our model differs from the Aiyagari model in that the distribution of individual wealth is completely degenerate, thus ruling out any wealth-redistribution effects of a capital tax.====Second, we show analytically that within the standard parameter space and in the absence of wealth-redistribution effects, the Ramsey planner will never tax capital in the steady state, despite capital overaccumulation.====Third, our analysis provides clarification of the critical role of government debt in achieving the MGR and influencing the trade-offs of a capital tax between aggregate allocative efficiency and individual allocative efficiency. Specifically, we show that whether the MGR holds or not depends critically on the Ramsey planner's ability to issue bonds as an alternative store of value (aside from capital) for households to buffer idiosyncratic risks. In particular, the MGR would hold in our model if and only if the government can amass a sufficiently large stock of bonds to enable households to achieve full self-insurance. Once households are fully self-insured, the equilibrium interest rate in our model equals the time discount rate (====). In such a case, aggregate allocative efficiency and individual allocative efficiency are simultaneously achieved by the Ramsey planner. However, when it is impossible to equalize the interest rate and the time discount rate—either because of unbounded variance of the idiosyncratic shocks or due to an ad hoc debt-limit constraint on the government's capacity to issue bonds—the MGR ==== hold. Despite the failure of the MGR, however, the optimal capital tax rate is ==== zero in the steady state (even in the case where the government cannot issue bonds).==== Hence, the MGR appears to have no bearing on the planner's long-run capital tax scheme.====Fourth, we use numerical analysis to show that the Ramsey planner nonetheless opts to tax capital along the transition path—so as to front-load consumption by arbitraging between the relatively low interest rate and the time discount rate. In particular, the larger the elasticity of intertemporal substitution, the higher the short-run tax rate. In fact, the levels of aggregate consumption, the capital stock, and hours worked in a Ramsey steady state are lower than their counterparts in the laissez-faire competitive equilibrium. Hence, the welfare gains under optimal policies derive primarily from higher consumption in the short run and the improved distribution of household self-insurance positions in the long run. Notice that in our numerical analyses the Ramsey transition path is calculated based on a proven Ramsey steady state, in contrast to the existing numerical literature that calculates the Ramsey transition path based on an unproven Ramsey steady state that may or may not exist. Our numerical exercises also serve as independent verification of our theoretical analyses.====Last but not least, in the absence of a binding debt limit, the optimal debt-to-GDP ratio in our model is determined by a positive wedge times the aggregate saving rate implied by the MGR. This wedge is an increasing function of the extent of individual allocative inefficiency and would vanish only if idiosyncratic risk approaches zero or markets become complete. Namely, the optimal debt-to-GDP ratio in our model is zero if (and only if) households are fully self-insured and no longer borrowing constrained in a competitive equilibrium. This result suggests (again) that in the absence of wealth-redistribution effects, the single most important role of government debt is to improve individuals' self-insurance positions through which the MGR is achieved (if possible). In other words, in the absence of any wealth-redistribution effects, the MGR does not appear to be the primal concern of the Ramsey planner, because aggregate efficiency is the consequence of individual efficiency in Aiyagari-type models; so it is never optimal to tax capital in the steady state simply to achieve aggregate allocative efficiency even though it is feasible to do so (as long as other forms of distortionary taxes such as a labor income tax are available).====These results are intuitive. On the one hand, it is the lack of an insurance market that induces agents to overaccumulate capital to self-insure against idiosyncratic consumption risk. In the absence of wealth-redistribution effects, taxing capital in the steady state would permanently hamper individuals' self-insurance—the lack of which is the root cause of aggregate allocative inefficiency—and is thus not a desirable tool to restore the MGR.====On the other hand, government bonds meet individuals' demand for precautionary saving without creating pecuniary externalities on the marginal product of capital. Hence, by substituting for (or crowding out) capital, government debt can satisfy the household buffer-stock saving needs and, at the same time, correct aggregate inefficiency due to capital overaccumulation. Most importantly, since the interest rate lies below the time discount rate—a hallmark feature of Aiyagari-type models, the marginal benefit of improving individual allocative efficiency is always larger than the discounted future marginal costs of debt financing by distortionary labor taxes. This is why the Ramsey planner opts to flood the asset market with a sufficient amount of bonds to satisfy (as much as possible) the full self-insurance demand from all households across all states. However, when debt limits exist, the Ramsey planner is unable to issue enough bonds to achieve a full self-insurance allocation; but, in spite of this, the planner will not levy a permanent tax on capital simply to achieve the MGR.====Although our model is just a special case of standard infinite-horizon HAIM models, it serves to demonstrate that the classical result of zero capital taxation obtained in the representative-agent literature can still hold in infinite-horizon HAIM economies with overaccumulated capital stock—as long as the redistributional channel of capital taxation is shut down.====The remainder of the paper is organized as follows. Section 2 describes the model, derives the competitive equilibrium, and provides sufficient conditions for the Ramsey planner to support a competitive equilibrium. Section 3 shows how to solve for the Ramsey allocation analytically and how to prove the existence of a Ramsey steady state. Section 4 performs numerical exercises to study transition dynamics and also to confirm our theoretical analyses. Section 5 provides a brief literature review. The last section concludes the paper with remarks for future research.",Optimal Ramsey taxation in heterogeneous agent economies with quasi-linear preferences,https://www.sciencedirect.com/science/article/pii/S1094202521000624,6 September 2021,2021,Research Article,71.0
Zeida Teegawende H.,"Brock University, Canada","Received 12 August 2019, Revised 29 July 2021, Available online 25 August 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.08.003,Cited by (1),"The Tax Cuts and Jobs Act (TCJA) significantly altered how business income is taxed in the US. This paper provides a quantitative assessment of the distributional and macroeconomic effects of the TCJA, both in the short run and in the long run, using a life-cycle model with ","Recently, the US tax code underwent a major overhaul with the enactment of the Tax Cuts and Jobs Act (TCJA).==== The key feature of this reform is the reduction of tax burden on businesses (corporate and non-corporate) in comparison with that of workers. This paper provides a quantitative assessment of the macroeconomic and distributional effects of the TCJA.====The TCJA is modeled by its three key provisions: a new 20-percent-deduction rate for pass-throughs,==== a drop in the statutory tax rate for corporations from 35% to 21% and the reduction to 37% of the top marginal tax rate for individuals from 39.6%. One important aspect of this law is the fact that tax cuts for individuals and pass-through businesses are temporary (with a sunset clause set for 2025), while corporate tax cut is permanent. Therefore, the long-run effects of TCJA are primarily driven by the reduction of the corporate tax rate, while the short-run effects combine different features of the tax reform.====The paper not only focuses on the macroeconomic effects of the reform but also its heterogeneous effects across individuals. I then consider a dynamic general equilibrium model of entrepreneurship built on Cagetti and De Nardi (2006), modified to feature a life-cycle dynamics. While households age and save for life-cycle purpose and retirement, they can invest in active businesses in a financially constrained environment. The possible switch between paid work and entrepreneurship makes their saving behavior tied to the endogenous nature of occupational choice. The need to overcome borrowing constraints to start up or to scale up a venture provides another reason for a high saving rate for would-be entrepreneurs and entrepreneurial households. Furthermore, when households undertake a spell in the business sector, they acquire an entrepreneurial experience which is essential in the subsequent business success. Therefore, they keep saving even more. This mechanism is also important to account for the existence of serial entrepreneurs found in the data. Using the Panel Study of Income Dynamics (PSID), Zeida (2020) highlights the importance of cumulative business experience on the entrepreneurial dynamics, and wealth and income distribution.====Under the TCJA, in the calibrated model, the economy experiences an increase of 0.19, 0.5 and 0.08 percentage points in annual GDP, capital stock and entrepreneurship growth for the first decade 2018-2027, respectively. Considering the temporary feature of the tax cuts for individuals and unincorporated businesses in the long-run analysis, shows that the aggregate output and capital stock increase by less than 0.3% in levels, and the pass-through entrepreneurial sector shrinks by 0.5%. Nonetheless, when the TCJA is implemented without any scheduled expiration the economy displays a level increase of 3% in output and 7.6% in capital stock in the long run, mainly due to the new 20-percent-deduction provision. In this scenario, the entrepreneurship rate is higher by 2% in the long run.====In this environment, the incentives stemming from the business income deduction's provision are the main driver of the TCJA when all three provisions are made permanent. They contribute to more than 70% of the increase in output and capital stock. The interest rate goes down by 10% in this case, which is more than proportional to the rise of 2.3% in the wage rate. Thus, the financial constraints are alleviated for would-be entrepreneurs making the outside option as a worker less attractive (positive extensive margin). For instance, entrepreneurial population and entry rate increase by 3%, while the exit rate is reduced by 2%. Moreover, the existing business owners take advantage of the tax break to scale up their investment. The average firm size then increases (positive intensive margin). The two effects complement each other inducing the economy to experience higher savings and output. On the other hand, allowing all provisions but the corporate tax to expire triggers macroeconomic aggregates to return to their baseline level. In the long run and from a purely efficiency perspective, there is a net advantage of extending all provisions since all aggregates experience positive growth rates.====I find that economic inequality is higher under the TCJA with permanent provisions. The Gini coefficients for income and wealth increase by 0.2% and 1.25%, respectively. Typically, individuals at the top of the income distribution take more advantage of the new provisions making them save more than the rest of the population. Thus provided that the TJCA is top and business-oriented, inequality goes up as a result. Moreover, the reform generates large heterogeneity within and between occupations. While the variance in consumption is higher on average in the reformed economy, that of the disposable income and asset are lower for almost all workers (namely, those outside the top 10th decile). Indeed, the large increase in income and asset dispersion for the top decile workers (dominant effect of the top marginal tax cut) leads to higher variability in consumption for all workers. Therefore, with more disposable income, inequality in consumption rises. On the other hand, the three dispersion measures rise among entrepreneurs. In fact, with lower tax liabilities, top entrepreneurs hold higher income, save and consume more than the remaining business owners, which amplifies the within inequality. As a result, the TCJA induces higher dispersion in the economy. In addition, the consideration of the after-tax income Gini also shows that the income is unequally distributed in the population.====To the extent that the business deduction provision is the main driver of the TCJA's effect when all provisions are extended, I argue that the new 20% deduction exacerbates the initial inequality already presents among business owners. Wealthier entrepreneurs are simply becoming wealthier. The welfare consequences are opposite for the two occupations. In fact, on average, entrepreneurs are better off while workers experience welfare losses even with a wage increase of 3%. The revenue-neutral condition in the long run wipes out the potential tax relief contained in the reform. Accordingly, the whole population suffers a consumption loss relatively to the ==== when accounting for the transitional dynamics. On the welfare basis, the TCJA does not have a majority support.==== These results hold regardless of the sunset clause for certain provisions of the TCJA.==== This paper is related to the large strand of literature on the quantitative evaluation of tax reforms using macroeconomic models. A first set of studies carries out hypothetical tax reform. Castaneda et al. (2003) show that abolishing estate tax will generate an output growth of 0.35%. Looking at the welfare consequences during the transition, Domeij and Heathcote (2004) study the effect of removing capital taxation both in representative agent model and Aiyagari-type models. A second set of papers also performs thought experiments in occupational choice model where agents can choose paid-work or running a business. Cagetti and De Nardi (2009) then discuss the effect of repealing estate taxation on wealth distribution in this environment. Meh (2008) emphasizes the positive effects of eliminating the corporate tax rate. In the presence of idiosyncratic investment risk, Panousi (2010) finds positive effects of capital taxation. Using an infinite horizon model, Kitao (2008) analyzes the incentives when capital income is reduced and also when business income is preferentially taxed with a flat tax rate. In the same vein, Boháček and Zubrickỳ (2012) examine a flat tax reform with different exemption levels in an entrepreneurship model. A recent paper by Kaymak and Poschke (2016), analyzes the historical changes in the actual tax and transfer system combined with those in the wage distribution and focuses on the rise in wealth inequality over half a century in the US. In this paper, I evaluate key provisions of the TCJA.====Barro and Furman (2018) use a standard neoclassical framework with infinitely lived agents and perfect foresight to evaluate the effects of the TCJA through the user cost approach with a representative firm. Sedláček and Sterk (2019) extend Barro and Furman (2018)' work by using heterogeneous firms setup.==== Using reduced form estimates from several recent studies, Mertens (2018) projects the impact of the TCJA on U.S. GDP growth in the next couple of years. The level of GDP is predicted to be 1.3% higher by 2020. On the other hand, the current paper allows agents to be affected by earnings shocks and choose their optimal occupation over their lifecycle. It also carries out distributional and welfare analysis which is absent from Barro and Furman (2018), Mertens (2018) and Sedláček and Sterk (2019). More importantly, the model considered in this paper has first-order implications on macroeconomic aggregates. The extended institutional details of the new tax law are more closely followed by the analyses of U.S. federal agencies such as the Joint Committee on Taxation (JCT, 2017), the Congressional Budget Office (CBO, 2018), and the Penn Wharton Budget model (PWBM, 2018). Therefore, I see the current analysis as complementary, since the results lie in the same ranges.====The interplay between entrepreneurship, macroeconomic aggregates (output, capital formation, and prices), and inequality among households has been stressed in the literature (Quadrini, 2000; Gentry and Hubbard, 2004; Cagetti and De Nardi, 2006; Buera, 2009). The framework here shares the same mechanism and adds a complementary entrepreneurial experience channel.====Furthermore, entrepreneurship can have a legal form of organization margin, allowing business owners to elect their firms as pass-throughs or C-corporation for tax and financing purpose (Chen et al., 2018; Dyrda and Pugsley, 2018). For instance, Chen et al. (2018) find that with this margin the reduction of the corporate tax rate from 35% to 20% induces a 1.3% long-run change in the output level. Since the main focus is on business activities that are closely related to households characteristics (demographics and financial constraints), I abstract from this channel in the baseline setup.====The rest of the paper is laid out as follows. The model and its recursive formulation are presented in section 2. Section 3 discusses model calibration. The evaluation of aggregate and distributional effects of the TCJA reform is carried out in section 4 and section 5 emphasizes its transitional dynamics in comparison with the federal agencies' estimates. Section 6 concludes.",The Tax Cuts and Jobs Act (TCJA): A quantitative evaluation of key provisions,https://www.sciencedirect.com/science/article/pii/S1094202521000600,25 August 2021,2021,Research Article,72.0
"Jang Youngsoo,Yum Minchul","Shanghai University of Finance and Economics, China,University of Mannheim, Germany","Received 16 October 2020, Revised 7 July 2021, Available online 8 August 2021, Version of Record 6 September 2022.",https://doi.org/10.1016/j.red.2021.07.004,Cited by (0),"Long hours worked associated with higher hourly wages are common to many occupations, known as nonlinear occupations. Over the last four decades, both the share of workers in nonlinear occupations and their relative wage premium have been increasing. Females in particular have been facing rising experience premiums, especially in these types of occupations. We quantitatively explore how these changes have affected the female labor supply over time using a quantitative, dynamic general equilibrium model of ","Nonlinear occupations describe a rather prevalent type of work in the modern economy, where employees receive higher hourly wages for longer hours of work (Goldin, 2014; Erosa et al., forthcoming). As these occupations provide greater rewards to individuals who work longer hours and penalize those who work shorter hours, earnings increase ==== with hours worked. In this paper, we highlight several significant changes regarding these types of occupations.==== The fraction of people—especially women—working in nonlinear occupations has increased over the last four decades, while positive wage premiums for these occupations have increased steadily. This suggests that the relative demand for nonlinear occupations has been increasing. Further, the large gender gaps in experience premiums that existed four decades ago have narrowed significantly, again especially for nonlinear occupations.====What are the implications of these changes to the relative demand for nonlinear occupations and to rising experience premiums on the recent evolution of female labor supply? In particular, we ask if these changes help to account for the ever-rising average number of hours worked per female worker (i.e., the intensive margin), which differs notably from the stagnating employment rate (i.e., the extensive margin) of recent years.==== At first glance, and given that there are increasingly more women than men in these more remunerative nonlinear occupations, it may be expected that both changes have contributed to the shrinking gap in the wages of women relative to men. These factors in turn could induce greater female labor supply at both margins. On the other hand, the increasing importance of nonlinear occupations could hinder the participation of more women who would be unwilling to work long hours. We address this quantitative question using a version of the neoclassical growth model with heterogeneous agents in which occupational differences arise endogenously.====Specifically, we build upon the model of Erosa et al. (forthcoming) that combines occupational choice (Roy, 1951) and endogenous labor supply, whereby the two model occupations differ by their degree of nonlinearity. Our model is essentially a dynamic version of their model, and is based on a standard heterogeneous agent incomplete markets framework (Huggett, 1993; Aiyagari, 1994)—a workhorse macroeconomic model used to study distributional issues.==== Compared with the parsimonious static model of (Erosa et al., forthcoming), our dynamic environment is more advantageous as it enables us to specify origins for and the different nature of the nonlinearities in question.====Broadly speaking, nonlinearities are shaped by the dynamic returns to working long hours and the presence of part-time penalties. For each specific occupation, a worker can be upgraded stochastically if she worked in the same occupation during the previous period and worked greater than or equal to the (occupation-specific) upgrade threshold number of hours. Once a worker is upgraded, she additionally earns the (occupation-specific) return to experience. Similarly, part-time penalties also vary by occupation and are modeled as a proportional tax on earnings for those who work less than the full-time threshold number of hours. These thresholds are designed to capture the barrier nature of the nonlinearities. On the whole, these differences across occupations—along with individual state variables such as idiosyncratic productivity, comparative advantages, experience, household assets, and preference types—affect the employment decision, occupational choice, and hours of work conditional on occupational choice in the model.====We calibrate the model to US data using a standard approach by matching the relevant statistics obtained from the Current Population Survey (CPS) during the initial period of 1976–1985.==== Following (Erosa et al., forthcoming), we categorize occupations found in the data into two groups based on mean hours worked at the occupational level. Without assuming any further conditions as part of this categorization, our calibration results distinguish those occupations with higher mean hours from those with lower mean hours along three dimensions: (i) the threshold number of hours for an upgrade is higher; (ii) the return to experience is higher; and (iii) the part-time penalty is higher. Since nonlinear occupations would indeed provide greater compensation for working longer hours while disfavoring shorter hours (Goldin, 2014), our estimation and model calibration confirms that the occupations in the former group are more likely to be nonlinear and the latter more likely to be linear. We further confirm that our model can deliver the salient facts that nonlinear occupations have higher mean wages and higher wage dispersions.====Next, we use our model to quantify the role that changes in nonlinear occupations have in explaining the evolution of the female labor supply. We are particularly interested in the underlying factors responsible for the continued rise of the intensive margin labor supply and the stagnating employment rates of recent years. We investigate changes in select driving forces. These include not only the key interests of our paper—returns to experience and nonlinear-occupation-biased technical changes—but also factors that are known to be important for determining the female labor supply, such as relative wage changes (Heathcote et al., 2010b; Kaygusuz, 2010; Jones et al., 2015; Bick et al., 2019) and preference shifts (Fernández et al., 2004; Fogli and Veldkamp, 2011; Fernández, 2013). This allows our model to replicate the observed changes in gender wage gaps and aggregate hours over time.====The first notable finding from our decomposition analysis is that rising returns to experience are quantitatively important when accounting for the rising intensive margin of the female labor supply. Specifically, our model predicts that the model-implied increment of 240 annual hours worked per worker from 1976–1985 to 2006–2015 (vs. 205 hours in the data) would be approximately 43% lower if the returns to experience were held fixed at their baseline levels (in 1976–1985). Secondly, our calibration results imply noticeable changes in demand factors that have increasingly favored nonlinear occupations over time. We find that this change naturally increases the share of women working in nonlinear occupations, but reduces the overall employment level—explaining why female employment has been stagnating in recent years. Quite a few women prefer short hours worked and would be willing to work in linear occupations. However, as linear occupations become less attractive due to technical changes favoring nonlinear occupations, more women are induced to leave the labor force altogether from linear occupations, as compared to those who leave them to work in nonlinear occupations. We also find that wage changes for women are very powerful in shifting more women to work (the extensive margin) but are not as important as the returns to experience in increasing labor supply at the intensive margin.====Finally, we conduct a counterfactual analysis by asking what would have happened to female labor supply trends if the barrier aspects of nonlinearities had gradually vanished. This analysis is motivated by Goldin (2014) who argues that these nonlinearities play the role of barriers in high-paying occupations and are thereby an important source of the gender wage gap—they prevent women from working in nonlinear occupations that pay higher average wages. In this experiment, we keep the changes in the returns to experience while adjusting for these nonlinearities by either (i) reducing the upgrade threshold number of hours or (ii) decreasing the full-time threshold number of hours.==== We find that the level of female employment indeed could have been significantly higher, especially if the requirement of working long hours was eliminated. This could have led to a 12 percentage point higher employment rate in 2006–2015. Because returns to experience have been rising—especially in nonlinear occupations—our model predicts a significantly more prominent increase in the number of women working in such occupations. However, we also find that this change is accompanied by significantly lower labor supplies at the intensive margin (9.5% lower in 2006–2015). Our exercise suggests that, while nonlinearities are indeed a quantitatively important barrier that lead many women out of the labor force, they also play an important role in providing an incentive for women to supply long hours.====A large body of literature has investigated the determinants of changes in female labor supply over time, as reviewed recently by Doepke and Tertilt (2016) and Greenwood et al. (2017).==== Our results are closely related to Olivetti (2006), Attanasio et al. (2008), and Park (2018), all of whom consider that returns to experience are a major determinant of female labor supply in a structural environment. Relative to these papers, our work differs in that we consider returns to experience separately in different occupations categorized by nonlinearities. We also document differential trends by occupation, and we investigate their implications for occupational choice—a channel that also shapes labor supply. Moreover, our findings shed light on the seemingly conflicting findings of Olivetti (2006) and Attanasio et al. (2008): the latter finds that returns to experience are unlikely to be quantitatively important factors in explaining the rising female labor supply, while the former finds a substantial role for returns to experience. Note that in our model incorporating both the intensive and extensive margins, we find that the effects of returns to experience work mostly through the intensive margin, with only limited effects on the extensive margin. Therefore, our results suggest that models without the intensive margin—as in Attanasio et al. (2008)—may understate the importance of returns to experience when accounting for overall female labor supply changes. At the same time—in comparison to Olivetti (2006)—our model incorporates idiosyncratic uncertainty and higher model frequency (annual vs. 10 years). With these features, we find that the role of returns to experience in explaining overall female labor supply is quantitatively not as strong, as compared to Olivetti (2006).====Erosa et al. (forthcoming) presented a model of occupational choice and labor supply, which they used to show that gender differences in home production responsibilities (in terms of time alone) can generate sizable gender gaps in various labor market outcomes like wages, hours, and occupational choice. As explained above, our model builds upon theirs by introducing the dynamic aspects necessary for us to specify the nonlinearities in more detail and to endogenize the labor supply decision along the extensive margin at the data frequency.==== Based on their insight into nonlinear occupations and cross-sectional labor supply at a given time, our paper produces a further contribution by quantitatively investigating the implications of these factors on labor supply changes over time.====We highlight that our novel findings regarding the role of nonlinear occupations and experience premiums are based on a model where the effects of the other factors are quantitatively in line with the previous literature. In particular, and fitting in with earlier theory highlighting the role of learning in shifting women's disutility of work (Fogli and Veldkamp, 2011; Fernández, 2013), our decomposition exercise finds that the role of preference shifts in explaining overall increases in female labor supply became increasingly important until 1996–2005, while they then became much weaker in 2006–2015.====The reminder of this paper is organized as follows. The next section presents the stylized facts regarding how labor supply and occupations (nonlinear vs. linear) have evolved over the last four decades using data from the CPS. Section 3 presents the model economy and defines the equilibrium. Section 4 explains how the model is calibrated and presents some of the properties of the baseline economy. Section 5 presents our decomposition analysis showing how different factors affect the observed trends in labor supply and gender wage gaps. Section 6 conducts a counterfactual experiment to quantify what would have happened to labor supply trends if nonlinearities had gradually vanished. Section 7 contains our conclusions.",Nonlinear occupations and female labor supply over time,https://www.sciencedirect.com/science/article/pii/S1094202521000582,8 August 2021,2021,Research Article,73.0
Galindo da Fonseca João,"Université de Montréal and CIREQ, 3150, Rue Jean-Brillant, Montreal, H3T 1N8, Canada","Received 15 February 2021, Revised 30 June 2021, Available online 3 August 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.07.003,Cited by (4), and induces a reallocation of resources to low productivity firms.,"Are there differences in the propensity to start a business and the quality of the business started based on current employment status? The answer to this question is crucial to understand the impact of policies intending to promote job creation by subsidizing entrepreneurship among the unemployed. Examples include the expenditure of 37.5 million Euros by France in 2009, with 40% of new businesses being started by the unemployed (European Commission (2010)). In Germany in 2004, spending on these policies totaled 2.7 billion Euros, representing 17.2% of expenditures in active labor market policies (Baumgartner and Caliendo (2007)). In the UK, such a policy has been responsible for creating nearly ==== new businesses per month since 2011 (Burn-Callender (2013)). In Canada in 2012, similar policies cost 118 million Canadian dollars, representing 10% of expenditures in active labor market policies (CEIC (2014)).====I evaluate these differences in propensity to start a business and business quality, and simulate the effect of these policies. I use a three step strategy. First, I build a model that allows me to derive predictions for these differences. Second, I test these predictions using state of the art data. Third, I use the model to evaluate the impact of a policy targeting entrepreneurship among the unemployed.====I propose a search model of endogenous entrepreneurship. Individuals are ex-ante homogenous and face a common arrival rate and distribution of quality of opportunities. The only difference between the unemployed and the employed is their outside option. I show that this leads to the unemployed being less selective on which business projects they implement. In equilibrium, the unemployed are more likely to start a firm but on the condition of doing so, they hire fewer workers. Moreover, they are more likely to exit entrepreneurship relative to an individual who started a business from employment. Finally, an additional implication is that higher wages decrease the entry rate into entrepreneurship of the employed by more than that of the unemployed. The reason is that wages represent the opportunity cost to entrepreneurship for the employed but not for the unemployed.====To test these implications I make use of full universe employer-firm-employee linked Canadian tax data. It contains three advantages relative to other datasets. First, it improves on employer-employee datasets by linking firms to their corresponding owner,==== which makes it fitting for studies of entrepreneurship. Second, individuals and firms are linked over time, allowing me to observe firm creation and pre-entry information on the entrepreneur. Third, each employee is linked to their employer firm. I leverage this information to use episodes of firm closure to identify random assignment to unemployment.====I find that unemployment doubles the probability of an individual to start a firm relative to that of an employed person.==== Next, I show that starting a firm following random assignment to unemployment is associated with 25.7% fewer workers and a 30% higher firm exit rate relative to starting a firm while employed. Finally, to test the last prediction, I follow Beaudry et al. (2012), Sand et al. (2016), Beaudry et al. (2018), and Green et al. (2019) and use variation in industrial composition across local labor markets as my source of identification for the effect of wages. I find that a 1% drop in wages increases the entry rate by 3.2 percentage points for wage workers and has no impact on unemployed individuals.====Next, the model is used to study the impact of a policy that promotes entrepreneurship among the unemployed. Consistent with stated objectives by policy makers, I focus on the impact on job creation. I quantify the impact on the aggregate economy of a policy that redistributes 5% of total unemployment insurance (UI) income to individuals that are unemployed and start a firm. This corresponds to an entrepreneur receiving 41% of their previous UI benefits during the first year of business. This magnitude is similar to the subsidy program in British Columbia, Canada, where entrepreneurs entering from unemployment receive their full UI benefits for the first 38 weeks of a business operation. I calibrate the model to match the estimated differences in propensity to start a firm and business quality.====The policy reduces the unemployment rate by 0.1 percentage points and decreases average firm productivity by 1.19%. Intuitively, the policy induces the creation of low productivity firms by the unemployed. As the share of firms created by the unemployed increases, productivity falls. The result is a shift in resources from high productivity firms created by the employed to low productivity firms created by the unemployed. Finally, due to a smaller unemployment pool, labor market tightness increases by 1.91%. Higher tightness raises the value of employment, making the employed more selective on which projects to implement. Higher selectivity among the employed further reallocates resources from high to low productivity firms. I conclude that the policy has a small negative effect on the aggregate economy.====There is a large literature on entrepreneurship and firm dynamics====; however, this is the first paper to study the total impact of policies promoting entrepreneurship among the unemployed via a subsidy. While these policies have been promoted as cheap tools to improve job creation, there has been no study on their impact on productivity and the unemployment rate. Caliendo and Künn (2011) and Baumgartner and Caliendo (2007)) also investigate the impact of policies that help the unemployed transitioning to entrepreneurship, but do not look at aggregate outcomes. By highlighting how the relationship between unemployment and entrepreneurship matters for policy this paper contributes to the literature studying this relationship (Donovan (2014), Block and Wagner (2010), Evans and Leighton (1989), Von Greiff (2009) and Røed and Skogstrøm (2014)). Finally, this paper also relates to the development literature studying subsistence entrepreneurship, which often uses measures of involuntary entrepreneurship, such as self-employed with no employees (Earle and Sakova (2000) and de Mel et al. (2008)) or education (Poschke (2013)). Instead of concentrating on the notion of subsistence entrepreneurship, I focus on the role of involuntary unemployment for entrepreneurial outcomes.====Section 2 presents the model. Section 3 describes the data and the source of variation to identify unemployment. Section 4 reports the main empirical results. Section 5 explains the parametrization strategy and reports the policy counterfactual result. Section 6 concludes.","Unemployment, entrepreneurship and firm outcomes",https://www.sciencedirect.com/science/article/pii/S1094202521000569,3 August 2021,2021,Research Article,74.0
"Fornino Michele,Manera Andrea","Massachusetts Institute of Technology, Department of Economics, E52-480, 50 Memorial Dr, Cambridge, 02142, MA, United States","Received 19 May 2020, Revised 24 June 2021, Available online 28 July 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.07.002,Cited by (5),"We study the economic incentives for automation when labor and machines are perfect substitutes. Labor may still be employed in production, even when it is a costlier input than robots on a productivity-adjusted basis. This occurs if firms face idiosyncratic risk, adjusting the stock of machines is costly, and workers can be hired and fired quickly enough. Even though labor survives, jobs become less stable, as workers are hired in short-lived bursts to cope with shocks. We calibrate a ====, multi-industry version of our model to match data on robot adoption in US manufacturing sectors, and use it to compute the employment and labor share consequences of progress in automation technology. A fall in the relative price of robots leads to relatively few jobs losses, while reductions in adjustment costs, or improvements in relative robot productivity, can be far more disruptive. The model-implied semi-elasticity of aggregate employment to robot penetration (number of robots per thousand employees) ranges between 0.01% and 0.12%, depending on the underlying source of increased robot adoption, consistent with findings in the empirical literature. In an extension, we show that reduced-form hiring and firing costs unambiguously depress long-run employment.","Over the last few years, the progress in robotics, software engineering, and AI, coupled with the secular decline in the labor share of output, has sparked a discussion on whether machines will progressively replace humans in performing tasks (Acemoglu and Restrepo, 2018c; Autor, 2015; Autor and Salomons, 2018; Berg et al., 2018; Graetz and Michaels, 2018; Sachs and Kotlikoff, 2012). In this paper, we investigate the long-run consequences of improvements in automation technology under the deliberately extreme assumption that robots and humans are perfect substitutes within tasks, to provide an upper bound for potential employment losses.====The recent literature has stressed a number of reasons why robots and human workers might not be perfect substitutes. Autor (2015) discusses the ability of workers to perform the multiple and differentiated tasks that typically constitute a job. Acemoglu and Restrepo, 2018b, Acemoglu and Restrepo, 2018c, Acemoglu and Restrepo, 2020 highlight the importance of human comparative advantage in carrying out specific tasks, a point that is also made by Berg et al. (2018) and Graetz and Michaels (2018). Finally, there might be some non-routine occupations, or categories of workers that are poised to benefit from automation (Berg et al., 2018; Sachs and Kotlikoff, 2012; Sachs et al., 2015). While we acknowledge the importance of these mechanisms, we abstract from them to focus on tasks where workers and robots can indeed be perfect substitutes. In this sense, we describe a worst-case scenario for workers employed in low-skilled, routine occupations.====Our baseline model follows the spirit of the task-based framework adopted by Acemoglu and Restrepo (2018b), and focuses on a firm choosing whether to automate a single task where humans and robots are perfect substitutes. We describe how human labor can survive thanks to its ====, which we identify as its distinctive comparative advantage.==== Labor survives alongside robots under three main assumptions that set our framework apart from the existing literature. First, we introduce demand shocks by assuming that firms face idiosyncratic risk. Shocks create a source of demand for flexible inputs that can readily adjust to a volatile environment. Second, we model robots as having capital-like features that impair their rapid deployment in production, in contrast to the standard assumption of a rental market that allows for immediate adjustments of the robot stock. Finally, we assume that employees can be hired and fired more easily than robots can be bought, installed, and sold. Accordingly, firms respond to positive shocks using the more flexible factor, but generally employ ==== workers and machines. Notably, firms employ both workers and machines even if labor and robots are perfect substitutes, and even if robots are cheaper on a productivity-adjusted basis.====The dynamics implied by our model reproduce the empirical dichotomy between firm-level and aggregate effects of automation on employment. At the firm level, investment in automation technology has been associated to higher wages and employment (Acemoglu et al., 2020; Aghion et al., 2020; Bonfiglioli et al., 2020; Koch et al., 2019). However, aggregate labor demand at the sector or geographic levels has been found to fall with increased robot penetration (Acemoglu and Restrepo, 2020; Dauth et al., 2019). We also show that the survival of production-line employment comes at the cost of reduced job stability, which hearkens back to the findings in Eggleston et al. (2021); Rutledge et al. (2019). Simulated time series from our model highlight that, in an automated world, labor is only hired in short-lived “bursts” to cope with sudden increases in the desired production scale.====Our modeling choices are informed by data on robot costs. Fig. 1 reports data from Sirkin et al. (2015) detailing the main cost items for the setup of a spot-welding robot in the U.S. automotive industry. The figure highlights three main facts that feature in our model and calibration. First, in line with the broader evidence in the report by the International Federation of Robotics (2017) and Korus (2019), the purchase price of robots has been trending down, and it is projected to keep doing so in the future. Second, purchase costs represent but a small fraction of the total cost of a robotic system, which is mostly made up of installation-related costs, reflected by the adjustment costs in our model. Third, the nature of these adjustment costs, and particularly those related to programming and “peripherals”, suggest that robot systems have a firm-specific component that might affect their redeployment to different contexts.====In order to gauge the quantitative implications of our theoretical findings, we develop a multi-industry, general equilibrium version of our baseline model, which we calibrate to match data on the adoption of robots between 2010 and 2014. This exercise reveals that, in line with the evidence in Fig. 1, robot adjustment costs might indeed be sizable. This result stems from the low aggregate semi-elasticity of robot penetration—the number of robots per thousand employees—to purchase prices observed in the data. Under our calibration, we establish that even a dramatic reduction in the relative price of robots causes only a modest fall in aggregate employment, with changes in the technical substitutability and flexibility of robots posing a more substantial threat. Notably, general equilibrium effects mitigate employment losses, as they imply a fall in the equilibrium wage. Our model generates a semi-elasticity of aggregate employment to robot penetration which ranges between 0.01% and 0.12%, depending on the underlying source of increased robot adoption. These magnitudes are in line with the quantitative findings of Acemoglu and Restrepo (2020), who compute a semi-elasticity of 0.2%.====Our quantitative results are robust to three alternative calibration strategies. First, we account for uncertainty in robot price data—a crucial threat to the estimate of price elasticity. We do so by re-calibrating the multi-sector model for a wide range of potential robot price changes (between 50% and 150% of our baseline). Second, we assess the robustness of our results to account for the fact that the low price elasticity that informs our baseline calibration might arise from slow investment responses to current and anticipated future price changes. To this end, we propose an alternative calibration that matches the increase in robot penetration observed between 2004-2014, assuming that this change occurs along a perfect-foresight transition of robot prices to lower levels, announced in year 2000. Finally, we allow for uncertainty surrounding firms' growth prospects by introducing non-stationary shocks. All these alternative strategies broadly confirm our quantitative findings.====We further extend our theoretical results in two directions. First, we analyze an extension featuring labor adjustment costs, which we interpret as reduced-form labor market frictions. Second, we show that our qualitative findings are robust to using linear or fixed costs of adjustment. In line with our theory, we find that high hiring and firing costs dampen labor's flexibility comparative advantage, and increase the long-run displacement of production-line workers. This suggests that removing strict employment protection measures could be an effective policy to safeguard unskilled jobs in the long run, counter to what intuition might suggest. The rigid-labor extension also shows that when the transition to lower robot prices is gradual, stricter labor market regulations can induce firms to anticipate the adoption of robots to smooth out workforce adjustment costs. This result speaks to empirical evidence suggesting that higher unionization is associated with higher robot penetration at the current stage (Acemoglu and Restrepo, 2018a).==== Our modeling framework relates to two distinct strands of theoretical literature. The first deals with automation and its long-run impact on employment and factor shares. The second relates to modeling investment under uncertainty and costly reversibility. Our findings also speak to the recent empirical literature on the effects of automation on employment and wages.====In the theoretical literature, several papers view automation as a form of factor-augmenting innovation: labor-augmenting according to Bessen (2020); capital-augmenting according to Sachs and Kotlikoff (2012); Sachs et al. (2015); Nordhaus (2021); Berg et al. (2018). These studies either impose exogenous technological limits on the extent of automation, or conclude in favor of a long-run demise of labor, which sees its factor share falling to zero. Our stance is decidedly closer to the task-based approach pioneered by Zeira (1998) and later adopted by Acemoglu and Restrepo, 2018b, Acemoglu and Restrepo, 2018c and Graetz and Michaels (2018). In this setting, labor and robots are perfect substitutes within a subset of tasks. In our paper, we focus on this subset, and analyze the consequences of advances in automation when there are no technological limits to substitution.====All the above contributions make two common assumptions: robots are rented on the market; and firms operate in a deterministic environment. These features allow for bang-bang solutions in favor of either robots or labor when perfect substitution is possible. Our framework departs from both these assumptions, bridging the literature on automation with that on investment under uncertainty and costly reversibility (Abel, 1983; Pindyck, 1988, Pindyck, 1991; Caballero, 1991; Abel and Eberly, 1996, Abel and Eberly, 1997; Dixit and Pindyck, 1994; Stokey, 2009). Our theoretical model is closest to Abel and Eberly (1996), in that we assume decreasing returns to scale, perfectly flexible labor, and solve our model in continuous time. However, we depart from the standard investment literature by adopting a production function that features perfect substitutability between labor and capital. We rely on results summarized in Achdou et al., 2017, Achdou et al., 2014 for our numerical solution method.====Our findings reproduce a number of features of the growing empirical literature on automation. In our framework, increased robot penetration leads to lower aggregate employment, even if investment in robots is positively correlated with productivity and employment at the level of the individual firm, consistent with recent studies. In particular, Acemoglu et al. (2020), Bonfiglioli et al. (2020), Koch et al. (2019), Aghion et al. (2020) find that employment generally increases at the level of the firm undertaking the automation effort. By contrast, Acemoglu and Restrepo (2020); Acemoglu et al. (2020) and Dauth et al. (2019) estimate negative employment effects of automation at various levels of aggregation. Finally, Eggleston et al. (2021) show that an increase in robot adoption is associated with an increase in temporary work and employee headcount.",Automation and the future of work: Assessing the role of labor flexibility,https://www.sciencedirect.com/science/article/pii/S1094202521000570,28 July 2021,2021,Research Article,75.0
"Hosseini Roozbeh,Kopecky Karen A.,Zhao Kai","Department of Economics, Terry College of Business, University of Georgia, 620 South Lumpkin Street, Athens, GA 30602, United States of America,Research Department, Federal Reserve Bank of Atlanta, 1000 Peachtree St. NE, Atlanta, GA 30309, United States of America,Department of Economics, University of Connecticut, 365 Fairfield Way, Storrs, CT 06269-1063, United States of America","Received 7 June 2019, Revised 1 July 2021, Available online 9 July 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.07.001,Cited by (11),"We construct a unified objective measure of health status: ====, defined as the cumulative sum of all adverse health indicators observed for an individual. Using this index, we document four stylized facts on health dynamics over the life cycle and show that they are robust to other ways of constructing the index. We also compare the frailty index with self-reported health status and find significant differences in their dynamics and ability to predict health-related outcomes. Finally, we propose and estimate a stochastic process for frailty dynamics over the life cycle accounting for mortality bias. Our frailty measure and dynamic process can be used to study the evolution of health over the life cycle and its economic implications.","One of the most challenging aspects in the study of life cycle health dynamics is the lack of consensus on what health is and how exactly it should be measured. As a result, what researchers observe are noisy and incomplete signals about health. These signals can be divided into two broad categories. One category consists of people's self-assessment of their health or the extent to which health affects their activities. The other category consists of a long array of objective health-related indicators, ranging from past medical diagnostics to current health conditions and health-induced limitations. Both categories of signals have great potential to inform us about the evolution of the latent health status of an individual over her life cycle. The challenge, however, is to tease this information out in a concise and tractable way.====In this paper we study the evolution of health over the life cycle using a measure of health called the frailty index. The index aggregates information from the wide range of health indicators in the data into a unitary measure that is easy to construct. Using the frailty index, we document four stylized facts about life cycle health dynamics. We then show that these findings are robust to variations in how the index is constructed that are inspired by other measures of health used in the literature. In particular, we show that the variations have relatively little impact on our findings on life cycle health dynamics or on the ability of the index to predict health-related outcomes. We also compare the frailty index to a commonly used measure of health status in the economics literature: self-reported health status (SRHS). We document significant differences in health dynamics and prediction ability between the two measures. Finally, we present a statistical model of frailty dynamics over the life cycle and show how to estimate the model using a simulated method of moments procedure that allows us to account for selection bias due to mortality.====The frailty index, or frailty for short, is simply the accumulated sum of all adverse health events that an individual has incurred. It can be treated as a continuous variable and is used extensively in the gerontology literature.==== The idea behind its construction is as follows. As individuals age, they accumulate health problems, or deficits, ranging from symptoms to clinical signs, and laboratory abnormalities to diseases and disabilities. Mitnitski et al. (2001) and Mitnitski et al. (2002) have demonstrated that health status can be represented by combining deficits in an index variable called a frailty index. Mitnitski et al. (2005) and Goggins et al. (2005) find that the frailty index is comparable between databases even when it is constructed using different lists of deficits. They also find that the frailty index is a better predictor of mortality and institutionalization than age is.====Following the guidelines described in Searle et al. (2008), we construct a frailty index for individuals in the Panel Study of Income Dynamics (PSID). Since 2003, the PSID contains a rich set of survey questions on various aspects of individuals' health conditions, allowing us to include a broad range of deficit variables in our index. The variables span three general categories: restrictions and difficulties with activities of daily living (ADLs) and instrumental activities of daily living (IADLs); mental and cognitive impairments; and medical diagnoses and measurements. Examples of deficits in the first category are difficulties eating, dressing, walking across a room, or shopping without assistance. Examples from the second category are diagnoses of memory or psychological problems. Examples from the third category are diagnoses of high blood pressure, cancer, or obesity. The index is normalized by the sum of all deficits considered such that it lies between 0 and 1. Therefore, a frailty index of 0.2 means that a person has accumulated 20 percent of all deficits potentially observed.====We focus on the PSID dataset for two reasons. First, it allows us to document facts about health dynamics over the entire adult life cycle. Second, given the long panel structure of the PSID, it is an especially useful dataset for estimating a stochastic frailty process. However, the stylized facts that we present are not unique to the PSID data. In an extensive online appendix we show that the same set of facts emerge when obtained using a similarly constructed frailty index based on data from either the Health and Retirement Study (HRS) or the Medical Expenditure Panel Survey (MEPS). Given the similarities in frailty across the datasets, we also use the HRS and MEPS data to provide additional results that cannot be obtained using PSID data. For example, we use the HRS to study the relationship between frailty and mortality, and the MEPS to study the relationship between frailty and total medical expenditures.====Using frailty as our measure of health, we document the following facts about health dynamics over the life cycle. First, mean frailty increases with age, decreases with education, and is higher for women than for men. Also, differences in mean frailty are larger by education than by gender. Second, cross-sectional dispersion in frailty rises with age both overall and when controlling for gender or education, is declining in education, and is slightly higher for women than for men. Third, the cross-sectional distribution of frailty is right skewed both overall and when conditioning on age. This skewness is driven by two features of frailty in the data. First, many individuals have a frailty value of zero, meaning they have none of the deficits considered; the fraction of such individuals gradually declines with age. Second, at all ages, there is a long thin right tail of highly frail individuals; thus, cross-sectional variation in frailty is relatively larger among unhealthy individuals. Finally, frailty is highly persistent. We show that over a two-year period, individuals are much more likely to remain in the same frailty quintile than to move to a different one. Moreover, the probability of moving to a new quintile declines rapidly as its distance from the current one increases.====An alternative measure of health that is commonly used by researchers is SRHS. In the PSID (as in many other surveys) respondents are asked if their health is ‘excellent’, ‘very good’, ‘good’, ‘fair’, or ‘poor’. Frailty and SRHS differ in several ways. Frailty is constructed using objective measures of health, while SRHS is a subjective one. In addition, frailty is a cardinal measure of health that can be treated as a continuous variable, whereas SRHS is a coarse ordinal measure of health. In addition to these qualitative differences between the two measures, we document several quantitative differences. First, SRHS indicates a less rapid decline in health with age as compared to frailty. Second, there is more variation in frailty than in SRHS. Specifically, we document substantial variation in frailty within all SRHS categories even at younger ages. The variation in frailty within SRHS categories is informative about several health-related outcomes, indicating that it reflects true variation in underlying health. Third, we find that frailty is more persistent than SRHS; the conditional probability of remaining in the same SRHS category two years later is lower than the probability of remaining in comparable frailty categories. Finally, we show that frailty tends to perform better than SRHS at predicting health-related outcomes. In particular, the frailty index outperforms SRHS in predicting total medical expenses, mortality, nursing home entry, and social security disability insurance recipiency. However, we also find that both measures have independent predictive power, indicating that each measure provides information about true underlying health that is not contained in the other measure.====Frailty aggregates information from several objective health indicators into a single index. However, it is not the only way to exploit multiple health variables to construct a unitary measure of health. One alternative approach used in the literature, see Poterba et al. (2017), is to take the first principal component of the indicators as the measure of health. Another approach, see Blundell et al. (2020), is to use a subjective indicator of health instrumented by objective indicators. Both approaches can be thought of as alternative ways to construct the weights in the frailty index. For instance, instead of equally weighting each deficit variable, the first principal component of the deficits chooses the weights to maximize the proportion of the variance in the variables that is captured by the index. Similarly, using a subjective indicator of health that has been instrumented by objective indicators is equivalent to choosing the deficit weights to minimize the distance between the index and the subjective health indicator. We show that our facts on health dynamics over the life cycle are robust to these two alternative ways of constructing the frailty index. Given our finding that SRHS has independent power in predicting health-related outcomes, we also show that the facts are robust to including it directly as an additional deficit variable in the frailty index. Finally, we find that all four versions of the frailty index that we consider perform similarly well in predicting health-related outcomes.====In the final section of the paper we propose a statistical model of frailty dynamics over the life cycle that is consistent with our four stylized facts. The model allows for a mass of individuals with zero frailty that gradually declines with age. The dynamics of nonzero log frailty are governed by a deterministic component common across individuals, a fixed individual-specific component, and a stochastic component consisting of an AR(1) shock and a purely transitory shock. The process can easily be embedded into quantitative life cycle models.==== To estimate the model we employ a simulated method of moments procedure that accounts for selection bias due to mortality. This procedure targets several empirical moments. In particular, the parameters governing the individual-specific and stochastic components are identified by the variance-covariance profile of log frailty in the PSID data. We show that after having accounted for mortality effects, the model is able to replicate well both moments targeted in the data and the overall cross-sectional distributions of frailty by age. Finally, we estimate the model separately first by gender and then by education subgroups. The results indicate little variation in frailty dynamics by gender but significant variation across education groups. For instance, there is less overall variation in frailty and the shocks to frailty are significantly less persistent among college graduates than among those without a college degree.====Our paper contributes to a large literature that studies health dynamics over the life cycle and their economic implications. Most of the analyses reported in this literature use SRHS to measure health. For instance, Cole et al. (2019) use SRHS to study the effect of labor and health insurance market policies on the evolution of the cross-sectional health distribution. Capatina (2015) uses SRHS to calibrate a structural model and quantify the impact of health on labor supply, asset accumulation, and welfare. Braun et al. (2017) assess the welfare gains from means-tested old-age social insurance programs in a framework where health dynamics (measured as transitions across SRHS categories) impact medical expenses, mortality, and the risk of losing one's spouse. See also Kitao (2014), French and Jones (2011), and De Nardi et al. (2010), among others. One innovative paper in this literature is De Nardi et al. (2018), who use the persistence of the self-reported ‘bad’ health status to measure the severity of health shocks. They use their estimated health process to quantitatively evaluate the lifetime consequences of bad health. Another notable paper is Ameriks et al. (2020), who use SRHS augmented to include an additional category of health characterized by the need for help with ADLs to explore the drivers of old-age saving behavior.====In the empirical literature that studies the relationship between health and labor supply it is common to augment subjective measures of health, such as SRHS, with objective health indicators.==== In particular, several papers in this literature essentially use the approach we describe above of instrumenting subjective measures with objective ones.==== For instance, the approach is used by Blundell et al. (2016), Bound et al. (1999), and Disney et al. (2006) to document the differential impacts of various health dynamics on retirement. Blundell et al. (2020) compare objective health measures, subjective health measures, and subjective ones instrumented with objective ones and find that conditional on using a large enough set of objective measures, they all imply relatively similar effects of health on the labor supply of older workers.====Other papers in the literature use a more restrictive set of objective health indicators to measure health. For instance, Gilleskie et al. (2017) use body mass to measure health status and study its impact on wages in a life-cycle model. Robinson (1996) and Friedberg et al. (2014) use information on ADLs and IADLs to estimate transition rates across long-term care need states at the monthly frequency. Their model has been used both by researchers (see, for instance, Lockwood (2018)) and the insurance industry to measure long-term care risk. Amengual et al. (2017) use information on ADLs and IADLs to construct an objective discrete measure of health and estimate a panel Markov switching model of old-age health dynamics.==== By using objective indicators of health conditions, these studies avoid the disadvantages of subjective health measures. However, as argued by Blundell et al. (2020), the objective health indicators used in these studies, while perhaps well suited to study particular types of health-related events, provide an incomplete view of overall health since they cover only a subset of health conditions. The frailty index, in contrast, serves as a more comprehensive summary of an individual's health status.====Other notable approaches to measuring health include Dalgaard and Strulik (2014), who, also inspired by the gerontology literature, model health evolution over the life cycle as a deterministic process of deficit accumulation to study the cross-country link between longevity and income known as the Preston curve. This model has been used by Schünemann et al. (2017a) to study the role of gender-specific preferences in accounting for gender differences in life expectancy; by Schünemann et al. (2017b) to study the impact of deteriorating health on the value of life; and by Kopecky and Koreshkova (2014) and Ozkan (2017) to estimate health shock processes by targeting survival probabilities and medical expenditures.====Our paper is also related to a small literature that has estimated stochastic processes of health dynamics using measures of health similar to the frailty index or the variations of it we consider. Using SRHS instrumented by objective measures, Bound et al. (1999) document that health shocks are highly persistent and estimate an autoregressive ordered probit model of health. In the context of studying the relationship between health and labor supply, Blundell et al. (2016) estimate a stochastic health process similar to ours using a health measure that is also based on instrumenting subjective measures with objective ones. They estimate the process using HRS data on 50- to 66-year-olds and do not control for mortality bias. Overall, their estimated parameters are similar to ours, although their AR(1) component is slightly less persistent. Finally, Mitnitski et al. (2006) estimate a stationary discrete Markov process of frailty using two waves of the Canadian Study of Health and Aging (CSHA).====Finally, this paper is related to the literature on estimating earnings and medical expenditure processes. See, for example, Storesletten et al. (2004) and Guvenen (2009) for the estimation of earnings processes, and Hubbard et al. (1995) and French and Jones (2004), who estimate medical expenses processes. Our approach to estimating a frailty process draws heavily from this literature, which has focused on simpler statistical models that can be easily incorporated into quantitative life-cycle models.====The rest of the paper is organized as follows. In Section 2 we present the frailty index as a measure of health and document four stylized facts on life cycle frailty dynamics. In Section 3 we show that these facts are robust to alternative ways of constructing the frailty index, and we compare frailty and SRHS. In Section 4, we present and estimate a dynamic stochastic process for frailty over the life cycle first by using the whole sample and then separately for gender and education subgroups. Section 5 concludes.",The evolution of health over the life cycle,https://www.sciencedirect.com/science/article/pii/S1094202521000533,9 July 2021,2021,Research Article,76.0
"Hull Isaiah,Olovsson Conny,Walentin Karl,Westermark Andreas","Sveriges Riksbank, Sweden","Received 26 November 2019, Revised 9 June 2021, Available online 8 July 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.06.005,Cited by (0),"Using a new dataset of all Swedish housing transactions over the 2009-2017 period, we find that manufacturing's share of employment is positively associated with ","Existing work has shown that labor income risk shifts housing demand, potentially generating house price volatility (Adelino et al., 2018). Since manufacturing is a large and volatile sector in most high income countries, its decline as a share of employment and income since the 1970s has important implications for house price risk.==== We examine this relationship using a new dataset of all property transactions in Sweden over the 2009-2017 period.==== This is particularly important because other drivers of country-level house price volatility, such as financial crises, tend to be transitory; whereas the decline in the manufacturing share appears to be permanent. The decline in manufacturing also tends to be broadly-based geographically within a country, which is not true in general for other regional drivers of house price growth volatility. Furthermore, as recent work has shown (Kuhn et al., 2018), households with below-median income have historically held few assets besides housing. Thus, shifts in house price volatility have substantial implications for portfolio choice and welfare, since house price volatility is associated with consumption volatility.====The dataset we construct allows us to exploit geographic and time variation to identify the impact of manufacturing share on house price growth volatility and risk-adjusted capital gains. Furthermore, it also permits us to evaluate the channels through which dependence on manufacturing affects the housing market. In particular, we measure how house price growth volatility is affected by firm concentration and employment volatility.==== Our comprehensive geographic coverage enables us to measure volatility and risk-adjusted capital gains at all levels of geography. While most of our findings and simulation exercises focus on regional and national volatility, we will also examine how volatility varies within region.====Our dependent variable in most regression exercises is house price growth volatility. We measure it by first computing capital gains on repeat sales and then applying the Davidian and Carroll (1987) method to obtain a measure of instantaneous volatility with both time and geographic variation.==== The first exercise estimates the impact of manufacturing share at the region level in 2008 on our measure of volatility for housing transactions between 2009 and 2017. We find that a 10 percentage point (ppt) increase in the manufacturing share implies a 0.85 to 1.51 ppt increase in house price growth volatility. For the median property, this is equivalent to a 12% to 22% increase in house price growth volatility. These results are largely invariant to specification and remain significant whether we adjust standard errors for heteroskedasticity and autocorrelation or cluster them at the narrowest geographic unit. We also show that the results hold when volatility is aggregated up to the regional level in a cross-sectional regression. Furthermore, the dynamic regressions are robust to the inclusion of geographic fixed effects, which capture the impact of Saiz-style (2010) measures of housing supply elasticity on house price volatility. This suggests that the effect measured in our dynamic regressions is likely to be related to demand-driven factors, such as expected future income and employment.====In addition to measuring the impact of manufacturing share on house price growth volatility, we also try to determine the channels that mediate this relationship. The first channel we explore is employment growth volatility, which may be affected by dependence on manufacturing share at the national, regional, or local level. Higher employment volatility could generate fluctuations in housing demand, which would increase house price volatility. This relationship has been documented in existing work for manufacturing share and output volatility (Carvalho and Gabaix, 2013). Additionally, the literature has demonstrated an association between house price growth and manufacturing share (Case and Mayer (1996) and Howard and Liebersohn (2018)). We find that regional variation in employment growth volatility is positively associated with house price growth volatility. In particular, when we include employment growth volatility in a regression of house price growth volatility on manufacturing share of employment, we find that the magnitude of the coefficient on manufacturing share is reduced by 33%. Furthermore, removing manufacturing share doubles the magnitude of the coefficient on employment growth volatility. This suggests that manufacturing share may partially affect house price volatility through employment growth volatility.====Another channel we examine is the impact of firm concentration on house price volatility.==== In our sample, for instance, 9 of 15 of the largest employers in Sweden are manufacturers, even though manufacturing employs less than 15% of the workers. Thus, employment in areas dominated by manufacturing might be more vulnerable to firm-specific shocks. We test this hypothesis by evaluating how firm concentration affects house price growth volatility. We do this by constructing local Herfindahl-Hirschman Indices (HHIs). A high HHI value implies high firm concentration, indicating that local employment and income are more exposed to firm-specific shocks. Our preferred regression specification includes year-quarter-region fixed effects, time-varying local controls, and property level controls. We find that a one standard deviation increase of the local HHI index is associated with a 1.25 to 1.57 ppt increase in house price growth volatility. For the median property, this is equivalent to a 18% to 23% increase in house price growth volatility. These findings are largely invariant to the choice of specification and are robust to choice of standard error adjustment.====Finally, we evaluate whether the house price growth volatility associated with manufacturing is compensated for by higher capital gains and find that it is not. A 10 ppt increase in manufacturing share is associated with a 0.23 ppt reduction in the housing capital gains Sharpe ratio, which suggests that the decline in manufacturing's share since the 1970s may have made housing a better investment. Similarly, a doubling of firm concentration is associated with a Sharpe ratio reduction of 0.13 ppt.====Beyond our empirical results, we also aggregate our estimates up to the national level and examine the implications of the decades-long decline in manufacturing's share of employment. We show that our results imply a 0.28 ppt decrease in housing capital gains volatility over the 2010-2017 period. In a more speculative exercise, we also show how this could explain part of the reduction in house price growth volatility during the Great Moderation in high income countries, such as Sweden, the U.S., the U.K., and Japan.==== In particular, the 16 ppt manufacturing employment share reduction in Sweden since 1970 could account for a 2.4 ppt (35%) decline in house price growth volatility. Similarly, the 17.5 ppt decline in manufacturing share in the U.S. since 1970 would account for a 2.5 ppt decline in house price growth volatility. It would also account for volatility reductions of 3.6 ppt in the U.K. and 1.6 ppt in Japan. Furthermore, it is possible that this could have improved the attractiveness of homeownership.====The paper is organized as follows. Section 2 describes the data. Section 3 describes our main empirical specification and results. Section 4 examines the channels through which manufacturing affects house price volatility. Section 5 extends our main result. Section 6 discusses the aggregate implications of our results. And finally, Section 7 concludes.",Manufacturing decline and house price volatility,https://www.sciencedirect.com/science/article/pii/S1094202521000557,8 July 2021,2021,Research Article,77.0
Kim Heejeong,"Department of Economics, Concordia University, Canada","Received 27 August 2019, Revised 22 June 2021, Available online 8 July 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.06.006,Cited by (0),"What drove large declines in aggregate quantities during the ====? I study this question by building a dynamic stochastic overlapping generations economy in which households hold both low-return liquid and high-return illiquid assets. In this economy, I explore how aggregate quantities and the distribution of households respond to a recession driven by an increased risk of a further economic downturn.==== is essential in driving these aggregate results. Comparing my model to an economy without illiquid assets, I show that household differences in both liquid and illiquid assets play a crucial role in amplifying the effects of an increase in disaster risk.","As seen in Fig. 1, during the Great Recession, the U.S. economy experienced sharp declines in aggregate quantities; the largest falls in consumption and investment, relative to trend, were around 4 and 18%, respectively. These are unusually large declines in aggregate quantities given the relatively small drop in measured TFP, around 2%.==== Furthermore, the micro data shows that most households increased their savings rates during this recession.====Existing quantitative macroeconomic business cycle models have difficulty explaining such large falls in aggregate quantities, in particular consumption, with the observed small fall in TFP. For example, in their study of the Great Recession, Krueger et al. (2016) only explain a 2.5% drop in aggregate consumption, following a 4% drop in TFP. In addition, they predict a counterfactual fall in households' savings rates in such a recession.====In this paper, I provide a new quantitative framework that can reconcile macro- and micro-observations seen in the Great Recession. Specifically, in contrast to existing work, I explore aggregate dynamics when households fear a further economic downturn — economic disaster — while in a recession. In the face of such increased disaster risk, precautionary savings rise and consumption decreases. Importantly, this occurs whether or not there is an actual fall in income.====The assumed rise in the risk of an economic disaster, although parsimonious, is consistent with several salient features of the Great Recession. First, using the Michigan Survey of Consumers, De Nardi et al. (2012) find a significant fall in the expected real income growth for households during the Great Recession. Consistent with their empirical findings, in my model economy, a rise in the probability of economic disaster decreases households' expected future income. Second, estimating the distribution of aggregate shocks to the economy using capital returns in 2007 and 2009, Kozlowski et al. (2020) indeed find a significant rise in left-tail aggregate risk in 2009, compared to the pre-crisis distribution. Such an increase in left-tail risk is reproduced by an increase in disaster risk in my model economy. However, it is important to note that an increased probability of a disaster on its own fails to explain sharp declines in aggregate quantities. As I will show later, an important channel to amplify such an aggregate shock is household differences in liquid and illiquid asset holdings.====I study the Great Recession in a quantitative dynamic stochastic general equilibrium overlapping-generations (DSGE OLG) framework in which households' asset holdings vary with respect to returns and liquidity. An OLG framework is essential to explain the realistic consumption-savings behavior of households, as infinite-lived agent models generate counterfactually high expenditure and low savings rates. In this framework, I explore how aggregate quantities and the distribution of households, over asset holdings, respond to a recession driven by an increased risk of a further economic downturn and a persistent negative TFP shock.====The model economy has three important features. First, households not only make consumption-savings decisions but also choose how to allocate savings across two types of assets: low-return liquid assets and high-return illiquid assets. As in Kaplan and Violante (2014), households have to pay fixed transaction costs to adjust illiquid wealth. These fixed costs determine a region of inaction where illiquid assets are not adjusted. Costly portfolio adjustment implies liquidity risk in wealth, and households with different portfolios have different abilities to smooth consumption. Second, the aggregate economy faces a time-varying aggregate risk of economic disaster. Economic disasters are modeled as catastrophic declines in TFP, which lead to sharp declines in both wages and the price of illiquid assets as well as a large unemployment risk. I assume that the probability of such a disaster sharply increased during the Great Recession.==== Lastly, markets are incomplete; households face both uninsurable idiosyncratic earnings and unemployment risk. Unemployment risk, an important force for precautionary savings, rises in a recession, as in Krusell and Smith (1998) and Krueger et al. (2016).====The calibrated economy reproduces much of the distribution of net worth, illiquid assets, and liquid assets seen in the 2007 SCF data. Moreover, while not targeted, fixed transaction costs for illiquid assets lead to model predictions for the share of illiquid assets, as a fraction of total assets, across the distribution of households, over wealth and age, that resemble the data. The model's disaster shock process is also consistent with the frequency and severity of disasters reported in Barro (2006).====Following a rise in disaster risk and an empirically consistent fall in TFP, the benchmark model successfully predicts sharp falls in aggregate quantities during the Great Recession. In particular, it explains a 3% decline in aggregate consumption and a 16% drop in investment while, in the data, the corresponding declines are 3.6% and 18%, respectively. A heightened risk of disaster leads to a large negative wealth effect, which operates differently across households varying in age and wealth. For young and wealth-poor households, whose primary income source is labor earnings, a higher probability of a sharp fall in future wages and higher unemployment risk during an economic disaster decrease their expected future income. For old and wealthy households, which hold more illiquid assets, a fall in the expected future return on their savings decreases their expected future income. Overall, an increased probability of lower expected future income gives rise to a larger fall in current consumption and an increase in precautionary savings. Moreover, as illiquid assets are costly to adjust, households increase their precautionary savings in safe liquid assets, which allow for better consumption smoothing.====In the model, illiquid assets correspond to physical capital in firms. A rapid drop in investment results from the portfolio adjustment of some wealthy households toward safe liquid assets. While many households do not change their holdings of illiquid assets, wealthy hand-to-mouth households, which save most of their wealth in illiquid assets, are more likely to liquidate part of their illiquid assets to smooth consumption in a recession. This amplifies the reduction of investment in physical capital.====Importantly, I find that allowing household differences in their holdings of both liquid and illiquid assets is crucial for explaining sharp falls in aggregate quantities following heightened disaster risk. In an otherwise comparable economy but with liquid physical capital assets, an increase in disaster risk has little effect on aggregate dynamics relative to a recession driven by a shock to current TFP alone. In this single asset economy, the negative wealth effect from an increase in disaster risk is largely offset by a substitution effect. Heightened disaster risk reduces the expected future return to capital, and thus savings. Without illiquidity in assets, households strongly respond to a fall in the expected return to savings by decreasing their savings and increasing consumption. In contrast, in the benchmark economy with illiquid assets, costly portfolio adjustment weakens the standard substitution effect. Specifically, around 80% of households do not adjust their holdings of illiquid assets in a recession. This sharply decreases the magnitude of the substitution effect and amplifies the negative wealth effect compared to a model without illiquid assets.====Finally, to explore how well the model predicts households' behavior during the Great Recession, I evaluate the model's consistency with the changes in the joint distribution of household income, consumption, and expenditure rates seen in the PSID. Specifically, I examine the change in the annualized growth rates of disposable income and consumption, across wealth quintiles, between normal and recession times. I also study the percentage point change in expenditure rates. First, my model economy is consistent with a slowdown in the growth of disposable income and consumption, across all wealth quintiles, during the Great Recession. Second, similar to the expenditure rate changes observed in the PSID over this period, a significant fraction of households increase their savings rates in a model recession accompanied by an increased risk of further economic downturn. A higher disaster risk leads to strong precautionary savings across households, allowing the model to better explain the rise in savings rates during the Great Recession compared to other existing business cycle models, such as Krueger et al. (2016).==== Lastly, I show that, although they hold a negligible share of total wealth in the economy, wealth-poor households still play a significant role in determining the change in aggregate consumption during the recession.====The numerical method developed to solve the model may be of independent interest. First, I develop a two-stage approach to solve decision rules that involve a portfolio choice. This two-stage approach defines an intermediate value function over cash-on-hand, the choice of illiquid assets, and idiosyncratic types. Next, I extend the backward induction method of Reiter (2002, 2010) to solve a stochastic life-cycle model with a bivariate cross-sectional distribution of assets.==== Compared to other existing methods for solving dynamic stochastic general equilibrium with heterogeneous agents, this method has two advantages. First, this method does not linearize the model and thus does not rely on ==== in terms of aggregate shocks, as in Ahn et al. (2018). This allows the model to have a direct role for aggregate risk in household decisions and their distribution. Second, the backward induction method is robust when there is a large shock, as it allows the distribution of households to vary in rich ways. This is especially important in a model with multiple assets when many households show large changes in their asset holdings following aggregate shocks.====The remainder of the paper is organized as follows. Section 2 discusses the related literature. Section 3 presents the model economy. Section 4 discusses the calibration, and Section 5 discusses numerical methods. Section 6 presents the quantitative results for aggregate and household dynamics. Section 7 concludes the paper.","Inequality, disaster risk, and the great recession",https://www.sciencedirect.com/science/article/pii/S1094202521000545,8 July 2021,2021,Research Article,78.0
Cochrane John H.,"Hoover Institution, Stanford University, 434 Galvez Mall, Stanford CA 94305, USA","Received 15 October 2020, Revised 30 May 2021, Available online 17 June 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.06.001,Cited by (3),"I construct a simple model with ==== and ==== targets, closed by fiscal theory of the price level with long-term debt and fiscal and ==== rules. Fiscal surpluses rise following deficits, to repay accumulated debt, but surpluses do not respond to all values of unexpected inflation and deflation. This specification avoids common puzzles and produces reasonable responses to fiscal and monetary policy shocks. It allows an easy translation of any new-Keynesian model, and it allows one to study a whole sample with active fiscal policy.","This paper advances the fiscal theory of monetary policy, bringing us closer to a realistic model useful for policy analysis. A “fiscal theory of monetary policy” uses active fiscal policy, the government debt valuation equation, in place of active monetary policy, an interest rate rule that induces explosive dynamics, to complete the determination of inflation and output in an otherwise standard macroeconomic model. (The active-passive terminology is from Leeper (1991).)====I develop the model and I analyze of the effects of fiscal and monetary policy shocks. The model can produce reasonable responses to such shocks. The responses illustrate a variety of interesting mechanisms.====The central innovation is a fiscal policy process in which the government can repay deficits with subsequent surpluses, partly or in full, yet fiscal policy remains active. In one interpretation, the government raises fiscal surpluses in response to increases in the value of debt brought on by past deficits and by higher real interest rates, but the government does not respond to changes in the value of debt resulting from unexpected inflation that differs from the unexpected value of a stochastic inflation target. If a big deflation were to break out, for example, the government would not raise taxes or cut spending to repay the higher real value of debt, generating a real windfall for bondholders. The government would ignore the rise in the real value of its debt, or the government might instead run an inflationary fiscal stimulus. That expectation stops the deflation from breaking out in the first place.====Current fiscal-theory models, reviewed below, implicitly specify that surpluses either respond to all changes in the value of debt, generating passive fiscal policy, or to none at all, generating active fiscal policy. Under that specification of active fiscal policy, along with positively correlated fiscal disturbances, the government cannot borrow in real terms, as it cannot promise to repay deficits by subsequent surpluses. Instead the government finances deficits entirely by inflating away outstanding debt. Deficits ==== the value of debt. Inflation is large, volatile, countercyclical (higher in recessions) and correlated with deficits. The real returns of government bonds are volatile, lower in recessions, and offer stock-like average returns. These counterfactual predictions are not present in these models' passive-fiscal specification. But since surpluses then validate any value of unexpected inflation, fiscal policy cannot help to determine unexpected inflation.====By generalizing the active-fiscal regime to allow partial or full repayment of debts, the fiscal policy specification of this paper can expand the applicability of the active-fiscal regime. Indeed, the surplus process is so flexible that ==== equilibrium of an active-money specification can be rewritten as an equilibrium of the active-fiscal specification and vice versa.====This “observational equivalence” is an invitation to easily construct fiscal theory models by importing standard new-Keynesian ingredients. It then invites us to look at and evaluate fiscal foundations of those models and to ask quite different policy questions.====This observational equivalence opens the door to an alternative to the whole approach of measuring labeling periods by equilibrium-selection regime, as periods “monetary dominance” vs. “fiscal dominance,” and ascribing good or bad outcomes to that switch. For example, a typical result is to label the period after 1980 as active-money, and the 1970s as passive-money, and to understand the inflation of the 1970s centrally as an unfortunate result of that determinacy regime. But if we can equally describe all the data with either determinacy regime, then we need not make this diagnosis. Instead, we may return to understanding economic performance as a result of different shocks, or of policy-rule parameter shifts within a determinacy regime. Parameter regimes need not imply equilibrium-selection or determinacy regimes.====I choose minimal additional ingredients that exhibit a plausible model, to examine the effect of the fiscal policy specification in a transparent and well-understood environment, and to argue that the general framework is a plausible foundation for more detailed model-building. I specify long-term nominal government debt with a geometric maturity structure. Long-term debt helps the model to produce a negative response of inflation to unexpectedly higher nominal interest rates. I use standard textbook new-Keynesian IS and Phillips curves, despite their well-known empirical shortcomings. This specification allows me to focus on the effects of the novel fiscal specification. Fiscal and monetary policy follow standard rules, responding to output and inflation, plus persistent disturbances.====The main calculations are model responses to persistent fiscal and monetary policy shocks. A deficit shock leads to a protracted inflation, and via the Phillips curve it leads to an output expansion. When monetary policy endogenously reacts to inflation, monetary policy moderates the initial inflation and output responses, by spreading inflation forward. The protracted inflation response contrasts with simple fiscal theory models that produce an unrealistic one-time price-level jump. Deficits also lead to a long string of future surpluses which repay the accumulated debts, and the surplus responds to debt in equilibrium. Both observations could lead one to falsely infer a passive fiscal regime. An unexpected monetary policy shock leads to a protracted disinflation, and an output decline. Policy rules again smooth the responses.====In sum, a completely active-fiscal theory of monetary policy model can be easily built, can surmount classic criticisms, can produce reasonable responses, can evaluate policies, and can avoid pathological predictions.",A fiscal theory of monetary policy with partially-repaid long-term debt,https://www.sciencedirect.com/science/article/pii/S1094202521000430,17 June 2021,2021,Research Article,80.0
Cochrane John H.,"Hoover Institution, Stanford University, 434 Galvez Mall, Stanford, CA 94305 USA","Received 10 December 2020, Revised 29 May 2021, Available online 17 June 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.06.002,Cited by (3),"Unexpected inflation devalues nominal government bonds. It must therefore correspond to a decline in expected future surpluses, or a rise in their discount rates, so that the real value of debt equals the present value of surpluses. I measure each component using a vector autoregression, via responses to inflation, recession, surplus and discount rate shocks. Discount rates account for much inflation variation, for the cyclical pattern of inflation, and why persistent deficits often do not cause inflation. Long-term debt is important. In response to a fiscal shock, smooth inflation slowly devalues outstanding long-term bonds.","The real value of nominal debt equals the present value of real primary surpluses. Higher inflation devalues nominal government debt. Higher inflation must therefore correspond to lower surplus/GDP ratios, lower GDP growth, or higher discount rates for government debt. I develop a set of linearized identities that expresses this identity. I measure the components via impulse-response functions of a simple vector autoregression (VAR).====I look first at an unanticipated movement in inflation. Two thirds of the total inflationary effect of that shock corresponds to a change in discount rates, one third from a change in growth, and essentially none to a change in surplus/GDP ratios.====I look next at a shock in which both inflation and growth move unexpectedly and together. This exercise is motivated by events such as 2008-2009. There is a big recession, with large and persistent deficits. Yet inflation falls, raising the real value of nominal debt. How can this be? Well, perhaps people expect higher subsequent primary surpluses to pay back the cumulated deficits, and more. Aside from its implausibility, I do not find this pattern in the data. But nominal and real interest rates on government debt fall sharply, which raise the value of government debt, a deflationary force. I find that the decline in expected returns is large and persistent enough quantitatively to account for inflation shocks in a recession, and vice versa in a boom.====I also examine persistent shocks to surpluses and shocks to discount rates. These shocks come with essentially no inflation. Shocks to surpluses are highly correlated with shocks to discount rates, so the surplus and discount rate terms of the present value formula largely offset. Viewed in ex-post terms, persistent deficits come at the same time as low returns. Low returns bring back the value of debt, without needing repayment via later surpluses, or devaluation via an initial inflation. The strong correlation between discount rates and deficits provide fiscal roots of the absence of inflation in the presence of large variation in surpluses and discount rates.====The first and third observations are not contradictory. There are multiple sources of variation in the data. Not all business cycles are alike. When we isolate a shock to inflation, we see events in which discount rates and deficits do not offset. When we isolate a shock to discount rates or deficits, we see a different slice of data, in which they do offset and there is not much inflation or deflation.====I also find an important role for long-term debt. Simple models focus on one-period debt, and price-level jumps devalue such debt. With long-term debt, a slow inflation can devalue long-term bonds when they come due. Expectation of such future inflation lowers nominal bond prices, restoring present value balance in place of a price-level jump. This mechanism is evident in the data, with expected future inflation accounting for large fractions of changes in the present value of debt.====I interpret the results through the lens of the fiscal theory of monetary policy: models with interest rate targets, fiscal theory of the price level, and potentially sticky prices, as described in Cochrane (2020a), Cochrane (2020b). (More literature below.) In this interpretation, changes in expected surpluses and discount rates ==== unexpected inflation. In this interpretation, we study the fiscal roots rather than the fiscal consequences of inflation. This paper establishes a set of facts that will be useful for constructing such models. My causal language below refers to this interpretation.====But the identities whose terms I measure hold in almost all macroeconomic models used to quantitatively address inflation, and therefore form a widely useful set of stylized facts for monetary and fiscal interaction. The computations of this paper are deliberately “measurement without theory”. I do not estimate any structural parameters, identify any structural shocks, or test one model vs. another. A “shock” only means a movement in a variable that is not forecast by the VAR, without structural interpretation.====In particular, standard new-Keynesian/DSGE models posit an opposite causality. Equilibrium-selection policy by the central bank determines unexpected inflation. Fiscal policy reacts “passively”, raising or lowering surpluses to validate inflation-induced changes in the value of government debt. These fiscal underpinnings are not often examined, but they should be as they are also important parts of the model, just as monetary-fiscal coordination is important to classic monetarist thought. The results of this paper can also be interpreted as measures of the fiscal adjustments to inflation that a standard new-Keynesian model must envision. The fact that discount rates do much of the adjusting, and the measured time-path of surpluses following inflation shocks, are important fiscal underpinnings of such models.====Since the analysis is based on identities, and since I make no effort to identify structural shocks of a model or exogenous policy shocks, the empirical results do nothing to establish one or another causal story. But which element in an identity moves – whether surpluses or discount rates account for inflation-induced variation in the value of government debt – is still an interesting measurement, that bears on the construction of any theory.====More narrowly, this paper addresses a common attempt at armchair refutation of fiscal theory: We have huge debt and deficits, and no inflation. Debt and deficits increase in recessions, where inflation declines. The theory must be wrong. No. First, a low real interest rates quantitatively account for the disinflation and rise in the value of government debt in recessions. Second, since the government debt valuation equation holds equally in conventional monetary theories, if there is a puzzle in the fiscal foundations of inflation, it applies equally to conventional theories. It does not reject fiscal theory in favor of those other theories.====As a paper on pure facts, I do not offer here theory or evidence on ==== surpluses or expected returns on government bonds vary as they do. Given their variation, inflation makes fiscal sense.",The fiscal roots of inflation,https://www.sciencedirect.com/science/article/pii/S1094202521000442,17 June 2021,2021,Research Article,81.0
Kolasa Marcin,"SGH Warsaw School of Economics, Al. Niepodległości 162, 02-554 Warsaw, Poland,International Monetary Fund, 700 19th Street, NW, Washington, DC 20431, United States","Received 14 May 2020, Revised 5 June 2021, Available online 10 June 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.06.003,Cited by (0),.,"Despite ongoing development of local financial markets and progress in inflation control achieved during the last decades, foreign currency lending remains an important feature of the financial structure in many countries, raising concerns among policy makers. According to the Financial Soundness Indicators published by the IMF, the (unweighted) average share of foreign currency and foreign-currency-linked borrowing in total gross outstanding loans amounted in 2014 to nearly 30% in the 66 surveyed countries (see Table 1). While this proportion is non-negligible even among developed euro area economies, it is particularly high in Central and Eastern Europe (CEE), where it comes close to a half. As Table 2 reveals, the currency substitution in this region is not restricted to the sector of non-financial firms, but also prevalent among households, and in particular related to financing house purchases. While the euro adoption by Slovenia, Slovakia and the three Baltic states effectively solved this problem in these economies, it took strong actions by the government or financial supervision in other countries to curb lending in foreign currency to households.====A number of papers have tried to rationalize the observed widespread and persistent dollarization of lending. Most of this literature focuses on the supply side factors related to the functioning of international financial markets and banks.==== While all these channels might help understand the currency composition of loans to firms, their relevance for borrowing by households is limited. Household debt is predominantly domestic and not international, which means that preferences of foreign lenders should not be relevant in this context. Indeed, based on the bank-level survey data, Brown and De Haas (2012) argue that foreign currency lending to households in emerging Europe has not been driven by foreign banks due to their easier access to foreign wholesale funding. If anything, lending in foreign currency should be rather less attractive to banks because of higher credit risk. However, the demand-side rationalizations existing in the literature are also not very appealing in this context. In particular, households usually earn in domestic currency and hence natural hedging cannot be the major motive in choosing the foreign currency. As a result, popular explanations for dollarization of household debt are based on some irrational behavior, borrowers' limited ability to assess exchange rate risk or speculative motives, like those created by the empirically observed violation of the uncovered interest rate parity (UIP) or implicit government bailout guarantees in case of severe exchange rate depreciation (Ranciere et al., 2010).====This paper offers an alternative and novel explanation for why foreign currency loans, and foreign currency mortgages in particular, have become so popular in some countries whenever banks made them available at non-discriminatory terms, as it was the case in the CEE region. At the heart of our argument is what we call the debt limit channel, which, whenever the exchange rate is expected to depreciate, allows credit constrained agents to backload the real payments on their loans (i.e. effectively hold more debt over the loan duration) if they choose to borrow in foreign currency.====To illustrate how this backloading works, let us use a simple deterministic example. Consider a financially constrained agent who at period 0 can take a 2-period adjustable-rate loan denominated either in local or foreign currency.==== The amount that can be borrowed is independent of denomination and equal to 100 units of either domestic or foreign currency, assuming that the nominal exchange rate at time 0 equals unity. Loans are repaid in equal principal payments of 50 units of either local or foreign currency, depending on the chosen denomination. The real interest rate is constant and normalized to zero. The nominal interest rate charged on outstanding debt in the domestic currency is 1% for period 1 and 0% for period 2, while in the case of a foreign currency loan it equals zero in both periods. Financial markets, at which the agent cannot directly participate, ensure no arbitrage between one-period holdings of either of the two currencies, which implies that the nominal exchange rate depreciates by 1% in period 1 and then remains constant. With these assumptions, the cash flows related to loan repayment, expressed in local currency units, are ==== in period 1 and ==== in period 2 for domestic currency borrowing, and ==== in both period 1 and 2 for foreign currency borrowing. Note that the present discounted value of both streams of payments is the same and equal to ====, so what makes them distinct is their different distribution over time: exchange rate depreciation effectively backloads real loan payments.====Naturally, if the agent in our example was not financially constrained, she would be indifferent between the alternative repayment schedules. For a borrower that is constrained over the loan duration, it does matter how much debt she can effectively hold at every point in time. Hence, another way of looking at our mechanism is by noting that a multi-period adjustable rate loan is equivalent to a sequence of one-period loans, but with a special nominal rollover commitment. This commitment is different for local and foreign currency borrowing because exchange rate movements affect the local currency value of the latter. As a result, exchange rate depreciation acts as if the agent has a higher debt limit for the debt rollover if she borrows in foreign currency. This can be also seen in our illustrative example, in which outstanding debt at the end of period 1, expressed in local currency units, is equal to 50 in the case of local currency borrowing, and 50.5 if borrowing is denominated in the foreign currency.====This simple example can be extended to a more realistic one, in which a mortgage contract is taken for more than two periods, and where additionally agents are allowed to borrow short term subject to some standard constraint, e.g. up to a fraction of their labor income. The logic presented above survives as long as the nominal exchange rate is expected to depreciate (the interest rate differential is positive) and the short-term borrowing constraint is binding at some time in the future. A foreign currency loan is more attractive as it essentially allows to hold more debt. One of the insights from this discussion is that foreign and domestic currency loans granted at apparently equal terms (loan-to-value ratio, repayment schedule) are not perfect substitutes to credit constrained agents, even if there is no risk and the interest rate parity holds exactly, i.e. the exchange rate depreciates whenever there is a positive difference between domestic and foreign interest rates. Hence, our argument does not rely on some speculative behavior of (usually considered risk-averse) households, like betting on an appreciation of the local currency, or their misperception of exchange rate risk. Instead, for the debt limit channel to bias household mortgage choices towards foreign currency, the following conditions must hold: (i) the difference between domestic and foreign interest rates is positive, (ii) borrowers are credit constrained, (iii) mortgage debt is long-term, (iv) collateral constraint applies to new borrowing rather than outstanding debt, i.e. it is established on newly purchased assets. We argue that the available evidence indicates that these conditions fit very well to the mortgage markets in the CEE region.====If we leave a deterministic world and allow for uncertainty, borrowers' portfolio choices additionally reflect risk considerations. If households earn in local currency, exchange rate fluctuations can make foreign currency borrowing relatively risky. In particular, unexpected exchange rate depreciation negatively affects the balance sheets of those who hold foreign currency debt. For sufficiently high risk, the benefits of backloading debt repayment may turn out to be too weak to justify mortgage debt dollarization. Hence, any evaluation of the empirical relevance of the debt limit channel must confront its strength with a realistic description of risk.====To this end, we formalize the simple example and discussion offered above by embedding it into a quantitative general equilibrium framework. In this model, agents can borrow from abroad either in domestic or foreign currency, face uncertainty about their income and nominal exchange rate movements, and their borrowing is subject to a collateral constraint that depends on the value of newly purchased housing, hence resembling new mortgages as e.g. in Garriga et al. (2017). When calibrated to Poland, which can be considered a representative CEE economy, the model produces a sizable share of foreign currency mortgage debt. It also generates a number of predictions consistent with the empirical evidence from the CEE region.====Because of the presence of house prices in the collateral constraint, the model features a pecuniary externality: households do not internalize how their total borrowing and its currency composition affects house prices. Comparing the obtained decentralized outcome to allocations under constrained-optimal time-consistent policy as in Bianchi and Mendoza (2018) reveals that the former is characterized by overborrowing and bias towards foreign currency. To our knowledge, the latter result is new in the literature and suggests that a substantial part of the observed dollarization of mortgages in the CEE region was inefficient and warranted at least some intervention by a financial regulator.====Apart from the papers already mentioned, this work is related to several strands of the literature, of which we list only a couple of representative contributions. The determinants of mortgage dollarization in the CEE region was investigated empirically by e.g. Barajas and Morales (2003), Luca and Petrova (2008), Cuaresma et al. (2011) or Fidrmuc et al. (2013). General equilibrium analysis of the currency composition of borrowing by emerging market economies is offered by Korinek (2011), while Brzoza-Brzezina et al. (2017) use a DSGE model to examine the effect of foreign currency borrowing on monetary and macroprudential policy transmission, treating the composition of household debt as exogenous. However, both of these papers have contracts lasting only one period. Modeling collateral constraints in housing was pioneered by Iacoviello (2005), and the multi-period extensions can be found e.g. in Garriga et al. (2017) or Bluwstein et al. (2020). Pecuniary externalities associated with collateral prices are studied by e.g. Mendoza (2010), Jeanne and Korinek (2010), Bianchi (2011) or Benigno et al. (2013). However, these papers deal with total overborrowing and not with (possibly inefficient) currency composition of debt.====The rest of this paper is organized as follows. Section two presents the model and defines its decentralized and constrained-efficient equilibrium. Section three conducts a quantitative and qualitative analysis with the model. Section four concludes. Additional details on the model solution and data are delegated to the Appendix.",Equilibrium foreign currency mortgages,https://www.sciencedirect.com/science/article/pii/S1094202521000454,10 June 2021,2021,Research Article,82.0
Yao Yao,"Victoria University of Wellington, New Zealand","Received 15 December 2019, Revised 26 May 2021, Available online 6 June 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.05.005,Cited by (0),"This paper examines the role of social and cultural norms regarding fertility in women's HIV risk in Sub-Saharan Africa. Fertility is highly valued in most African societies, and premarital fertility is often encouraged to facilitate marriage. This incentivizes unprotected premarital sex and escalates HIV risk. I build a rich lifecycle model linking women's decisions concerning sex, fertility and education to HIV infection and calibrate it to match Kenyan data. Quantitative results show that premarital fertility motives account for 13 percent of HIV prevalence among young Kenyan women and that a combination of fertility motives and contraception cost accounts for nearly one-third of this prevalence. Various policies are evaluated. While an HIV treatment subsidy would reduce HIV prevalence to the greatest extent, its effectiveness would be severely mitigated by increased risky sex.","HIV/AIDS is a major health risk facing young women in Sub-Saharan Africa (SSA). Premarital sex is arguably the leading cause of their infection. However, there was no decline in the share of women engaging in premarital sex or having premarital pregnancy in many SSA countries when HIV spread rapidly in the 1990s. In this paper, I argue that social and cultural norms regarding fertility may have played a substantial role in risky sexual behavior of young African women. I build a quantitative lifecycle model linking women's decisions concerning sex, fertility and education to HIV risk and calibrate it to match Kenyan data. I find that the motives to bear premarital children account for 13 percent of the HIV prevalence among young women in Kenya and that a combination of fertility motives and contraception cost accounts for nearly one-third of this prevalence. I evaluate three policies with the same program cost. While an HIV treatment subsidy would reduce HIV prevalence to the greatest extent, its effectiveness would be severely mitigated by increased risky sex.====Fertility, or the ability to bear children, is highly valued in most African societies. Childless women suffer discrimination, stigma and ostracism (Sembuya, 2010). For unmarried women, premarital fertility is often encouraged in order to facilitate marriage (Meekers and Calvès, 1997). Strong motives to bear children, however, intensify women's exposure to HIV infection by incentivizing unprotected sexual activities. The risk of these activities increases if they are premarital, a time when men commonly have several partners and are less committed to the relationships. Indeed, overlapping sexual relations are considered to be a major cause of the severity of the HIV epidemic in Africa (Epstein and Morris, 2011). Unprotected sex also enhances the chance of other sexually transmitted infections (STIs) which are found to increase the HIV transmission rate dramatically (Oster, 2005). Consistent with this hypothesis, data from the Kenyan Demographic and Health Surveys (KDHS) show that a woman who has a premarital pregnancy is significantly more likely to have been HIV infected, and that better-educated women are less likely to have a premarital pregnancy, suggesting a lower HIV risk for advantaged women through fertility motives.====Taking the observations delineated above into account, I construct a lifecycle model that relates a woman's decisions concerning sex, fertility and education to HIV risk. The key element of the model is the decision to choose premarital fertility over premarital sex protection. A high level of protection lowers the probability of having a premarital birth and reduces HIV infection risk, but incurs a higher cost of contraception, especially the utility cost, and the lack of a premarital birth lowers the chance of getting married. The model also incorporates a premarital sex-type choice between committed and casual relations. Choosing casual sex may bring more transfers of income from partners, but delays marriage and increases HIV risk; however, the utility cost of contraception is lower in casual sex than in committed sex. The model features four key events following Poisson processes. The first is a premarital birth, which may facilitate marriage but raises childrearing costs. The second is marriage, which may increase transfers from the partner and enhance the enjoyment of children; it also provides a relatively safe haven against HIV infection. The third is HIV infection, which lowers productivity and increases mortality. The last event is death, which is more likely to occur when one is HIV infected. The probabilities of all these events depend crucially on women's premarital sex decisions. As such, the model incorporates two main channels of HIV risk – a fertility channel and a sex-type channel. The interaction of these two channels is critical for understanding behavior and risk. For example, women who choose committed sex tend to use less protection. Since they are the ones who favor earlier marriage and more children, they outweigh the marital prospect over the high contraception cost and less transfer in a committed relationship. The modeling of the two dimensions of risky activity is in line with Duflo et al. (2015), which, through a field experiment in Kenyan schools, underscores the distinction of the sex-type choice from that of unprotected sex as an essential component of risky behavior.====Importantly, these two channels can work differentially for women with different opportunities of education, which helps to explain the very mixed findings of the relationship between education and HIV infection in the empirical literature. Specifically, women with better educational opportunities are more likely to choose casual sex with a high level of protection, as childrearing and early marriage would undermine their education efficacy. On the contrary, women with fewer educational opportunities tend to have more premarital children and are more likely to choose committed sex, due to a lower opportunity cost of childrearing and a better incentive to enter marriage earlier. Thus, the model suggests two different ==== HIV risk factors for the two groups of women – a higher risk from the fertility channel for the less-educated and a higher risk from the sex-type channel for the better-educated.====The model is calibrated to match Kenyan data, with key moments taken from the KDHS regarding women's sex, fertility, marriage and HIV prevalence. I categorize women by their level of education and preference for children, and solve the lifetime decisions of each group numerically. The calibrated model matches data on premarital fertility, marriage age and HIV prevalence for women of both the more- and less-educated groups very well. I then conduct counterfactual experiments pertaining to fertility motives, and policy experiments of income, education and HIV treatment subsidies. All policies are implemented based on the same program cost, so that their effectiveness is comparable.====Counterfactual experiments show that fertility motives play a substantial role in the sexual behavior and HIV risk of young Kenyan women. If premarital births did not facilitate marriage, the fraction of women with premarital fertility would drop by more than half and the HIV prevalence rate of young women would drop by 13 percent. When both premarital fertility motives and contraception cost were eliminated, the premarital fertility rate would drop by over 80 percent and HIV prevalence of young women by nearly one-third, which are considerably more than the decline when fertility motives or contraception cost alone were removed.====Moreover, while the literature highlights measures such as marriage promotion in reducing HIV transmission (Greenwood et al., 2017), my experiments indicate that the effect of such policy depends on the way of promoting marriage. While an exogenous increase in marriage probability, which can be viewed as an improvement in matching efficiency of the marriage market, would reduce HIV prevalence by lowering marriage age directly, a pure increase in the utility of marriage would incentivize more premarital childbearing, increasing risk. The latter result is in contrast to that of Greenwood et al. (2017) which shows that raising the utility of marriage would lead to safer sex, without taking premarital fertility motives into account.====Three additional policies are evaluated based on the same program cost: an income subsidy that directly transfers cash to women, an education policy that grants more educational opportunities and supports women for education conditional on their school attendance and a treatment subsidy that subsidizes HIV-infected women for receiving antiretroviral therapy (ART). Results show that, among the three policies, the ART subsidy would be the most effective in reducing HIV prevalence, as the treatment can also effectively curb HIV transmission, generating a large positive externality. The HIV prevalence of young Kenyan women would be 41 percent lower when all the HIV positive women were treated. However, the effectiveness of the policy would be severely mitigated by increased risk-taking activities. The education policy would be the most effective in raising premarital sex protection, especially for those who initially had no educational opportunity. Although it would also induce casual sex, the policy would lower the HIV prevalence of young women by 14 percent. The income subsidy would have little effect on either premarital pregnancy or HIV infection.====Furthermore, to explain the decline in HIV prevalence in Kenya since the late 1990s, I extend the model to incorporate two facts in the data: an improvement in knowledge about HIV prevention and an improvement in knowledge about condom sources. The extended model features imperfect knowledge about HIV transmission which improves from 1998 to 2008 and a reduction in the utility cost of contraception during these years. It explains the trend of condom usage and HIV prevalence well.====This paper contributes to the literature in three main aspects. First, it documents a new channel of HIV risk for young African women through premarital fertility choices, which has a substantial influence on sexual behavior and HIV outcomes but has largely been ignored in the economics literature. Second, while lifecycle models have the advantage in investigating intertemporal choices and dynamic responses, they have rarely been used for studying disease infection. Even the more general choice-theoretic models related to disease infection were few prior to the recent emergence of the COVID-19 literature. This paper contributes to this literature by developing a rich theoretical model in which various dimensions of behavior, such as sex, fertility and education, and HIV risk are interrelated in a dynamic lifecycle setting. The modeling of both fertility and sex-type channels also provides an explanation for the differential risk faced by women at different education levels. Third, with the diagnosis of the new fertility mechanism, the paper sheds new light on HIV policy through various counterfactual and policy experiments based on the quantitative model.====This paper is first related to the literature on choice-theoretic models of infectious diseases. While the significance of the COVID-19 pandemic inspired a rise of this literature in the past year, use of choice-theoretic models was previously uncommon in studying infectious diseases, despite their usefulness in investigating behavioral responses and policy implications compared with the epidemiological models; and most existing ones were not designed for quantitative analysis (e.g., Kremer, 1996; Magruder, 2011).==== The research most relevant to my paper is Greenwood et al. (2019b). Both papers build dynamic lifecycle models with rich dimensions of sexual behavior to study HIV infection and are suitable for quantitative analysis.==== In their paper, the authors develop a general equilibrium model in which agents of two genders engage in a directed search in different sexual markets, including markets of long-term sex (i.e., marriage, with no protection), casual sex with condoms and unprotected casual sex, facing different transfers and HIV infection risks in different markets. They highlight the advantage of a general equilibrium framework relative to epidemiological models and small field experiments in studying behavioral responses and endogenizing aggregate HIV risk. My paper differs from their paper in the following main aspects. First and most important, Greenwood et al.'s paper does not model fertility (although they do model protected versus unprotected sex), which is the emphasis of my paper.==== Since unprotected sex inevitably leads to fertility, and fertility plays such an important role in an African woman's life, missing the dimension of fertility can mislead the results. My paper complements their paper in modeling rich fertility behavior; that is, I not only consider that the desire to have children can motivate marriage, but also that the desire to get married can incentivize premarital fertility and thus unprotected sex, which poses additional risk to young women. Second, my paper models richer interactions of decision making. In Greenwood et al.'s paper, agents make choices of the sexual market to engage in, mainly out of concerns of HIV risk as well as utility and transfer from different types of sex. In my paper, agents' incentives go beyond HIV risk and sex: they have a desire to have children, which has pros and cons; they have an incentive to invest in education, which affects their future income; and they are eager about marriage, but do not all want to get married early. Thus, when they make decisions concerning fertility and sex type, they face more tradeoffs; even these two major decisions per se interact directly, as the cost of birth control differs across sex types. Such inclusion of various lifetime dimensions in a theoretical framework renders more realistic and richer insights for policy analysis. The third main difference, which is the limitation of this paper, is that it does not have a general equilibrium model and only explicitly models the behavior of one gender – female. This may cause a bias in policy analysis since the effect of behavior change only affects individual risk but not aggregate risk through the feedback between individuals. Yet, the bias might be limited given the various lifetime aspects and tradeoffs modeled in the paper, so that HIV is only one of the many considerations in a woman's decision making. Thus, I focus on women's lifecycle decisions and risks in this paper, leaving the modeling of a general equilibrium framework with both HIV and fertility (especially premarital fertility) and both genders to future work.====Next, this paper is related to the empirical literature on the relationship between HIV and fertility. While the literature has studied the impact of HIV on fertility in Africa and has produced very mixed findings (Fortson, 2009; Juhn et al., 2013; Young, 2005, 2007; Boucekkine et al., 2009; Kalemli-Ozcan, 2012), none of them have examined the effect of fertility on HIV risk and thus may be subject to a reverse causality problem.====This paper is also related to the broader literature investigating socioeconomic factors of the HIV epidemic in Africa (e.g., Oster, 2012a; Corno and De Walque, 2012). On the high HIV rates among young African women, factors such as transactional sex (Epstein, 2008), premarital sexual relations (Meekers and Calvès, 1997), spousal search behavior (Magruder, 2011) and the configuration of sexual networks (Pongou and Serrano, 2013) are argued to have played a role. Greenwood et al. (2017) claim that marriage provides a haven for safer sex, and thus policies aimed at encouraging marriage may mitigate HIV transmission. While my study shares the same feature that sex within marriage is low risk, it highlights that the motives to enter marriage through childbearing can enhance risk.====In addition, a strand of literature documents a limited behavior change to the rising HIV risk in Africa, especially in the aspects of premarital sex involvement and condom usage (Oster, 2005; Thornton, 2008; Gallant and Maticka-Tyndale, 2004; McCoy et al., 2010). Oster (2012b) proposes explanations of OLS bias and low non-HIV life expectancy in Africa, and Kerwin (2014) proposes fatalistic response by a subset of the population. My paper complements their studies by offering a potentially new explanation from the perspective of marriage and premarital childbearing.====Furthermore, the literature has studied the relationship between HIV and education empirically. The results are mixed. While some find a positive relationship between education and HIV infection, due to a positive association between education and premarital sex (Fortson, 2008; Case and Paxson, 2013), others find the relationship is either insignificant (De Walque, 2009) or negative (Alsan and Cutler, 2013). Some studies document a dynamic relationship between the two (De Walque, 2007; Iorio and Santaeulalia-Llopis, 2016). The lack of consensus on the empirical relationship between education and HIV suggests the existence of multiple channels through which education and HIV risk are linked, and thus calls for a theoretical framework to disentangle these channels. My paper fills this gap.====Finally, this paper is connected to the broader literature studying the impacts of social norms and institutions on sexual behavior and fertility (Tertilt, 2005, 2006; Schoellman and Tertilt, 2006; Doepke and Tertilt, 2018; Doepke and Kindermann, 2019; Greenwood and Guner, 2010). But this literature does not link fertility to HIV.====The paper proceeds as follows. Section 2 introduces the background and evidence about HIV and fertility in Africa, with a focus on Kenya. Section 3 presents the model and Section 4 the calibration, followed by counterfactual and policy experiments in Sections 5 and 6, respectively. Section 7 provides an extension of the model for explaining the trend of HIV prevalence, and Section 8 concludes.",Fertility and HIV risk in Africa,https://www.sciencedirect.com/science/article/pii/S1094202521000429,6 June 2021,2021,Research Article,83.0
"Geromichalos Athanasios,Herrenbrueck Lucas","University of California – Davis, United States of America,Simon Fraser University, Canada","Received 3 May 2019, Revised 7 April 2021, Available online 29 May 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.05.002,Cited by (8),", the effects of money demand shocks, and the existence and persistence of a ==== is positive.","We propose a model for macroeconomic analysis that is parsimonious, tractable, consistent with the microfoundations of asset liquidity, and also consistent with a set of facts that have been challenging to model in a unified framework: (1) monetary policy is implemented via intervention in financial markets; (2) few assets serve as media of exchange (“money”) but most assets can be sold when needed, thus acting as ==== substitutes to money; (3) lower policy interest rates tend to increase output and investment; and (4) there is no tight link between policy interest rates and inflation, neither in the short run nor in the long run. The model can serve as a framework to study many topics in macro- and monetary economics. Here, to demonstrate the breadth of possible applications, we use it to address the following questions: the short-term effects of money demand shocks, the links between inflation, interest rates, and asset returns, optimal monetary policy in the long run, and the existence and persistence of a liquidity trap where interest rates are zero but inflation is positive.====Why is such a model needed? The model which has been most widely used as a guide to policy, the New Keynesian model, features a cashless economy where the driving friction is price stickiness. As a result, the model is not well suited to modeling monetary issues such as money demand shocks, or intervention in financial markets (it is cashless); moreover, its ability to explain a liquidity trap has been challenged.==== These issues are easier to address in a New Monetarist model where the driving frictions make liquidity emerge naturally (Lagos et al., 2017). However, that branch of the literature has mostly focused on inflation and the real balance effect, at the expense of a realistic model of interest rates, their central role in monetary policy, and their effect on the economy.====Our model combines New Monetarist insights about asset liquidity with the structure of the neoclassical growth model – the main workhorse model of macroeconomic aggregates. Hence, we call it the Liquidity-Augmented Model of Macroeconomic Aggregates (LAMMA). Due to frictions that we will describe precisely, a need for a medium of exchange arises in the economy, and in the model this role is played by fiat money. Government bonds and physical capital cannot be used as media of exchange, but they too are liquid, as agents with a need for money can sell their bonds and capital in a secondary asset market. The government controls the quantities of money and liquid bonds, and can therefore conduct open-market operations in that secondary market to target the price of liquid bonds, which is arguably the empirically relevant approach.====Our first result is that monetary policy has real effects at all frequencies, even in steady state. We can express the long-run effects as a function of two rates: the expected inflation rate, and the interest rate on liquid bonds. Expected inflation makes people economize on money balances, which gives rise to two opposing forces. One is the inflation tax which falls on the productive economy, and this force tends to make money and capital complements in general equilibrium. The second force is the fact that as a somewhat liquid asset, capital can be an imperfect substitute to money (the Mundell-Tobin effect). The end result could be overinvestment or underinvestment, and which force prevails depends on the second instrument of monetary policy: the interest rate on bonds. Raising this rate by selling bonds in the asset market makes bonds a more desirable store of value, hence investment and output fall. Lowering this rate, by buying bonds in the asset market, does the opposite. With the right interest rate, investment and output are at their first-best levels; hence, while the Friedman rule is ==== optimal long-run policy in this economy, it is not the ==== such policy.====Second, the model clarifies that the distinction between interest rates on liquid and illiquid assets is crucial for understanding the role of monetary policy, and for empirical analysis of its effects. Highly liquid assets are the closest substitutes for money, hence their returns can be set independently from inflation, even in the long run. For this reason, we should not expect there to be an empirically stable, structural Fisher equation for less-than-perfectly-illiquid assets. Depending on the monetary policy regime and the trends affecting the economy, the empirical slope of inflation on interest rates can be greater than one, less than one, or even negative. Using long-run data, we demonstrate that this effect holds for capital assets as well; inflation and interest rates affect capital returns separately, and with coefficients close to those predicted by the model.====When capital is hard to trade, the economy can be in a liquidity trap, which we define as a situation where (i) the policy interest rate is at a lower bound, (ii) output and investment are below their optimal levels, and (iii) raising interest rates would make things worse (equivalent to saying that it would be desirable to lower interest rates further). This liquidity trap formalizes the long-held notion that saving is not automatically translated into investment, but requires a well-functioning financial system and an unconstrained interest rate. In such a trap, a variety of fiscal schemes may help, but there is also a simple monetary remedy: increase inflation permanently. In addition, there is nothing “short-run” about our mechanism; hence, there is no contradiction between a liquidity trap and stable, even positive, inflation. This fits with the experience of developed economies in the last decade (three decades in Japan), where near-zero interest rates have coexisted with stable inflation.====Finally, we solve our model in the short run as a dynamic-stochastic general equilibrium model. We discuss various schemes of monetary policy implementation, but focus on the most relevant one for the modern era: interest-rate setting monetary policy, implemented via intervention in secondary asset markets, together with endogenous money growth. Next we calibrate the model and simulate the economy's response to several real and financial shocks, including a shock to the fraction of agents who need money to pay for goods (that is to say, a money demand shock); we show that the shocks have plausible effects on macroeconomic outcomes. It is worth emphasizing that the results discussed so far are representative of the topics that the LAMMA can address, but by no means exhaustive. One contribution of the paper is to offer a ==== where researchers can study monetary policy in a theoretically consistent and empirically relevant way, while at the same time allowing them to include any feature that one could have included in the neoclassical growth model itself.====Conceptually, our paper is related to Tobin (1969). Writing in the inaugural issue of the ====, he proposes a “general framework for monetary analysis”: ====Our paper is part of a literature that studies how liquidity and monetary policy can shape asset prices, based on the New Monetarist paradigm (Lagos and Wright, 2005; Lagos et al., 2017). In papers like Geromichalos et al. (2007), Lester et al. (2012), Nosal and Rocheteau (2013), Andolfatto and Martin (2013), and Hu and Rocheteau (2015), assets are ‘liquid’ in the sense that they serve directly as media of exchange (often alongside money).==== An alternative approach highlights that assets may be priced at a liquidity premium not because they serve as media of exchange (an assumption often defied by real-world observation), but because agents can sell them for money when they need it (Geromichalos and Herrenbrueck, 2016; Berentsen et al., 2014, Berentsen et al., 2016; Mattesini and Nosal, 2016).==== Herrenbrueck and Geromichalos (2017) dub this alternative approach ==== liquidity. In this paper, we make use of the indirect liquidity approach because it provides a natural way to mimic how central banks implement monetary policy in reality: they intervene in institutions where agents trade assets in response to short-term liquidity needs. That is exactly what the secondary asset market in our model represents.====One central question for us is the effect of monetary policy on capital, and we have argued that the dual role of capital, as a productive factor (affected by the inflation tax) and a liquid asset (competing with money as a store of value), is key to this (see also Herrenbrueck, 2014). Most of the recent literature has focused on one of these channels at a time. Aruoba et al. (2011) analyze how capital responds negatively to the inflation tax. In Rocheteau et al. (2018b), entrepreneurs can finance investment using money or credit, thus inflation also tends to depress investment. On the other hand, Lagos and Rocheteau (2008), Rocheteau and Rodriguez-Lopez (2014), and Venkateswaran and Wright (2014) explore the idea that capital could be valued for its potential liquidity properties as a substitute to money, which makes inflation cause ====accumulation of capital unless offset by a negative externality or capital tax.====Our final important departure from most of the New Monetarist literature is that we interpret the yield on liquid bonds as the main monetary policy instrument. Traditionally, this literature has emphasized money growth and the real balance effect of expected inflation. The list of papers that do focus on the yield on liquid bonds is growing and now includes Andolfatto and Williamson (2015), Dong and Xiao (2017), Rocheteau et al. (2018a), and Dominguez and Gomis-Porqueras (2019). These papers do not include capital or the effect of monetary policy on investment.====Our paper is also related to a large literature on the effect of monetary policy on macroeconomic aggregates in the presence of financial frictions. Notable examples include Bernanke and Gertler (1989), Cúrdia and Woodford (2011), and in particular Kiyotaki and Moore (2019) who also consider a multi-asset model where money has superior liquidity relative to other assets. Unlike us, they do not explicitly model bonds (and, hence, the conduct of monetary policy through intervention in the market for these bonds). Finally, our paper is related to the literature on policy in a liquidity trap, including Krugman (1998), Eggertsson and Woodford (2003), Andolfatto (2003), Werning (2011), Williamson (2012), Guerrieri and Lorenzoni (2017), Altermatt (2017), and Caramp and Singh (2020).====The paper is organized as follows. Section 2 introduces the model. Section 3 focuses on steady states: we analyze equilibria, discuss policy options, and apply the results to long-run questions such as the liquidity trap and the effect of inflation on capital. In Section 5, we return to the stochastic economy: we calibrate the model and solve it in log-linearized form, compare the effects of various real and financial shocks, and discuss short-term policy options. Section 6 concludes, and additional details are provided in the Appendix.",The liquidity-augmented model of macroeconomic aggregates: A New Monetarist DSGE approach,https://www.sciencedirect.com/science/article/pii/S1094202521000399,29 May 2021,2021,Research Article,84.0
"Cuñat Alejandro,Deák Szabolcs,Maffezzoli Marco","University of Vienna, Austria and CES-ifo,University of Exeter, UK,Università Bocconi, IGIER, and BIDSA, Italy","Received 19 February 2018, Revised 26 April 2021, Available online 27 May 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.05.001,Cited by (0), and the ,"This paper studies the dynamic response of government revenues to income tax cuts in an environment in which countries engage in commodity trade. In particular, we construct a model in which two Ramsey economies specialize according to their factor abundance. We show that the long-run negative effect of a reduction in a country's capital-income tax rate on government revenues is much smaller than in the standard closed-economy Ramsey model.====The different behavior of the closed and open economies can be understood in terms of the different ways their sectorial factor allocation mechanisms work. A reduction in the capital-income tax rate raises the after-tax return to capital, thus creating an incentive to accumulate capital. Under autarky, an increase in the aggregate capital-labor ratio implies higher sectorial capital intensities; the diminishing marginal productivity of capital therefore reduces the return to capital and thereby the incentive to accumulate. In the open economy, instead, capital-labor intensities do not respond to exogenous==== increases in the aggregate capital-labor ratio that much, as resources are reallocated from labor-intensive to capital-intensive industries. This enables the open economy to accumulate capital without affecting the gross return to capital as much as under autarky. Obviously, this generates a stronger reaction of capital income to the initial tax cut, and therefore reduces the negative impact of the tax cut on government revenues.====The paper's main contribution consists of a quantitative assessment of the relevance of this intuition. For this purpose, we calibrate our dynamic two-country model with the US and the rest of the world in mind, and compute the short-run and long-run responses of government revenue to tax cuts. We relate our results to two issues, dynamic scoring and the Laffer curve, that have been treated recently in the literature.====First, Mankiw and Weinzierl (2006) criticize the way the revenue impact of tax changes is conventionally calculated. It is usually referred to as ====, because it ignores the feedback effect from tax changes to any macroeconomic variables. However, it is static only from a macroeconomic point of view, because feedback effects from microdynamic behavior are incorporated into the forecast.==== Mankiw and Weinzierl (2006) take a firm stand in favor of ====: they use a closed-economy Ramsey model to show that the short-run response of government revenues to tax-rate changes is always stronger than the long-run response.==== When scoring a proposed legislation, the Congressional Budget Office and the Joint Committee on Taxation have also been reporting a range of possible budgetary feedback from changes to underlying macroeconomic activity alongside the conventional scoring estimates since 2003. Since 2015 these dynamic effects have also been incorporated into the official score for major tax legislation.====Second, in a more recent reference, Trabandt and Uhlig (2011) use the neoclassical growth model to characterize the shape of the Laffer curve in the US and Europe. They find that both the US and Europe are on the upward sloping side of the Laffer curve; however, they point out that Europe is quite close to the Laffer curve's ‘slippery slope,’ that is, its downward sloping side.====Regarding dynamic scoring, we find that our dynamic trade model generates a much larger response on the factor accumulation side to a tax cut than in the autarky model, as we discussed above.==== In our benchmark calibration, for example, a capital tax cut is almost fully able to finance itself in the long run, whereas the dynamic response to the tax cut in the autarky economy only compensates for 61% of the short-run revenue loss. As for the Laffer curve, we find that the US is almost on the Laffer curve's ‘slippery slope’: the actual average US tax rate on capital income is 27.3%, while the revenue-maximizing tax rate in our model, the peak of the Laffer curve, equals 27.9%. In contrast, under autarky, the peak of the Laffer curve occurs at a tax rate of 46.3%.====The main intuition of our paper is based on Ventura (1997), who shows, in the context of the neoclassical growth model, that the negative effect of capital accumulation on the return to capital is reduced by free trade. Although insightful and elegant, the Ventura model turns out not to be a very useful workhorse for performing a quantitative exercise of the kind we have in mind, since it yields international factor price equalization. First of all, this is obviously not a very realistic feature when contrasted with the data; secondly, the treatment of steady states in the presence of taxation becomes somewhat tricky, as the equalization of before-tax interest rates implies different after-tax interest rates if capital-income taxation differs across countries. Therefore, for our calibration exercise we produce a model based on Cuñat and Maffezzoli (2007), which is a dynamic generalization of Dornbusch et al. (1980). This set-up enables us to model Heckscher-Ohlin trade with trade frictions and therefore no factor price equalization (both important features in reality) in a rather straightforward way. See Davis and Weinstein (2001) and Romalis (2004) for empirical evidence supporting this model's predictions.====The link between taxation and international trade is obviously not new.==== Whalley (1980) is a good example of a “Computable General Equilibrium” model of taxation and international trade, which compares the welfare implications of tax policies under autarky and trade.==== Baxter (1992) shows that changes in taxation can affect cross-country specialization patterns within a dynamic model of Ricardian comparative advantage. This is also the case in our model: comparative advantage is influenced by taxation through its effects on factor accumulation. More interestingly, we show that the quantitative effects of taxation depend on an economy's ability to reallocate resources according to the evolution of its comparative advantage. Mendoza and Tesar (1998) studies tax reforms in a one-good, two-country dynamic model calibrated to U.S. and European tax policies, focusing on the role of intertemporal trade in the transmission of fiscal policy shocks across countries. In the same framework, Mendoza and Tesar (2005) discuss the issue of international tax competition, and Mendoza et al. (2014) and Auray et al. (2016) study the effects of fiscal adjustments to restore fiscal solvency in response to debt shocks. Bianconi (1995) studies tax policy in a neoclassical two-country dynamic model with integrated capital markets analytically. Kim and Kose (2014) and Choi and Kim (2016) study the welfare implications of revenue-neutral fiscal reform programs for developing economies in a multi-sector model of a small open economy. In comparison with these references, we ignore capital mobility; but the international dimension we exploit, goods trade and changes in production structures, also yields striking results from a quantitative perspective. More recently, Andersen (2007) studies tax competition in a static Ricardian trade model, and Epifani and Gancia (2009) studies the empirical relationship between international trade and the size of the government. The value added of our work here is the treatment of fiscal policy effects in a dynamic setting.====As far as the empirical evidence is concerned, Johansson and Olaberria (2014) look at the effects of labor taxes on industrial specialization in 26 sectors of 37 OECD and non-OECD countries. They find a significant relationship between the labor tax wedge and the intensity of labor in production. This suggests that countries with higher labor taxes tend to specialize in industries that are less labor intensive. Our exercise is instead about the effects of capital taxation, but the economics of the long-run effects of factor-income taxation on specialization patterns is likely to be similar across types of production factors.====The rest of the paper is structured as follows: section 2 lays out a rather general dynamic trade model; in section 3 we develop some intuition by working out a very particular case, while in section 4 we simulate a more realistic version of the model; section 5 checks the sensitivity and robustness of our results; finally, section 6 presents our concluding remarks.",Tax cuts in open economies,https://www.sciencedirect.com/science/article/pii/S1094202521000405,27 May 2021,2021,Research Article,85.0
"Gechert Sebastian,Havranek Tomas,Irsova Zuzana,Kolcunova Dominika","Macroeconomic Policy Institute, Düsseldorf, Germany,Institute of Economic Studies, Faculty of Social Sciences, Charles University, Prague, Czechia,CEPR, United Kingdom of Great Britain and Northern Ireland,Czech National Bank, Czechia","Received 11 May 2020, Revised 3 May 2021, Available online 21 May 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.05.003,Cited by (21),We show that the large ,"A key parameter in economics is the elasticity of substitution between capital and labor. Among other things, the size of the elasticity has practical consequences for monetary policy, as Fig. 1 illustrates. In the SIGMA model used by the Federal Reserve Board, the effectiveness of interest rate changes in steering inflation doubles when one assumes the elasticity to equal 0.9 instead of 0.5, yielding very different policy implications. We choose the SIGMA model for the illustration because, as one of very few models employed by central banks, it actually allows for different values of the elasticity of substitution. Almost all models use the convenient simplification of the Cobb-Douglas production function, which implicitly assumes that the elasticity equals one. If the true elasticity is smaller, these models overstate the strength of monetary policy and should imply a more aggressive campaign of interest rate cuts in response to a recession (Chirinko and Mallick, 2017, make a related argument). In this paper we show that the Cobb-Douglas specification is at odds with the empirical evidence on the elasticity.====Aside from convenience, the other reason for the widespread use of the Cobb-Douglas production function is that, at first sight, empirical investigations into the value of the elasticity have produced many central estimates close to 1. When each study gets the same weight, the mean elasticity reported in the literature reaches 0.9—at least based on our attempt to collect all published estimates, in total 3,186 coefficients from 121 studies. But we show that the picture is seriously distorted by publication bias. After correcting for the bias, the mean reported elasticity shrinks to 0.5. This correction alone can imply halving the effectiveness of monetary policy in a structural model, as shown by Fig. 1.====The finding of strong publication bias predominates in our results. The bias arises when different estimates have a different probability of being reported depending on sign and statistical significance. The identification builds on the fact that almost all econometric techniques used to estimate the elasticity assume that the ratio of the estimate to its standard error has a symmetrical distribution, typically a ====-distribution. So the estimates and standard errors should represent independent quantities. But if statistically significant positive estimates are preferentially selected for publication, large standard errors (given by noise in data or imprecision in estimation) will become associated with large estimates. Because researchers command plenty of degrees of freedom in estimation design, a large estimate of the elasticity always emerges if the researcher looks for it long enough, and an upward bias in the literature arises. A useful analogy appears in McCloskey and Ziliak (2019), who liken publication bias to the Lombard effect in psychoacoustics: speakers increase their effort in the presence of noise. Apart from linear techniques based on the Lombard effect, we employ recently developed methods by Ioannidis et al. (2017), Andrews and Kasy (2019), Bom and Rachinger (2019), and Furukawa (2021), which account for the potential nonlinearity between the standard error and selection effort.====All the aforementioned techniques assume that in the absence of publication bias there is no correlation between estimates and standard errors: meta-analysis has its origins in medicine, where the exogeneity of the standard error is rarely questioned. In economics, however, the standard error can be endogenous for three reasons: it is itself an estimate (measurement error), publication bias may work through reporting artificially high precision (reverse causality), and some unobserved method choices may systematically influence both the point estimate and the corresponding standard error (omitted variables). No technique commonly used in economics meta-analyses allows us to get rid of the assumption. We employ study fixed effects, which filter out between-study differences, likely the most important source of endogeneity. We also employ the number of estimates as an instrument for the standard error, but some method choices can still be correlated with the size of the data set in primary studies.====A more fundamental solution is provided by psychology, where the newly developed p-uniform* technique (van Aert and van Assen, 2021) analyzes the distribution of p-values instead of estimates and standard errors. The foundation of p-uniform* is the statistical principle that p-values are uniformly distributed at the mean underlying effect size: that is, when testing the hypothesis that the estimated coefficient equals the underlying effect. The idea of p-uniform* is to find a coefficient at which the distribution of p-values is approximately uniform; this is done by recomputing the reported p-values for different possible values of the underlying effect and then comparing the resulting distribution to the uniform one. Following this principle, the technique's test for publication bias evaluates whether p-values are uniformly distributed at the precision-weighted mean reported in the literature. All tests, including p-uniform*, suggest strong publication bias that substantially exaggerates the mean reported elasticity.====The studies in our dataset do not estimate a single population parameter; rather, the precise interpretation of the elasticity differs depending on the context in which authors derive their results. We collect 71 variables that reflect the different contexts and find that our conclusions regarding publication bias hold when we control for context. Because of the richness of the literature on the elasticity of substitution, we face substantial model uncertainty with many controls and address it by using Bayesian (Eicher et al., 2011; Steel, 2020) and frequentist (Hansen, 2007; Amini and Parmeter, 2012) model averaging. We investigate how the estimated elasticities depend on publication bias and the data and methods used in the analysis. Our results suggest that three factors drive the heterogeneity in the literature: publication bias (the size of the standard error), source of variation in input data (cross-country vs. industry-level variation), and identification approach (whether or not information from the first-order condition for capital is accounted for). Estimations using systems of equations tend to deliver results similar to those of single-equation approaches focused on the first-order condition for capital. In addition, the normalization of the production function used in recent studies typically brings much smaller reported elasticities, by 0.3 on average. We also find that different assumptions regarding technical change have little systematic effect on the reported elasticity.====As the bottom line of our analysis, we construct a hypothetical study that uses all the estimates reported in the literature but assigns more weight to those that are arguably better specified. The result represents a mean estimate implied by the literature but conditional on the absence of publication bias, use of best-practice methodology, and other aspects related to quality (such as publication in a leading journal or a large number of citations). In this way we obtain an elasticity of 0.3 with an upper bound of the 95% confidence interval at 0.6. Though certainly not the definitive point estimate for the elasticity, it is the best guess we can make when looking at half a century of accumulated empirical evidence.====Defining best-practice methodology is subjective, and different authors will have different preferences on study design. But to arrive at 0.3, it is enough to hold two preferences: i) using variation across industries is superior to using variation across countries (which is substantiated, e.g., by Nerlove, 1967; Chirinko, 2008) and ii) including information from the first-order condition for capital is superior to ignoring it (and, for example, focusing exclusively on the first-order condition for labor). To put these numbers into perspective, we once again turn to the Fed's SIGMA model, which employs a value of 0.5 for the elasticity of substitution (Erceg et al., 2008). This calibration corresponds to the mean estimate in the literature corrected for publication bias, without discounting any estimates based on data and methodology. The model employed by the Bank of Finland (Kilponen et al., 2016), on the other hand, uses the elasticity of 0.85, which is close to the mean estimate in the literature without correction for publication bias. The calibration closest to our final result is that of Cantore et al. (2015), who use a prior of 0.4. Their posterior estimate is even lower, though, at below 0.2.====The elasticity of substitution between capital and labor is central to a host of problems aside from monetary policy. Our understanding of long-run growth depends on the value of the elasticity (Solow, 1956). The sustainability of growth in the absence of technological change is contingent on whether the elasticity of substitution exceeds one (Antras, 2004). Klump and de La Grandville (2000) suggest that a larger elasticity of substitution in a country results in higher per capita income. Turnovsky (2002) argues that a smaller elasticity leads to faster convergence. Nekarda and Ramey (2013) argue that the countercyclicality of the price markup over marginal cost also depends on the elasticity of substitution. The elasticity represents an important parameter in analyzing the effects of fiscal policies, including the effect of corporate taxation on capital formation, and in determining optimal taxation of capital (Chirinko, 2002).====But perhaps most prominently, the elasticity of substitution is a key parameter in the literature on the labor share. The evidence of a declining labor share has in fact revived general interest in estimating the elasticity because some of the explanations depend critically on the value of the elasticity (====). Oberfield and Raval (2014) categorize these explanations into two groups: (1) mechanisms decreasing the labor share via changing factor prices and (2) mechanisms decreasing the labor share via changing technology. Regarding group (1), the explanations put forward by Piketty (2014) and Karabarbounis and Neiman (2014) hold only when the elasticity surpasses one. Then the global decline in the labor share can be attributed to an increasing capital-labor ratio, either via capital deepening (Piketty, 2014) or as a response to falling investment prices (Karabarbounis and Neiman, 2014). With ====, however, declining prices of capital and increased capital accumulation raise the labor share. Yet, as we show in this paper, ==== is consistent with the bulk of the empirical estimates of the elasticity. In this context, Glover and Short (2020a) assert that capital deepening cannot explain the observed decline; they point to issues that led to the high elasticity estimates of Karabarbounis and Neiman (2014). Regarding group (2), alternative explanations stress changes in automation, offshoring, directed technological change (as in Oberfield and Raval, 2014; Eden and Gaggl, 2015; Koh et al., 2016), a slowdown in labor productivity (as in Grossman et al., 2017), a rise in concentration (Autor et al., 2017), and demographic changes (Glover and Short, 2020b); explanations that do not hinge on high values of ====.====The elasticity also has important effects on the short-run dynamics of the labor share. This channel can be illustrated by computing the response of the labor share to a labor-augmenting technology shock, as we do in Fig. 2 based on the model developed by Cantore et al. (2014) and Cantore et al. (2015). In the case of the Cobb-Douglas production function the labor share remains constant, while with ==== the share decreases after a labor-augmenting shock. As the figure illustrates, the response is highly sensitive to changes in ====. A model with a lower elasticity, consistent with our results, is able to match the actual dynamics of the data on the labor share better than the Cobb-Douglas case (Cantore et al., 2015).====The remainder of the paper is structured as follows: Section 2 briefly discusses how the elasticity of substitution is estimated; Section 3 describes how we collect estimates of the elasticity from primary studies and provides a bird's-eye view of the data; Section 4 examines publication bias; Section 5 investigates the drivers of heterogeneity in the reported elasticities and calculates the mean elasticity implied by best practice in the literature; Section 6 concludes the paper. Appendix A illustrates the working of publication bias and basic meta-analysis tools via a Monte Carlo simulation. The data, code, additional details, and robustness checks are available in an online appendix at ====.",Measuring capital-labor substitution: The importance of method choices and publication bias,https://www.sciencedirect.com/science/article/pii/S1094202521000387,21 May 2021,2021,Research Article,86.0
"Chen Chaoran,Restuccia Diego,Santaeulàlia-Llopis Raül","York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada,University of Toronto and NBER, 150 St. George Street, Toronto, ON M5S 3G7, Canada,Universitat Autonoma de Barcelona, BGSE and CEPR, Plaça Cívica s/n, Bellaterra, Barcelona 08193, Spain","Received 5 February 2021, Revised 22 April 2021, Available online 28 April 2021, Version of Record 31 May 2022.",https://doi.org/10.1016/j.red.2021.04.006,Cited by (16),"We assess the effects of land markets on misallocation and productivity both empirically and quantitatively. Exploiting variation from a land certification reform across time and space in Ethiopia, we find that certification facilitates rentals and improves agricultural productivity. We calibrate a quantitative ==== with heterogeneous household farms facing institutional costs to land markets using the micro panel data. The effect of a counterfactual reallocation from no rentals to efficient rentals increases zone-level agricultural productivity by 43 percent on average. While our estimated institutional costs are strongly associated with land certification across zones, there are nontrivial residual frictions to rental market activity, implying that land certification only partially captures the overall effects of rentals. A full certification reform accounts for about one-fourth of the overall productivity gains from land rentals. This result highlights the importance of comprehensive reforms alleviating frictions to land transactions beyond the granting of certificates.","What are the effects of land markets on resource allocation and agricultural productivity? Despite the importance and large efforts devoted into understanding land markets, the answer to whether land markets improve resource allocation and productivity remains elusive (Deininger and Feder, 2001). We study the effects of land rental markets on agricultural productivity using evidence from a land certification reform in Ethiopia together with a quantitative macroeconomic model that captures institutional costs beyond access to land certification. We show that rentals significantly improve agricultural productivity by reducing misallocation and that the empirical effects of a land certification reform only capture a fraction of the overall effects of land markets.====Ethiopia provides a unique and relevant context to investigate the effects of land markets on productivity. From 1974 until the early-1990s, the Communist government in power expropriated and uniformly redistributed all of the rural land in the country, and prohibited land transactions by law. Although land ownership still resides with the state, an ongoing land certification reform allows land to be reallocated across farmers via rentals up to a limit and with restrictions (Holden and Ghebru, 2016). The land certification reform, and hence the lifting of barriers to land rental market activity, was decentralized and implemented by local governments with different intensity and timing across zones (i.e., sub-regions) as opposed to being contemporaneously implemented by the central government (Deininger et al., 2008), providing an interesting source of variation. Using representative panel data that catches the reform in the 2010s, in two waves 2013/14 and 2015/2016, we find large variation in the fraction of land parcels with a certificate (land certification share) and rental market activity across space and time.====We provide microeconomic empirical evidence that increases in land rental market activity improves resource allocation by comparing zones that do and do not increase land rentals. Since rentals are an endogenous outcome of changes in institutional costs, we explicitly exploit the variation in land certification shares across space and time in order to infer the empirical effects of land certificates on rentals and misallocation. Using a difference-in-difference strategy we compare zones (treatment group) for which certification shares increase between the 2013/14 and the 2015/16 waves with zones (control group) for which certification shares do not increase. We find that land certification significantly facilitates land rentals and improves resource allocation. We assess the exogeneity of the certification reform by showing no pre-trend differences and through a placebo test.====We emphasize that the empirical effects of the land certification reform may not reflect the entire scope of how land markets alleviate misallocation and improve productivity. For instance, there may be a lagging behavior between the granting of land certificates and farms' engagement in land rental activity due to the potential lack of trust in the institutional reform (Ostrom, 2010). It could also be that there are other frictions impeding the efficient allocation of land through rentals, in addition to lacking certificates. To assess the broader effects of land markets on resource allocation, we develop a quantitative model with endogenous rentals, where household-farms are heterogeneous in their permanent productivity and face zone-specific land-market institutional costs. We calibrate our baseline economy, including zone-specific institutional costs and the joint distribution of productivity and land endowments, to the 2013/14 wave of data and refer to this allocation as status quo.====We conduct two main quantitative experiments. First, we vary the land-market institutional costs to match the 2015/16 moments of land rentals and refer to the implied allocations as reform. We then estimate the relationship between rentals and productivity using the model-generated status quo and reform allocations and find that the difference-in-difference estimates are very similar to those obtained from the empirical data. While the calibrated institutional costs encompass all frictions to land markets, they are highly correlated with the land certification shares across zones, providing further evidence that the certification reform helps facilitate rentals. We also find that the history of reform matters since zones in which certificates are granted earlier experience a more rapid decline in institutional costs, suggesting a lag between the granting of certificates and land reallocation. Second, using the empirical relationship between land certification and institutional costs, we vary land certification shares in each zone from zero to 100 percent and find that this counterfactual full certification reform increases zone-level agricultural productivity by 11.4 percent on average. We contrast this result with an alternative counterfactual obtained by varying institutional costs in each zone from prohibitively high to zero, which captures changes in allocations from no rentals to efficient rentals. The overall effect of rentals is an increase of zone-level agricultural productivity of 43.3 percent on average. We conclude that land certification only partially captures the effects from land markets, accounting for about one-fourth of the overall effects from rentals.====A critical aspect of the political discourse on land policy in poor countries is whether land rentals enhance or reduce farm income inequality (Deininger and Binswanger, 1999; André and Platteau, 1998; Otsuka, 2007). A complete empirical assessment of the effects of land rental markets on inequality is challenging as it requires data that is typically not available such as rental payments both paid and received by each farm. This limitation is also present in our data that only contains rental payments paid to rent in. For this reason, we alternatively use our model-generated status quo and reform allocations to construct measures of within-zone farm-income inequality. We find that land certification does not increase zone-level inequality; instead, the increase in average farm income is particularly stronger for poor households.====Our paper relates to a macroeconomic literature on agricultural productivity and international income differences.==== The measurement of the extent of misallocation in poor countries has been emphasized using micro panel data (Restuccia and Santaeulàlia-Llopis, 2017; Gollin and Udry, 2021). Our focus is on the changes in misallocation due to the effects of land markets as opposed to the level of misallocation. We have identified underdeveloped land markets as one source of factor misallocation (Restuccia and Rogerson, 2017), relating to the role of institutions on development (Acemoglu et al., 2001; Banerjee et al., 2002; Banerjee and Iyer, 2005).====Land reforms have been studied extensively.==== For instance, Adamopoulos and Restuccia (2020) study a land reform in the Philippines that caped the maximum size of land holdings and intervened in the land market to redistribute excess land to landless and smallholders. Using a quantitative model and micro data before and after the reform, Adamopoulos and Restuccia (2020) show a substantial negative effect of the reform on agricultural productivity and average farm size. Our focus is on a land certification reform with effects through land rental market activity rather than size and on exploiting actual variation in reform intensity across time and space. Similarly, Chen (2017) and Gottlieb and Grobovšek (2019) study the effects of untitled land and communal land tenure on agricultural and aggregate productivity. These articles assess quantitatively the effects of hypothetical land rights reform, whereas we assess the effects of land reform from observed changes across time and space in Ethiopia. More broadly, we contribute to the literature on land reforms not only by providing causal evidence of land certification on rentals and productivity, but also by assessing how much land certification reform accounts for the overall effects of land rentals using theory that is consistent with the empirical effects. Our results show that focusing on the empirical causal effects of a specific land policy can substantially underestimate the overall effects of land markets. We also contribute to a growing literature that integrates micro empirical evidence with quantitative theory to assess macro development.====In Section 2, we describe the data, the institutional background, and the land market activity in Ethiopia. Section 3 presents the theoretical framework and the qualitative effects of land markets. In Section 4, we discuss the empirical evidence of the effects of land certification reform. Section 5 quantifies the effects of land markets on resource allocation and productivity, contrasting the effects of land certification with the overall effects of land rentals. In Section 6, we study the inequality implications of land reform. Section 7 concludes.",The effects of land markets on resource allocation and agricultural productivity,https://www.sciencedirect.com/science/article/pii/S1094202521000351,28 April 2021,2021,Research Article,87.0
"Chang Yongsung,Hong Jay H.,Karabarbounis Marios,Wang Yicheng,Zhang Tao","Seoul National University, Republic of Korea,FRB Richmond, United States of America,Peking University, HSBC Business School, China,Ragnar Frisch Centre for Economic Research, Norway","Received 20 September 2020, Revised 15 April 2021, Available online 23 April 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.04.004,Cited by (4),"Based on administrative data from Statistics Norway, we find economically significant shifts in households’ financial portfolios around individual structural breaks in labor-income volatility. According to our estimates, when income risk doubles, households reduce their risky share of financial assets by 5 percentage points, thus tempering their overall risk exposure. We show that our estimated risky share response is consistent with a standard portfolio choice model augmented with idiosyncratic, time-varying income volatility.","How do households respond to income risk? According to the standard theory (starting with Aiyagari, 1994), households save more, work more, or adjust their financial portfolios in response to uninsurable wage risk. This paper studies the last channel empirically and quantitatively: when faced with higher wage risk, households lower the share invested in risky assets, thus tempering their overall risk exposure (Merton, 1971; Kimball, 1993; Constantinides and Duffie, 1996; Heaton and Lucas, 1996, Heaton and Lucas, 2000).====The purpose of our analysis is twofold. First, based on administrative panel data from Statistics Norway, we confirm a significant negative relationship between wage risk and the risky share of financial assets by further elaborating on the method developed by Fagereng et al. (2017b).==== Second, we introduce idiosyncratic stochastic volatility (e.g., second moment) shocks to the wage process in an otherwise standard life-cycle model of portfolio choice (e.g., Cocco et al., 2005). We then ask whether the standard model can reproduce the portfolio response we estimated from the data. This is a new attempt, since existing quantitative models of portfolio choice do not reflect the idiosyncratic and time-varying component of income risk.====Households in Norway are obliged to report detailed information about their income and wealth to the tax authority every year. As a result, our data set includes a complete description of households' labor income and financial assets as well as their allocation to safe and risky financial accounts. We merge the households' income and financial data with other data regarding labor market status, demographic characteristics, and, more importantly for our analysis, employer information.====We employ two techniques to generate a cleaner estimate for the response of portfolio to income risk. First, we identify the “structural’’ breaks in income volatility, which are the periods when an individual worker experiences the largest change in the standard deviation of income growth. By looking at big events, we can potentially avoid noisy variations to obtain cleaner estimates. Second, we use firm volatility as an instrumental variable to isolate orthogonal events to households—an innovative method pioneered by Fagereng et al. (2017b). Indeed, a recent literature suggests that a substantial portion of the residual variation in earnings is predictable and reflects individual choices rather than risk (e.g., Primiceri and van Rens, 2009; Guvenen and Smith, 2014). Misinterpreting the predictable variations in income as risk is likely to bias the estimated response of the portfolio toward zero.====We estimate a clear negative response of risky share to income volatility. When income volatility (that is not anticipated by households) doubles, a typical (median) worker decreases her risky share by 5 percentage points (over a 4-year horizon), whereas the ordinary-least-squares (OLS) estimate predicts a mere 0.4 percentage point reduction in the risky share. Thus, isolating pure income risk from noisy or endogenous variations of income volatility increases (in absolute terms) the estimated response of the portfolio by an order of magnitude.====To test whether a standard model of portfolio choice is consistent with our estimated response, our benchmark model features: (i) a life-cycle economy with incomplete asset markets, (ii) a portfolio choice between risk-free bonds and risky equity (e.g., Cocco et al., 2005 and Gomes and Michaelides, 2005), (iii) an exogenous borrowing limit, (iv) labor earnings that consist of a mix of heterogeneous income profiles (Guvenen and Smith, 2014) and uninsurable stochastic shocks, and finally (v) our highlighted new element, idiosyncratic shocks to income volatility (stochastic volatility). We show that our model—calibrated to various income and financial moments from the Norwegian panel—reproduces the portfolio response we see in the data fairly well. According to our structural model, the welfare cost of time-varying income risk is large: 4% in consumption-equivalent units. The welfare gain from being able to adjust a financial portfolio (between risky and risk-free assets) in response to such risk is about 1%.====The marginal contribution of our analysis relative to the literature can be summarized as follows: (i) We propose a new way to identify the structural breaks in income volatility, which can be related to various life-cycle events. (ii) We expand the instrumental variable approach pioneered by Fagereng et al. (2017b) by combining it with structural breaks.==== According to our analysis, using structural breaks further reduces the influence of frequent noisy events, generating a response 16% larger than what the instrument alone can generate. (iii) Relative to Fagereng et al. (2017a), our model explicitly introduces shocks to the second moment—which is not explored by previous papers====—and analyzes its marginal effect on risky shares. (iv) We show that the life-cycle model of portfolio choice augmented with income volatility successfully reproduces the marginal portfolio response we see in the data. (v) Finally, the structural model enables us to compute the welfare cost of time-varying income risk and the welfare gains from adjusting the portfolio’s composition to such risk.====The rest of the paper is structured as follows. Section 2 estimates households' portfolio response to income risk using the administrative data from Norway. Section 3 builds a structural model augmented with idiosyncratic, time-varying income volatility. In Section 4 we calibrate the model to match the key statistic from the Norwegian panel and perform a quantitative analysis. In Section 5 we conduct the welfare analysis. Section 6 explores a few alternative model specifications (with respect to adjustment costs in portfolio choice, the stochastic process of income volatility, and imperfect information). Section 7 concludes.",Income volatility and portfolio choices,https://www.sciencedirect.com/science/article/pii/S1094202521000338,23 April 2021,2021,Research Article,88.0
"Grazzini Jakob,Massaro Domenico","Department of Economics and Management, University of Pavia, Italy,Department of Economics, Management and Quantitative Methods, University of Milan, Italy,Complexity Lab in Economics, Milan, Italy","Received 27 February 2019, Revised 12 April 2021, Available online 19 April 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.04.003,Cited by (0),"We investigate the sources of changes in GDP volatility observed from 1966 to 2018. We develop a ==== model and calibrate it to US data to characterize the contribution of micro level productivity shocks, inter-sectoral linkages and households' behavior to aggregate volatility. Our results show that changes in sectoral volatility played an important role in shaping GDP volatility and that asymmetries in the economy had a different impact on aggregate volatility over time. Moreover, we show that, despite an increase before the financial crisis of 2007, aggregate volatility has remained low until 2018.","GDP volatility in the US exhibited a high degree of time variation in the second part of the last century. The years ranging from 1960s to early-1980s were characterized by a surge in the volatility of GDP, which was then followed by a sharp decline which lasted until the mid-1990s (see e.g. McConnell and Perez-Quiros, 2000; Stock and Watson, 2002). We refer to these distinct periods respectively as “great volatility” and “great moderation”. The “great moderation” led to a prolonged period of low volatility, temporarily interrupted during the global financial crisis (see Fig. 1). After the crisis, aggregate volatility decreased even further and the US economy went through a period of “great moderation (again)”.====Traditional explanations of GDP fluctuations relied on economy-wide disturbances, such as aggregate productivity shocks (see e.g. Lucas, 1977). The origins of such aggregate shocks are, however, still a matter of debate. A significant branch of the literature, influenced by the work of Long and Plosser (1983) and Horvath, 1998, Horvath, 2000, advanced the hypothesis that fluctuations at the macro level may originate from micro level, e.g. sectoral, shocks. Dupor (1999) argues instead against the microeconomic origins of aggregate fluctuations on the basis of a diversification argument: in an economy with a large number of sectors, the aggregate effect of idiosyncratic sectoral shocks should decay at a rate proportional to the square root of the number of sectors. More recently, Gabaix (2011) pointed out that when firms' size is heterogeneous and distributed according to a Zipf law, aggregate fluctuations may emerge from shocks originated at the micro level: idiosyncratic shocks to a handful of very large firms do not wash out in the aggregate and decay at a much lower rate than predicted by the diversification argument. The work of Di Giovanni et al. (2014) provides empirical evidence for the importance of firm-specific shocks in explaining aggregate fluctuations. Moreover, Acemoglu et al. (2012) show that the diversification argument does not hold in the presence of asymmetric input-output linkages: production complementarities may lead to the amplification of sector-specific shocks and generate aggregate volatility.====Focusing on the origins of the large fall in GDP volatility associated with the “great moderation”, several papers provided empirical evidence on the importance of sectoral sources in explaining aggregate fluctuations. Proposed explanations typically connect changes in aggregate volatility to either changes in the weights of different sectors in the economy or changes in the volatility within sectors. McConnell and Perez-Quiros (2000) relate the decline in GDP volatility occurred around 1984 to a decline in the volatility within the sector of durable goods. Carvalho and Gabaix (2013) link changes in aggregate volatility to changes in the structure of the economy described by the time variation of sectoral sales over GDP. Moro (2012) explains the “great moderation” by means of a structural change described by an increase in the size of the services sector (less intensive in intermediate inputs) relative to the manufacturing sector (more intensive in intermediate inputs). Moro (2012) also hypothesizes that the time variation of total factor productivity (TFP) at the sectoral level played an important role in shaping aggregate volatility in the first half the 2000s.====In this paper we consider both a time-varying structure of sectoral weights in the economy and time-varying sectoral TFP volatility, and study how their interaction shaped GDP volatility. We frame our analysis in a model with ==== sectors using capital, labor and intermediate goods to produce gross output along the lines of Long and Plosser (1983), Jones (2011) and Carvalho and Gabaix (2013). Our theoretical framework links the dynamics of GDP volatility to the following time-varying terms: (====) the weights of each sector in the economy defined as “Domar weights”, i.e. sectoral gross nominal output over GDP (Domar, 1961), ==== sectoral TFP volatilities, ==== a multiplying factor which depends on labor and capital supply decisions of households in the economy.====We calibrate the model to US data from 1960 to 2018 and perform a series of counterfactual exercises in order to isolate (====) the impact of time-varying sectoral TFP volatilities on GDP volatility, ==== the amplification of idiosyncratic sectoral productivity shocks due to the presence of production linkages, ==== the role played by heterogeneity in the Domar weights. Such heterogeneity reflects asymmetries in the weights of different sectors in the production system. Using our calibrated model we are able to identify the causes of changes in aggregate volatility observed in the last 50 years.====Our main findings can be summarized as follows. First, sectoral TFP volatilities are an important driver of GDP volatility. As an example, we find that the evolution of aggregate volatility from 1995 onwards has been essentially caused by changes in sectoral volatilities rather than changes in the structure of the production network. However, sectoral TFP volatilities alone are not able to fully describe the behavior of aggregate fluctuations. Second, the presence of an input-output network amplifies sectoral-level fluctuations. In fact, the higher the degree of overall intermediate input-intensity of production, the stronger the amplification effect. Third, the impact of heterogeneity in the Domar weights has greatly changed over time. In particular, from 1970 until mid-1980s and from early 2000s until mid-2010s, asymmetry in sectoral weights had an amplifying effect on volatility, with a rather strong impact around 1980. In the rest of the sample, the asymmetric structure of the production system had an almost neutral, or even dampening effect on aggregate volatility. These different impacts cannot be explained by different levels of asymmetry alone, but a major role is played by changes over time in the correlation between Domar weights and sectoral TFP volatilities. For example, our findings show that during the “great moderation” the level of asymmetry remained almost unchanged, while the correlation between Domar weights and TFP volatilities became more negative. Finally, we show that the dynamics of GDP volatility can be largely explained by the contribution of just a few sectors.====Our paper contributes to the literature on the microeconomic origins of aggregate fluctuations. It is most related to Carvalho and Gabaix (2013), who introduce a measure of “fundamental volatility”, i.e. volatility derived only from sectoral shocks, and show that GDP volatility tracks fundamental volatility. In Carvalho and Gabaix (2013), the evolution of fundamental volatility only depends on sectoral weights changing over time, while sectoral TFP volatilities are constant. We build on their theoretical framework and introduce time-varying micro-level TFP volatilities.==== We decompose changes in fundamental volatility to measure the relative contributions of changes in sectoral TFP volatilities and in the Domar weights, finding that changes in idiosyncratic volatilities play a predominant role in explaining variations in fundamental volatility. In fact, we find that changes in Domar weights cannot account for movements in aggregate volatility observed between 1995 and 2018. Moreover, also during the period 1960s – 1990s, Domar weights can only partially account for observed changes in aggregate volatility. Furthermore, our results suggest that the time-varying interaction between Domar weights and sectoral TFP volatilities is a key driver of aggregate volatility. Therefore, explanations of the volatility dynamics based only on changes of sectoral weights in the economy or only on changes in sectoral volatilities may overlook the important role played by the interaction between these two factors.====This paper also contributes to the debate on the alleged end of the “great moderation”. A strand of the literature suggests that the financial crisis marked the end of the “great moderation” period (see e.g. Taylor, 2011; Stock and Watson, 2017). Other contributions do not find any structural break in volatility (see e.g. Gadea et al., 2018; Charles et al., 2018), suggesting instead that the peak in volatility observed during the crisis was an episode rather than a structural increase in GDP fluctuations. We contribute to this debate by showing that aggregate volatility indeed increased in the years before the crisis. This increase was relatively small compared to the “great volatility” period and mainly caused by an increase in TFP volatility of finance-related sectors. However, aggregate volatility sharply declined after the crisis, leading to a new period of very low volatility. This suggests that the financial crisis was an isolated episode in a period of generally low volatility.====The rest of the paper is organized as follows. In Section 2 we develop the theoretical model underpinning our empirical analysis. In Section 3 we discuss the data used for our analysis and the calibration of the model. In Section 4 we discuss the counterfactual analysis that will then be used in Section 5 to explain the history of US GDP volatility in the last 50 years. Section 6 concludes.","Great volatility, great moderation and great moderation again",https://www.sciencedirect.com/science/article/pii/S1094202521000326,19 April 2021,2021,Research Article,89.0
"Jovanovic Boyan,Ma Sai","New York University, United States of America,Federal Reserve Board, United States of America","Received 10 August 2019, Revised 3 April 2021, Available online 19 April 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.04.002,Cited by (9),"This paper documents several facts on the real effects of economic uncertainty. First, higher uncertainty is associated with a more dispersed distribution of output growth. Second, the relation is highly asymmetric: A rise in uncertainty is associated with a sharp decline in the lower tail of the growth distribution whereas it has a much smaller and insignificant impact on its upper tail. We build a model in which growth and uncertainty are both endogenous: rapid adoption of new technology raises economic uncertainty and may occasionally cause measured productivity to decline. The equilibrium growth distribution is negatively skewed and higher uncertainty leads to a thicker left tail.","The contemporary macro literature often finds uncertainty about the future to be an important driver of economic fluctuations. A growing body of work proposes uncertainty as a cause of economic slowdowns and sluggish recoveries. For example, Bloom (2009) and Bloom et al. (2018) argue that higher uncertainty stems from the process governing technological innovation, which subsequently causes a decline in real activity.====Empirically, this evidence has been found to be robust to the use of various proxy variables such as implied stock volatility (VIX), economic policy uncertainty (EPU) from Baker et al. (2016), or a broad-based measure of macroeconomic and financial uncertainty, as in Jurado et al. (2015) (JLN) and Ludvigson et al. (forthcoming) (LMN). However, these papers usually investigate the impact of higher uncertainty on mean growth either via a linear forecasting regression or a structural vector autoregression (SVAR). Consequently, they are silent about the effect of uncertainty on the volatility or other ==== moments of the growth distribution, and this may underestimate the impact of economic uncertainty on growth downside risk.====In this paper, we provide evidence that uncertainty is highly correlated with the higher moments of the growth distribution. Fig. 1 depicts the contemporaneous relationship between JLN macro uncertainty and 36-month rolling window average growth of industrial production (hereafter “IP”) in the left panel, IP growth ==== in the middle panel, and IP growth ==== in the right panel. While uncertainty is highly negatively correlated (====) with mean growth as the literature has shown, we find that uncertainty is also highly correlated with growth volatility (==== and growth skewness (====). Therefore, higher uncertainty is not only associated with lower mean growth but also contributes to a more dispersed and negatively skewed growth distribution.====We estimate the distribution of future real growth of IP as a function of uncertainty measures using quantile regression methods.==== We document three stylized facts. First, higher economic uncertainty is associated with a more dispersed and left-skewed future growth distribution. A one-standard-deviation increase in uncertainty statistically significantly increases the interquartile range of the one-month ahead annualized growth distribution by 2%, and decreases the lower 5th percentile by 5%. This indicates that the marginal effects of higher uncertainty significantly raise growth downside risk.====Second, the response of IP growth to changes in uncertainty is highly asymmetric, the response being much higher when uncertainty rises than when it falls. An increase in uncertainty is associated with a larger decrease in the lower tail of the growth distribution while it has a much smaller impact on its upper tail. These results suggest that higher uncertainty could lead to an abrupt economic decline whereas lower uncertainty does not necessarily rebound the economy from the recession.====Motivated by this evidence, we present and estimate an endogenous growth model that generates uncertainty. Equilibrium growth results from the adoption of technologies of uncertain quality. A technology's “quality” refers to how closely the technology's needs match the economy's input endowments. Since technology needs are unpredictable, its rapid irreversible adoption raises uncertainty and can even lead to a decline in output and, hence, to negative growth.====The model follows the putty-clay tradition of Johansen (1959), and assumes irreversible technological commitment, building on Ramey and Ramey (1991) and Jovanovic (2006). It derives closed-form expressions for equilibrium growth and its distribution, and studies the relationship between uncertainty and higher moments of the growth distribution. Uncertainty thus affects growth because of technological commitment and the ==== mismatch between the technology and the firm's inputs. Firms adopt technologies the exact character of which they do not know; a firm may have to commit to the scale of its factory and to the size of its labor force, as argued by Ramey and Ramey (1991), or it may have to commit to the skill and occupational composition of the labor force. The skill gap is distributed symmetrically but the ==== of that gap is quadratic and maximal losses from technology adoption exceed the maximal gains. The firm's growth rate is therefore negatively skewed as observed in the data.====In our model a new technology's match to a firm's asset endowments is revealed only after the firm commits to that technology; if it postpones investment and waits, the firm cannot see the shock even partially. As a result, a rise in the variance of the rate of return to technological advancement reduces the growth rate of output and the reduction is so large that the volatility of growth declines too. This is in partial contrast to models of irreversible investment in which information about an investment's return accumulates while the investment is being postponed: Bernanke (1983), McDonald and Siegel (1986), and Dixit et al. (1994) in which the variance of a firm's output growth is lower while the option to wait is being exercised, and it then may rise when the option to invest is exercised. In our model the volatility of output growth declines if the rate volatility of the return to investment rises. Moreover the negative relation between the shock variance and the resulting growth-rate variance is negative regardless of whether the change in the shock volatility is permanent or transitory.====The model provides an analytic characterization of the equilibrium growth distribution. Consistent with our empirical evidence, higher uncertainty about the newly adopted technologies is associated with lower average growth as well as a more dispersed and negatively skewed growth distribution. When we vary the shock variance while constraining the parameters to target the long run mean and standard deviation of growth, the model generates a positive relation between asset price volatility, as measured from the estimated option prices of a Lucas (1978) representative security, and the thickness of the lower tail of the growth distribution. Using quantile regression estimates, we further construct the Growth-at-Risk measure and study its interaction with asset pricing volatility, occupational mobility, and capacity utilization. To the best of our knowledge, we are the first to provide a theoretical framework to study the impact of uncertainty on higher moments of the growth distribution both empirically and theoretically.",Uncertainty and growth disasters,https://www.sciencedirect.com/science/article/pii/S1094202521000314,19 April 2021,2021,Research Article,90.0
"Miao Jianjun,Wang Pengfei,Zhou Jing","Department of Economics, Boston University, 270 Bay State Road, Boston, MA 02215, United States of America,Peking University HSBC Business School, University Town, Nanshan District, Shenzhen, 518055, China,School of Economics, Fudan University, 600 Guoquan Road, Shanghai, 200433, China","Received 5 June 2020, Revised 12 March 2021, Available online 19 April 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.03.005,Cited by (0),We provide an estimated ,"The 2008 financial crisis generated renewed interest in asset bubbles (Martin and Ventura, 2012; Miao and Wang, 2018). It is often argued that the Federal Reserve's excessively low interest rate policy in the early 2000s helped cause a housing bubble in the United States and that the collapse of the bubble led to an unprecedented global financial crisis. Emerging economies seem particularly vulnerable to this type of “bubble-driven” crisis. When a bubble emerges, asset prices, investment, and capital inflows all enjoy booms and then suddenly suffer remarkable reversals. Examples include Chile and Mexico in the early 1980s, Argentina and Mexico in the 1990s, and Southeast Asia, namely, Malaysia, Indonesia, Thailand, and South Korea, in the 1990s. Since interest rates in emerging economies are strongly driven by interest rates in advanced economies (Neumeyer and Perri, 2005; Uribe and Yue, 2006; and Maćkowiak, 2007), some observers have argued that strong expansionary monetary policies implemented by major advanced economies in the aftermath of the 2008 financial crisis would cause bubbles and inevitable crises in emerging countries. Given the prevalence of “bubble-driven” crises in emerging countries, understanding and quantifying the role of foreign interest rates on bubbles is therefore very important in accounting for the business cycle fluctuations in these countries.====The goal of our paper is to develop and estimate a dynamic stochastic general equilibrium (DSGE) model of asset bubbles to understand the impact of foreign interest rate shocks on macroeconomic fluctuations and asset prices in emerging economies. We extend the recent development of theories of asset bubbles in infinite-horizon models with idiosyncratic shocks and credit frictions to a small open economy model. In addition to one-period bonds, firms can also trade an intrinsically useless asset. If all agents believe that the intrinsically useless asset has value, it serves as self-insurance against idiosyncratic shocks by providing liquidity for firms to finance real investment.==== This belief can be self-fulfilling and results in a bubbly equilibrium with stock prices of firms being inflated by the bubble. On the other hand, when no one believes that the intrinsically useless asset has value, this belief can also be self-fulfilling and results in a bubbleless equilibrium.====We first provide a steady-state analysis of the impact of the foreign interest rate on the bubble in an infinite-horizon small open economy. We prove that a low foreign interest rate causes capital inflow and a low domestic interest rate, thereby fueling a domestic asset bubble. The size of the bubble decreases with the foreign interest rate. When the foreign interest rate is sufficiently high, the asset bubble can burst. This result extends the existing results of Tirole (1985), Santos and Woodford (1997), Miao et al. (2015b), and Miao and Wang (2018) for closed economies.====Our most important contribution is to quantitatively evaluate the impacts of foreign interest rate shocks by taking our model to the Mexican data over the period 1990Q1-2011Q4 using Bayesian estimation. To the best of our knowledge, our paper provides the first estimated DSGE model with asset bubbles in a small open economy. To provide a fair evaluation of foreign interest rate shocks, we also include four other types of exogenous shocks often used in the literature (long- and short-run productivity shocks, preference shocks, and foreign demand shocks) to fit five time series of the demeaned foreign interest rate, the growth rate of the sum of US and Canadian real GDP, and the growth rates of Mexican real GDP, real investment, and real consumption. We find that our estimated model matches the Mexican business cycle data including stock prices reasonably well. In particular, our estimated model matches the salient features of high volatilities of consumption and stock prices relative to output, strong countercyclical trade balance, and procyclical stock prices. In addition, our estimated impulse responses to a foreign interest rate shock from the model match those from the data remarkably well. Following a rise in the foreign interest rate, real stock prices decline substantially, while GDP, investment and consumption all decrease significantly at the same time. Accompanied by significant real depreciation, the trade balance increases significantly. The magnitude of these responses in the model is very close to the VAR evidence in the data. As we only use a subset of data for our Bayesian estimation, fitting all VAR responses provides a very convincing post-estimation check for the model.====Our estimation suggests that foreign interest rate shocks play a nontrivial role, particularly in explaining the variation in the net exports/GDP ratio (33.9% in the impact period), stock prices (27.9% in the impact period), and investment (23.6% in the impact period). The trend shocks to productivity are also important. They explain the bulk of the variation in the net exports/GDP ratio (43.0% in the impact period), stock prices (39.7% in the impact period), and investment (16.0% in the impact period). Our key insight is that asset bubbles provide a powerful amplification and propagation mechanism, which is essential for these results. When the bubble channel is shut down, the correlation between the net exports/GDP ratio and GDP reduces to −0.14 from −0.53, too weak compared to the data (−0.50). The correlation between stock prices and consumption becomes −0.46, producing an incorrect sign that is in contrast to those in the model with bubbles (0.37) and in the data (0.53). Furthermore, the correlation between the net exports/GDP ratio and stock prices increases to 0.71 from −0.66, clearly in conflict with the data (−0.45). Moreover, the relative volatility of stock prices decreases from 4.82 to 2.25, significantly below the 6.39 observed in the data. In addition, in this case, both foreign interest rate shocks and trend productivity shocks play essentially no role in explaining the variation in the net exports/GDP ratio, stock prices, and investment.====To further investigate the importance of the amplification and propagation mechanism generated by bubbles, we also estimate a model without asset bubbles. We find that this model performs much worse. In particular, it cannot match the high volatilities of consumption and stock prices or the significant countercyclicality of trade balance. The model without asset bubbles also produces an incorrect sign of the correlation between stock prices and real consumption. Furthermore, the estimated volatilities of the long- and short-run productivity shocks and preference shocks are several times larger than their counterparts in the model with bubbles, highlighting the necessary role of bubbles in amplifying and propagating exogenous shocks to macroeconomic fluctuations. After a rise in the foreign interest rate, the responses of stock prices, GDP, consumption, investment, net exports and the real exchange rate show the same signs as the data, but their magnitudes are too small. Note that we have excluded stock prices from both estimations to avoid the natural disadvantage of the bubbleless model to produce volatile asset prices. Nevertheless, the data still favor the model with asset bubbles based on the marginal density of macroeconomic data alone.====Our paper is related to three strands of the literature. First, our paper builds on the literature on international real business cycles (RBCs).==== Aguiar and Gopinath (2007) argue that the long-run productivity shock in a standard RBC model is important to explain the high consumption volatility and the countercyclical trade balance in emerging markets. By contrast, García-Cicco et al. (2010) find that when estimated over a long sample, the RBC model driven by permanent and transitory productivity shocks performs poorly at explaining observed business cycles in Argentina and Mexico along a number of dimensions. Neumeyer and Perri (2005) and Uribe and Yue (2006) argue instead that the introduction of foreign interest rate shocks coupled with financial frictions is important to explain the empirical regularities of emerging economies. Chang and Fernández (2013) estimate a model with financial frictions that includes all three types of shocks using Bayesian methods. They find that financial frictions play a dominant role in amplifying conventional productivity shocks and, less markedly, interest rate shocks; trend shocks, in contrast, play a very minor role. This literature typically ignores asset prices and does not study the high stock market volatility and the comovement between stock prices and the real economy. We contribute to this literature by showing that asset bubbles are important to explain these facts. This contribution is important because neoclassical models do not provide a fully adequate explanation of asset price movements (Schmitt-Grohé and Uribe, 2012). Our estimated model shows that both long-run productivity shocks and interest rate shocks are important drivers of business cycle fluctuations in emerging economies.====Second, our paper is related to the recent literature on sudden stops (e.g., Calvo, 1998; Gopinath, 2004; Martin and Rey, 2006; Gertler et al., 2007; Mendoza, 2010; Fernández and Gulan, 2015; Aoki et al., 2016; and Perri and Quadrini, 2018).==== This literature views credit frictions as the central feature of the transmission mechanism that drives sudden stops. Mendoza (2010) also emphasizes the amplification and asymmetry of macroeconomic fluctuations that result from the debt-deflation transmission mechanism. Asset prices in this literature typically refer to capital prices or Tobin's Q. By contrast, firms can own both capital and bubble assets in our model. The stock market value of firms contains a fundamental component, equal to Tobin's Q multiplied by the capital stock, and a bubble component, equal to the value of the bubble asset. The movement of both Tobin's Q and asset bubbles contributes to stock market fluctuations. Our model can hence produce the aforementioned stylized facts of a “sudden stop” when foreign interest rates suddenly change from low to high, causing a boom and bust in the real economy through the expansion and contraction of bubbles.====Third, our paper is related to the recent literature on asset bubbles in open economies (e.g., Caballero and Krishnamurthy, 2006; Ventura, 2012; Basco, 2014; and Martin and Ventura, 2015a, Martin and Ventura, 2015b). This literature typically adopts the overlapping-generations (OLG) framework. Like our paper, this literature emphasizes the importance of credit constraints. Martin and Ventura, 2012, Martin and Ventura, 2015a, Martin and Ventura, 2015b also discuss the crowding-in and crowding-out effects of asset bubbles, similar to those in our paper. Our paper differs from this literature in the addressed questions and modeling details. More importantly, unlike the OLG models, our model is in the DSGE framework, which can confront data using Bayesian estimation. In our infinite-horizon framework, credit constraints are essential for the emergence of asset bubbles, as in Kiyotaki and Moore (2019), in the sense that a bubble could not emerge without credit constraints. By contrast, a bubble can still emerge in OLG models without credit constraints, and their presence allows bubbles to emerge in dynamically efficient economies (Farhi and Tirole, 2012; and Martin and Ventura, 2012). Our infinite-horizon DSGE model complements the existing OLG models.====Finally, our paper is related to studies that quantitatively investigate the importance of asset bubbles. A notable example is the paper by Miao et al. (2015a), who show that asset bubbles driven by sentiment shocks help explain the high stock price volatility and the comovement between the stock market and the real economy in a closed economy model. Our study differs from their work in three respects. First, the international real business cycle patterns we attempt to account for in this paper are completely absent in their closed economy setup. Second, the foreign interest rate affects the existence condition of asset bubbles, which is different from that in the closed economy model of Miao et al. (2015a). Third, they emphasize the role of the sentiment shock in stock markets, whereas we focus on the role of the foreign interest rate shock in this paper.",Asset bubbles and foreign interest rate shocks,https://www.sciencedirect.com/science/article/pii/S1094202521000302,19 April 2021,2021,Research Article,91.0
Zhou Xiaoqing,"Federal Reserve Bank of Dallas, United States of America","Received 28 July 2019, Revised 2 March 2021, Available online 7 April 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.03.004,Cited by (3),This paper studies the transmission of the major shocks in the U.S. housing market in the 2000s to consumption and ,"In the years leading up to the 2008 financial crisis, the U.S. housing market experienced sharply rising house prices, declining mortgage rates and an expansion of credit. As a result, households aggressively borrowed against their homes and household debt reached an unprecedented level. Mortgage borrowing by existing homeowners has been widely viewed as instrumental for the propagation of housing market shocks to the broader economy in the 2000s. Since household borrowing is closely related to spending, an important question is to what extent shocks to the housing market affect aggregate household expenditures. To date, the literature has focused on the response of consumption to these shocks. The response of residential investment (which includes construction, improvements and maintenance), in contrast, has received little attention. As Fig. 1 shows, residential investment, like consumption, co-moved strongly with mortgage borrowing during the last housing cycle.====In this paper, I explore empirically and theoretically how consumption and residential investment responded to the major shocks in the U.S. housing market in the 2000s. While my analysis corroborates existing studies on the consumption responses, it shows that residential investment was more responsive to these shocks than consumption, as measured by elasticities and the implied contributions to GDP growth. This result holds both in the data and in a structural model that is consistent with the micro-level evidence on collateralized borrowing and household expenditures. Thus, residential investment played a key role in the propagation of housing market shocks to aggregate expenditures during the last housing cycle.====The first part of the paper provides detailed empirical evidence on the responses of consumption and residential investment to three major housing market shocks: shocks to house prices, mortgage rates and credit condition. Identifying the causal effects of these shocks is known to be difficult. I employ empirical strategies recently developed in the literature to identify each shock using geographically disaggregated data. To facilitate the comparison across shocks and outcomes, I use the same measures of expenditures at the metropolitan statistical area (MSA) level for each shock.====There are three key empirical findings. First, the magnitudes of the consumption responses are consistent with estimates in the existing literature. Second, the response of residential investment to each of these shocks is much larger than that of consumption. Third, even after taking into account the much lower share of residential investment in aggregate expenditures, the implied contribution of residential investment to output growth is larger than that of consumption. For example, in response to a one standard-deviation shock to the house price, all else equal, consumption contributes a 0.3 percentage point (pp) increase to annual GDP growth, while residential investment contributes a 0.4 pp increase. In response to mortgage rate or credit supply shocks, the contribution of residential investment relative to that of consumption is even larger. These results jointly suggest that residential investment was as important as consumption, if not more important, in driving fluctuations in U.S. aggregate expenditures in the 2000s.====This raises the question of whether a standard macro model with consumption and housing investment can explain this empirical evidence. If it can, what are the mechanisms that give rise to the large responses of residential investment at the aggregate level? More importantly, is the model supported by the micro-level evidence on the link between borrowing and expenditures, as well as the heterogeneity of this link across households? To answer these questions, I develop a quantitative life-cycle model building on the workhorse consumption model of Berger et al. (2018). This type of model has been shown to be able to capture the heterogeneous consumption responses as in the microdata. The novel feature of my model is that households can choose between different types of housing investment (moving to a new home or making home improvements). This model feature allows me to quantify the use of mortgage borrowing for different kinds of housing investment as in the data and to study which type of investment is more responsive to shocks.====The other elements of the model are more standard. Over the life cycle, households earn income, accumulate assets, and make spending and borrowing decisions. There are two consumption goods (housing and non-housing) and three assets (liquid savings, houses and mortgage debt). Households face uninsurable labor income risks, collateral constraints and liquidity constraints. Mortgage contracts are long-term, making it costly to deviate from the contracted debt level. These features ensure that the model generates rich heterogeneity in household balance sheets, consumption and housing investment, and that the model can be directly evaluated by microdata.====Data simulated from the steady state of the model match salient life-cycle and cross-sectional patterns in the microdata. In particular, the model generates a strong correlation between homeowners' mortgage borrowing and their housing investment, indicating the substantial use of mortgage loans by existing homeowners for housing investments (either through home equity extraction or new home-purchase loans). Moreover, these correlations are strongest among young homeowners. These patterns arise in the model because young households start their life with an affordable home, the size of which is below the target level. Thus, additional investments are desirable. However, the income of young households is too low and they do not have much liquid savings. As a result, households accumulate their home equity by gradually paying down the current mortgage and making small housing investments. When they have accumulated enough home equity, they pay a fixed cost to tap this equity and reinvest it in housing in a lumpy way. This explains the correlation between borrowing and housing investment in the data and illustrates how households use debt to move up the property ladder in the absence of aggregate shocks.====When shocks to the housing market increase the borrowing capacity of households (e.g., higher house prices, lower mortgage rates and easier access to credit), households push forward their housing investments or increase the amount they would have invested otherwise, which leads to large responses of residential investment at the aggregate level. Using the model, I quantify the impacts of the three housing market shocks on consumption and residential investment and show that they are consistent with the empirical evidence. The model suggests that each shock spurs the borrowing of young households on average, and that the spending on housing investment increases most for those who raise their mortgage debt.====In the last part of the paper, I consider the joint impact of the three housing market shocks in the context of the U.S. economy in the 2000s. For this purpose, I simulate the evolution of aggregate consumption and residential investment by feeding the time series of real house prices, real mortgage rates, and a measure of credit condition into the model. The simulated paths match broadly the boom-bust cycles in the corresponding U.S. data. Further analysis suggests that housing investments by young homeowners disproportionately accounted for residential investment fluctuations over this period, whereas their consumption did not appear to have driven aggregate consumption dynamics. In other words, residential investment is more closely related to the changing borrowing capacity of young households in the 2000s than consumption.====To formalize this policy implication, I conduct a counterfactual analysis that assesses the extent to which volatility in aggregate expenditures would have been reduced if households' borrowing ability had been restricted through higher down-payment requirements. The results show that both the consumption growth volatility and the residential investment growth volatility would have declined, but the decline would have been larger for residential investment growth. For example, if households had been required to provide at least 50% of their house value as a down payment in 2000 (compared to the actual down-payment requirement of 20%), the volatility of consumption growth in the 2000s would have fallen by 28%, whereas the volatility of residential investment growth would have fallen by 58%.",Mortgage borrowing and the boom-bust cycle in consumption and residential investment,https://www.sciencedirect.com/science/article/pii/S1094202521000272,7 April 2021,2021,Research Article,92.0
"Atkinson Tyler,Plante Michael,Richter Alexander W.,Throckmorton Nathaniel A.","Research Department, Federal Reserve Bank of Dallas, 2200 N Pearl Street, Dallas, TX 75201, United States of America,Department of Economics, William & Mary, P.O. Box 8795, Williamsburg, VA 23187, United States of America","Received 9 February 2021, Revised 25 March 2021, Available online 1 April 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.03.003,Cited by (1)," uncertainty regularly fluctuates in the data. Theory suggests complementarity between capital and labor inputs in production can generate time-varying endogenous uncertainty because the concavity in the production function influences how output responds to productivity shocks in different states of the economy. This paper examines whether complementarity is a quantitatively significant source of time-varying endogenous uncertainty by estimating a nonlinear ==== with a ==== and exogenous volatility shocks. When matching labor share and uncertainty moments, we find at most 16% of the volatility of uncertainty is endogenous. An estimated model without exogenous volatility shocks can endogenously generate all of the empirical variation in uncertainty, but only at the expense of significantly overstating the volatility of the labor share.","Macroeconomic uncertainty regularly fluctuates in the data. The literature typically accounts for these fluctuations in business cycle models using shocks to the variance of an exogenous variable, such as productivity, while holding its conditional mean fixed. That produces estimates for the responses of output to an ==== increase in uncertainty, but it is silent about whether uncertainty ==== fluctuates over time in response to first-moment shocks and the state of the economy.====Straub and Ulbricht (2019) show that variance-preserving shifts in the productivity distribution across firms cause endogenous fluctuations in the cross-sectional dispersion of output when capital and labor are gross complements in production because each firm's output becomes a concave function in productivity.==== We build on their work in two ways. First, we show that complementarity also generates endogenous fluctuations in aggregate uncertainty—the expected forecast error volatility of future aggregate output. Complementarity can endogenously generate fluctuations in aggregate uncertainty because the concavity in the production function influences how output responds to first moment shocks in different states of the economy. For example, a positive labor productivity shock generates a larger change in output when the capital-to-labor ratio is high compared to when the capital-labor ratio is low because complementarity increases the marginal product of labor when capital is abundant. The differences in the responses imply that forecasts for output and hence the level of uncertainty also depend on the initial state, creating time-varying endogenous uncertainty.====Second, we examine whether the endogenous variation in aggregate uncertainty is quantitatively significant. To conduct our analysis, we estimate a real business cycle model with a constant elasticity of substitution (CES) production function, exogenous volatility shocks to productivity, and real frictions in the form of habit persistence in consumption and investment adjustment costs. We estimate the nonlinear model using a global solution method and a simulated method of moments that targets uncertainty, real activity, and labor share moments. The model generates movements in aggregate uncertainty through exogenous volatility shocks and endogenous fluctuations due to complementarity in the production function. Allowing for exogenous and endogenous movements in uncertainty lets the data determine the relative importance of the two mechanisms.====We begin by looking at the predictions of the model without exogenous volatility shocks to focus on the effects of complementarity. We find that the model without exogenous volatility can successfully match either the uncertainty moments or the labor share moments, but not both. When the labor share moments are excluded from the set of empirical targets, the model is able to endogenously generate all of the empirical variation in aggregate uncertainty, confirming the theory that gross complementarity can generate significant variation in aggregate uncertainty. However, the estimated CES is only 0.14, well below almost all estimates in the literature, and the volatility of the labor share is about six times higher than it is in the data. Alternatively, when we exclude the uncertainty moments, the labor share moments pin down the estimated value of the CES, which increases to 0.49. However, the higher estimate leads to far less concavity in the production function, so the model is able to generate only about 20% of the volatility of aggregate uncertainty in the data.====We then turn to our baseline model with exogenous volatility shocks, which offers a competing mechanism for the variation in uncertainty. The model jointly matches the volatility of uncertainty and labor share dynamics, and the CES estimate is the same as when we match labor share moments without exogenous volatility shocks. However, a forecast error variance decomposition reveals that endogenous uncertainty explains at most 16% of the variation in aggregate uncertainty. A lower CES would increase the strength of the endogenous uncertainty channel, but it would also cause the model to overstate the empirical volatility of the labor share. Despite the theoretical appeal of complementarity, these results show that it plays a limited role in generating the fluctuations in macro uncertainty. However, the model underpredicts the cyclicality of uncertainty, which suggests there is a potentially important role for other sources of time-varying endogenous uncertainty.====We also find the dynamics of real activity under a Cobb-Douglas production function are similar to those implied by our estimated CES production function. Under a Cobb-Douglas production function with constant returns to scale, the labor share is constant, which motivates using a more general CES production function. However, we find the empirical volatility of the labor share leads to a CES estimate that is not small enough to generate meaningful differences in the dynamics implied by the two production functions. This suggests a Cobb-Douglas production function provides a reasonable approximation of business cycle dynamics, even when accounting for nonlinearities.====  A large literature has estimated the elasticity of substitution between capital and labor. Klump et al. (2012) surveys reduced-form estimates of the CES. Across 17 studies, the average CES is 0.58, well below the unit elasticity implied by Cobb-Douglas production. Oberfield and Raval (2021), who account for substitution across plants, firms, and manufacturing industries, estimate a CES for the U.S. manufacturing sector between 0.5 and 0.7. We obtain a similar CES estimate of 0.49 in our baseline model using a different framework and identification strategy. We pin down the CES in a one-sector business cycle model using quarterly variation in the labor share and macro uncertainty. When we do not target the labor share, the CES falls to 0.14. This estimate is consistent with Cantore et al. (2015), who estimate a New Keynesian model with a CES production function but without targeting labor share dynamics. While these estimates generate more endogenous variation in uncertainty, they cause the model to over-predict the volatility of the labor share.====Several papers have also estimated a CES above unity (Karabarbounis and Neiman, 2014; Eden and Gaggl, 2018; Hubmer, 2020). This work identifies the CES by focusing on long-run trends, such as the decline in the labor share or the relative price of investment goods. Our estimation procedure focus on business cycle dynamics and therefore matches the short-run properties of the de-trended labor share.==== However, the two sets of estimates are not necessarily inconsistent, as it is eminently plausible that the long-run elasticity is higher than the short-run elasticity we estimate.====Several papers study alternative sources of endogenous uncertainty. One segment emphasizes the role of a financial sector, where the severity and duration of financial crises are stochastic. Most papers focus on crises that result from financial frictions, collateral constraints, or the zero lower bound constraint on the nominal interest rate (Mendoza, 2010; Brunnermeier and Sannikov, 2014; He and Krishnamurthy, 2019; Plante et al., 2018), while others incorporate the role of firm default (Gourio, 2014; Navarro, 2014; Arellano et al., 2019). A separate segment of the literature examines the implications of incomplete information. Some feature learning with aggregate shocks (Van Nieuwerburgh and Veldkamp, 2006; Fajgelbaum et al., 2017; Saijo, 2017), while others focus on firm-specific shocks (Straub and Ulbricht, 2015; Ilut and Saijo, 2021). In these models, adverse shocks under asymmetric learning reduce economic activity and make it harder for agents to learn about the economy, amplifying equilibrium dynamics. Finally, recent papers have shown that search and matching frictions can generate uncertainty (Ilut et al., 2018; Bernstein et al., 2020). Consistent with many of these mechanisms, uncertainty in our model occurs at business cycle turning points. One major benefit of our mechanism is that it is easy to incorporate into any model.====There are also papers that study the effects of exogenous volatility shocks. For example, this literature has examined volatility shocks to technology (Bloom, 2009; Leduc and Liu, 2016), fiscal policy (Born and Pfeifer, 2014; Fernández-Villaverde et al., 2015), monetary policy (Mumtaz and Zanetti, 2013), and the real interest rate (Fernández-Villaverde et al., 2011). We build on this literature by examining the role of exogenous volatility shocks in driving aggregate uncertainty. Our model accounts for exogenous and endogenous sources of uncertainty, and our estimation disciplines the stochastic processes by matching the volatilities of both real activity and aggregate uncertainty. This allows the data to decide which source is driving the empirical variation in uncertainty.====Our paper also makes two technical contributions to the literature. First, we estimate our model using a global solution method, which is crucial to calculate aggregate uncertainty and account for state-dependance. Cantore et al. (2015) estimate a similar model but use a linear solution method and focus on issues besides uncertainty. Similarly, the exogenous uncertainty literature typically applies third-order perturbation methods. Second, we are the first to discipline uncertainty dynamics in a business cycle model by directly linking to uncertainty dynamics in the data. We rely on the real uncertainty series in Ludvigson et al. (2020) over other popular measures of uncertainty (e.g., realized volatility, indexes based on keywords in print or online media, survey-based forecast dispersion) since it is based on the same statistic we use to measure uncertainty in our structural model.====The paper proceeds as follows. Section 2 explains the underlying nonlinearity in a simplified setting. Section 3 presents our quantitative model. Section 4 re-examines the source of the nonlinearity in our quantitative model. Section 5 presents our estimation results, and Section 6 concludes.",Complementarity and macroeconomic uncertainty,https://www.sciencedirect.com/science/article/pii/S1094202521000284,1 April 2021,2021,Research Article,93.0
"Bloise Gaetano,Vailakis Yiannis","Yeshiva University, New York, United States,University of Rome III, Rome, Italy,Adam Smith Business School, University of Glasgow, United Kingdom","Received 1 June 2020, Revised 5 March 2021, Available online 17 March 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.03.001,Cited by (0),We extend and refine ===='s contraction approach to ====.,"In this paper we provide a characterization of the canonical Eaton and Gersovitz (1981)'s model of sovereign debt under time-varying interest rates and growth. In particular, as in Aguiar and Amador (2019), we show that equilibrium obtains as the unique fixed point of a contraction mapping, provided that interest rates are high relative to growth. The contraction property is the reflection of the more traditional arbitrage argument, originally inspired by Bulow and Rogoff (1989) and recently exploited by Auclert and Rognlie (2016) and Bloise et al. (2017). The contraction method immediately yields existence and uniqueness, unifying distinct strands of literature on Eaton and Gersovitz (1981)'s model.====Aguiar and Amador (2019) study a ==== representation of equilibrium under a constant positive interest rate. In particular, as in Aguiar et al. (2019), they consider a sort of minimum expenditure program, in which the amount of outstanding debt is minimized subject to ensuring a given level of utility to the sovereign. This dual transformation of the program allows them to show that equilibrium is determined as a fixed point of a contraction mapping, so establishing uniqueness by means of conventional recursive techniques. To gain a suitable space for the application of the contraction mapping principle, they impose an exogenous upper bound on consumption, a restriction that cannot account for economic growth, as it is instead the case in the quantitative literature (see, among others, Aguiar and Gopinath (2006) and Arellano (2008)).====We extend Aguiar and Amador (2019)'s contraction mapping approach to a more general environment with time-varying interest rates and growth. However, differently from them, we work with the more natural ==== representation of equilibrium and instead consider an adjusted (extended) metric on a suitable space of candidate equilibrium value functions. This seems preferable because the contraction property obtains straightaway. It emerges as the counterpart of the pure arbitrage argument originally used by Bulow and Rogoff (1989): any optimal plan with high borrowing can be replicated with lower level of debt along with an additional saving policy yielding a positive return.====Our general treatment identifies the critical assumption driving equilibrium uniqueness, so implicitly suggesting a channel towards multiplicity. The contraction property requires that long-term interest rates exceed economic growth, and it might fail otherwise. Thus, equilibrium uniqueness heavily relies on the widespread hypothesis of a constant positive interest rate.==== By contrast, old and recent empirical studies (Abel et al. (1989), Blanchard (2018) and Jordà et al. (2019)) document that a low safe rate of interest, relative to growth, is more the historical norm than the exception for developed countries. A time-varying interest rate, recurrently falling below economic growth, reproduces these empirical patterns without necessarily permitting Ponzi games. Under these alternative conditions, sovereign debt markets are prone to self-fulfilling crises (multiple equilibria), as argued by Bloise and Vailakis (2019).====Aguiar and Amador (2019)'s contraction method, and the refinement we provide in this paper, might have other applications in macroeconomics. We indeed notice a rather enlightening feature of the approach: contrary to conventional applications, utility discounting plays no direct role in establishing the contraction property, which only relies on high interest rates relative to growth. Hence, the technique might also succeed in other frameworks in which discounting fails because of time non-additive or non-expected utility. Contraction obtains, irrespectively of discounting, provided that suitable lower and upper bounds exist.====Independently of the contraction approach, we enlarge the set of known conditions for a unique equilibrium in Eaton and Gersovitz (1981). Indeed, we only assume that the interest rate dominates growth in the long term rather than at all contingencies. Therefore, in the short term, and even for prolonged phases, interest rates might remain negative net of growth. To encompass these situations the analysis requires a major innovation, though the logic still rests upon a replication strategy firmly established in the literature. Auclert and Rognlie (2016), for instance, adopt a ‘mimicking at a distance’ policy in which bond positions are increased by a constant amount corresponding to the maximum distance between distinct default thresholds. This simple arbitrage is unsuitable when the interest rate falls recurrently below growth. The replication policy becomes necessarily time-varying, involving plans of accumulation and decumulation of bonds over time depending on contingencies. The existence of these plans can hardly be established constructively and it is here that our method demonstrates its full potential.====We briefly relate the contribution in this paper to our previous work on debt sustainability. In all these papers we apply a ==== method, developed in Bloise et al. (2021), in order to capture tendencies of time-varying interest rates relative to growth. This machinery only serves to separate regimes for long-term interest rates without any direct substantive implication. Our objectives in the papers are rather different: in Bloise et al. (2021) we argue that, at a competitive equilibrium under incomplete markets, debt can be sustained by the reputation for repayment alone; in Bloise and Vailakis (2019) we show that low interest rates expose sovereign debt markets to an increased risk of self-fulfilling solvency crises; in this paper we extend and refine Aguiar and Amador (2019)'s ==== approach to Eaton and Gersovitz (1981)'s equilibrium. The papers are complement and, all together, they suggest that the conditions necessary for contraction are dramatically understated by the (==== heuristic) assumption of a constant positive interest rate.====The paper is organized as follows. In section 2, we set up the model as in the established literature, but allowing for risk-averse creditors and temporal variation of interest rates. In section 3, we construct the primal equilibrium operator and introduce the adjusted metric. Finally, in section 4, we show that the equilibrium operator is a contraction in the adjusted metric. To capture long-term tendencies of interest rates, relative to growth, we adopt the ==== method developed in Bloise et al. (2021). This permits to deal with time-varying interest rates at a level of tractability comparable with that of the more traditional constant interest rate. For completeness, in Appendix A, we verify robustness of our uniqueness theorem. In Appendix B, we provide a method to verify conditions on the dominant root by means of the spectral radius. In Appendix C, finally, we clarify that the contraction property does not require the interest rate to exceed (permanently) growth at all contingencies.",On sovereign default with time-varying interest rates,https://www.sciencedirect.com/science/article/pii/S1094202521000193,17 March 2021,2021,Research Article,94.0
"Harding Martín,Klein Mathias","Bank of Canada, 234 Wellington Street, Ottawa, ON, K1A 0G9, Canada,Sveriges Riksbank, Monetary Policy Department - Research, SE-103 37 Stockholm, Sweden","Received 22 May 2018, Revised 11 February 2021, Available online 4 March 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.02.013,Cited by (0),"This paper investigates the interrelation between household balance sheets, collateral constraints, and ====. Using data on the U.S. economy, we estimate a monetary ==== with financial frictions and occasionally binding borrowing constraints. The model implies stronger effects of ==== interventions when the borrowing constraint is binding compared to situations when it turns slack. In a prediction analysis we find that, out of a set of alternative plausible endogenous model variables, the level of household net worth is the single best predictor of the tightness of the borrowing constraint, which implies that monetary policy is more effective when household net worth is low. We test this model prediction in the data and provide robust empirical evidence on asymmetric effects of monetary policy across the household net worth cycle that validates the model predictions. A contractionary monetary policy shock leads to a large and significant fall in economic activity during periods of low household net worth. In contrast, monetary policy shocks have only small and mostly insignificant effects when net worth is high.","Since the beginning of the 2000s, private household net worth has fluctuated substantially in the U.S. economy. As a fraction of disposable personal income, household net worth increased from 550% in 2002 to almost 680% at the outbreak of the 2008 financial crisis. Due to the massive collapse in house prices, the ratio fell back to 560% in 2011.==== A growing number of mostly theoretical studies interprets this significant adjustment in household balance sheets as the central element to understand the boom and bust period that ended with the Great Recession. Moreover, empirical contributions show that the evolution of households' financial position is crucial for understanding the propagation and amplification of economic shocks and policy interventions.==== In this paper, we contribute to this literature by showing that shifts in the financial position of households significantly affect the transmission mechanism of monetary policy.====Despite the important role of household balance sheets in shaping macroeconomic outcomes, little is known about whether the effectiveness of monetary policy depends on household net worth dynamics. This issue is of particular interest because unconventional monetary policy interventions and massive changes in household net worth evolved in parallel since the financial crisis. If borrowing constraints play an important role for households' saving-consumption decisions and the tightness of collateral constraints varies considerably with the households' net worth position, monetary policy may indeed have asymmetric effects across the household net worth cycle.====Against this background, our contribution in this paper is twofold: first, we rely on the DSGE model by Guerrieri and Iacoviello (2017) (GI, henceforth), which on top of the standard New Keynesian ingredients features financial frictions on the household side. In the model household balance sheets influence how monetary policy shocks transmit to the economy. Specifically, we estimate the model and illustrate that monetary policy has stronger effects when borrowing constraints bind and household net worth is low. Second, we test this result on U.S. data and find robust empirical evidence supporting the model predictions.====We use the estimated DSGE model to study the determinants of when borrowing constraints bind. Specifically, we generate artificial data from the model and conduct a prediction analysis to shed light on which endogenous model variable best predicts the tightness of the borrowing constraint. We look at several possible candidates commonly highlighted in the literature (see, e.g., Drehmann and Tsatsaronis, 2014; Iacoviello, 2015) as measures of financial excess, such as household leverage, debt, net worth, house prices, and credit-to-GDP gaps. We find that the level of household net worth is the single best predictor of the borrowing constraint being binding or becoming slack. This result implies that monetary policy is significantly more effective in periods where household net worth is low. More specifically, the responses of output and aggregate consumption are amplified by more than 50% in periods where net worth is low compared to periods where it is high.====The model provides us with a framework in which the interrelation between household balance sheets, borrowing constraints, and monetary policy can be investigated in great detail. The model features two types of households with heterogeneous saving-consumption preferences, which generates borrowing and lending. Borrowing households face a housing collateral constraint that limits borrowing to a maximum fraction of housing wealth. Importantly, this constraint binds only occasionally rather than at all times, implying that the propagation and amplification of economic shocks in general and exogenous monetary policy interventions in particular depend on the endogenous degree of financial frictions. In the model, the effect of a monetary policy shock is significantly larger when the borrowing constraint is binding compared to a situation in which it turns slack. The magnitude of this amplification depends chiefly on households' expectations about the duration of slack borrowing constraints: the longer the expected slack duration, the larger the amplification effects.====The intuition for these asymmetric effects can be summarized as follows: when the constraint is slack, standard adjustments common to New Keynesian DSGE models occur. Because nominal prices are sticky, the central bank - controlling the short-term interest rate - influences the ex-ante real interest rate. An increase in the nominal rate leads to an increase in the real rate, which in turn reduces aggregate demand and puts pressure on firms to gradually adjust prices to a lower level. Thus, when borrowing constraints are turned off, a monetary tightening has mild contractionary effects. However, there are two additional channels that gain importance when the constraint is binding: the Fisher effect and the collateral effect.==== The fall in prices induced by the monetary policy shock raises the cost of debt services for constrained households, which induces a redistribution of resources from borrowers to savers. Because borrowers have a higher marginal propensity to consume, aggregate demand falls more strongly compared to the slack constraint case, when they can smoothen the shock out by taking on more debt. In sum, asymmetric responses following a monetary policy shock are driven by financially constrained households, which are forced to cut back consumption when an adverse shock hits the economy.====In the second part of the paper, we test this model prediction of asymmetric effects of monetary policy across the household net worth cycle on empirical data. To investigate the effects of monetary policy shocks conditional on the household net worth cycle, we estimate state-dependent impulse responses of aggregate variables to exogenous monetary policy interventions using local projections as proposed by Jordà (2005). The estimated responses are allowed to depend on whether household net worth is high or low. To measure the stance of monetary policy during the zero lower bound period, we use the shadow federal funds rate constructed by Wu and Xia (2016). Thereby, we take the significant adjustment in household balance sheets that occurred after the Great Recession explicitly into account. In our baseline estimation, we rely on a timing restriction to identify monetary policy shocks.====The empirical results strongly support the theoretical predictions. When private household net worth is low, an increase in the short-term interest rate leads to large and significant decreases in GDP, private consumption, and investment. In contrast, monetary policy shocks have mostly insignificant effects on economic activity during a high household net worth state. In our baseline estimation, the maximum GDP response is twice as large in a low household net worth state as the corresponding GDP response in a high net worth state.====These empirical results are robust to alternative definitions of low and high household net worth periods, different ways of identifying monetary policy shocks, and changes in the sample. Moreover, we show that positive and negative monetary policy shocks are fairly evenly distributed across low and high household net worth states, which implies that our findings are not driven by the nature of the shocks. Additionally, we conduct an analysis based on more disaggregated data. For this purpose, we construct monetary policy shocks at the level of U.S. geographical states by relying on the identification approach proposed by Nakamura and Steinsson (2014). The state level estimates confirm our findings at the aggregate level: the effects of monetary policy shocks are significantly amplified during periods of low household net worth.====Notably, our findings prove to be robust when we condition on three other prominent state variables. First, previous studies find that the state of the business cycle affects the impact of a monetary policy shock (Angrist et al., 2018; Tenreyro and Thwaites, 2016). However, we show that in periods of low household net worth, a contractionary monetary policy shock induces a significant fall in aggregate activity both in economic expansions and in recessions. Moreover, the cumulative effects are considerably larger when compared to the respective responses in high net worth states. Second, in a related paper, Alpanda and Zubairy (2019) find that the level of household debt influences the effectiveness of monetary policy interventions. We show that the effects of a monetary policy shock are amplified in periods of low household net worth, both when household debt is high and low. In contrast, during high household net worth periods, a monetary tightening induces mostly insignificant effects irrespective of the level of household debt. Third, our results are robust when we condition on financial stress in the economy. When household net worth is low, a contractionary monetary policy shock induces a significant decline in economic activity in tranquil times but also in periods of financial stress. By contrast, during a high household net worth episode, monetary policy only has a significant impact on the economy when financial stress is low. Overall, our findings suggest that the household net worth cycle is of first order importance for the effectiveness of monetary policy interventions whereas the state of the business cycle, the level of household debt, and financial stress only play a secondary role.====Our paper contributes to the growing literature on the role of household balance sheets for understanding the impact of macroeconomic shocks. Mian and Sufi (2012) show that those U.S. counties that experienced the largest increase in housing leverage before the financial crisis, suffered from more pronounced economic slack in the post-crisis period. Jordà et al. (2016) find that more mortgage-intensive credit expansions tend to be followed by deeper recessions and slower recoveries, while this effect is not present for non-mortgage credit booms. Di Maggio et al. (2017) and Wong (2019) show how households' heterogeneous financial profiles affect the transmission of monetary policy. We contribute to this literature, first, by showing how borrowing constraints matter for the transmission channel of monetary policy in the context of a standard New Keynesian model of the business cycle and, second, by providing extensive empirical evidence that households' financial position is key to understand the effects of monetary policy when looking at U.S. data. Our work is complementary to the papers by Cloyne et al. (2020) and Gelos et al. (2019), who consider household-level survey data to assess the role of households' balance sheets for the effectiveness of monetary policy. In addition, our paper provides guidance for empirical work on which data to focus on to characterize the time-varying tightness of borrowing constraints.====The remainder of the paper is organized as follows. Section 2 gives an overview of the structure of the DSGE model and presents results of the model estimation. Moreover, we investigate the transmission mechanism of monetary policy depending on the tightness of the borrowing constraints and discuss the findings of our prediction analysis. In Section 3, we conduct an empirical analysis and find strong support for the theoretical predictions. Finally, Section 4 concludes.",Monetary policy and household net worth,https://www.sciencedirect.com/science/article/pii/S109420252100017X,4 March 2021,2021,Research Article,95.0
Abbott Brant,"Queen's University, Canada","Received 24 April 2019, Revised 10 February 2021, Available online 2 March 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.02.012,Cited by (1),The effect of incomplete markets on parental investments is investigated. Uninsured risk and ,"Since the seminal work of Cunha and Heckman (2007), a large body of literature has explored issues related to the dynamic process of skill formation in children. One motivation for estimating models of the skill production function has been to help interpret the large estimated effects of early childhood policies reported in the literature (see Heckman and Mosso (2014) for a survey). Estimated models of the skill formation technology are usually embedded in broader models of parental decision making in order to simulate the effects of policy reforms.==== One approach has been to extend the standard incomplete markets life-cycle model to include multiple periods of parental investment in children's human capital. A common finding in these analyses is that policy interventions targeted to younger children, i.e. preschool aged, are the most effective, much like what is estimated in the empirical policy evaluation literature. Examples of analyses within an incomplete markets framework include Cunha (2013), Caucutt and Lochner (2020), Lee and Seshadri (2019) and Daruich (2018), as well as the seminal work by Restuccia and Urrutia (2004).====An important result regarding skill production functions is that parental investments made at different stages of a child's development are complements (Cunha et al., 2010). This dynamic complementarity means that human capital investments made early in a child's life positively affect the productivity of later investments, and vice versa. While this result would support early childhood policy interventions when parents are myopic, the argument may not follow when parents are optimizing agents. In the standard incomplete markets framework parents are indeed rational agents, and their decisions take dynamic complementarity into account. As such, it is not clear why incomplete financial markets, per se, result in early policy interventions being more effective than general ones. This paper provides analytical and quantitative results that help clarify the key mechanisms embedded within incomplete markets models that lead to these policy conclusions.====This paper develops a model in which parents make annual investments in their child's human capital in the form of both time and expenditure. Throughout the investment process parents face possibly binding credit constraints and uninsurable wage risk. Parents are altruistic and may transfer wealth to their child directly, in addition to making human capital investments. These wealth transfers are restricted to be non-negative, which implies that human capital investments may be inefficient.==== The model yields an expression describing the dynamics of parental investment decisions. By comparing these dynamics to those that arise under full insurance, novel analytical results on how incomplete markets distort the timing of parental investment are derived. When the model has the feature that good realizations of wage shocks lead to both increased parental consumption and increased investment in children, parents tend to delay those investments relative to what they would choose under full insurance.====The main analytical results are derived by borrowing methods from asset pricing. In particular, the dynamics of investment in children under the counterfactual assumption that parents have access to Arrow-Debreu securities serves as a basis for comparison. Investment dynamics in this full-insurance model are similar to those with uninsured risk, but the analysis shows that there may be differences. A wedge that measures differences in investment timing with and without full-insurance is derived and decomposed into two terms. The first is associated with the Lagrange multiplier that enters the consumption Euler equation when a household is credit constrained. Whenever the borrowing constraint binds investment in the child is delayed compared to the full-insurance model. The second wedge term is associated with uninsured wage risk, which induces uncertainty about future investment levels. This second term may be positive or negative depending on how the realization of wage shocks will affect investment choices. In the case that wage shock realizations and investments are positively correlated, the second wedge term will be negative and also contribute to delayed investment in children (relative to full-insurance). The reason for delayed investment in such a case is that parents take precautionary action, purchasing more of a riskless asset to insure against future investment risk.====Data on parental investment and child ability from the Panel Study of Income Dynamics Child Development Supplement (PSID-CDS) are used to calibrate the model.==== The method of Agostinelli and Wiswall (2016a, 2016b) is employed in order to make use of several noisy measures of unobserved latent skills. Because the dynamics of parental investment are the focus of the paper, the model is calibrated to match data on parental time and goods investment dynamics, estimates of the dynamics of abilities, as well as other characteristics of households. Because the skill production function cannot be directly estimated, one of the ways the data are utilized is by fitting misspecified auxiliary models of the skill production function, and then matching these to their model counterparts to achieve indirect identification. Calibration also involves estimating a stochastic wage process. This process includes individual fixed effects, which subsume both ability and the elasticity of wages with respect to ability. However, restrictions on the distribution of skills within the quantitative model allow the elasticity to be separately identified as part of the calibration.====The calibrated model is then used to carry out several optimal policy experiments, in which the richness of policy tools available to the planner increases successively across experiments. The welfare criterion considered by the planner accounts for transitional dynamics, and a long-run government budget constraint must be satisfied. First, a standard non-targeted reform of the tax and transfer system is considered in order to establish a baseline with optimal redistribution. This reform increases welfare by the equivalent of a 2.4% consumption increase for all households. When the planner can also subsidize investments in children and target tax policies to parents the consumption equivalent welfare gain is more than twice as large. Much of the additional welfare gain is derived from a large improvement in the ability distribution, where mean ability increases by 37.7% and proportional variation in skills decreases by 14.5%. Finally, policies targeted to parents are allowed to vary with the age of their child, which yields substantial further welfare gains, equivalent to a 5.8% increase in consumption over the benchmark. These further welfare gains are again attributable to improvement of the ability distribution, and, importantly, this incremental improvement in skills is associated with a reduction in the investment wedge described above. Thus, age-dependent tax, transfer and subsidy policies can lead to superior skill, output and welfare outcomes by reducing distortions in the timing of parental investment induced by incomplete markets.====Two closely related papers are those of Cunha (2013) and Caucutt and Lochner (2020), who also study investments in children when markets are incomplete. Caucutt and Lochner focus on a two-period model of goods investment that includes an intergenerational borrowing constraint, parental credit constraints and transitory earnings risk.==== They provide important results on how binding credit constraints at different stages of life, both for the parent and the child, can lead to underinvestment. The current paper builds on these results by showing how investment timing relates to possibly binding credit constraints and uninsured wage risk. Analytical and quantitative results here show that uninsured parental wage risk can distort investments away from the early years even if credit constraints never bind, provided that the intergenerational transfer constraint could possibly bind. The Caucutt and Lochner results also hold in the model as binding credit constraints exasperate the distortion of parental investment timing. From this we gain new insight into why early policy interventions are more effective than later ones within this type of model.====The remainder of this paper proceeds as follows. Section 2 introduces the child relevant parts of the model, and section 3 derives associated analytical results. Section 4 provides the remaining details of the quantitative model, while the identification strategy and calibration are presented in section 5. Section 6 explores how the incomplete markets frictions distort parental investment timing. Section 7 details the numerical optimal policy experiments, and section 8 concludes.",Incomplete markets and parental investments in children,https://www.sciencedirect.com/science/article/pii/S1094202521000144,2 March 2021,2021,Research Article,96.0
Zhang Shiyun,"Aarhus University, Department of Economics and Business Economics, Fuglesangs Allé 4, building 2632, 228,8210 Aarhus V, Denmark","Received 5 November 2018, Revised 16 February 2021, Available online 25 February 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.02.011,Cited by (1),"This paper studies the relationship between immigration and crime by applying the ==== crime model. Although the relationship between immigration and crime has been widely debated, there is no theoretical explanation that can define the impact of immigration on crime. This model constructs two channels through which immigrants affect the host country's crime rate: composition (direct) channel and labor market (indirect) channel. These two channels provide explanations for the ambiguity of immigration effects on crime rates. An extension of the model with skill bias and imperfect substitution between skilled and unskilled labor has more sophisticated numerical results based on the United States (U.S.) labor market and immigration. A more generous unemployment insurance system for immigrants increases both the unemployment and crime rates. An extended period of incarceration and a deportation policy reduce crime rates but have no significant impact on labor market outcomes.","Since the 1970s, immigrants have moved continuously to the United States (U.S.). The most significant wave of immigrants was between 1990 to 2010, increasing the population of immigrants from 19.8 million to 40 million, respectively. However, research on the impact of this wave of immigrants on crime rates is not conclusive. Alonso-Borrego et al. (2012) find that immigration has a positive correlation with crime rates, whereas Wadsworth (2010) argues that immigrants reduce crime rates. Bell et al. (2013) provide evidence that waves of asylum seekers in the United Kingdom led to a rise in property crime rates, but migration flows from the A8 countries had opposite effects.==== Neither wave had any effect on the rate of violent crimes. Bianchi et al. (2012) and Spenkuch (2014) find that immigration positively correlates only with property crimes. However, this literature is not able to explain how immigrants affect crime rates.====This is a theoretical study on the effects of immigration on crime rates. By using the Pissarides labor search model and Engelhardt et al. (2008) criminal behavior model, there are two channels through which immigrants affect crime rates. The first channel is called the composition channel. An increase in the immigrant population directly affects the host country's composition of labor force. The propensity to commit crimes varies for immigrants and native workers: compared with native workers, unemployed immigrants have more difficulties when they search the labor market, while employed immigrants have a higher value of employment in the labor market. As a result, unemployed immigrants tend to commit more crimes while employed immigrants are less likely to commit crimes among the work force. When the share of immigrants increases, employed immigrants drive the crime rate down, while unemployed immigrants increase the crime rate directly. The second channel is called the labor market channel because it operates through labor market friction. Firms' expected profits increase when more immigrants seek employment in the labor market. Such a compositional change in the labor force compels firms to create more jobs that benefit both native and immigrant workers in terms of employment and wages. Thus, unemployed workers' incentive to commit a crime—regardless of immigration status—decreases because they can get hired faster in labor markets with more vacancies. However, employed workers are more likely to commit crimes because the value of employment decreases in labor markets. Thus, analytically, the overall effect of immigration on the crime rate is ambiguous.====The existing literature concludes that workers' criminal behavior regarding property crimes relates strongly to labor market outcomes. Burdett et al. (2003) and Burdett et al. (2004) document that low-wage workers commit more crimes than those with higher wages, and a high unemployment rate leads to higher crime rates. Engelhardt (2010) states that workers with fewer unemployment benefits commit more crimes. Therefore, it is reasonable to link immigration and criminal behavior via labor markets.====The main difference between immigrants and native workers is unemployment utility. Immigrants earn less than native workers. According to the Current Population Survey (CPS), a monthly survey of households is conducted by the U.S. Bureau of Census for the Bureau of Labor Statistics, and during the 1990s, the wage gap between immigrants and native workers was about 20% in the U.S. It is reasonable to consider that this wage gap is attributable to the low unemployment value of immigrants for two reasons. First, immigrants have limited access to the federal social security system, so they cannot have the same unemployment income and benefits as natives. Second, immigrants lack social networks and communication skills and can have cultural conflicts in the host country.====Given these constraints, immigrants are compelled to search more intensively for employment than natives. Chiswick et al. (2000) document that foreign-born workers invest more than native workers in job search and transferability of skills. They also point out that the unemployment insurance benefits for foreign-born workers are $61.75, compared with the one for native workers are $78.59 in the 1990s. Frijters et al. (2005) states that immigrants are less successful than natives when they search in the labor market in the host country. Daneshvary and Weber (1991) also show that immigrants lack information of the U.S. labor market. These evidences show that the search costs of immigrants are higher than native workers. As a result, immigrants enjoy less leisure when they are unemployed. According to the Annual Social and Economic supplement (ASEC) of the CPS from 1994 to 2009, the mean unemployment income of foreign-born workers was lower than that of native workers. The data shows that from 1994 onward, the mean unemployment income of immigrants with college degrees and above is $1281.29, lower than native workers with the same educational attainment. The mean unemployment income of immigrants without any college degree is $264.28, lower than that for no-college-degree native workers. A lower value of unemployment leads to higher profits for firms, as in the baseline Diamond-Mortensen-Pissarides (DMP) model. Immigrants have a lower unemployment value than do natives, so unemployed immigrants will tend to commit more crimes. Employed immigrants receive a higher surplus from employment than do natives. Therefore, employed immigrants are more selective than employed natives when they encounter criminal opportunities. Among all the workers, employed immigrants are the least likely to commit crimes, while unemployed immigrants are the most likely to do so. Any increase in the number of immigrants directly affects the composition of the workforce. Moreover, increase in the number of employed immigrants decreases the crime rate, but the increase in the number of unemployed immigrants drives up the crime rate directly.====The criminal behavior of workers also affects the crime rate. In this paper, criminal behavior follows the model of Engelhardt et al. (2008). As immigration changes workers' distribution directly, these changes also affect labor markets and workers' criminal behavior via labor markets. Workers encounter criminal opportunities at random, but they commit crimes only when the payoff is sufficiently high. An increase in immigrant flows does not change the criminal behavior of workers explicitly, instead, it creates more jobs in the labor market. Both employed and unemployed workers respond differently to job creation resulting from immigration. With more jobs in the markets, unemployed workers prefer remaining unemployed instead of getting involved in criminal activities because they can find jobs faster, as it increases the value of unemployment. Employed workers, however, commit more crimes because their jobs become less valuable. The opposite effect of an increase in immigration on the criminal behavior of both employed and unemployed workers may explain the ambiguity of the impact of immigration on crimes observed in empirical studies.====This paper calibrates the model to the U.S. labor market data and crime report data in the 1990s. The model predicts that with the wave of immigrants in the 2000s, the overall unemployment rate decreased by 0.3857 percentage points, skilled native workers' wage increased by 0.21%, and unskilled native workers' wage increased by 0.15%. The overall crime rate decreased by 0.2334 per 1,000 population, which means that the total number of criminal offenses decreased by approximately 68,805 cases nationwide. Specifically, with the wave of skilled immigrants seen only in the 2000s, the overall crime rate decreased by 0.3054 per 1,000 population, and the overall unemployment rate reduced by 0.1479 percentage points. With the wave of unskilled immigrants, the overall unemployment rate will decrease by 0.2495 percentage points, but the overall crime rate increases by 0.0603 per 1,000. For the numerical exercise, I also extend the model with imperfect substitution between both skilled and unskilled labor. The crime rate decreases by 0.1666 offenses per 1,000 with the increase in the number of immigrants. In particular, with an increase in the number of skilled immigrants, the crime rate decreases by 0.2342 offenses per 1,000 but increases by 0.0660 offenses per 1,000 with the increase in the number of unskilled immigrants.====Finally, this paper studies several relevant public policies. First, I consider the impact of giving immigrants access to a more generous unemployment insurance system, so that they receive the same unemployment benefits as natives. This policy raises the unemployment rate by 1.1564 percentage points. It lowers the skilled native wage by 0.72% and unskilled native wage by 0.44%, while increasing the overall crime rate by 0.3726 offenses per 1,000 population. Second, an extended duration of incarceration and deportation policies reduce the crime rate by increasing the opportunity cost of committing a crime. A longer period of incarceration affects the criminal behavior of both natives and immigrants. The crime rate declines by 20.31 offenses per 1,000 when the average jail sentence is extended from 16 months to 48 months. The difference following a change in the deportation policy is that deportation only affects immigrant criminals. With the deportation policy, the crime rate drops with 1.38 offenses to 4.26 offenses per 1,000, depending on country of origin. Both incarceration and deportation policies have little effect on labor market outcomes.====This is the first paper to study the impact of immigration on both labor market outcomes and crime in a search and matching framework. I extend the Engelhardt et al. (2008) criminal behavior model with skill bias and the immigrant population. A closely related paper on immigration is Chassamboulli and Palivos (2014), which studies a model with two frictional labor markets with skill bias and imperfect substitution between skilled and unskilled labor. The authors show that an increase in the number of immigrants can raise natives' wages and reduce unemployment. Compared to their work, the main contribution of this paper is discussing the impact of immigration on crime rates. In this paper, criminal behavior of workers follows Engelhardt et al. (2008). As immigration changes workers' distribution directly, these changes also affect labor markets and the criminal behavior of workers via labor markets. As the paper shows, this novel mechanism is important for understanding the consequences of migration policies on the labor market and crime rates.====Other related literature is as follows. Dai et al. (2013) also look at the relationship between immigration and crime. There are two main differences between their paper and this paper. Firstly, they do not have search framework, so there is no unemployment in Dai et al. (2013). Secondly, they show that the overall effect of immigration on crimes is ambiguous analytically but do not do a numerical exercise to show which effect dominates. The numerical exercise in this paper shows the dominating channel given different group of immigrants. Chassamboulli and Palivos (2013) introduce unskilled immigrants only. Immigrants only show up in the unskilled labor market in the host country and compete with unskilled natives, while there are only native workers in the skilled labor market. Skilled native workers benefit from unskilled immigrants in terms of wages and employment, while the impact of unskilled immigrants on the unskilled labor market outcomes is ambiguous.====Chassamboulli and Peri (2015) focus on the effects of illegal immigrants on labor market outcomes with a two-country model. They endogenize the migration behavior of legal and illegal immigrants from Mexico, i.e., Mexican immigrants can choose either to stay in Mexico or to migrate to the U.S. In their paper, the presence of illegal immigrants encourages firms to create more jobs, so the unemployment rate in the U.S. decreases, and the wages of natives increase. Ortega (2000) and Liu (2010) also study the impact of immigration in a search and matching framework. Ortega (2000) constructs a two-country model in which workers decide whether to either search for employment in their own country or migrate. He proves that the migration equilibria Pareto dominates the non-migration equilibrium. Liu (2010) finds that illegal immigrants lower the job-finding rate in the labor market and force native workers to accept lower wages.====In Section 2, this paper describes frictional labor markets with skilled and unskilled immigrants. In Section 3, the steady-state equilibrium of the model is solved. In a steady-state equilibrium, an increase in the number of immigrants affects the composition of the labor force. The impact of immigration on labor market outcomes and crimes is discussed in Section 4. In Section 5, the model is calibrated to the U.S. labor market data and crime report data in the 1990s. The simulation with an increase in the number of immigrants and comparison to the data are reported in Section 6. Section 7 discusses the outcomes of the policy. Section 8 extends the model with skill bias and imperfect substitution between skilled and unskilled labor. Section 9 concludes the paper.",Immigration and crime in frictional labor markets,https://www.sciencedirect.com/science/article/pii/S1094202521000168,25 February 2021,2021,Research Article,97.0
"Ortigueira Salvador,Siassi Nawid","School of Economic Sciences, Washington State University, United States of America,Institute of Statistics and Mathematical Methods in Economics, TU Wien, Wiedner Hauptstrasse 8-10, 1040 Wien, Austria","Received 5 March 2019, Revised 16 February 2021, Available online 25 February 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.02.010,Cited by (2),"Eligibility and benefits for anti-poverty income transfers in the U.S. are based on both the means and the household characteristics of applicants, such as their filing status, living arrangement, and marital status. In this paper we develop a dynamic structural model to study the effects of the U.S. tax-transfer system on the decisions of non-college-educated workers with children. In our model workers face uninsurable idiosyncratic risks and make decisions on savings, labor supply, living arrangement, and marital status. We find that the U.S. anti-poverty policy distorts the cohabitation/marriage decision of single mothers, providing incentives to cohabit. We also find quantitatively important effects on savings, and on the labor supply of husbands and wives. Namely, the model yields a U-shaped relationship between the earnings of one spouse and the labor supply of the other spouse, a result that we also find in the data. We show that these U-shaped relationships stem in part from the current design of anti-poverty income programs, and that the introduction of an EITC deduction on the earnings of secondary earners—as proposed in the 21st Century Worker Tax Cut Act—would increase the employment rate of the spouses of workers earning between $15K and $35K, especially of female spouses.","The U.S. federal government spent about $264 billion on mandatory means-tested income security programs in 2019.==== The bulk of the tax credits and income transfer programs to assist low-income households are not only earnings-, income- and assets-tested, but they also depend on the living arrangement and the filling and marital status of applicants. In this paper we assess the effects of the U.S. tax-transfer system on low-income households along the following response margins: consumption/savings; labor supply; living arrangement; and marital status. We measure effects that have not been sufficiently examined within the framework of structural models of household decision making, especially the effects on cohabitation versus marriage, and on the labor supply of husbands and wives.====To this end, we develop a dynamic model for non-college-educated workers with children. Workers, females and males, receive uninsurable idiosyncratic shocks to their labor productivity. In addition, adult females are also subject to fertility shocks; they choose consumption, savings, labor supply, and whether or not to form a two-adult household provided they have an offer from a male. When a two-adult household is formed, either as cohabitants or as a married couple, they are assumed to share risks (intra-household risk sharing) and to solve a joint decision problem. Cohabiting couples get married as soon as they are better off than under cohabitation. The endogenous choices of both the living arrangement and marital status allow us to assess how tax-transfer reforms shape the composition of the population by household type: single mothers, cohabiting couples, and married couples.==== We embed in the model the federal individual income taxes, payroll taxes, two federal tax credits—the Child Tax Credit (CTC) and the Earned Income Tax Credit (EITC)—and two income assistance programs—the Temporary Assistance for Needy Families (TANF) and the Supplemental Nutrition Assistance Program (SNAP).==== We model these tax-transfer programs introducing all the kinks and non-convexities stemming from means testing, as well as the different treatment of applications based on living arrangement and filing status. We calibrate the model to match moments from a sample of ==== one- and two-adult households with children formed by non-college-educated workers. Our sample of households is drawn from the 2014 Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS). We first use the model to assess the impact of the incentives and disincentives introduced by the tax-transfer programs on the decisions of households across the stationary distribution of the model. Then, we evaluate a reform of the EITC along the lines proposed in the “21st Century Worker Tax Cut Act”, a legislative proposal introduced to the 113th Congress on March 26, 2014. The Act proposed the introduction of a new EITC deduction on the earnings of the secondary earner in a married couple.====We find important effects on labor supply. While some are in line with those found in previous studies, the new endogenous margins introduced in our model allow us to identify effects of the tax-transfer system that have been overlooked in previous structural models. As an example of the former, we find that the U.S. tax-transfer system provides strong incentives to work for lone mothers. In the stationary solution of our model, a low productive, working lone mother of two children gets a working subsidy of about 15 percent of her earnings. Overall, our model yields a lone mothers' participation elasticity of 0.63, which is in line with values estimated in the empirical literature.==== The new labor supply effects identified in our model result from joint decision making within two-adult households. For example, since taxes and transfers for married couples are based on family earnings, and transfer programs contain phase-in and phase-out regions, a hump-shaped relationship emerges between the earnings of a married parent and the employment tax rate of his/her spouse. Similarly, a worker's marginal tax rate varies with his/her spouse's earnings. As a result, the joint determination of the labor supply of husbands and wives creates U-shaped relationships between the earnings of a married parent and the employment rate and work hours of his/her spouse. Neglecting joint decision making within two-adult households would result in an underestimation of the effects of public assistance reforms.==== In Section 2 of this paper we use a sample of non-college-educated married couples with children drawn from the ASEC, and present evidence of a statistically significant, U-shaped relationship between husbands' earnings and their wives' employment rate, like the one generated by our model.====Our model finds that the current tax-transfer system has marriage disincentives for most non-college educated households with children. By design, the only difference between cohabitation and marriage in our model is the tax-transfer system they face. This allows us to assess how taxes and transfers shape lone mothers' decisions regarding cohabitation and marriage, as well as the decision of cohabiting couples regarding their marital status. For lone mothers, a two-adult household brings both insurance (risk sharing) and economies of scale, but also a change in taxes and transfers, so her choice of the living arrangement will depend on her own wealth and labor productivity, and on her suitor's wealth and productivity. Overall, we find that the current tax-transfer system provides incentives to cohabit: Most lone mothers are more likely to accept a cohabiting offer than a marriage offer. Only those with very low labor productivity are more likely to accept marriage than cohabitation when the offer comes from the father of her children. Since the U.S. tax-transfer system exhibits a differential treatment of cohabiting couples where the male is the biological father of the children and of those where he is not, we model both types of cohabiting couples.==== We find marriage acceptance rates that are up to 15 percentage points lower than cohabitation acceptance rates among lone mothers. TANF introduces important disincentives to marry for lone mothers. We find that removing TANF would increase marriage, at the cost of cohabitation, especially among lone mothers with low labor productivity. Low-income cohabiting couples also face disincentives to marry, especially those where both members of the couple work. For instance, since the EITC for married couples is based on family earnings, the within-couple distribution of potential earnings ability is a key determinant of the marriage bonus/penalty. Under the current tax-transfer system, we find that most low-income two-earner couples are better off as cohabitants, and that transitions from cohabitation to marriage are mostly chosen by couples where the female does not work. Removing the EITC from the tax-transfer system would yield a marked increase in marriage rates, especially among low-income working couples. This result is consistent with reduced-form empirical evidence on the marriage effects of the EITC. For instance, expansions in the EITC have been shown to yield a decline, although small, in new marriages (Rosenbaum, 2000; Eissa and Hoynes, 2003; Herbst, 2011).====Finally, we find sizable effects of the tax-transfer system on savings. The two tax credits—the EITC and the refundable part of the CTC—contain investment income limits; however, distortions to savings spring mainly from the TANF and SNAP asset limits. There is substantial variation in asset limits across U.S. states, ranging from $1K to no limits. In our model we use a $2K asset limit, which is the national weighted median value, and show that there is bunching at this threshold among lone mothers in the stationary solution of the model. This implies that asset limits contribute to crowding out household self-insurance through savings. (See Hubbard et al., 1995 for early work showing that asset-based social insurance discourages saving.)====In our assessment of the EITC deduction on the earnings of secondary earners proposed by the “21st Century Worker Tax Cut Act” for married couples, we find that it flattens the U-shaped relationship between one spouse's earnings and the other spouse's employment rate. A 50 percent deduction increases the average employment rate of both husbands and wives (1.3 and 6.9 percent, respectively), and the fraction of two-earner married households by 10 percent. However, it decreases their hours worked by 0.9 and 1.4 percent, respectively. The deduction also affects the choice of the living arrangement by increasing the marriage rate, thus reducing the population shares of lone mothers and cohabiting couples. The cost of this new deduction to the government is about $36 per household in the population of non-college-educated workers with children.====Our paper is related to a growing literature that uses structural models to study the effects of the tax-transfer system on household decisions. Instead of providing an exhaustive review of this literature, we simply highlight how our work complements previous lines of research. Greenwood et al. (2000) present a highly stylized model of labor supply, marriage, and fertility and find that welfare reduces marriage. In their framework, only single mothers receive welfare, and taxes are lump sum. The model abstracts from a consumption/savings decision and from cohabitation. Keane and Wolpin (2010) study how government income transfers interact with preference heterogeneity and labor and marriage opportunities in accounting for the observed differences in the behavior of black, Hispanic and white young women. They find important differences in the structural parameters across these three groups, including differences in their preferences for marriage. Welfare transfers are found to augment the differences in employment between whites and minorities created by the structural parameters. Their model abstracts from a consumption/savings decision and cohabitation, and assumes that upon marriage husbands remain unresponsive to household conditions. Chan (2013) develops a model for the labor supply and program participation of single mothers. He uses the model to estimate the contributions of policy and the economy to the observed increase in their labor supply from 1992 to 1999, a period of important welfare reforms and high economic growth. Policy is found to explain only 15 percent of the increase, while the economy explains 50 percent. The model does not include a consumption/savings decision and abstracts from opportunities to form two-adult households. Athreya et al. (2014) and Koşar (2019) introduce a consumption/savings decision in their models to study the effects of the EITC on the labor supply and human capital accumulation of single mothers. They do not, however, allow for cohabitation or marriage. Blundell et al. (2016) also consider endogenous savings in their study of the effects of the Income Support Program and the Working Families Tax Credit in the U.K. on female education and labor supply. In their framework, marriage is exogenous and husbands are modeled as shocks—they make no decisions and remain unresponsive to household conditions. There is no cohabitation. Guner et al. (2020) assess the effects of expansions in child care subsidies and child care tax credits (which are contingent on work) versus expansions in the Child Tax Credit (which is not contingent on work). They develop a rich, deterministic model of savings, female labor force participation, and male and female hours worked. In contrast to our model, the living arrangement (single or married) is fixed exogenously, and welfare transfers (TANF and SNAP) are approximated by linear functions of household income. Finally, Ortigueira and Siassi (2013) study the labor supply of husbands and wives under joint decision making in an environment with unemployment risk. However, they do not model the tax-transfer system, and abstract from the endogenous determination of both the living arrangement and marital status.====The remainder of the paper is organized as follows. Section 2 presents new empirical evidence related to family labor supply. Section 3 presents the model. Section 4 describes our sample of households, calibrates the model and assesses its fit with the data. Section 5 examines the effects of the tax-transfer system. The evaluation of the EITC reform is conducted in Section 6.","The U.S. tax-transfer system and low-income households: Savings, labor supply, and household formation",https://www.sciencedirect.com/science/article/pii/S1094202521000156,25 February 2021,2021,Research Article,98.0
Chen Yu,"Department of Economics, University of Calgary, Social Science building, 2500 University Dr NW, Calgary, AB T2N 1N4, Canada","Received 13 August 2019, Revised 20 February 2021, Available online 25 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.009,Cited by (0),"I show that the labor-market crowding out of less-educated workers can be understood as the labor-market response to an adverse-selection problem. When high-skilled workers apply for less skill-intensive jobs, adverse selection arises when employment contracts cannot systematically discriminate against education level, even though overqualified workers are more likely to quit. In order to separate workers, the equilibrium distorts the labor-market outcomes of less-educated workers with an inefficiently high unemployment rate. Furthermore, the distortion creates a market value of post-secondary vocational education, because it acts as an entry barrier and protects less-educated workers from the competition of overqualified college graduates.","There is a common perception that when workers are competing for the same scarce jobs, high-skilled workers displace less-educated workers by accepting less skill-intensive jobs in a phenomenon known as crowding out (e.g., Thurow (1975), Acemoglu and Autor (2011), Beaudry et al. (2016) and Barnichon and Zylberberg (2019)). However, little evidence has been found to support the view of direct crowding out. In fact, firm-level studies show that workers with higher education are not more productive at less skill-intensive jobs than less-educated workers.==== Moreover, compared to workers whose education fits the skill requirements of their jobs, overqualified workers are found to have lower job satisfaction, to search more actively on-the-job, and to have higher quit rates, in the same type of jobs.==== It is unclear why less-educated workers are the ones high-skilled workers displace, given that employers have concerns that overqualified workers are likely to use their current jobs as stepping stones towards better jobs.====This paper shows that the crowding out of less-educated workers can be understood as a labor-market response to a problem of adverse selection. This novel mechanism takes into account the interaction between the workers' job search decisions when unemployed, and their dynamic career paths. The problem arises when high-skilled workers have incentives to apply for jobs where employers anticipate filling these positions with less-educated workers, while at the same time employment contracts cannot perfectly exclude overqualified workers. As a response, the labor market equilibrium is distorted with inefficiently high unemployment rate of less-educated workers. In adopting this mechanism, this paper argues that the requirement of vocational education and regulations on licensing can help mitigate labor market inefficiencies as these are costly entry barriers for some occupations.====The study begins by analyzing a frictional labor market with on-the-job search. Search is directed in the sense that all agents take as given the trade-off, such that a job with a higher wage is associated with a lower matching rate for workers. While all workers can apply for less-skilled jobs, only college graduates can fulfill the requirements of a skilled job. The dynamic nature of the on-the-job search implies distinct career paths for workers. Unlike high school graduates, whose career ladder only goes as far as less-skilled jobs, college graduates apply for these jobs as stepping stones toward jobs that require more skills. With better on-the-job search opportunities, college graduates are willing to accept a relatively lower wage as long as the job-finding rate is sufficiently high. In contrast, high school graduates are willing to wait longer for a job that pays a relative higher wage, given that they have longer expected job tenures in less-skilled jobs.====Adverse selection arises when college graduates have incentives to apply for jobs where employers anticipate applicants who are high school graduates who will have lower job turnover.==== Further, when the productivity gain from hiring a worker with higher education is limited, especially for jobs where tasks can be performed by following some well-defined procedures, an overqualified worker generates lower expected profits. In this sense, as college graduates have higher expected quit rates, they are “lemons” for employers whose jobs are less skill intensive.====The key assumption in the model is that the same employer is not allowed to offer different contracts for the same type of jobs where matching productivity does not vary across workers. Under fixed wage contracts, this assumption implies that any existing wage dispersion within identical type of jobs is caused by different paying policies across employers, a result of market segmentation. For the same type of jobs, some employers choose to pay high wages, while others offer low wages, anticipating that their posted wages will only attract certain types of workers, given that workers' job search incentives vary, based on their education, outside options, and employment status. The assumption implies that the same employer cannot discriminate qualified workers based on their educational background even though overqualified workers are relatively more likely to quit.====I show that in a competitive search equilibrium, high school and college graduates are sorted into different markets when applying for less-skilled jobs. Among these jobs, college graduates are attracted to ones that are easier to find, have higher turnover rates and pay lower wages====; while high school graduates prefer jobs with better compensation and lower turnover, but these types of jobs are also more difficult to obtain. This is consistent with the empirical observation that the share of college graduates is higher in the low-skilled occupations with low wages, little benefits, and high turnover rates.==== The separation, however, could be costly and generate inefficiently high unemployment for high school graduates. I show that under certain conditions, college graduates have incentives to search for the same job as high school graduates and, as a response, equilibrium contracts offered to high school graduates are distorted so that they are less attractive to college graduates. In this case, the equilibrium wage offered to high school graduates is inefficiently high and the corresponding job finding rate is inefficiently low. College graduates are discouraged from applying for these jobs because they are less willing to trade off the job-finding rate for a higher paying stepping-stone job. As a result, high school graduates are displaced from employment, even though they do not directly compete with college graduates for the same job.====The separation of the equilibrium relies on firms' capacity constraint to meet with workers within a period, and also whether firms can impose selective rules in hiring. Under bilateral meeting and multilateral meeting where firms do not impose any selecting rule, high school graduates and college graduates within the same market have the same chance of matching. The distribution of attracted job candidates is then important in determining firms' profits, and the equilibrium separates workers ==== into different markets. When firms meet with multiple workers and can also screen out overqualified workers through hiring processes such as interviews, the negative externalities caused by an additional college graduate on the matching rate of high school graduates decrease as the number of job candidate a firm can interview goes up. The equilibrium is then separating if the capacity constraint is high.====Understanding the distortion mechanism has a novel implication on the demand of education. To investigate this implication, the baseline model is extended to include educational choice. In particular, I consider the post-secondary vocational education, which primarily focuses on providing prospective workers with occupationally specific preparation. The adverse selection problem creates a demand for vocational education, which acts as an costly entry barrier and screens out college graduates by imposing the requirement of separated education. Employers wishing to fill less-skilled jobs are able to offer non-distorted contracts to workers who have obtained a vocational credential. Aside from the value of creating human capital, introducing vocational education into the labor market is welfare improving, as it offers diverse post-secondary educational choices and makes a worker ==== better off.====The idea that high-skilled workers can crowd out less-educated workers goes back to Thurow (1975), where less-educated workers are displaced out of employment when higher educated workers are ranked over less-educated workers in job competitions. In more recent work, such as Acemoglu and Autor (2011) and Beaudry et al. (2016), negative shock that directly affects high-skilled workers can crowd out less-educated workers as the high-skilled workers moved down the occupational ladder. To the sharp contrast of these competitive models, where displacement of less-educated workers is efficient skill-upgrades, the crowd-out effect in this paper is an inefficient market distortion, implying roles of labor market and educational policies for welfare improvements.====Another group of literature studies the crowding out of less-educated workers with random matching models (e.g., Albrecht and Vroman (2002) and Dolado et al. (2009)). The random matching assumption forces workers with different backgrounds compete directly for the same employers. Instead, this paper takes into account that workers with different career paths have incentive to direct their search towards employers offering different wages, for the same job. Barnichon and Zylberberg (2019) build a search model where high-skilled applicants are systematically hired over less-skilled ones. This paper, however, follows the observations that high-skilled applicants are not always preferred by employers with less-skilled jobs, and endogenously generates crowding out without imposing any ranking condition.====This paper builds on the literature that studies adverse selection in a competitive search environment. The model framework is developed from Chen et al. (2019), which extends the previous work by Guerrieri et al. (2010) with the dynamics of on-the-job search, and diverts from Chen et al. (2019) by incorporating the interaction between heterogeneous workers. While the majority of the literature studies adverse selection with unobserved productivity of workers, this paper focuses on the heterogeneity of a worker's expected job duration. Even though it is contracting limitation rather than asymmetric information that causes adverse selection in this paper, the mechanism can be applied to more general situations, where unobserved heterogeneity regarding the expected job duration affects the labor market outcomes of certain groups such as women and senior workers. Carrillo-Tudela and Kaas (2015) studies the effects of adverse selection with unobserved workers' abilities on the mobility of workers with random matching framework. While they focus on the firms' willingness to separate their applicants based on the degree of information frictions, their framework makes it difficult to address the distortion on the employment opportunities as analyzed in this paper.====The rest of the paper is organized as follows. Section 2 lays out the model. Section 3 characterizes the equilibrium with the crowding-out effect and discuss its labor market implications. Section 4 checks the robustness of the separating equilibrium under multilateral meetings. Section 5 extends the model with educational choices and discusses the implication of the crowding-out mechanism on the choice of educations. Section 6 concludes.",A directed search model of crowding out,https://www.sciencedirect.com/science/article/pii/S1094202521000132,25 February 2021,2021,Research Article,99.0
"Gao Xiaodan,Zhao Jake","Renmin University of China, China,Peking University HSBC Business School, China","Received 11 August 2020, Revised 19 February 2021, Available online 24 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.008,Cited by (2),High-tech firms hold substantial cash reserves. We build a parsimonious ,"Non-financial businesses in the United States hold 38% of monetary aggregate M1 and 22% of monetary aggregate M2. Among them, publicly traded high-tech firms maintain a sizable level of cash reserves (Bates et al., 2009; Begenau and Palazzo, 2021). Their median and mean cash-to-assets ratio are 0.32 and 0.37, respectively, during the 2000s. In light of the fact that high-tech firms are key drivers of innovation, productivity, and economic growth, it is important to understand the determinants of their saving behavior and then to explore the role of corporate saving in innovation.====Previous studies attribute the large cash stocks to various factors relevant for R&D intensive industries: R&D adjustment costs (Brown and Petersen, 2011), financial frictions caused by the non-collateralizability of R&D (Falato et al., 2013), knowledge spillover (Qiu and Wan, 2015), innovation uncertainty (Lyandres and Palazzo, 2014; Hsu et al., 2016), and intense market competition (Ma et al., 2020; Lyandres and Palazzo, 2016). One of the primary goals of our paper is to consider all of these possible explanations in the literature, identify the main determinant(s) of high-tech firms' strong cash demand, and quantify their impacts.====In addition, the role of corporate saving in mitigating the distortion caused by financial frictions remains inconclusive and depends on the persistence of productivity processes (Buera et al., 2011; Midrigan and Xu, 2014; Moll, 2014). In particular, the distortion-correction role of corporate saving is effective only if productivity is sufficiently persistent. This paper re-examines this issue in the context of high-tech industries in which productivity is endogenized.====Toward this end, we build a dynamic industry equilibrium model of R&D and cash with endogenous entry and exit. In the model, firms hire labor to produce differentiated products and compete in the product market. They invest in R&D to improve their competitive positions, but face knowledge spillover and innovation risks. Also, adjusting R&D expenditures incur costs. The capital market is assumed to be imperfect, so firms have no access to debt financing due to prohibitive collateralization of R&D; they can, however, raise funds through equity issuance (Brown et al., 2009; Hall and Lerner, 2010; Hall et al., 2016). Equity inflows (equity issuance) and equity outflows (dividend distribution) are costly. This generates a precautionary motive for holding cash, as in Zhao (2020). That is, to avoid external financing costs in case of a liquidity shortage, firms accumulate internal funds. Meanwhile, the presence of other frictions—R&D adjustment costs, knowledge spillover, innovation uncertainty, and market competition—affects the strength of the precautionary motive through the cost of cash saving and the amount of cash flows. But to be clear, the precautionary motive we have in mind here is quite broad. Internal funds can also be used to respond more efficiently to positive shocks.====To examine the model's quantitative implications for research intensive firms' cash-saving policies, we estimate the model and validate it by demonstrating its ability to match relevant data moments. We then use the structural estimates obtained to assess how each factor affects cash and R&D choices jointly. Each factor is encapsulated by one or more estimated parameters. We change one parameter at a time and perform several sets of counterfactual analyses: (i) increasing the intensity of each mechanism by 5%, (ii) increasing the intensity of each mechanism such that mean firm value changes by 1%, and (iii) shutting down each channel in turn. Interestingly, shutting down any one of the main channels can produce considerable effects on cash—this is probably why there are such diverse explanations in the literature. However, at the margin, the relative strengths of the mechanisms are quite different.====Innovation uncertainty has the strongest effect, and knowledge spillover and financial frictions are also significant drivers behind high-tech firms' strong cash demand. These results support the empirical findings reported by Falato et al. (2013), Qiu and Wan (2015), and Hsu et al. (2016). However, our results deviate from previous studies along several dimensions. First, the endogenized productivity process allows us to decompose and interpret innovation uncertainty at a deeper level. Our model differentiates between innovation volatility and efficiency which are both under the umbrella of innovation uncertainty, and we show that the spread of the conditional productivity shock is the key to explain the large cash balance. In addition, knowledge spillover affects cash policy through a channel different from that hypothesized by Qiu and Wan (2015). In particular, our model suggests that in response to an increase in knowledge spillover, firms cut R&D investments. The decline in precautionary cash demand, however, is fully offset by the increase in the substitution away from R&D to cash (substitution effect) and the increase in income that spillover brings (income effect). This generates a pattern in line with the somewhat surprising empirical observation that cash holdings increase with knowledge spillover.====R&D adjustment costs play a smaller role in generating cash holdings than the other factors, because the absence of adjustment costs only reduces the real price of investment—and thus affects the level of R&D expenditures. As long as financial frictions and income uncertainty are present, cash will be valuable regardless of the magnitude of the adjustment costs. R&D investment is also naturally persistent since productivity improvements made by R&D are stochastic. Firms are never certain their R&D investment will successfully lead to higher productivity, so they also have no reason to change the R&D level unless there are big enough shocks.====Market competition has important but non-monotonic impacts on high-tech firms' cash policy, regardless of the sources of competitive pressure—higher product substitutability or fixed operating costs. The non-monotonicity arises from the interaction of different forces. There is higher cash demand due to the precautionary motive when market competition is greater. But firms also tend to increase R&D spending as market competition intensifies. As a result, cash savings drop and exits rise precipitously when market competition becomes too large since firms still want to maintain their level of R&D expenditures. More precisely, the substitution from cash to R&D and the income effect from lower cash flows overpower the precautionary motive to save.====We further examine the role of corporate saving in influencing high-tech firms' R&D investment. Our result partially deviates from that derived from previous macroeconomic studies. We find that corporate saving corrects the distortion caused by financial frictions, and the efficacy of self-financing does not depend heavily on productivity persistence. This key difference is generated from our model assumption which reflects the empirical facts that large firms are the main providers of business R&D in the United States and they can issue equity to save (Eisfeldt and Miur, 2016). This assumption disconnects savings from income and allows firms to accumulate cash even if the productivity process is not sufficiently persistent. Investors are also willing to support low-productivity but research-intensive firms. Over 50% of firms have negative net income in our sample of high-tech firms. Similarly, in our model with endogenous productivity, it is rational to keep high R&D firms with low productivity operating in most cases.====This paper contributes in several ways. First, it complements corporate cash studies by systematically analyzing the determinants of high-tech firms' substantial cash reserves. Previous structural work mainly focuses on average firms and examines their saving motives (Armenter and Hnatkovska, 2016; Boileau and Moyen, 2016; Nikolov and Whited, 2014; Riddick and Whited, 2009; Begenau and Palazzo, 2021). For high-tech firms, a number of factors have been empirically identified; however, little is known about their quantitative importance, which is the key step toward understanding high-tech firms' cash-saving decisions as well as the role of corporate saving in innovation. Furthermore, these factors have been examined in isolation, and this may cause erroneous conclusions to be reached. We show in our study that some results from previous work are overturned once all the features are analyzed jointly.====Second, our paper explores the role of firms' self-financing in mitigating distortions from financial frictions when productivity dynamics are endogenized, while most previous work assumes a stochastic productivity process which is beyond the control of firms (Buera et al., 2011; Midrigan and Xu, 2014; Moll, 2014). Our paper has the potential to deliver a deeper understanding of the importance of corporate savings in correcting misallocation. Instead of analyzing the issue through the traditional channel—physical capital allocation across firms and firms' entry and exit decisions, our paper emphasizes the channel through firms' R&D investment which has direct impacts on productivity dynamics.====Third, our paper develops a novel and computationally tractable framework to study firms' R&D choices. Specifically, it proposes a theoretically appealing approach to model the transition probabilities that govern the evolution of endogenous productivity. In contrast to previous studies that restrict firms from moving beyond the technology level that is closest to their current one (Ericson and Pakes, 1995; Xu, 2008; Hashmi and Van Biesebroeck, 2016), our approach allows for the realization of different levels of innovation—radical innovation vs. incremental innovation—and neatly captures a wide range of possible innovation dynamics. In the process of the estimation of our model, we also provide some of the first structural estimates for R&D adjustment costs.====The remainder of the paper is structured as follows. Section 2 lays out a dynamic industry equilibrium model of R&D and cash with endogenous entry and exit. Section 3 provides intuition about how considered R&D features influence high-tech firms' cash and R&D choices and reports the estimation results of the model. Section 4 quantitatively evaluates the importance of each factor in explaining high-tech firms' cash demand and discusses the role of corporate savings in high-tech industries. Section 5 concludes.",R&D dynamics and corporate cash saving,https://www.sciencedirect.com/science/article/pii/S1094202521000119,24 February 2021,2021,Research Article,100.0
Gradstein Mark,"Department of Economics, Ben Gurion University, Israel; CEPR, CESifo, IZA","Received 22 September 2020, Revised 18 February 2021, Available online 24 February 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.02.007,Cited by (0), configuration. These results are interpreted in the light of existing evidence pertaining to such differentials in the US.,"Mounting evidence accumulated in recent years attests that cultural traits, such as cognitive (“hard”, having to do with technical, man-machine, or administrative aptitude) as well as non-cognitive (“soft”, as manifested in the context of social interactions) personality features, affect productivity and, more generally, material prospects, see e.g., Almlund et al., 2011; Bowles et al., 2001a, Bowles et al., 2001b; Heckman and Kautz, 2012; Heckman et al., 2006; Kautz et al., 2017. This research broadens our understanding of the nature of human capital and its implications for individual economic performance. It, in particular, indicates that, not only do cultural traits affect future material prospects, but they can also be affected by (family) environment and schooling. Much less clear, however, is the role of these traits for the macro economy, although given the broadly recognized importance of human capital for modern economic growth, one would expect it to be substantial.====In this paper, we make a step in this direction by incorporating the effect of cultural traits on productivity in a simple growth model. An important feature of this model is the stipulation that human capital endowments and productivity as shaped by one's cultural traits complement each other in affecting material prospects. Another crucial assumption is that both cultural traits and endowments are, to some extent, endogenously determined by the parents caring, among other things, for their offspring's material future. The central role of the family in determining one's personality and a sense of identity has long been acknowledged by sociologists and, more recently, economists.==== Complementarity between parental decisions in regard to cultural traits and endowments creates a mutually reinforcing propagation mechanism, implying that the economy typically exhibits multiple steady states, convergence to which depends on initial condition, specifically, on the initial income distribution. This is a novel feature relative to a standard modeling that typically generates income convergence in the absence of external effects.====We then apply the presented analytical framework to explore ethnic income differentials. To this end, the model is adjusted to include two ethnic communities, advantaged and disadvantaged, as differentiated by initial income. In addition to own cultural traits, productivity is also allowed to depend on social interactions, in particular, it hinges on the level of social integration among the communities. We find that initial ethnic differentials are intertemporally persistent. Further, this context, too, exhibits multiple steady states, whereby a relatively larger fraction of the disadvantaged community finds themselves in a low steady state; and its high steady state income level is smaller than that of the advantaged community. These disadvantages are found to be functions of the initial income distribution across the communities and of the level of social integration.====Exploring the effect of these factors leads to several insights. Thus, a uniform (community neutral) equalizing redistribution of initial income reduces ethnic differentials at the steady state, as does the increase in the level of social integration. In contrast, the increase in the marginal product of capital endowment enhances these differentials. These results are interpreted as being consistent with empirical regularities pertaining to the evolution of the racial income gap in the US. We point out to several facts documented in the literature in this regard. One is the remarkable overall stability of this gap for the past five decades (since the Civil Rights Act of 1965). Then the slow but steady decrease in the level of racial residential segregation during that period, along with the progress in income rank of the Black community. Finally, the reduction over this period in the share of national income captured by that community as a result of the general increase in income inequality.====The presented framework can be reconciled with these facts provided that over the covered period both the level of social integration and the marginal product of capital endowment have increased – the latter due, for example, to the skill biased technological change. These changes, leading to opposite effects on the racial gap, can cancel each other implying the remarkable phenomenon whereby the overall gap has remained the same, despite the moderate improvement in the income rank of the Black community. In a sense, the latter, while moving forward, has stayed in place, because of the detrimental macro economy changes.====The paper, therefore, aims at making several interrelated contributions. One is the introduction of cultural traits and, specifically, of productivity relevant soft skills, and their formation in a growth model. Another is exploration of persistent income inequality in a growth model incorporating cultural attributes. We also explore the role of social interactions in affecting long run inequality. Finally, more specifically, application of this framework to account for macroeconomic evolution of ethnic differentials, which also enable us to consider the role of social integration. This last contribution also makes it possible to meaningfully relate the model parameters to observable empirical regularities.====The paper proceeds as follows. The next section reviews related literature. Section 3 presents the baseline model, analysis of which is contained in Section 4. Section 5 then outlines the model extension to explore ethnic differentials along the economy's growth path; presents its steady state analysis; and discusses relevant empirical facts pertaining to the racial gap in the US in the light of the model's insights. Section 6 concludes with brief remarks.","Cultural attributes, income inequality, and ethnic differentials",https://www.sciencedirect.com/science/article/pii/S1094202521000120,24 February 2021,2021,Research Article,101.0
"Luk Paul,Zheng Tianxiao","Hong Kong Institute for Monetary and Financial Research, Units 1005-1011, 10th Floor, One Pacific Place, 88, Queensway, Hong Kong,International Monetary Fund, 700 19th St NW, Washington, DC 20431, United States","Received 24 June 2019, Revised 27 January 2021, Available online 17 February 2021, Version of Record 10 March 2022.",https://doi.org/10.1016/j.red.2021.01.004,Cited by (2),.,"Standard macro-finance models often assume a uniform debt structure where collateralized credit is the main channel for the propagation and amplification of economic shocks. While this approach may help in building tractable theoretical models, it ignores the fact that firms are financed by different types of debt, and, importantly, these debts present different cyclical patterns across the business cycle. In this paper, we aim to explain the driving forces behind the dynamics of different debt instruments and study whether, and how, the conclusions of standard models of financial frictions change when firms have access to different types of debt.====We start by documenting the empirical patterns of firms' debt structures using U.S. firm level data. We find that firms operate with different debt structures. High credit quality firms rely almost exclusively on unsecured debt, while low credit quality firms use a large share of secured debt, confirming the findings by Rauh and Sufi (2010). We also find that unsecured and secured debt have different dynamics during the business cycle. Unsecured debt is strongly procyclical while secured debt is at best weakly procyclical, consistent with the empirical findings by Azariadis et al. (2016).====To explain the cyclicality of secured and unsecured debt, we build a tractable dynamic stochastic general equilibrium model with debt heterogeneity. In the model, firms borrow secured or unsecured debt from perfectly-competitive lenders subject to idiosyncratic productivity shocks and costly-state-verification problems similar to Bernanke et al. (1999) (henceforth BGG). In the secured debt contract, the lender takes over the firm's assets in the event of default, and the borrower exits with nothing left. In the unsecured debt contract, the lender receives no payment in the event of default and the borrower keeps a fraction of revenue and keeps operating.==== Firm's track record of default evolves endogenously over time and can be observed by lenders. Specifically, firms with a default record will be punished by being excluded from using unsecured credit in the future and will only borrow in the secured debt market. This is undesirable because we assume secured debt is costly to initiate, and borrowers prefer unsecured debt contracts.====The characteristics of secured and unsecured debt contracts affect firms' incentives to borrow. Similar to BGG, the optimal secured (unsecured) debt contract can be characterized by a threshold level of idiosyncratic productivity below which the borrower defaults. Under this setup, borrowers are subject to a moral hazard problem. They reap the benefits of potential upside risk, ====, when idiosyncratic productivity is above the threshold, and do not need to bear the full costs of downside risk, ====, when idiosyncratic productivity is below the threshold and default takes place. Importantly, the moral hazard problem is more severe for secured debt borrowers. In the downside risk scenario, secured debt borrower has “less skin in the game” compared to unsecured debt borrower. Secured debt borrower goes bankrupt with zero continuation value regardless of the indebtedness, whereas unsecured debt borrower stays in business with a penalty on their track record and a loss of continuation value. As a result, secured borrowers care less about downside risk compared to unsecured borrowers.====The contractual framework also implies that lenders face different incentives when they supply credit in the two debt markets. A lender in the secured debt market worries less about default, simply because the lender is able to recover a fraction of secured borrowers' asset in the event of default, but cannot recover anything from unsecured borrowers.====The above reasoning holds over the business cycle. In response to a negative productivity shock, the capital stock in the economy decreases, and the expected return on capital increases, which drives up borrowers' demand for credit.==== Secured borrowers, who worry less about downside risks, increase their credit demand more than unsecured borrowers. Meanwhile, with higher expected returns, secured debt lenders are happy to increase credit supply more than unsecured debt lenders. Therefore, during economic downturns, the leverage ratio of secured borrowers increases more than that of unsecured borrowers.==== For secured borrowers, a rise in the leverage ratio together with a fall in net worth in the economic downturn results in a relatively stable level of secured debt over the business cycle. In contrast, the increase in the leverage ratio for unsecured borrowers is more limited. Together with a fall in net worth, it implies a larger fall in unsecured debt in a downturn.====To explore the quantitative significance of this channel, we embed our contractual framework into a standard dynamic stochastic general equilibrium model and calibrate the model based on US corporate debt data. With a TFP shock and a shock to the cross-sectional dispersion of idiosyncratic firm productivity similar to Christiano et al. (2014), our model-simulated moments feature strongly procyclical unsecured debt and weakly procyclical secured debt, consistent with US data.====A key implication of this paper is that the introduction of unsecured debt contracts amplifies the financial accelerator effect in BGG. In the one-sector BGG model, the financial accelerator effect exists because debt falls just when a firm's net worth falls, which amplifies the effects of the original shock. By contrast, in our model, in response to a negative shock, the increase in the leverage ratio in the unsecured debt market is rather limited, so the fall in debt is more pronounced. This leads to a quantitatively important amplification effect relative to BGG. Our results suggest that the standard one-sector BGG model may underestimate the amplification effects of the financial accelerator mechanism.====Finally, we consider several extensions of the model with more realistic features in the firm sector. We allow for (1) positive recovery ratios for lenders of unsecured debt; (2) exogenous upgrading of credit ratings; (3) predetermined productivity differences in the firm sector; and (4) a mixed debt structure in low-credit-rating firms. We find that our key finding is robust, that unsecured debt is more procyclical than secured debt in each of these extensions.====Our paper is related to two strands of literature. First, this paper is related to a vast literature incorporating financial frictions into macroeconomic models. This paper adopts a costly state verification approach because it is straightforward to endogenize default. See Carlstrom and Fuerst (1997), Bernanke et al. (1999), Christiano et al. (2014) and Nuno and Thomas (2017). By contrast, default is eliminated as an equilibrium outcome in models in which financial frictions arise due to limited enforcement problems (see for example Kiyotaki and Moore (1997), Meh and Moran (2010), Jermann and Quadrini (2012) and Gertler and Karadi (2011)). Moreover, the theory of Kiyotaki and Moore (1997) implies that secured debt is strongly procyclical, which cannot explain the cyclicality of secured and unsecured debt in the data.====Second, there is a large theoretical literature on corporate debt structure, following Diamond (1991), Besanko and Kanatas (1993) and Boot and Thakor (1997). This literature focuses on the determinants of a firm's financing based on bank debt versus corporate bonds. For instance, Diamond (1991) argues that high credit quality firms have good reputations allowing them to avoid the additional costs of bank debt associated with monitoring. Our model is in this spirit. Chemmanur and Fulghieri (1994), Bolton and Freixas (2000) and De Fiore and Uhlig (2011) argue that banks have an information advantage about a firm's profitability. Such information is particularly useful for assessing the risk of low-quality borrowers. Empirically, Denis and Mihov (2003) find that credit quality is a major determinant of a firm's debt structure, with higher credit quality firms choosing public debt and lower quality firms choosing bank loans. Rauh and Sufi (2010) show that high credit quality firms rely exclusively on unsecured debt; whereas low credit quality firms rely more on secured debt. This literature, however, does not study the macroeconomic effects of corporate debt structure.====A few papers discuss debt structure and its relation to the macroeconomy. De Fiore and Uhlig (2015) assume that bank monitoring yields useful information about relatively low productivity firms. They find that the flexibility in substituting alternative debt instruments by firms reduces macroeconomic volatility. In Crouzet (2017), firms borrow partly through banks because banks are more flexible in debt restructuring. The paper argues that since bond finance cannot be restructured in the future, firms switching from bank finance to bond finance will deleverage, which increases the negative macroeconomics effects of a shock to the banking sector. Our paper addresses different aspects of a firm's debt choice by studying secured versus unsecured debt to explain the cyclicality of these different types of debt contracts. In terms of aggregate implications, we argue that unsecured borrowers have less volatile leverage ratios, so fluctuations in debt are amplified. Therefore, the amplification due to the financial accelerator mechanism is stronger in an economy with a large fraction of unsecured debt.====The work of Azariadis et al. (2016) is most relevant to ours. Their model features multiple equilibria driven by unsecured debt and relies on sunspot shocks to generate persistent and highly volatile dynamics of macroeconomic variables. They argue that fluctuations in unsecured debt, but not in secured debt, are driven by sunspot shocks, and that sunspot shocks account for around half of output volatility. In this paper, we show that the nature of secured and unsecured debt contracts implies that borrowers and lenders of unsecured debt are more cautious, and therefore the leverage ratios of unsecured borrowers are less volatile. Our simulation results demonstrate that even with only fundamental shocks, our endogenous mechanism can account for the relative procyclicality of unsecured debt observed in US data.====The rest of the paper is organized as follows. Section two provides empirical analysis. Section three presents the full model with debt heterogeneity. Section four explores the key properties of credit contracts and explains theoretically why they result in different cyclical movements in secured and unsecured debt. Section five describes calibration of the model. Section six discusses the model properties and quantitative results. Section seven compares the benchmark model with a standard BGG model. Section eight discusses four extensions to our benchmark models. Section nine concludes.",Dynamics of secured and unsecured debt over the business cycle,https://www.sciencedirect.com/science/article/pii/S1094202521000041,17 February 2021,2021,Research Article,102.0
"Navarro Lucas,Tejada Mauricio M.","Department of Economics and Finance, Universidad Nacional de Córdoba, Bv. Enrique Barros - Ciudad Universitaria, Córdoba, X5000HRV, Argentina,Department of Economics, Universidad Alberto Hurtado, Erasmo Escala 1835 office 211, Santiago, 8340539, Chile","Received 18 January 2020, Revised 4 February 2021, Available online 11 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.004,Cited by (1),"This paper studies the impact of a minimum wage policy in a labor market with a private and a public sector. We develop a two-sector search and matching model with minimum wage and heterogeneous workers in their human capital. We structurally estimate the model using data for Chile, a country with a large fraction of employment in the public sector and a binding minimum wage. Counterfactual analysis shows that institutional features of public sector employment reduce labor market frictions and mitigate the negative effect of the minimum wage on unemployment and welfare.","There are two interesting facts of labor markets in developing economies. First, the public sector accounts for a large fraction of employment (Mizala et al., 2011). Second, there is a large proportion of workers earning wages around the minimum wage levels (Maloney and Mendez, 2004; Boeri et al., 2008; Boeri, 2012). The Chilean labor market is particularly interesting because its public sector is a large employer and the wages distributions in both the public and the private sectors have a large density around the minimum wage. The minimum wage in Chile is particularly relevant compared with other OECD countries.==== Chile's national minimum wage applies to all workers between 18 and 65 years and is adjusted once a year. Based on the sample of prime-age, full-time, urban, employed, male workers used in this paper, 13.5% of the workers are employed in the public sector. In addition, the minimum wage seems to be binding, as 31% and 18% of those employed in the private and the public sector, respectively, earn up to 1.2 minimum wages.====This suggests that increases in the minimum wage do not only have a direct impact on the wage bill in both sectors, but also affect employment in both sectors differently, if public sector vacancies are filled according to rules that differ from those applied in the private sector. Therefore, a question that arises is: To what extent does the existence of a public sector employer affect the impact of the minimum wage in the labor market? In particular, how would the minimum wage policy affect labor market outcomes in an economy with private and public sector jobs? To explore these questions, we develop a search and matching model with private and public sector employment and a minimum wage policy. We then structurally estimate the model to match the Chilean data and perform different counterfactual and policy experiments to understand the main mechanisms driving our results.====Specifically, we introduce a minimum wage policy (Flinn, 2006, Flinn, 2011) in a two-sector, public and private, labor market model with endogenous heterogeneity in workers' human capital level. In the model, the labor market is segmented by human capital groups, and search is random within each human capital segment. We assume that productivity is match specific. In other words, when two parties meet there is a productivity draw from a distribution of productivity. Depending on the productivity draw and the minimum wage, the match is realized or not and wages are eventually determined. On the one hand, if the minimum wage is binding but the productivity draw is such that it is in the interest of the parties to make the match, the worker is paid the minimum wage. On the other hand, if productivity is low enough, then the match is not formed; while if it is high enough, then the minimum wage is not binding and wages are determined by Nash Bargaining. In the public sector, the minimum productivity requirement is an exogenous policy parameter that can differ from the minimum wage. We also follow the ideas of Gomes (2015) and Albrecht et al. (2019) to assume that the public sector can pay a wage premium over the private sector wages. A free entry condition determines private sector vacancies, while public sector vacancies are determined according to an exogenous employment target in the public sector.====We estimate our model by maximum likelihood methods using Chilean data from the National Socio-Economic Characterization Survey (CASEN) of 2013 and the Social Protection Survey (SPS) of 2009-2015. We follow the standard identification strategies of Flinn and Heckman (1982) and Flinn (2006) to estimate a search model with endogenous contact rates, using only supply side data. Since the model can be decoupled in two parts, the supply and the demand sides, the estimation is performed separately. This is done by exploiting the fact that the only link in the model between both sides of the market are the contact rates.====The estimation results indicate that workers in the Chilean labor market encounter private sector vacancies more frequently than they do public sector vacancies, the public sector premium favors unskilled workers, and hiring standards are by far less restrictive in the public sector than in the private sector. Our policy and counterfactual experiments suggest that a larger minimum wage increases low skilled workers' incentives to accept jobs in the public sector, where the hiring requirements and vacancies are not affected by the minimum wage policy and also where the wage policy is more generous than in the private sector. As a result, public sector employment acts as a buffer, weakening the negative effects of the minimum wage on unemployment and welfare. Among the different institutional factors of the public sector, we find that the less restrictive hiring rule largely explains the buffer effect of public sector employment as we increase the minimum wage. On the contrary, the public sector wage policy reduces this buffer effect of public sector employment. The down side is that the existence of the public sector negatively affects the private sector productivity as the minimum wage increases.====There is a vast literature on the effects of the minimum wage on the labor market. Neumark and Wascher (2008) provide a thorough review of the literature and conclude that there is a lack of consensus on the employment effects of the minimum wage. This has motivated a resurgence of interest in this topic (Meer and West, 2016; Harasztosi and Lindner, 2019). For the case of Chile, Silva (2017) analyzes the interaction between labor protection in the form of firing costs and the minimum wage in a similar setting as this paper. Regarding the effect of the minimum wage in the public sector, the literature is restricted to a few empirical papers and it is also inconclusive (Lemos, 2007; Gindling and Terrell, 2007, Gindling and Terrell, 2009; Alaniz et al., 2011). To the best of our knowledge, there is no literature analyzing the effect of the minimum wage policy in an economy with a large employer like the public sector.====A few papers introduce public sector employment in search and matching models. Burdett (2011) and Bradley et al. (2017) develop search models with on-the-job search à la Burdett and Mortensen (1998) and public sector employment. Quadrini and Trigari (2007), Michaillat (2014), Gomes (2015), Gomes (2018), Albrecht et al. (2019), Chassamboulli and Gomes (2019) and Chassamboulli and Gomes (2021) use, instead, Diamond-Mortensen-Pissarides type models. None of these papers consider the effects of a minimum wage policy in a labor market with private and public sectors.====As mentioned, our paper is related to Albrecht et al. (2019), who develop a model with public and private sectors and a continuum of worker types to analyze the effects of public sector employment policies. However, we instead focus on how the public sector affects the impact of the minimum wage policy on labor market outcomes in a model where search markets are endogenously segmented by human capital groups, i.e. formal education levels. In this aspect, our model has some features of Chassamboulli and Gomes (2019), who also model a labor market with private firms and a public sector, where two types of workers search for jobs in markets segmented by education levels. While their paper focuses on the analysis of education decisions in the context of hiring and wage setting practices of the labor market, we focus on the impact of the minimum wage. As in Albrecht et al. (2019), Gomes (2018), and Chassamboulli and Gomes (2019), we assume that public sector employment policies are exogenous.====The paper is organized as follows. In the next section, we present the model and characterize the equilibrium. In section 3, we discuss the estimation procedure and the identification strategy. We also present the estimation results and evaluate the model fit. Section 4 presents the results of policy and counterfactual experiments, and section 5 compares the main results with those of two model extensions. Finally, section 6 concludes.",Does public sector employment buffer the minimum wage effects?,https://www.sciencedirect.com/science/article/pii/S1094202521000089,11 February 2021,2021,Research Article,103.0
Kim Heejeong,"Concordia University, Canada","Received 23 February 2019, Revised 4 February 2021, Available online 11 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.006,Cited by (2),To what extent does heterogeneity in education contribute to ,"Education is an important determinant of earning capacity and of the earning risk that households face. Then, to what extent do differences in education contribute to wealth inequality and life-cycle savings, and through which pathways? That is, are highly educated workers different because, on average, they make more (the deterministic component of wages) or because they face different wage shocks (the stochastic component of wages)? This is an important question as, in the incomplete-market models, the nature of wage differences across households largely determines households' savings behavior and, thus, the distribution of wealth.====This is the first paper to study the implications of skill-specific wage processes with both between- and within-group wage dispersion on wealth inequality and life-cycle savings. In contrast to Huggett et al. (2011), by allowing idiosyncratic shock processes to differ by education levels, I show that differences in both the deterministic and stochastic components of wages are important determinants for wealth inequality and life-cycle savings.====I estimate skill-specific wage processes for college- and non-college educated workers using the Panel Study Income Dynamics (PSID) data. Here, I allow both deterministic between-group and skill-varying stochastic within-group wage dispersion. Between-group wage dispersion includes college wage premia and skill-specific labor market experience premia. Within-group wage dispersion consists of both persistent and transitory wage shocks. The estimated results show that, in the benchmark year 2004, skilled households received a college wage premium and faced steeper age-wage profiles than unskilled households. Moreover, skilled workers faced more volatile persistent wage shocks, compared to unskilled workers, while the persistence of shocks and the variance of transitory shocks were relatively similar across education groups.====I study the implications of these estimated wage processes on wealth inequality and life-cycle savings in an incomplete-markets overlapping-generations general equilibrium model with college education choices and elastic labor supply. Households make their college education decisions before the start of their working life. Education is costly and households have access to college loans. As a result of their college choices, they face wage processes over their working lives that are specific to their education levels. Specifically, skilled workers benefit from a permanently higher hourly wage – the college wage premium and labor market experience premium – and face more volatile wage shock compared to unskilled households. After retirement, households receive social security benefits proportional to their earnings.====The calibrated economy successfully reproduces the joint distribution of wealth and college degree attainment seen in the 2004 SCF. In particular, while untargeted, the benchmark economy explains the relatively high fraction of skilled households in the top distribution of wealth and the high probability of being wealthy for college graduates. Moreover, a borrowing limit and the resource costs of college give rise to total annual education costs of around ====, and 81% of college graduates holding student loan debt, similar to the data.====I find that explicitly modeling skill-specific between- and within-group wage differentials is important for understanding the wealth inequality seen in the data. For instance, the benchmark economy with skill-specific wage processes generates a wealth Gini of 0.72 and 53% of total wealth held by the top 10% of households. By comparison, in the 2004 SCF data, the corresponding numbers are 0.77 and 63%, respectively. In contrast, in an alternative model with a common wage process, essentially an Aiyagari model with elastic labor supply, the wealth Gini drops to 0.64, and the share of wealth held by the wealthiest 10% of households falls by 9 percentage points from the benchmark economy.====The more skewed distribution of wealth in the benchmark economy is mainly driven by the savings of skilled households. Skill-specific wage processes affect the saving behavior of skilled households through two channels. First, the deterministic between-group wage difference – the higher hourly wage and the steeper age-wage profile for the skilled worker – leads to a high level of earnings, increasing their savings. Second, a higher within-group wage dispersion provides a better opportunity for the skilled worker to become wealthy. This is because of the following: a more volatile persistent component of wage shock for the skilled implies a higher probability of both favorable and unfavorable wage shocks. As elastic labor supply allows skilled households to insure themselves against downside wage risk by increasing their hours worked, their more volatile wage shock leads to a higher level of earnings and, thus savings, driving further inequality.====Allowing wage shock to differ by skill levels is also crucial to understanding the large differences in the life-cycle wealth between skilled and unskilled households seen in the data. When, instead, a common wage shock is assumed, unskilled households have counterfactually high volatility in the persistent component of their wage shocks. This allows them to realize a significant amount of earnings without a college education, only if they are lucky. Thus, common wage shock for skilled and unskilled households places too much emphasis on luck in determining individual wealth. This narrows the gap in realized earnings and wealth across education groups. In contrast, by deviating from this, the benchmark economy partly endogenizes uninsurable wage risk through the discrete choice of skills and better explains the difference in the life-cycle savings of skilled and unskilled households.====To explore the relative importance of between and within-group wage differentials for the distribution of wealth and life-cycle savings, I conduct two main experiments. In the first, I abstract from between-group wage dispersion, but households still face skill-specific wage shock processes. In the second experiment, I assume that all households face a common wage shock but allow for college wage premia and different age-wage profiles. Using the first experiment, I show that between-group wage dispersion is critical in the college education decision. Without the observed benefits of college, households do not pursue a college degree, and thus wealth inequality falls sharply. In the second experiment, I find that a more volatile post-education persistent wage shock for the skilled is an important driver for the distribution of wealth at the top as it provides a better opportunity to become wealthy for the skilled workers.====Given that wage inequality has been rising in the U.S., the natural question that arises is how the rising wage inequality has affected wealth inequality. To expose this, I solve the model economy with the 1984 skill-specific wage processes. In 1984, the college wage premium was half of what it was in 2004, and the persistent component of the wage shock was much less volatile for the skilled laborer than in 2004.==== With these 1984 estimates, wealth inequality sharply falls. For instance, the 1984 economy only explains a wealth Gini of 0.66 compared to 0.72 for the 2004 economy. The share of wealth held by the top 10% of households also falls by 8 percentage points compared to the 2004 economy. I further explore the relative importance of between- and within-group wage differentials for such differences. I find that the more skewed distribution of wealth in 2004 is mainly driven by a more dispersed persistent wage shock for the skilled laborer.====The remainder of the paper is organized as follows. Section 2 discusses the related literature. Section 3 presents an empirical analysis, including the estimation of wage processes. Section 4 summarizes the model economy. Section 5 discusses the calibration. Section 6 presents quantitative results. Section 7 shows the implications of the rise in wage inequality. Section 8 concludes.","Education, wage dynamics, and wealth inequality",https://www.sciencedirect.com/science/article/pii/S1094202521000107,11 February 2021,2021,Research Article,104.0
Kim Myunghyun,"Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul, 03063, Republic of Korea","Received 14 November 2019, Revised 3 February 2021, Available online 10 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.005,Cited by (5),This paper studies international transmission of U.S. ,"U.S. monetary policy has significant effects on the prices of commodities==== such as oil, basic metals, and lumber.==== Since changes in commodity prices affect commodity-exporting and commodity-importing countries differently (see Bodenstein et al. (2011) and others), international transmissions of U.S. monetary policy shocks to the two groups may be different as well. I show empirically that in response to an expansionary monetary policy shock to the U.S., a commodity-importing country,==== the output of commodity-exporting countries increases by more than that of commodity-importing countries. This result can be explained by a rise in commodity prices and a larger increase in the exports of commodity-exporting countries compared to those of commodity-importing countries in response to the shock. I construct a three-country dynamic stochastic general equilibrium (DSGE) model that includes commodities while accounting for the idiosyncrasies of country-specific commodity trade structures. According to the simulation results of the model, the aggregate output of the commodity exporter rises more compared to that of the commodity importer in response to an expansionary U.S. monetary policy shock. Since the shock increases U.S. demand for commodities and hence commodity prices, the exports of the commodity exporter increase by more than those of the commodity importer. Therefore, U.S. monetary policy has stronger spillover effects to the commodity exporter compared to the importer, which is consistent with the empirical findings.====The importance of the international transmission of monetary policy through changes in commodity prices has been raised many times in international meetings since 2008. For example, the Delhi Declaration, issued at the 2012 BRICS==== Summit, stated that “excessive liquidity from the aggressive policy actions taken by central banks in advanced countries to stabilize their domestic economies had been spilling over into emerging market economies, fostering excessive volatility in capital flows and commodity prices.” According to Takáts and Vela (2014), at the Meeting of Deputy Governors in Basel in 2014, several central banks argued that the monetary policies in advanced countries directly affect commodity prices, and hence macroeconomic conditions in emerging economies. From these sources as well as many empirical studies, we can expect that monetary policy in a large economy like the U.S. has strong impacts on commodity prices, and that changes in commodity prices triggered by monetary policy in a large economy affect economic conditions in other countries. As a result, many empirical studies have taken commodity prices into consideration to properly analyze international monetary transmission (e.g. Canova (2005), Maćkowiak (2007), Dedola et al. (2017)).====As an example of this mechanism, suppose that there are three countries: the U.S.; Germany (commodity importer); and Saudi Arabia (commodity exporter), and two goods: cars (manufactures) and oil (commodity). The U.S. and Germany produce cars, and Saudi Arabia produces oil. An expansionary monetary policy shock to the U.S. will increase U.S. demand for all goods and the supply of U.S. cars. The extra U.S. demand will boost world demand for oil and will push up the price of oil. The price of German cars will increase as well. The price of German cars, however, will go up by less than that of oil because the supply of U.S. cars has increased and because U.S. and German cars are far more substitutable than cars and oil which are complementary. Furthermore, since an increase in the German car price relative to the U.S. car price will dampen the increased U.S. demand for German cars, German exports to the U.S. will not rise greatly. Despite a rise in oil prices, however, there will be a large rise in Saudi Arabian oil exports to the U.S. because the increased U.S. car demand and the increased supply of U.S. cars will require more oil consumption and more oil inputs in production, respectively. Hence, it would be expected that the shock would have much more stimulative effects on Saudi Arabia than on Germany.====The primary goal of this paper is to examine whether U.S. monetary policy shocks have stronger effects on commodity-exporting countries than on commodity-importing countries. Specifically, the goal is to show that in the model an expansionary monetary policy shock to the U.S. leads to a larger increase in the aggregate output of commodity-exporting countries compared to commodity-importing countries, consistent with the empirical evidence in Section 2.====This paper begins by providing empirical evidence that a U.S. monetary policy shock has larger effects on commodity-exporting countries than on commodity-importing countries. For the empirical analysis, I use structural vector autoregressions (SVARs) with block exogeneity and with sign and zero restrictions. I first show using the SVAR with block exogeneity that commodity prices increase in response to a surprise monetary easing in the U.S., that the exports and trade balance of commodity exporters rise by more than importers, and that the output of exporters also goes up by more than importers. I also show that these results are robust to identification methods of the SVAR by providing very similar responses of these variables to an expansionary U.S. monetary policy shock obtained from the SVAR with sign and zero restrictions.====Building on the empirical evidence, I construct a three-country DSGE model. Specifically, I add commodities, while accounting for different commodity trade structures, to a standard three-country DSGE model. In the model, although country ==== (representing the U.S.) produces commodities, its production of commodities is not enough to meet domestic demand. Thus, it always imports commodities from commodity-exporting countries. Country ==== (representing commodity-importing countries among the G7 countries====) is the same as country ====. Country ==== (representing the major commodity-exporting countries in the world====) produces commodities to meet both domestic and overseas demand, and thus always exports commodities. Furthermore, the household consumption basket includes commodities as a complement to manufactures,==== and firms producing manufactures use commodities as an input.====According to the simulation results of the model, an expansionary monetary policy shock to the U.S. brings about a bigger increase in the aggregate output of the commodity-exporting country than that of the commodity-importing country. To be specific, the shock to the U.S. increases its aggregate output and consumption. Accordingly, U.S. demand for imports of commodities and foreign manufactures rises, which leads to increases in the prices of commodities and foreign manufactures. Owing to the increased price, despite the increased demand, U.S. consumption of foreign manufactures decreases, and hence its imports of foreign manufactures fall. However, in spite of the increased price, U.S. commodity consumption goes up thanks to the complementarity between commodities and manufactures in consumption. Conversely, since the price of U.S. manufactures is relatively low, consumption of U.S. manufactures in all three countries increase, which causes an increase in the output of U.S. manufactures. U.S. imports of commodities for manufacturing production jump. The increased U.S. commodity imports for consumption and production enable the exports of the commodity-exporting country to rise by more compared to that of the commodity-importing country. As a consequence, the aggregate output of the commodity-exporting country goes up by more than that of the commodity-importing country.====Furthermore, according to the simulation results, if the commodity-exporting country adopts a pegged exchange rate regime—i.e. its currency is pegged to the U.S. dollar—U.S. monetary policy shocks have stronger impacts on both commodity-exporting and commodity-importing countries than when the exchange rate regime of the commodity-exporting country is floating. If the U.S. becomes a net energy exporter as per the expectations of the U.S. Energy Information Administration (EIA), the effects of the shocks on the commodity-exporting country will become weaker but those on the commodity-importing country will become stronger.====The policy implications from the analyses are clear. Since the outputs of commodity-exporting countries are more greatly affected by U.S. monetary policy than those of commodity-importing countries, their policymakers, especially those in commodity-exporting countries whose currencies are pegged to the U.S. dollar, should monitor U.S. economic conditions and monetary policy more carefully to stabilize their economies. Furthermore, if the U.S. becomes a net energy exporter as per the EIA's expectation, U.S. monetary policy will have stronger effects on commodity-importing countries than now, which means that U.S. economic conditions and monetary policy are becoming more and more important to policymakers in commodity-importing countries in stabilizing their economies.====This paper is closely related to the literature on international monetary transmission. The most well-known framework for this is the Mundell-Fleming-Dornbusch (MFD) model. According to the MFD model, a monetary loosening in the home country leads to a rise in domestic demand, which boosts the home country's imports from the foreign country (the demand-augmenting effect). However, the monetary loosening also brings about an exchange rate depreciation, which increases the foreign country's imports from the home country (the expenditure-switching effect). Since these two effects are opposite, the net effect of monetary policy in one country on the other country is ambiguous. The other popular frameworks were developed by Svensson and van Wijnbergen (1989) and Obstfeld and Rogoff (1995). Their theoretical frameworks with micro-foundations have become the main analytical machinery of international monetary transmission. These models emphasize that a monetary loosening in the U.S. increases future U.S. price levels, making future goods more expensive relative to current goods. This brings about intertemporal substitution in favor of current goods, including current foreign goods, which has positive effects on foreign output. With regard to empirical studies, many papers (e.g. Kim (2001), Faust and Rogers (2003), Dedola et al. (2017)) show that U.S. monetary policy has positive spillover effects on other countries.====This paper is organized as follows. Section 2 empirically investigates the transmission mechanism by which U.S. monetary policy shocks have stronger effects on commodity-exporting countries than on commodity-importing countries. Section 3 describes the three-country DSGE model with commodities. Section 4 presents the model calibration and analyzes how U.S. monetary policy shocks affect commodity-exporting and commodity-importing countries differently. Section 5 concludes the paper.",Transmission of U.S. monetary policy to commodity exporters and importers,https://www.sciencedirect.com/science/article/pii/S1094202521000090,10 February 2021,2021,Research Article,105.0
"Markiewicz Agnieszka,Raciborski Rafal","Erasmus University Rotterdam, Tinbergen Institute, the Netherlands,European Commission, Belgium","Received 25 March 2019, Revised 14 December 2020, Available online 10 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.01.001,Cited by (1),"We show that the drop in the equity premium since the 1970s can partially be explained by the shifts in the level and composition of U.S. income inequality. To show it, we use a framework that extends the standard production-based Consumption ==== by allowing for heterogeneity of agents, who differ in their ability to hold financial assets and their labor shares of income. The top income group, capital owners, own the firms and provide labor and the rest of the economy is populated by workers who consume their labor income and income from risk-free government and corporate bonds. Intuitively, an increase in the share of capital in income rises the riskiness of consumption and predicts higher equity premium. A rise in the share of capital owners' non-risky labor income leads to lower excess return. Time-series U.S. equity premium regressions and cross-country excess return comparison significantly and robustly validate predictions of the model. The quantitative experiment of shifting capital and labor income shares of capital owners explains one third of the observed reduction in the U.S. equity premium. The reason is that, during the last five decades, capital owners benefited from higher average growth in their non-risky labor income relative to the capital income.","Equity premium on the U.S. stock market fell from 10.6% during the 1950s and 1960s to 7.5% since the 1970s. At the same time, the U.S. economy has witnessed an unprecendented increase in income inequality.==== Are these two trends related? Since equities are predominantly held by households at the top of the income distribution, changes in their income shares should have an impact on steady state equity prices.==== But is an increase in inequality consistent with a general fall in equity premium?====We argue that the answer to this question does not only depend on the level, but also on the source of income inequality shifts. Over the recent years, two income share trends benefited the wealthier households. First, as is shown in Fig. 2, the labor share of income of top decile in the U.S. increased from 25% to 36%, between 1970 and 2014. Second, as shown in the bottom panel of Fig. 1, the capital share of income, predominantly held by the rich households, increased from 35% to 43%, over the same period. The paper shows that these two shifts have opposite theoretical implications for the equity premium. Intuitively, an increase in the share of income derived from labor constitutes a hedge against stock market fluctuations in terms of consumption risk. As a consequence, an increase in the labor share of income of stock holders should reduce equity premium. On the other hand, higher share of income derived from capital exposes shareholders to additional consumption risk. Therefore, it will typically coincide with higher demanded equity premia. The combined effect is, hence, ambiguous.====To judge the quantitative impact of the observed income level and composition shifts on equity premium we build a stylized general equilibrium model. The model extends the standard RBC setting used in the production-based Consumption Capital Asset Pricing Model (CCAPM) literature by allowing for heterogeneity of agents, who differ in their labor shares of income and their ability to hold financial assets. The top income group (capital owners) owns the economy's financial wealth—a setup that roughly approximates the highly-skewed distribution of U.S. financial wealth. The rest of the economy is populated by workers who consume their labor income and income from risk-free government and corporate bonds. In order to replicate the most salient stock market stylized facts (sizable equity premium, high Sharpe ratio and price-equity ratio), we equip the model with several additional features that increase the sensitivity of capital owners to stock market risk: a high and time-varying coefficient of risk-aversion (similar to Greenwald et al., 2016), capital adjustment costs (e.g. Uhlig, 2007 and Jermann and Quadrini, 2012) and financial leverage (Jermann and Quadrini, 2012). The model is calibrated to match the financial market and real economy statistics and income shares observed over the period of interest.====Based on the empirical evidence on the persistence in income inequality shifts, our main quantitative exercise compares two stationary steady states characterized by different level and composition of capital owners' income shares.==== We calibrate the model's baseline steady state to the U.S. post-war economy between 1947 and 1970 with the average equity premium on S&P500 shares of 10.6%. The second steady state is characterized by higher labor and capital income shares in top income decile in line with the shifts that occurred over 1970-2014. We find that this alternative calibration reduces equity premium to 9.7%, implying that the shifts in income distribution and composition over the last five decades can explain about one third of the observed reduction in the U.S. equity premium, over the same period. Clearly, the upward shift in top decile labor income share has dominated the simultaneous increase in capital income share, reducing the overall riskiness of the top decile income and exerting a downward pressure on equity premium. Further experiments show that, absent the capital income share increase, the shift in labor income share alone would have been associated with an even larger fall in excess return of 1.6 percentage points.====The predictions that a higher capital share of income is associated with higher equity premium and that a higher labor share of income of capital owners is negatively correlated with the excess returns can be confronted more directly with the data. We test these predictions using cross-country and U.S. time-series data. Because, as in the model, we are interested in the long-run shifts, we rely on the long-run regressions to estimate the relationship between equity premium and income shares'. We show that U.S. capital income inequality is positively, significantly and robustly associated with the real excess returns while labor share of income of capital owners, defined as top decile or top quintile, is negatively related to the real excess return. An international comparison delivers similar results as, in a panel of 17 OECD countries, 5-year changes in equity premia are positively related with 5-year changes in capital share of income.====Our paper draws on a number of strands in the economic literature. The theoretical model we use for linking shifts in income shares to equity premium builds on the literature embedding risky asset markets into RBC models, similar to the work of Jermann (1998), Boldrin et al. (2001) and Danthine and Donaldson (2002 and 2008), who do not distinguish between workers and capital owners. Guvenen (2009) and Guvenen and Kuruscu (2006) extend the otherwise standard RBC framework by introducing two types of consumers, which differ by their elasticity of intertemporal substitution. In our paper, the elasticity of intertemporal substitution is kept identical for both types of households.====Our modeling framework is also closely related to Lansing (2015), who develops a production-based asset pricing model with two types of households, high concentration of productive capital and “redistributive shocks” to the shares of income. The focus of Lansing's paper is on high post-war level of equity premium and the model is shown to be able to reproduce up to two thirds of it. Our focus, instead, is on the longer-term, structural shifts in income shares and the resulting long-term ==== trend in the equity premium. Furthermore, in Lansing (2015), the high equity premium is primarily driven by distribution shock raising dividends' volatility. In our model, we obtain a high excess return by incorporating a time-varying risk-aversion in capital owners' utility function in spirit of Greenwald et al. (2016). In contrast to Greenwald et al. (2016), however, our model mirrors the empirical composition of income and therefore, in addition to capital income, capital owners earn wage income.==== This modification is essential to quantify the joint impact of shifts in income shares because they predict the opposite movements in the equity premium. Models ignoring capital owners' income derived from labor will, therefore, tend to overestimate the equity premium and struggle to account for its decades-long downward trend.====Several recent papers study the link between income and wealth inequality and equity premium. Walentin (2010) argues that an increase in stockholders' share of aggregate labor income reduces the covariance between stockholders' total income growth and dividend growth and therefore leads to the lower equity premium, but he ignores the empirical shift in the capital share. In an incomplete markets OLG framework, Favilukis (2013) shows that the observed rise in wage inequality, decrease in participation costs, and loosening of borrowing constraints can jointly explain substantial increases in wealth inequality and stock market participation, a decline in interest rates and the expected equity premium, as well as a prolonged stock market boom. Toda and Walsh (2020) show theoretically and empirically that an increase in the wealth share of stock-holders reduces the equity premium. Gomez (2018) documents that when stock returns are high, inequality increases but higher inequality predicts lower stock returns.====By studying the link between income shares and asset returns, we also contribute to the fast-growing literature emphasizing the importance of wealth dispersion and resulting capital income inequality in the U.S. and other developed economies. Kacperczyk et al. (2019) show that capital income inequality is large and growing fast, accounting for a considerable portion of total income inequality in the U.S. In addition, Saez and Zucman (2016) demonstrate an increased correlation between top labor and top capital incomes in the U.S. data. Our framework is motivated by these recent empirical observations and therefore models capital owners also as high labor income earners.====The paper is organized as follows. In Section 2, we describe a set of stylized facts on changes in income inequality and equity premium. In Section 3, we describe the model and its main intuition. Specifically, we explain the model mechanisms linking shifts in income shares to equity premium. Section 4 describes our empirical strategy including calibration of the model, its quantitative performance and the comparison of two model economies characterized by different level and composition of income inequality. Section 5 further confronts the model predictions with the data via a set of time-series and panel regressions. Section 6 concludes.",Income inequality and stock market returns,https://www.sciencedirect.com/science/article/pii/S1094202521000016,10 February 2021,2021,Research Article,106.0
"Cairó Isabel,Fujita Shigeru,Morales-Jiménez Camilo","Board of Governors of the Federal Reserve System, United States of America,Federal Reserve Bank of Philadelphia, United States of America","Received 30 December 2020, Revised 27 January 2021, Available online 10 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.001,Cited by (5)," in expansions. We identify two key channels through which the model delivers this result: (i) procyclical values of non-market activities and (ii) wage rigidity. The smaller the value of the extensive-margin labor supply elasticity is, the stronger the first channel is. Wage rigidity helps because it mitigates increases in the return to market work during expansions. Our estimated model replicates well the behavior of transition rates between the three labor market states and thus the stocks, once both features are in place.","Search and matching models pioneered by Mortensen and Pissarides (1994) are now a standard tool to analyze labor market fluctuations. The focus of this literature, especially since Shimer (2005), has been mostly on the sources of cyclical variations in labor demand.==== Developments in the U.S. labor market since the Great Recession, however, have spurred renewed interests among policymakers in the need for a better understanding of cyclical fluctuations in labor supply margins. This new trend is exemplified by numerous speeches by the Federal Reserve officials that engage in extensive discussions on underlying drivers of the movements in the labor force participation rate (LFPR).====In this paper, we extend a canonical search and matching model by adding an extensive-margin labor supply decision. Our aim is to develop a tractable yet quantitative framework that allows us to analyze labor market dynamics as a result of equilibrium responses of both the labor demand (job creation) and the labor supply (labor force participation) margins. In doing so, we also reevaluate the longstanding puzzle in macroeconomics that the calibrated value of the Frisch labor supply elasticity required to replicate aggregate employment fluctuations is measurably larger than the values suggested by the micro-level evidence (see, for example, Chetty et al. (2011) for a review of this literature). We revisit this issue in a framework with search frictions, while it has traditionally been studied in models with a frictionless labor market. We show that our model replicates salient cyclical features of transition rates between three labor market states (employment, unemployment, and nonparticipation) and that small labor supply elasticities are in fact ==== to match their business-cycle comovements. Throughout the paper, we emphasize the importance of the following two concepts for our results: (i) procyclicality values of non-market activities (as in Chodorow-Reich and Karabarbounis (2016)) and (ii) equilibrium wage rigidity (as in Hall (2005)). Note that our contribution is ==== to provide a new channel for labor demand magnification, but to demonstrate that these two concepts are also crucial to understand the cyclical variations in the participation margin.====We first summarize cyclical properties of labor market transition rates and stocks with respect to an impulse to labor productivity by estimating a simple vector auto-regression (VAR). In addition to the well-known cyclical pattern in transition rates between employment and unemployment, we show that the transition rate at which nonparticipants join the unemployment pool (NU rate) is countercyclical, while the exit rate from unemployment to nonparticipation (UN rate) is procyclical. Note that the behavior of these transition rates is particularly informative about the cyclicality of the participation margin, as they represent the pace of worker flows into and out of nonparticipation. The VAR evidence also reveals that transition rates between employment and nonparticipation are procyclical in both directions. All of these patterns are highly consistent with the existing literature analyzing unconditional data moments of transition rates (e.g., Elsby et al. (2015), Krusell et al. (2017)), but we present these properties more explicitly in relation to the change in market-work productivity. This approach is appropriate, given that our model focuses on the tradeoff between values of market work and home production/leisure over the business cycle.====Our model features the representative household that makes the participation decision. In the model, nonemployed household members differ with respect to their productivity at home, based on which the household optimally allocates them to either active job search (unemployment) or nonparticipation (home production). The equilibrium determines the two key endogenous variables, labor market tightness and the participation margin. The latter is represented by the threshold value of home productivity, above (below) which a nonemployed household member stays out of the labor force (joins the unemployment pool). When looking at the empirical evidence through the lens of the model, we find that the participation margin must be strongly countercyclical. That is, the household must be less willing to send an additional member to the unemployment pool in expansions than in recessions. The aforementioned countercyclicality of the NU rate and procyclicality of the UN rate directly reflect this mechanism. The conclusion that the participation margin must be strongly countercyclical underscores the importance of adopting the flow approach in modeling labor market dynamics, as opposed to the stock approach. In the stock approach, the (weak) procyclicality of the LFPR is likely to be interpreted as indicating the procyclicality of the participation margin. Our paper thus highlights the importance of analyzing transition rates as primitives in order to understand the key economic mechanisms underlying the cyclicality of the LFPR.====To be more specific, consider the household's decision as to whether to send an additional member to the unemployment pool or to keep the member as a nonparticipant. In the model, there are two channels affecting this participation decision. The first channel is through the returns to market work: higher labor market tightness (and thus a higher job-finding rate) and higher wages during expansions motivate the household to send more members to the unemployment pool. The second channel is through the cyclical fluctuations of the opportunity cost of participation, i.e., values of non-market activities (leisure and home production). Under standard preferences, a higher employment share in the household results in higher marginal values of leisure and home production (measured in market-goods consumption), thus keeping the household from sending more members to the labor force. The countercyclicality of the participation margin implies that the second channel must dominate the first one. The model can achieve this (i) if elasticities of labor supply are ====, which implies a larger increase in values of non-market activities, and (ii) if wages do not rise as much in expansions. We adopt a simple form of equilibrium wage rigidity, proposed by Hall (2005), which not only enhances labor demand fluctuations but also plays a key role in replicating labor-supply responses. Note that, in models with a frictionless labor market, small labor supply elasticities and the lack of movements in wages imply the lack of employment variability. In models with search frictions, however, movements in the participation margin do not need to result in changes in employment. Moreover, in such an environment, the lower the extensive-margin labor supply elasticity is and the larger the complementarity between home production and market-goods consumption is, the more countercyclical the participation margin becomes without compromising labor market volatility. Our results indicate that, in models with search frictions, wage rigidity and procyclical values of non-market activities together can provide a coherent mechanism that simultaneously accounts for labor demand and labor supply responses over the business cycle.====Our estimated model matches the overall cyclical patterns of ==== transition rates across the three labor market states and the behavior of labor market stocks (the unemployment rate, the employment-to-population ratio, and the LFPR). Several notable results are as follows. First, our model reproduces the observation that the LFPR exhibits a small procyclical variation over the cycle, even though underlying transition rates all exhibit large volatilities. Second, our model matches the empirical pattern that separation rates from employment into unemployment (EU) and into nonparticipation (EN) move in opposite directions. Although our model assumes a constant separation rate out of employment, the share of separations flowing into the unemployment pool increases in downturns (and thus the share of the other flow falls). In replicating this pattern, the countercyclical participation margin again plays a key role. We show that the countercyclicality of the separation rate into unemployment contributes significantly to unemployment fluctuations. This is notable because, in two-state models, a constant separation rate implies no contributions of the EU rate to unemployment fluctuations, contrary to the data. Relatedly, our model maintains the strong negative correlation between unemployment and vacancies, known as the Beveridge curve.====Our model does have some weakness. In particular, in response to the positive productivity shock, the model predicts that the LFPR falls slightly in the short run before it starts increasing, whereas the data favor no short-run response instead of a decline. This weakness of the model comes down to the countercyclical participation margin that responds too sharply in the short run, resulting in some misalignments between empirical and model responses that determine the pace of entries into and exits from the labor force. We show, however, that either assuming flexible wages or linear utility (both of which mitigate the countercyclicality of the participation margin) results in far worse performance.====  Earlier attempts that incorporate the extensive-margin labor supply decision into search models include Tripier (2003) and Veracierto (2008). They find that unemployment tends to become procyclical, once the participation margin is endogenized. In their models, the participation margin is procyclical because of flexible wages and a large elasticity of labor supply. Their models are different from ours in many dimensions, but we show that a similar result tends to arise in our model when wages are flexible. More recently, Shimer (2012) studies the properties of a model similar to Tripier's. He pays close attention to the role of wage rigidity as we do in our paper. However, he does not relate his findings to elasticities of labor supply. In Shimer's baseline model, the split between unemployment and nonparticipants is perfectly elastic and thus rigid wage is not enough to mitigate the procyclical force in unemployment. Haefke and Reiter (2011) develop a search model with heterogeneous workers and endogenous participation decisions, and evaluate its quantitative performance in light of micro evidence on labor supply elasticities. Based on a steady-state analysis, they also find that small elasticity values are consistent with fluctuations of labor market stocks under empirically plausible degrees of wage rigidity. Galí (2010), Galí et al. (2012), and Campolmi and Gnocchi (2016) study New Keynesian models with search frictions and endogenous participation.==== Their models consider a richer environment with more frictions and shocks. We study a simpler environment to emphasize key economic mechanisms. Importantly, none of the papers cited so far try to match the cyclicality of transition rates. We tackle this challenging task of matching the cyclicality of both transition rates and stocks. Ferraro and Fiori (2019) study asymmetric business cycles in a heterogeneous-agent model with endogenous participation, in which labor supply is perfectly elastic. Their model matches the volatility and cyclicality (measured as the correlation with output) of labor market transition rates by exogenously making the opportunity cost of employment strongly procyclical. Last but not least, Krusell et al. (2017) develop a heterogeneous-agent search model with endogenous participation and look explicitly at transition rates, especially those between unemployment and nonparticipation. They emphasize the role of wealth heterogeneity and associated composition effects in explaining the cyclicality of these rates. Because we study a representative-agent environment, we necessarily abstract from such composition effects but provide complementary channels. Furthermore, they do not consider the link between the cyclicality of labor force participation flows and extensive-margin labor supply elasticities.====As emphasized above, an important element of our model is the procyclicality of values of non-market activities. In the context of a two-state model, Chodorow-Reich and Karabarbounis (2016) show that this procyclicality reduces labor demand by making surplus less responsive to business cycle shocks, presenting more challenges to the resolution of the unemployment volatility puzzle (Shimer (2005), Costain and Reiter (2008), and Hagedorn and Manovskii (2008)). We show that this feature is actually necessary in models with endogenous labor force participation in order to match the cyclicality of labor market flows and stocks.====We organize this paper as follows. Section 2 presents the empirical evidence. Section 3 develops the model, which is then estimated in Section 4. Section 5 contains the paper's main quantitative results and discussions on the mechanism driving them. Section 6 concludes.",The cyclicality of labor force participation flows: The role of labor supply elasticities and wage rigidity,https://www.sciencedirect.com/science/article/pii/S1094202521000053,10 February 2021,2021,Research Article,107.0
"Li Wenli,Yu Edison G.","Research Department, Federal Reserve Bank of Philadelphia, United States of America","Received 4 May 2020, Revised 3 February 2021, Available online 10 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.003,Cited by (1),"We examine the impact of changes in the federal ==== treatment of owner-occupied housing stemming from the implementation of the Tax Cuts and Jobs Act (TCJA) in January 2018 on local housing markets. Using county-level ==== as shares of taxable income exceeded the national median. The results are robust when controlling for other changes in the tax reform. The areas with a high real estate tax burden also suffered from reductions in market liquidity after the reform. Fewer houses were transacted either in absolute numbers or as shares of total listings and houses stayed on the market longer before being sold. Importantly, we find that the housing market slowdown was accompanied by declines in local construction employment growth as well as multi-family building permits. Furthermore, on net more people moved out of these areas after the reform. Finally, we show that the act has already had political consequences. In the 2018 midterm Senate elections, more voters voted for Democratic candidates in areas with high real estate tax burden than they did for Republican candidates.","The Tax Cuts and Jobs Act (TCJA), which went into effect on January 1, 2018, made the most significant changes in the federal tax treatment of owner-occupied housing since the Tax Reform Act of 1986. Among the many changes in the act, itemized deductions of state and local taxes (SALT) from federal income tax, previously uncapped, are now limited to $10,000 for individuals and married couples filing jointly. This change undoubtedly affected different geographical areas differently as households' SALT obligation varies significantly with their residence. For instance, for the tax year of 2016, Internal Revenue Service tax data indicate that the average ratio of real estate taxes to adjusted gross income ranged from zero in Houston County, Georgia, King County, Texas, and Hayes County, Nevada to over 5 percent in Putnam County and Rockland County in New York.==== Although TCJA also contained other provisions that offset some of the impact from capping SALT tax deductions such as reductions in marginal tax rates for many households and increases in standard deductions,==== it is widely believed that for homeowners living in areas with high state and local taxes,==== capping SALT deductions lowered the value of tax-exempt imputed income from owning their houses.====In this paper, we provide the first nationwide analysis on the heterogeneous impact TCJA had on U.S. residential housing value as well as other economic consequences over the period between January 2018 and October 2019. The main part of our study tests how changes in the tax treatment of homeownership in TCJA affected house prices across different geographical locations. It is important to understand this impact because housing wealth constitutes almost half of American household wealth and about 80 percent of the housing wealth is in owner-occupied units. We use county-level house price data from Zillow, an online real estate data company, for our main analysis and a difference-in-difference estimation method across time and space for identification. As mentioned earlier, TCJA affected residents living in different counties differently due to differences in their local tax burden. For the benchmark analysis, we thus divide counties into two groups based on the average rate of local taxes relative to taxable income, those above the national median, which we term high real estate tax burden area, and those below the median, and analyze how the two groups behaved differently before and after the implementation of TCJA. We find that counties with high real estate taxes relative to income had slower house price growth during the first 22 months after the implementation of TCJA.====In our analysis, we utilize several measures of local home value including median home value per square foot, sale price, listing price per square foot and Zillow Home Value Index to fully take advantage of the different strength of these measures in terms of their coverage and construction methodology. We also calculate growth rates at both the (annualized) month-to-month frequency and the year-to-year frequency to capture different price growth volatility over different horizons, as year-over-year growth rates typically exhibit less volatility than month-over-month growth rates. Under our benchmark measure, annualized monthly growth rates of median home value per square foot, we find that counties with real estate taxes relative to income in the top half of the nation experienced a drop in home value growth rate of 0.9 percentage point per year or 18 percent during the first 22 months after the implementation of TCJA. This is equivalent to nearly $2,900 for the median house in the high tax counties. The other measures generate a decline in house value growth rates ranging from a low of 0.04 percentage point to a high of 3.5 percentage points.====We tested confounding factors and the parallel trend assumption for identification. There are other changes in the tax reform that might have affected the house price growth differentially across counties. In addition, a number of events took place in and around 2018 besides the implementation of TCJA. For instance, the Federal Reserve raised interest rates three times in 2017 and then another three times in 2018 before lowering them three times in the second half of 2019. Mortgage rates rose in response during most of our sample period, hurting areas with high house prices where households likely need to borrow large amount of mortgages. Additionally, TCJA was first introduced in the House in November 2017, two months before it was signed into law. As a result, households may have preemptively responded to the proposal. We address these concerns by including additional controls for these other events and by conducting placebo tests where we assume intervention occurred in the other months of our sample period than January 2018. We also randomly assign counties to states. We find that our estimates are robust to the introduction of additional controls as well as the random assignment of counties to states and there exist no statistically significant price effects associated with these other dates in the placebo tests.====Our main argument for constructing real estate tax burden, i.e., normalizing real estate taxes paid by taxable income, to proxy for different location's exposure to TCJA is to account for the fact that households living in expensive areas also have higher income on average.==== It is true, however, that TCJA imposes constraints on tax levels. We thus repeat our analysis using average real estate taxes paid while controlling for taxable income as well as the other factors included in the previous analyses. We find that counties with average real estate taxes paid in the top half of the nation experienced a decline of 1.6 percentage points or about 30 percent in house price growth during that period.====To investigate the heterogeneous response within the local housing market, we further divide the local market by its purchase prices relative to the area median and repeat our analysis using the corresponding house price index from CoreLogic Solutions. We find that the negative impact on house price growth rates was most felt within the medium range of the market. While the most expensive segment of the local housing market also suffered after the tax reform, the positive income effect it received from the reduction in income tax rates for high income brackets negated the magnitude.====Given that residential rental prices are typically closely tied to residential house prices, it is likely that the slower house price growth transmits to the rental market and leads to slower rental price growth. TCJA, however, adversely affects only homeowners while landlords can continue to claim all SALT taxes as business expenses. In addition, TCJA added a generous new business deduction for pass-through businesses which benefited small business owners such as landlords. Indeed, our analysis reveals that the tax reform did not have a negative impact on local rent price growth for three of the four rental price measures we use.====Housing market liquidity affects households' ability to buy and sell housing units. Asset prices tend to decline when liquidity is poor. Using several proxies for local housing market liquidity and the same difference-in-difference estimation technique, we find that, after TCJA, in areas with high real estate taxes relative to income, fewer houses were sold both in absolute numbers and relative to those listed and houses stayed on the market longer before being sold. In other words, TCJA reduced housing market liquidity in areas with high real estate tax burden. This deterioration in housing liquidity likely contributed to the severity of the decline in local house price appreciation rates.====Taken together, our analysis indicates that there were sizable slowdowns in local house price growth in high tax areas after TCJA took effect. We next investigate whether these declines had real economic consequences and how households have responded to its differential impact across geographical regions. For real economic consequences, we focus on employment in the local construction sector and building permits granted.==== Our study suggests that, after the reform, growth rates in local construction sector employment slowed and building permits granted for multi-family units also declined in areas with high real estate taxes relative to income. Given the sizable negative effects on house prices and real economic variables associated with TCJA, it is, therefore, not surprising that less than two years after the implementation of TCJA, more people have moved out of areas with high real estate tax burden after the reform relative to areas with low real estate tax burden. This effect is more pronounced for individuals with a mortgage, a proxy of their homeownership status. Furthermore, it is also not surprising that the act appeared to have had political consequences. During the 2018 midterm Senate elections, the share of voters who voted for Democratic candidates increased in areas with real estate tax burdens above the national median. This result holds irrespective of the party affiliation of the incumbent candidate or the Senate election results in 2016.====The paper proceeds as follows. In the next section, we conduct literature review. In Section 3, we summarize the changes in SALT deduction as a result of the TCJA and present a simple user cost model. Section 4 describes the data. Section 5 investigates the differential impacts of the tax reform on house price growth across different geographical areas. Section 6 studies the effects of the tax reform on rental prices and house market liquidity. Section 8 explores other real economic effects. Section 9 concludes.",Real estate taxes and home value: Evidence from TCJA,https://www.sciencedirect.com/science/article/pii/S1094202521000077,10 February 2021,2021,Research Article,108.0
"Ben Zeev Nadav,Ifergane Tomer","Ben-Gurion University of the Negev, Israel","Received 26 September 2019, Revised 19 January 2021, Available online 8 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.01.003,Cited by (0),"Firing restrictions are in use throughout the developed world but their role in the transmission of macroeconomic shocks into the real economy is mostly unstudied. We illustrate the theoretical role of these policies as amplifiers of macroeconomic shocks via labor-misallocation-induced output losses following an adverse shock. We use our model to derive an aggregation result which features a labor misallocation term and conduct a simulation exercise which demonstrates how misallocation can drive total factor productivity (TFP) down during recessions. We then perform a quasi-natural experiment which utilizes global credit supply shocks to study this amplifying role using a panel of 21 ==== economies. We show that strict firing restrictions are associated with a weaker initial response of the labor market, which is followed by a stronger and more persistent decline in real output as well as a slower return of real activity to pre-shock levels. The stronger output decline can be mostly explained by a stronger fall in aggregate TFP, which supports our theoretical predictions.","How do firing restrictions affect the transmission of macroeconomic shocks? Employment protection legislation (EPL) is a widely used set of policy devices in developed economies and it plays an important institutional role in modern labor markets. Most of the policy debate regarding EPL is centered around two main issues: its effects on long-term macroeconomic performance on the one hand, and its significance for microeconomic outcomes in the labor market, on the other hand.==== However, the use of such a policy device in times of economic adversity may alter the impact of macroeconomic shocks, influence their transmission mechanisms, and affect recovery. To fix ideas, throughout this paper, the term EPL will relate only to firing restrictions on regular workers and the two will be used interchangeably unless when relating to a particular restriction.==== Our aim in this paper is to explore the potential link between EPL and economic resilience. We use the latter term to refer to an economy's ability to withstand macroeconomic shocks. To accomplish this goal, this paper unfolds in three parts.====First, we demonstrate the capacity of firing restrictions to affect misallocation during a business cycle using a search and matching model which incorporates termination costs and advance notice. Our model builds upon the work of Lagos (2006), but allows for an endogenous choice of capital and includes a novel treatment of termination notice within a search model. The model enables us to derive an aggregation result that illustrates the capacity of firing restrictions to generate a cyclical decline in total factor productivity (TFP) stemming from labor misallocation, in addition to the policies' steady-state effects. Our aggregation result may be useful for future empirical works aimed at providing a more detailed decomposition of TFP and applying misallocation adjustments so as to construct purified aggregate technology measures as well as isolate the effects of different policies.====Second, we conduct a quantitative exercise that attempts to gauge the relative importance of the misallocation channel for cyclical dynamics. We calibrate our model to match key moments in an economy which features significant firing restrictions, namely France. We propose a novel (to the best of our knowledge) calibration strategy and show that it performs well in terms of matching steady-state moments of the earnings distribution which are not targeted moments. The quantitative exercise suggests that the cyclical misallocation channel is present and of meaningful magnitude. We relate our model to the literature on aggregate fluctuation in search models, i.e. Shimer's puzzle and discuss its effects in our setup.====Last, we utilize global shifts in credit conditions to conduct a quasi-natural experiment capable of uncovering the effects and propagation of such shifts into output and labor markets of economies exhibiting different levels of firing restrictions. The central motivation for our empirical approach rests on the fact that the recent global financial crisis had a considerable effect on developed economies and that most of these economies vary substantially with respect to their labor market policies. We carry out this analysis by estimating state-dependent impulse response functions to the shock for measures of real activity and labor market activity. Our identification approach adapts the local projections method developed in Jorda (2005) to a panel setting, as in Auerbach and Gorodnichenko (2012).====The main results from our empirical analysis can be summarized as follows. Firing restrictions reduce the initial effect of the shock on the labor market, leading to a smaller and slower rise in unemployment, a smaller drop in employment, and to more stability in terms of labor-force participation. However, from roughly the 1.5-year mark onwards, economies under strict firing restrictions experience a stronger and more persistent decline in real output. The drop in output is in the opposite direction to the effect on employment and too fast and sizable to be accounted for by a differential decline in capital stock, which is consistent with a drop in TFP taking place under the more restrictive regime. Such a drop is indeed evident in the data and is statistically significant. We further demonstrate that this sequence of differential responses in the labor market, real output, and TFP is statistically significant and robust to various choices of specifications and samples. These results are in line with those of the simulation exercise and suggest a TFP decline that is larger but falls within the same order of magnitude as our quantitative exercise implies. Interpreted through the lens of our theory, this amplification mechanism has its roots in EPL's contribution to increased misallocation of labor following an adverse shock.====Our results are of particular policy importance for the current COVID-19 crisis. Our theory indicates that countries in which firing restrictions are pervasive may experience a larger drop in aggregate TFP over the next few years. To ameliorate this adverse effect, our results provide support for a relaxation of these restrictions, at least temporarily, as a part of an economic exit strategy for the current crisis.==== This paper is most closely related to the literature on labor market institutions and their interaction with macroeconomic shocks. The work of Blanchard and Wolfers (2000) describes how changes in European unemployment data can be explained by the interactions the institutional factors in the labor market with various shocks. In addition to the long-term changes in unemployment, institutional factors had been linked to macroeconomic volatilities (e.g., Gnocchi et al. (2015) and Rumler and Scharler (2011)). The interaction between EPL and the business cycle has also been studied in Nunziata (2003) which demonstrates empirically and theoretically that strictness of EPL lowers the output elasticity of employment. Along this line, Duval and Vogel (2008) illustrate how strict EPL leads to more persistence in business cycle dynamics using output gap to identify cycles. The mechanism suggested by theory to explain this link between cyclical adjustment and EPL is that strict EPL should slow turnover dynamics and make the adjustment process to a shock longer as in Bentolila and Bertola (1990) and in Garibaldi (1998). The work of Messina and Vallanti (2007) provides support to this claim using firm-level data which indicates that strictness of EPL dampens the response of job destruction to the cycle, thus leading to less counter-cyclicality in job destruction.====This paper is also related to the literature which emerged after the Great Recession aimed at understanding the fashion in which different advanced economies have responded to what was generally considered as a global shock. Just following the Great Recession Ohanian (2010) examines the way in which Europe and the United States have experienced this shock using business cycle accounting. Ohanian's analysis points to the fact that in Europe the drop in the productivity deviation was more pronounced, while the United States had experienced little change in the productivity deviation but had experienced a sizable drop in the labor deviation relative to that which was present in Europe. Ohanian notes that this may be due to European firing restrictions which may lead to labor hoarding and lower measured productivity. This insight, which is revisited in Ohanian and Raffo (2012), is another motivation for this paper as it will demonstrate in detail how this may be the case and to what extent is this channel present. The relevance of labor market rigidities to the propagation of international business cycles is also discussed in the work of Perri and Quadrini (2018) which show that when the authors account for a variation in the adjustment costs of the labor input between the United States and the G6 countries, their model is able to provide a better match for the response patterns from the Great Recession.==== The contribution of this paper is twofold. First, the paper contributes to the empirical literature by conducting a comprehensive investigation of the link between firing restrictions and the transmission of credit supply shocks to several outcome measures, such as real output, privet consumption, investment, capacity utilization, TFP, unemployment, employment to population ratio, and labor-force participation. Our identification strategy in this paper differs from the aforementioned works due to the use of an identified shock and higher data frequencies to estimate non-linear, state-dependent impulse response functions which allow observing the restrictions' effect on the shock's transmission channel rather than exploring these effects on moments or long-term trends.====Second, this paper contributes to the theoretical literature by creating a very rich model of firing restrictions. The conceptual contribution of the model is the aggregation result provided in Section 2, which is a generalization of the result of Lagos (2006) with a richer institutional setup. Using a quantitative version of our model we illustrate the amplification channel by which firing restrictions lead to an increased level of misallocation following an adverse shock, thus lowering TFP and consequently leading to a more stark drop in output. Although we cannot directly observe the theoretical channel in the data, the model provides predictions that are broadly consistent with our empirical findings and allows for a better understanding thereof. As such, we view the empirical contribution of this paper as the most significant one, and will continue to explore the theoretical misallocation channel in our future work.==== The rest of the paper proceeds as follows. We begin in Section 2 by modeling the way by which firing restrictions affect aggregate productivity and affect the transmission of aggregate shocks. We proceed in Section 3 by conducting a quantitative calibration and simulation exercise by which we illustrate the potential amplification effect that results from increased labor misallocation following an adverse shock. We then move on to our empirical analysis. We describe the data used with an emphasis on the measure for EPL in Section 4. In Section 5 we present our econometric method. In Section 6 we present and discuss our results. The final section concludes. We relegate some of the more technical elements of the model to Appendix A, the robustness of our empirical results is discussed at length in Online Appendix B, and a detailed description of the datasets used is given in Online Appendix C.",Firing restrictions and economic resilience: Protect and survive?,https://www.sciencedirect.com/science/article/pii/S109420252100003X,8 February 2021,2021,Research Article,109.0
Matsumoto Hidehiko,"National Graduate Institute for Policy Studies, 7-22-1 Roppongi, Minato-ku, Tokyo, 106-8677, Japan","Received 9 December 2019, Revised 3 February 2021, Available online 6 February 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.02.002,Cited by (5),"This paper develops a quantitative small-open-economy model to study the determinants of the optimal pace of foreign reserve accumulation by developing countries. In the model, reserve accumulation depreciates the ==== and attracts foreign direct investment (FDI) inflows, which promotes productivity growth through endogenous firm dynamics. The economy is also subject to sudden stops in the form of an occasionally binding constraint on foreign borrowing, and accumulated reserves are used to prevent severe economic downturns. The model shows that two factors are the key determinants of the optimal pace of reserve accumulation: the elasticity of the foreign borrowing spread with respect to foreign debt and the entry cost for FDI. The model suggests that these two factors can explain a substantial amount of the cross-country variation in the observed pace of reserve accumulation.","The active accumulation of foreign reserves by developing countries, especially those in East and Southeast Asia, is one of the most prominent developments in the international financial system over the past 30 years. The left panel in Fig. 1 shows that the average reserve-to-GDP ratio across 67 developing countries has increased from lower than 10% before 1990 to nearly 25% by 2010. A large volume of literature investigates the motives for this active reserve accumulation, identifying two main motives: a precautionary motive to prevent severe economic downturns caused by sudden stops in capital inflows, and a growth strategy through exchange rate depreciation and export promotion.====These benefits of reserve accumulation are well understood, but there is a wide cross-country variation in how actively each country pursues these benefits. The right panel in Fig. 1 shows the average annual change in reserve holdings in terms of the ratio to GDP for 20 selected developing countries in 1991–2010. It illustrates that Asian countries such as China, Malaysia, and Thailand have been accumulating reserves equivalent to ==== of GDP per year on average, whereas many Latin American countries are accumulating reserves less than 1% of GDP. Despite active research on the optimal reserve policy throughout the past decade, existing studies do not report much about what the optimal pace of reserve accumulation for each country is and why different countries accumulate reserves at different paces.====This paper addresses these questions by developing a quantitative model of reserve accumulation. The main contribution of this paper is twofold. First, this paper provides a theoretical framework to explain the observed positive correlations between the pace of reserve accumulation, economic growth, and foreign direct investment (FDI) inflows across countries. The novelty of the model is that both reserve accumulation in normal times and reserve intervention during crises induce domestic investment and attract FDI through anticipation of fast and stable economic growth. Second, using the model, this paper identifies two determinants of the optimal pace of reserve accumulation: the elasticity of the foreign borrowing spread to foreign debt, and the FDI entry cost. The quantitative analysis shows that these two factors explain a substantial amount of the cross-country variation in the observed pace of reserve accumulation. To the best of my knowledge, this paper is the first to develop a quantitative model that can explain the cross-country variation in the pace of reserve accumulation.====The model describes a small open economy with tradable and non-tradable sectors. To capture the benefits of reserve accumulation, two features are introduced in the model: endogenous growth with FDI entry, and sudden stops in capital inflows. Endogenous growth is modeled based on the Schumpeterian growth model wherein productivity of intermediate firms in the tradable sector grows endogenously through new firm entry which replaces existing firms. FDI is introduced to capture the idea that reserve accumulation promotes growth in part by attracting FDI from developed countries as well.==== Sudden stops are modeled as an occasionally binding borrowing constraint on private foreign debt and working capital financing.====The government reserve policy comprises two types of interventions. First, in normal times when the borrowing constraint is not binding, the government collects taxes on tradable goods to accumulate reserves. As this reserve accumulation makes tradable goods relatively scarce compared with non-tradable goods, the relative price of non-tradable goods decreases, which is real exchange rate depreciation. Real depreciation increases the relative profitability of the tradable sector, thereby inducing a labor shift to the tradable sector and generating higher profits for intermediate firms.====Second, when the borrowing constraint becomes binding, the government provides accumulated reserves to bail out private agents. Government bailouts relax the binding borrowing constraint and prevent severe economic downturns, thereby preventing profits of intermediate firms from declining. Combined with reserve accumulation in normal times, reserve policy achieves high and stable profits for intermediate firms. Anticipating high and stable future profits, domestic agents invest more to create new firms, and foreign investors invest more to enter this country through FDI, both of which promote productivity growth. This is the key mechanism through which reserve policy attracts FDI and promotes economic growth.====Fig. 2 presents empirical facts that are consistent with this mechanism. The left panel shows that countries with a faster pace of reserve accumulation tend to have faster growth in GDP per capita. The right panel indicates a significant positive correlation between the pace of reserve accumulation and the FDI inflow-to-GDP ratio.==== The model therefore provides a theoretical framework to explain these positive correlations across developing countries observed in the data.====Fast and stable growth by reserve policy comes at a cost. As the government collects tax revenue to accumulate reserves, private agents borrow more from other countries to compensate for the loss of resources. In the model, the interest rate on foreign borrowing is debt-elastic, and the higher debt-to-GDP ratio leads to a higher interest rate spread.==== This higher interest rate spread prevents private agents from fully offsetting reserve accumulation by increasing foreign borrowing, thus lowering consumption in the short run. The optimal reserve policy therefore balances the costs of short-run austerity with the benefits of higher long-run consumption.====In the quantitative analysis, the model is calibrated to the average of the 20 developing countries in the right panel of Fig. 1. The first important result is that the optimal pace of reserve accumulation and its welfare impact crucially rely on two characteristics of each country: the debt elasticity of the foreign borrowing spread and the FDI entry cost. In countries with a high debt elasticity of the spread, even a small increase in private debt increases the spread substantially, which prevents private agents from increasing foreign debt, thus increasing the costs of short-run lower consumption. In countries with a higher FDI entry cost, reserve accumulation is not so effective in attracting FDI and therefore the growth-promoting effect is limited. In this case, the optimal pace of reserve accumulation is slower, and the welfare gain is limited.====Based on these results, this paper considers these two factors for evaluating the pace of reserve accumulation for each of the 20 countries. The debt elasticity of the foreign borrowing spread and the FDI entry cost are estimated for each country based on the data.==== Then the optimal pace of reserve accumulation is derived for each country and compared with the actual pace in the data. The second important result is that many of the sample countries are roughly in accordance with the optimal pace in the model, suggesting that these two factors can explain a substantial amount of the observed cross-country variation in the pace of reserve accumulation. The average optimal pace in the model across the 20 countries is 2.05% of GDP per year, whereas it is 1.72% in the data. The correlation coefficient between the optimal pace in the model and the actual pace in the data is 0.52. If the FDI entry cost is set to target each country's FDI inflow-to-GDP ratio in the data, then the average optimal pace is 1.82% and the correlation coefficient is 0.64. The welfare analysis also shows that if sudden stops never occur in the model, the optimal pace of reserve accumulation would be substantially lower than the observed pace in the data. This result suggests that mitigating crises constitutes an important part of the welfare effect by reserve policy.====The remainder of the paper is organized as follows. Section 2 reviews the related literature. Section 3 introduces the model. Section 4 discusses the mechanisms of how reserve policy works in the model. Section 5 presents the calibration of the model and the quantitative analysis. Section 6 investigates the key determinants of the optimal pace of reserve accumulation. Section 7 evaluates the actual pace of reserve accumulation by developing countries. Section 8 concludes the paper.","Foreign reserve accumulation, foreign direct investment, and economic growth",https://www.sciencedirect.com/science/article/pii/S1094202521000065,6 February 2021,2021,Research Article,110.0
Wu Wenbin,"Fanhai International School of Finance, Fudan University, Handan Rd 220, Shanghai, China","Received 13 September 2018, Revised 25 October 2020, Available online 28 January 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2021.01.002,Cited by (0),"Despite their prevalence in the microdata, sales (i.e., temporary price cuts) are often ignored by macroeconomists. If sales are taken into account, price rigidity is small in the data. Using the microdata underlying the ==== (CPI), I first demonstrate that sales of ==== have a substantial impact on the aggregate price index, and that the price index decreases gradually after these sales. To quantify the changes in the real effects of ","The real effects of monetary policy on economic activities are large in the short run (Christiano et al., 1999; Romer and Romer, 2004). The most common explanation in macroeconomics is price rigidity. As suggested by the standard New Keynesian model, price rigidity is the key feature that links the nominal side to the real side of the economy (Gali, 2008). Specifically, when prices adjust slowly, firms and retailers are unable to adjust prices fast enough to offset monetary policy shocks, thus generating large short-run real effects of monetary policy.==== However, prices are much more flexible than previously thought in microdata,==== where more than two-thirds of these price changes are due to temporary sales (Nakamura and Steinsson, 2008).====In this paper, I show that the real effects of monetary policy will be significantly overestimated if sales of durable goods are not taken into account. Compared to my benchmark model (two-sector menu-cost model with sales), the model without sales predicts that the real effect of monetary policy is 114% higher. I also find that the Calvo model calibrated to the frequency of regular price changes generates even larger monetary non-neutrality. It is not hard to imagine why sales of durable goods differ from sales of nondurable goods. For example, buying a dinner table on sale provides consumers more utility over a longer period of time compared to buying a soda on sale. Results in this paper differ from research in the macro literature. Anderson et al. (2016) use scanner data to show that although sales account for 95% of price changes in their data, sales do not represent a valid dimension of price adjustments, because they do not respond to wholesale cost changes. Klenow and Malin (2010) find that sales have little macro content, because they account for very little variance of the aggregate inflation. Using the microdata underlying the UK CPI, I show that their results hold for the nondurable sector. In the durable sector, however, the aggregate price index decreases gradually after sales, and more than one-third of the variance of the inflation is due to sales.====To quantify the changes in the real effects of monetary policy due to sales, I propose a two-sector menu-cost model based on Golosov and Lucas (2007) and Midrigan (2011). The model is a combination of the standard menu-cost model (see Golosov and Lucas, 2007; Gertler and Leahy, 2008, and Midrigan, 2011) and Barsky et al. (2007)'s two-sector model with durables and nondurables. To match the frequency and size of price changes, as well as the pattern of sales in the data, firms face two types of idiosyncratic shocks: permanent productivity shock and temporary cost shock. Permanent productivity shock is necessary in order to match the frequency and size of regular price changes in the data. Temporary cost shock captures all temporary changes in the cost, leading to temporary sales.==== The model is then calibrated to match salient moments in the data, including the frequency and size of regular price changes, the fraction of episodes of sales, and the size of sales.====How can the presence of temporary sales of durable goods reduce the real effects of monetary policy? Following a tightening monetary policy shock, households in the economy without temporary sales would expect prices to slowly decrease. Thus, compared to the steady state, they would purchase much fewer durables over a number of periods due to high elasticity of intertemporal substitution (EIS) of durables.==== Households in the economy with a high frequency of sales would be less affected, because the aggregate price drops quickly when retailers increase the size of sales. Sales of durable goods are more important, because their temporary presence has long-lasting effects due to the high EIS of durables.==== So, households in an economy with temporary sales would not respond as much.====My results are related to research arguing that temporary sales are not important for monetary analysis. For example, Kehoe and Midrigan (2015) develop a model to match the pattern of price changes in the BLS data. They find that temporary price changes have a small influence on the real effects of monetary policy. Guimaraes and Sheedy (2011) construct a macroeconomic model in which sales are the endogenous result of heterogeneous customers. They argue that sales do not matter for macro analysis, because sales are not only transitory but also staggered. Eichenbaum et al. (2011) argue that price rigidity takes the form of “reference price”. They construct a menu-cost model to match the pricing pattern in the data, which generates a similar monetary non-neutrality to the standard menu-cost model calibrated to the reference price changes. Anderson et al. (2016) use retailer scanner data to show that sales do not react to cost shocks and thus have a limited effect on macroeconomic variables. However, none of these studies consider sales of durable goods separately.====This paper is most related to Alvarez and Lippi (2016), Kryvtsov and Vincent (2014), and Hernaiz (2013). Alvarez and Lippi (2016) generalize Eichenbaum et al. (2011)'s model to a general equilibrium model, and find that temporary sales substantially reduce the real effects of monetary policy shocks. Kryvtsov and Vincent (2014) argue that sales significantly increase aggregate price flexibility, thus generating a much smaller effect of monetary policy. Hernaiz (2013) introduces price promotions in a NK model where consumers differ in their price sensitivity and in their search for promotions. He finds that sales significantly reduce the real effects of monetary policy. The focus of this paper is sales of durable goods, which differs from all these papers.====This paper proceeds as follows. In Section 2, I describe the data used in this paper, and demonstrate some empirical results about sales of durable goods. In Section 3, I develop a two-sector menu-cost model featuring sales and durable goods, and present an analysis of the model. In Section 4, I make conclusions.",Sales of durable goods and the real effects of monetary policy,https://www.sciencedirect.com/science/article/pii/S1094202521000028,28 January 2021,2021,Research Article,111.0
Smagghue Gabriel,"Banque de France, 31, rue de la croix des petits champs, 75001, Paris, France","Received 23 January 2019, Revised 7 December 2020, Available online 29 December 2020, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2020.12.001,Cited by (0),"I develop an extension of the neoclassical growth model in which firms are heterogeneous both in terms of labor share and productivity. In this model, distortions in the allocation of resources across firms can impact the labor share of national income. Using administrative firm-level data to calibrate the model, I show in particular that a removal of policies reducing the output price of more productive firms can generate a sizable decrease in the aggregate labor share (between 1 and 4 percentage points). My results suggest that the recent decline in the global labor share of income is both qualitatively and quantitatively consistent with an improvement in resource allocation across firms.","Government policies that impose unequal constraints across firms are widespread. Be it among policymakers or academics, these policies are suspected of having large damaging macroeconomic effects, particularly in terms of aggregate productivity and output. In this paper, I investigate the impact of heterogeneous policy distortions on a another major macroeconomic outcome: the labor share of income.====The concept of heterogeneous policy distortions includes a wide range of policies, from trade policy to fiscal policy or labor market regulation (see Restuccia and Rogerson (2017) for a review). The link between this type of distortions and the labor share rests upon a simple fact: bigger firms tend to have a smaller labor share (see Raval 2011 for evidence from the US, Gouin-Bonenfant 2018 for Canada as well as Fig. 1 for France). Consequently, any policy limiting the weight of bigger firms in the economy potentially causes a redistribution of national income towards labor. The main contribution of this paper is to show that this composition effect can be substantial, which contributes to our general understanding of the determinants of the labor share.====Many of the determinants suggested in the literature explain aggregate changes in the labor share through changes happening within firms.==== However, there is growing evidence that the bulk of changes in the labor share happens between firms rather than within firms. More specifically, it appears that most of the well documented decline in the global labor share results from a redistribution of market shares toward large, low-labor share firms (Autor et al. 2017; Kehrig and Vincent 2017). The literature has not yet reached a consensus as to what mechanism has caused this redistribution. Autor et al. (2017) hint at the role of technology and globalisation. Lashkari et al. (2018) document the role of the fall in the price of Information Technology capital goods. Gouin-Bonenfant (2018) points to the effect of productivity dispersion. In this paper, I provide simulation-based evidence that heterogeneous policy distortions, by redistributing resources across firms, can cause substantial changes in the aggregate labor share.====More specifically, my results show that considering labor share differences across firms to quantify the impact of heterogeneous distortions is primordial in the case of output price distortions, but not so much in the case of factor price distortions. Intuitively, this is because in the case of factor price distortion, most of the factor income redistribution happens within firms, as they adjust their capital-labor mix. There is no need to model labor share differences across firms to quantify this effect. By contrast, in the case of output distortion, factor income redistribution happens between firms, as market shares get redistributed across high and low labor share firms. To quantify this between-firm factor substitution, it is vital to have a model with heterogeneous labor shares.====The mechanism presented in this paper is important for understanding what movements in the labor share can tell us about economic efficiency. There is evidence that the global decline in the labor share observed over the last decades results from an increase in average markups (De Loecker and Eeckhout, 2017, De Loecker and Eeckhout, 2018), and thus potentially a loss of economic efficiency. On the contrary, my results suggest that this decline is consistent, both qualitatively and quantitatively, with a better allocation of resources towards large firms over time. In this sense, the mechanism I propose to rationalize the decline in the labor share is reminiscent of the improvement of allocative efficiency documented by Baqaee and Farhi (2020) for the US over the period 1997-2015.====In order to assess the quantitative importance of heterogeneous policy distortions for the labor share, I propose an extension of the neoclassical growth model with two-way heterogeneity across firms. In the steady-state, firm-level production functions vary both in terms of total factor productivity (TFP) and of factor elasticity. My model is therefore a generalization of Restuccia and Rogerson (2008) to the case of non-Hicks neutral technological differences across firms. I calibrate the model using French production data for the period 1995-2007. In particular, I set the level of correlation between the two idiosyncratic technological parameters so as to match the observed correlation between size and the labor share across firms. Then, I study the impact of policies which create dispersion across firms in terms of output price or factor price. By distorting the allocation of productive resources, such policies can impact the weight of capital-intensive firms in the economy. This results in a distortion of the aggregate relative demand for capital and labor which, ultimately, impacts the labor share of national income.====I model policy distortions as heterogeneous tax (or subsidy) rates across firms. I consider three types of tax: on output, on capital and on labor. Of the three, the tax on output is probably the best to showcase the role of labor share heterogeneity across firms. Indeed, an output tax does not impact the labor share of taxed firms because it does not distort their relative factor costs. Therefore, the only way a productivity-based output tax will impact the aggregate labor share is by reallocating market shares across firms with different labor shares. By contrast, heterogeneous tax rates on capital or labor will impact the aggregate labor share through a combination of within-firm factor substitution (taxed firm changes their mix of capital and labor) and between-firm factor substitution (market shares get reallocated toward high labor share firms). My simulations will allow me to get at the relative magnitude of these two effects.====I consider two complementary strategies to calibrate the policy distortions. First, I study a class of hypothetical distortions which consist in firm-specific implicit tax/subsidy. These generic policy distortions are simple and flexible. By considering different distributions of tax and subsidy rates across firms, I am able to trace out the general relationship between productivity-based distortions and the labor share. Second, I study the case of a real-world size-dependent regulation: the 50 employees threshold in France. As French firms exceed this threshold, they must provide their workers with multiple additional rights.====My results suggest that the effects of heterogeneous output distortions can be sizable. Taxing the 50% most productive firms results in an increase of the national labor share ranging from 1 percentage point (hereafter “p.p.”) to 2 p.p., for tax rates from 10% to 50%. These effects double when I take into account selection, that is the fact that taxing productive firms allows some unproductive firms with high labor share to survive in the economy. To put these numbers in perspective, the labor share in France was 5 p.p. larger than in the US in 2005. 5 p.p. also corresponds to the global drop in the labor share over the past 30 years, as estimated by Karabarbounis and Neiman (2014). My simulations suggest that the removal of productivity-based output distortions could explain from 20% to 75% of this evolution.====When it comes to capital price distortion, I find a large labor share response, from 1.6 to 5.8 p.p. Nevertheless, most of this effect is due to taxed firms substituting labor for capital (the within-firm substitution effect). By contrast, the reallocation of market shares from low to high labor share firms (the between-firm substitution effect) has a marginal contribution. This suggests that it is not crucial to take into account labor share differences across firms to quantify the labor share impact of heterogeneous capital distortions.====The same message emerges from simulating the removal of the 50 employees threshold. I find that this removal increases the labor share by 0.4 p.p. This means that the within-firm effect (previously regulated firms substitute labor for capital) dominates the between-firm effect (productive/low labor share firms expand in the economy).====Although this paper mainly adds to the labor share literature, it also contributes to other lines of research. First, it relates to the literature on the macroeconomic effects of heterogeneous policy distortions across firms, initiated by Restuccia and Rogerson (2008) and Hsieh and Klenow (2009).==== Many papers study the special case of size-dependent distortions (Guner et al. 2008; Braguinsky et al. 2011; Garcia-Santana and Pijoan-Mas 2014) both because they are widespread and suspected to be particularly distortionary. The general message of this literature is that resource misallocation caused by heterogeneous distortions can explain a large share of productivity differences across countries. By contrast, this paper is the first to propose a general study of the link between heterogeneous policy distortions and factor income distribution.====My paper also adds to the literature which documents the dispersion in capital intensity across firms and studies its implications. Raval (2011) establishes the positive relationship between firm size and capital intensity in the US. Using a sufficient statistic approach, Oberfield and Raval (2014) show that the aggregate elasticity of substitution is increasing in the dispersion of capital shares across firms. By contrast, I show that the joint distribution of firm size and capital intensity shapes the distributional effects of productivity-based and size-based government policies.====The rest of the paper is organised as follows. Section 2 describes the data and the motivating evidence. Section 2 sets out the general equilibrium model. In section 3, I present my calibration strategy and the results of my quantitative analysis of heterogeneous distortions. In Section 5 I analyse the sensitivity of my results to alternative modeling/calibration choices. Section 6 concludes.",Heterogeneous policy distortions and the labor share,https://www.sciencedirect.com/science/article/pii/S1094202520301125,29 December 2020,2020,Research Article,112.0
Szkup Michal,"University of British Columbia, Canada","Received 25 March 2019, Revised 9 December 2020, Available online 24 December 2020, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2020.12.002,Cited by (0),"This paper develops a micro-founded global games model of debt crises. I use this model to study which policies can help to prevent expectations-driven crises and how the desirability of such policies depends on market participants' expectations and the presence of ====. I show that endogenous expectations amplify the effects of government policies, so that even a small policy adjustment can have significant effects. I find that policy uncertainty may increase the range of situations in which government policies can help prevent a crisis but decrease their overall impact. Finally, I apply these insights to study two policies that are often at the center of political discussions: ==== (an increase in taxes) and government stimulus. I show that under plausible conditions an increase in ","One of the leading views on the recent European debt crisis, as exemplified by the above quotes, is that this crisis was driven by an interplay between poor economic fundamentals and self-fulfilling expectations.==== According to that view, the high borrowing costs experienced by countries such as Ireland, Italy, Portugal, and Spain during the European debt crisis had less to do with their weak economic fundamentals and more with creditors' self-fulfilling expectations about the faith of these countries. Indeed, Fig. 1 shows that during the period 2010–2013 Ireland, Italy, Portugal, and Spain (“crisis countries”) experienced a large increase in their borrowing costs, while Finland, France, and the UK (non-crisis countries) did not (the left panel), despite the fact that both sets of countries experienced a rapid build-up of debt during that period (the right panel). These observations motivated researchers to look beyond economic fundamentals to explain the drivers of the European debt crisis (see, for example, Bocola and Dovis (2016), De Grauwe and Ji (2013), or Saka et al. (2015)).====The European debt crisis was accompanied by debate regarding the best way to stop the crisis from unfolding further. This discussion focused on two policies: austerity and government stimulus (see Brunnermeier et al. (2016), Corsetti (2012), and Reinhart and Rogoff (2010)). The supporters of austerity pointed to increasing debt as the main economic fundamental which triggered the crisis; the supporters of stimulus emphasized that the crisis was accompanied by poor economic growth. While there exists a large and insightful literature that investigates fiscal policies in the context of self-fulfilling debt crises using models with multiple equilibria, that literature relies on sunspots as the equilibrium selection mechanism. Thus in these models government policies do not affect the likelihood of a crisis once a self-fulfilling crisis becomes possible.====In addition, the existing literature on self-fulfilling debt crises has so far abstracted from the role that policy uncertainty has on the effectiveness of fiscal policies in decreasing the likelihood of default. However, debt crises are often accompanied by substantial political uncertainty (see, for example, Hatchondo and Martinez (2010) or Panizza et al. (2009)). Indeed, the increase in borrowing costs faced by the periphery countries during the European debt crisis was accompanied by an increase in economic policy uncertainty. Fig. 2 depicts the index of economic policy uncertainty (the EPU index) for Ireland, Italy, and Spain for the period 2004–2017 as constructed by Baker et al. (2016) and shows that this index reached an all-time high in 2011, at the peak of the European debt crisis.==== It is an open question how the interaction of policy uncertainty with market participants' expectations alters the desirability of government policies aimed at preventing a self-fulfilling debt crisis.====Motivated by the above discussion, in this paper I ask (1) whether a government can implement policies that help to avert a crisis driven by ==== self-fulfilling expectations, and (2) how the desirability of such policies depends on market participants' expectations and on the presence of economic policy uncertainty. To answer these questions, I develop a stylized model that features self-fulfilling expectations, but where expectations are determined endogenously by fundamentals and government policies. The endogeneity of expectations differentiates this model from standard frameworks in the spirit of Calvo (1988) and Cole and Kehoe (2000), where expectations are determined by exogenous sunspots. I then use this model to analyze how endogenous expectations and policy uncertainty alter the effectiveness of government policies, with a special focus on an increase in taxes and a government stimulus.====This paper makes three contributions to the literature on self-fulfilling debt crises. First, it develops a novel framework that combines elements of micro-founded sovereign default frameworks, such as in Cole and Kehoe (2000), which feature multiple equilibria and rely on sunspots as an equilibrium selection mechanism, and global games models in the spirit of Morris and Shin (1998) (such as in Corsetti et al. (2006) or Morris and Shin (2006)), in which the equilibrium in unique. The resulting framework is rich in micro-foundations and, as a result, is better suited for policy analysis than earlier reduced-form global games models of debt crises. Furthermore, in contrast to standard sovereign default models of self-fulfilling crises, it links beliefs to fundamentals and government policies. Thus the current framework combines the strengths of the two approaches mentioned above.====Second, I use the model to analyze the effectiveness of an increase in taxes (“austerity”), and of a fiscal stimulus financed with debt, in preventing a debt crisis driven by endogenous self-fulfilling beliefs and policy uncertainty. Thus this paper contributes to the policy debate surrounding the European debt crisis by investigating how effective these policies are in decreasing the likelihood of a self-fulfilling debt crisis.====Finally, this paper provides a general characterization of the role played by endogenous self-fulfilling beliefs and policy uncertainty in determining effectiveness of government policies. This characterization applies to any government policy, not only those considered in the paper.====The paper consists of three parts. In the first part, I develop a simple model of self-fulfilling debt crises in which crises arise as a result of an interplay between poor fundamentals, foreign lenders' expectations, and domestic households' expectations. To model dispersed beliefs and to endogenize expectations about sovereign default, I assume that lenders and households do not observe the relevant fundamentals of the economy but instead only receive noisy private signals about it. This realistic assumption not only captures the uncertainty surrounding the state of the economy during crisis episodes but also transforms lenders' and households' expectations into endogenous equilibrium objects and restores the uniqueness of equilibrium.==== The resulting environment is rich enough to capture the main trade-offs faced by governments during debt crises. However, in contrast to standard models of self-fulfilling sovereign debt crises, it also links beliefs and expectations to economic fundamentals.====In the second part of the paper, I use the model to analyze which policies available to the government can decrease the ex-ante likelihood of a debt crisis (and possibly prevent a debt crisis). I show first that a change in the probability of default implied by ==== policy adjustment can be decomposed into the product of the “direct effect” (the initial effect of the policy change on the government's incentive to default, holding households' and lenders' beliefs constant) and the “beliefs effect” (the change in the government's default decision implied by the adjustment in households' and lenders' expectations). I show that the direct effect, which captures the fundamental economic forces of the model, determines whether a given policy decreases or increases the likelihood of a crisis. On the other hand, the beliefs effect, which captures the role played by expectations, acts as an amplification mechanism that always magnifies the initial response of the economy. These novel results indicate that if the government wants to avoid default, it can use expectations to its own advantage, as even a small policy change, when amplified by adjustments in expectations, can significantly decrease the likelihood of default.====I use the above observations to analyze the impact of an adjustment in the tax rate and the impact of a government stimulus on the probability of default. I show that as long as the initial level of taxes is not “very high,” an increase in the tax rate decreases the government's incentive to default by closing the government's financing gap when lenders are unwilling to provide the funding. On the other hand, I find that a stimulus (which I model as an increase in government investment financed with debt) tends to increase the likelihood of default. This is because the negative effect associated with an increase in government debt outweighs the positive effect associated with an increase in output. Thus my model suggests that austerity is typically a better policy for preventing a debt crisis.====In the third and final part of the paper, I introduce policy uncertainty into the model and analyze how its presence affects the above results. I find that the presence of such uncertainty tends to decrease the negative effect of austerity: Uncertain as to whether higher taxes will be implemented, households do not decrease their investment as much as they would otherwise. On the other hand, economic policy uncertainty decreases the benefits of a government stimulus: Unsure of whether a stimulus will be implemented, households do not expand their investment as much as they would otherwise. Thus the presence of economic policy uncertainty further strengthens the case for austerity relative to stimulus. However, I also find that economic policy uncertainty decreases the overall effect of each of those policies on the probability of default. In the extreme case, when a policy change is unexpected and agents' information is very precise, the beliefs effect is completely missing and government policies have no impact on the probability of default.====Finally, I conclude the paper by discussing the quantitative importance of beliefs effects, welfare consequences of the government's policies, and differences in predictions between my model with dispersed information and endogenous expectations and its version in which crises are driven purely by fundamentals and agents have common beliefs.==== The framework developed in the paper unifies two popular approaches to modeling self-fulfilling debt crises: the micro-founded general equilibrium approach of Cole and Kehoe (2000) and the game-theoretic approach of global games as in Corsetti et al. (2006) and Morris and Shin (2006). Uniqueness of equilibrium follows from the global games literature started by Carlsson and Damme (1993) and popularized by Morris and Shin, 1998, Morris and Shin, 2003.====Corsetti et al. (2006) and Morris and Shin (2006) were the first to use global games models to study sovereign debt crises. These papers focused on analyzing the effectiveness of IMF assistance in preventing a self-fulfilling debt crisis and the moral hazard that such assistance creates. In a parallel work, Zabai (2014) used global games to analyze how the government can use tax and borrowing policies to manage the probability of default in a model in the spirit of Calvo (1988). In contrast to the above work, the focus of this paper is on understanding the impact that endogenous expectations and policy uncertainty have on the effectiveness of fiscal policies.====Models of self-fulfilling crises have a long tradition in the literature on sovereign default, beginning with Sachs (1984) and Calvo (1988). Recent contributions include Corsetti and Dedola (2016) and Aguiar et al. (2013), who investigate how monetary policy can help to avoid a crisis. Lorenzoni and Werning (2018) focus on the role of the interest rate as the main driver of sovereign default. See also Bianchi and Mondragon (2018), Bocola and Dovis (2016), Conesa and Kehoe (2017), and Cooper (2013).====This paper is also related to the literature on sovereign debt in the spirit of Eaton and Gersovitz (1981). This line of research has focused on developing quantitative models of sovereign default that can account for the observed dynamics surrounding default episodes (see Aguiar and Gopinath (2006), Arellano (2008), Hatchondo and Martinez (2009), or Mendoza and Yue (2012), and references therein). Cuadra and Sapriza (2008) and Chatterjee and Eyigungor (2019) study the role of political uncertainty quantitatively. Arellano et al. (2017), Bianchi et al. (2017), De Ferra (2016), and Kaas et al. (2016) consider the effect of government fiscal policies within these frameworks. This strand of literature assumes away the possibility of belief-driven crises.",Preventing self-fulfilling debt crises: A global games approach,https://www.sciencedirect.com/science/article/pii/S1094202520301137,24 December 2020,2020,Research Article,113.0
"Berger David,Herkenhoff Kyle,Huang Chengdai,Mongey Simon","Duke University, United States of America,Federal Reserve Bank of Minneapolis, United States of America,University of Minnesota, United States of America,Kenneth C. Griffin Department of Economics, University of Chicago, United States of America","Received 29 April 2020, Revised 28 October 2020, Available online 27 November 2020, Version of Record 3 January 2022.",https://doi.org/10.1016/j.red.2020.11.003,Cited by (20),"We quantify how testing and targeted quarantine make it possible to reopen an economy in such a way that output increases while deaths are reduced. We augment a standard Susceptible-Exposed-Infectious-Recovered (SEIR) model with (i) virological testing, (ii) serological testing, (iii) permanently asymptomatic individuals, (iv) incomplete information, and (v) a reduced form behavioral response of reopening to changes in health risks. Virological testing allows for targeted quarantine of asymptotic spreaders. Serological testing allows for targeted release of recovered individuals. We fit our model to U.S. data. Virological tests every two weeks accommodate more aggressive reopening that more than halves output losses while keeping deaths below forecasts under the status quo. Serological tests are much less effective. Implementing testing against a fixed budget, low sensitivity tests that are cheap and used frequently, dominate perfect tests that are expensive and used less frequently.","During a pandemic, testing can accommodate targeted lockdown policies and in doing so allow for broad reopening policies that support economic output. We show this and quantify public health and economic outcomes by augmenting a standard epidemiological model with (i) virological testing, (ii) serological testing, (iii) temporarily and permanently asymptomatic individuals, and (iv) behavior that responds to public health outcomes. These are important for understanding the response to COVID-19 given three facts.==== First, there is a high rate of permanently asymptomatic infection. Second, individuals that develop symptoms are highly infectious in their pre-symptomatic stages. Third, depression of disease transmission rates across U.S. states absent large scale testing regimes has been well documented (Atkeson et al., 2020).====The two types of asymptomatic imply a role for both virological and serological tests. Once infected, both permanent and temporary asymptomatic types are unaware they are infectious. Virological testing of those without symptoms identifies these types and makes targeted quarantine possible. Mortality and disease transmission fall, creating room to reduce quarantine measures, which we refer to as ====. This mitigates the decline in output. Recovered types that never developed symptoms are unaware of their recovery. Serological testing of those without symptoms identifies these types and makes possible targeted release. Output increases, and disease transmission falls as recovered types crowd out interactions between susceptible and infected types, leading to fewer deaths. Which test provides better health and economic outcomes is an important quantitative question that we answer.====To quantify these mechanisms, we include the minimal necessary modifications to a standard epidemiological model. Fig. 1 provides a model schematic. We start with the SEIR model which has four states: Susceptible (S), Exposed (E), Infected (I) and Recovered (R). First, we split the exposed state into two groups: those who are temporarily asymptomatic and those are permanently asymptomatic, where only temporary asymptomatic cases develop symptoms before recovery or death. Second, we assume both types spread the disease.==== Third, we introduce incomplete information. Asymptomatic individuals do not know their own health state, nor does the planner. This creates a role for virological tests. Fourth, our two types of infection generate two types of recovered individuals: known recovered and unknown recovered. Incomplete information implies that unknown recovered and susceptible types cannot be distinguished. This creates a role for serological tests. Fifth, we assume that individuals exit quarantine in a way that depends on both (i) the level of deaths in the economy and (ii) government policy (e.g. reopening parks, gyms, etc.). We refer to the component of the quarantine exit rate determined by the government as ‘reopening’ policy. The endogenous component of the quarantine exit rate allows us to match time-series data on deaths and reproduction numbers and is consistent with theory (Farboodi et al., 2020).====We calibrate the model in order to construct counterfactual time-series for health outcomes and economic output. We assume that economic output is proportional to the number of healthy non-quarantined individuals. We adopt as many agreed-upon medical parameters from the literature as possible and calibrate the remaining model parameters. Since many parameters are unknown we conduct extensive robustness exercises. In our calibration we take as given the shutdown of the US economy over March and April 2020, and choose the rate of disease transmission and effectiveness of the shutdown to match the time-series of deaths between March and June. We then allow individuals to respond to changes in deaths by emerging out of quarantine so long as deaths stay depressed. This death-dependent exit rate from quarantine is calibrated to match mortality in July and August.====Our main counterfactual answers the question: ====. Testing saves lives as it comes coupled with targeted quarantine (release) measures in the case of virological (serological testing). In our calibrated benchmark with only the endogenous component of reopening, we forecast total deaths under the pandemic: ====. In counterfactual economies with testing at frequencies that range from one to 12 weeks, we then compute the permissible additional reopening such that deaths do not exceed ====. We then compare the benchmark and counterfactual economies in terms of the profile of infections and output losses.====Our main result is that by combining increased testing with targeted quarantine, it is possible to reopen the economy and produce significantly more while not exceeding the cumulative deaths the US is currently on track to incur. First, if virological tests are administered to individuals on average every two weeks, then 53 percent of the output lost under the no testing case can be saved. Even with monthly testing 20 percent of lost output is recovered. Second, we find that serological testing has quantitatively minor effects relative to virological tests, an important result for the selection of testing regimes. Simply put, with fewer than 16 percent of people ever becoming infected in the benchmark, there is little scope to improve output losses by identifying these individuals. We provide a range of robustness analysis. In particular we make the best case for serological testing by making the identification of antibodies necessary before release from quarantine, irrespective of past symptoms or virological tests. Even in this setting, serological tests have small effects.====Our third main result is to quantify the trade-off between test quality and frequency that a government may face when implementing a virological testing regime. Non-linearities in the model imply that frequency may be more important than quality. We consider two tests: (i) an at-home test that costs $10 with a 50 percent false negative rate, and (ii) an accurate lab test (Polymerase Chain Reaction or ‘PCR’ tests) that costs $40 with a zero false negative rate. For a fixed testing budget, at home testing can be done more frequently. Despite significantly lower accuracy, frequent at-home tests dominate the more accurate, expensive and infrequent PCR tests. Relative to PCR tests under a budget of $40 per person per month, at-home tests save over 130,000 lives. As a consequence of saving more lives, people exit quarantine at a higher rate and output declines by around 9 percent less.====We conclude with a discussion of how policies in the model differ from the US experience. We feature random testing of asymptomatic individuals at high frequencies. The US experience has been to mainly test symptomatic individuals. We provide data on testing rates and programs at private institutions—e.g. college campuses—which are entirely consistent with our model. We discuss potential reasons the US policy deviates so much from that of private institutions.====Our exercises qualitatively and quantitatively contribute an important point for economists integrating epidemiological and macroeconomic models. Analysis of an economic model with an epidemiological block but without public health interventions such as testing and targeted quarantine unavoidably feature a trade-off between mortality and economic activity: faster reopening increases output but increases mortality. Kaplan et al. (2020) label the set of such outcomes a ====. Combining widely advocated public health measures of testing and targeted quarantine, our model delivers lower mortality and higher economic activity, significantly shifting this frontier outwards. Theories of economic-activity vs. mortality trade-offs with only a lock-down policy available to the policy-maker are hence necessarily discussions of second best policies.====This paper has eight sections. Section 1 reviews the related SEIR literature and recent papers using this model to quantify the effects of the coronavirus pandemic. Section 2 reviews data on infection, mortality, testing and quarantine measures. Section 3 describes the model. Section 4 details calibration and model fit. Section 5 provides our main counterfactuals which compute output savings delivered from a combination of elevated testing and targeted reopening. Section 6 considers the quality-frequency trade-off. Section 7 shows how our results are robust to many assumptions regarding the virus. Section 8 compares the testing policy in the paper to that observed on college campuses in August, 2020. Section 9 concludes. An appendix contains details on additional robustness exercises.",Testing and reopening in an SEIR model,https://www.sciencedirect.com/science/article/pii/S109420252030106X,27 November 2020,2020,Research Article,115.0
"Ljungqvist Lars,Sargent Thomas J.","Stockholm School of Economics, Sweden,New York University, United States of America,Hoover Institution, United States of America","Received 16 April 2021, Available online 3 May 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2021.04.007,Cited by (8),The fundamental surplus isolates parameters that determine how sensitively unemployment responds to productivity shocks in the matching models of ==== (==== and this issue) under either Nash bargaining or alternating-offer bargaining. Those models thus join a collection of models in which diverse forces are intermediated through the fundamental surplus.,"To generate big responses of unemployment to productivity changes, matching models have been reconfigured in a variety of ways. Thus, Shimer (2005, p. 26) showed that to explain observed movements in unemployment and vacancies, “one must assume that the value of leisure is nearly equal to labor productivity,” a line of inquiry pursued with calibrations by Hagedorn and Manovskii (2008). Pissarides (2009, p. 1341) advocated departing from the standard assumption that matching costs are proportional to the duration of a vacancy: “a simple remodeling of the costs from proportional to partly fixed and partly proportional can increase the volatility of [labor market variables], virtually matching the observed magnitudes.” Wasmer and Weil (2004, p. 944) formulated a two-layer matching model in which firms must first match with bankers to obtain credit before matching with workers, and demonstrated “that credit frictions amplify [unemployment] volatility through a financial accelerator.” Hall and Milgrom (2008, p. 1673) abandoned Nash bargaining in favor of alternating-offer bargaining (AOB) that converts threat points from outside values into values from continued bargaining; that led them to conclude that “the limited influence of unemployment on the wage results in large fluctuations in unemployment under plausible movements in [productivity].” Given their diversity, one might be tempted to think that it would be fruitless to seek a single common channel through which the forces unleashed in all such reconfigurations must operate.====Remarkably, transcending these and many other matching models, there is a single intermediate channel – the fundamental surplus – through which economic forces affecting the responsiveness of unemployment to changes in productivity are funneled. The ==== is the difference between productivity ==== and a model-specific quantity ====, while ==== is called the fundamental surplus ====. The fundamental surplus determines one factor in a two-factor multiplicative decomposition of the elasticity of market tightness with respect to productivity.==== The decomposition comes from a comparative steady state analysis. The first factor satisfies a quantitatively small upper bound coming from a professional consensus about values of the exogenous elasticity of matching with respect to unemployment. That means that the second factor – the inverse of the fundamental surplus fraction – is the principle determinant of the elasticity of market tightness and hence, of how sensitively unemployment responds to productivity. A decomposition like this was first constructed by Shimer (2005) and Hagedorn and Manovskii (2008) in the context of a standard matching model with Nash bargaining and basic features. Ljungqvist and Sargent (2017) discovered and named the fundamental surplus by deriving such two-factor multiplicative decompositions for a wide class of matching models.====That the fundamental surplus shows up as an “uninvited guest” in a variety of models indicates its role in unveiling essential economic forces driving outcomes in an interesting ==== of models. It points like a laser at the heart of matching models: resources assigned to create vacancies. The fundamental surplus fraction is an upper bound on the fraction of a job's output that the ==== of market forces can allocate to vacancy creation. Thus, take a standard matching model with Nash bargaining. Here ==== is the value of leisure, as originally computed by Shimer (2005) and Hagedorn and Manovskii (2008). To motivate workers, the invisible hand has to award them at least the value of leisure; so that value cannot be allocated to vacancy creation. When Pissarides (2009) adds a fixed matching cost, it brings another deduction that must be made to arrive at the fundamental surplus. This deduction is based on an annuity payment that has the same expected present value as the fixed matching cost. Hence, in addition to the required compensation for lost leisure, the invisible hand's allocation of resources to vacancy creation is now constrained also to accommodate the fixed matching cost.==== Similar reasoning leads to the conclusion that an extra deduction to arrive at the fundamental surplus in the financial accelerator model of Wasmer and Weil (2004) entails the annuitized value of average credit search costs that firms incur before matching with workers. In the AOB model of Hall and Milgrom (2008), a novel finding is that a firm's cost of delay in bargaining suppresses the fundamental surplus. That might seem odd at first since no such cost is incurred in an equilibrium because the parties immediately reach an agreement. However, that agreement reflects how workers strategically exploit the firm's cost of delay under the alternating-offer bargaining protocol. Hence, the invisible hand's allocation of resources to vacancy creation is bounded both by the required compensation for lost leisure and by what workers attain under the AOB protocol as functions of a firm's cost of delay in bargaining. Derivations and interpretations of fundamental surpluses in more matching models appear in Ljungqvist and Sargent (2017) and an accompanying online appendix.====In all but one of the matching models analyzed by Ljungqvist and Sargent (2017), only the first factor in the multiplicative decomposition contains endogenous variables, i.e., the factor mentioned above that satisfies a quantitatively small upper bound. Hence, parameters that shape the second factor – the inverse of the fundamental surplus fraction – are the critical determinants of the elasticity of market tightness. The exception is the Hall-Milgrom AOB model. However, under the assumption that the exogenous job destruction probabilities under bargaining and production are the same, the Hall-Milgrom AOB model also features a second factor without endogenous variables. Imposing this parameter restriction, we produce what we call the ==== of the Hall-Milgrom AOB model. This version attains the ultimate Hall-Milgrom outcome not only of a limited but of a complete lack of “influence of unemployment on the wage” because a worker's outside value then vanishes. The approximating version of the Hall-Milgrom AOB model isolates the value of leisure and a firm's cost of delay in bargaining as the critical parameters for determining how sensitively unemployment responds to productivity. This is confirmed in stochastic simulations of the model across the unconstrained parameter space (i.e., without the simplifying parameter restriction). Actually, any parametric differences in job destruction probabilities during bargaining versus production act like nuisances when seeking to identify the truly critical parameters determining the elasticity of market tightness. Yes, a difference in job destruction probabilities does affect the elasticity but only if the critical parameters are such that the elasticity would be high in any case; but on its own, the parameterization of a difference in job destruction probabilities cannot yield a high elasticity without the support of the two critical parameters, i.e., the value of leisure and a firm's cost of delay in bargaining. In contrast, either the value of leisure or a firm's cost of delay in bargaining can be calibrated high enough to generate any market tightness elasticity regardless of the calibration of other parameters.====In this paper, we show that the matching models of Christiano et al. (2016 and this issue), henceforth CET, join the class of models for which the fundamental surplus is the key intermediating quantity. The analysis of CET's Nash bargaining model is almost identical to that of the Pissarides (2009) model, and CET's AOB model can be analyzed in the same way as can the Hall-Milgrom AOB model, only now with the addition of a fixed matching cost. It is remarkable how well the fundamental surplus isolates critical primitives when matching in the labor market is incorporated into CET's DSGE model with habit formation in preferences, adjustment costs in capital formation, capacity utilization costs, Calvo sticky price frictions, and a Taylor rule for monetary policy. Thus, for the AOB model, to attain what CET would view to be an empirically plausible model, it is necessary to assemble a combination of high values of the value of leisure, a firm's cost of delay in bargaining, and the fixed matching cost. If those parameters are set too low, the fundamental surplus fraction won't be small enough to make unemployment respond sensitively to productivity. Furthermore, to illustrate how different job separation probabilities during bargaining and production are just distractions in this context, we set them equal to each other and recalibrate a critical parameter of the fundamental surplus in order to generate the same fit to data as CET's estimated model. Thus, this alternative parameterization is an example of a configuration that we above called an ==== of the model. It generates virtually identical impulse-response functions with respect to 12 variables and three types of shocks as those computed by CET for the estimated model.====For readers who want to see a model in which the fundamental surplus is ==== central, Kehoe et al. (2019) present a matching model in which they correctly claim to “have abstracted from the standard mechanism of ==== [market production versus home production and vacancy creation].” A fruitful way to get under the hood of the Kehoe et al. model is to interpret their assumptions about parameters in terms of our decomposition of the elasticity of market tightness. First, they assume that key quantities are proportional to productivity, in particular, quantities that are deducted from productivity when calculating the fundamental surplus; this causes our second multiplicative factor comprising the fundamental surplus to vanish. And since the first multiplicative factor is bounded from above, it follows that the elasticity cannot be large (a point anticipated by Ljungqvist and Sargent (2017, p. 2664, footnote 28)). Second, assuming that a firm's cost of posting a vacancy is proportional to productivity makes the elasticity become zero. As a result of these assumptions, under standard preferences with constant relative risk aversion Kehoe et al. find that unemployment is unresponsive to productivity. To alter that outcome, they assume preferences that make risk aversion vary over a business cycle; their baseline model uses a version of Campbell-Cochrane (1999) preferences. Thus, while Kehoe et al. contribute to a literature on how assuming what David Backus mischievously called “exotic preferences” can generate fluctuations,==== their analysis is not within the general class of matching models with “standard preferences” maintained by CET.====A prelude in Section 2 accounts for how paths of the CET models and the fundamental surplus first crossed. Section 3 puts all the cards on the table and shows that the fundamental surplus works like a charm for CET. Sections 4 and 5 drill down into details of a proper fundamental surplus analysis of CET's AOB model. An example of how to disarm the fundamental surplus channel is presented in Section 6. Section 7 offers concluding remarks.",The fundamental surplus strikes again,https://www.sciencedirect.com/science/article/pii/S1094202521000363,3 May 2021,2021,Research Article,118.0
"Eggertsson Gauti B.,Egiev Sergey K.,Lin Alessandro,Platzer Josef,Riva Luca","Brown University, United States of America,NBER, United States of America","Received 28 January 2020, Revised 24 February 2021, Available online 22 April 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2021.04.001,Cited by (5)," and the ==== – that are more robust. Had these policies been in place in 2008, they would have reduced the output contraction by approximately 80 percent. If the Federal Reserve had followed average inflation targeting – which arguably approximates the policy framework announced in August 2020 – the output contraction would have been roughly 25 percent smaller.","The effective lower bound (ELB) on nominal interest rates has been widely studied in recent years. The standard way to analyse this problem is with dynamic stochastic general equilibrium (DSGE) models, in which the ELB is an inequality constraint on the nominal interest rate. However, inequality constraints complicate the application of standard solution strategies – for example perturbation methods. These methods approximate the behaviour of a dynamic nonlinear model around a point (usually via linearisation) using differentiability assumptions. Occasionally binding constraints pose a challenge for direct application of these methods.====In this paper, we present a toolkit aiming to facilitate the application of a generalised version of the solution method first used by Eggertsson and Woodford (2003), who analyse the ELB in the context of a two-state Markov process for the exogenous shocks with an absorbing state.==== We illustrate the algorithm in the canonical New Keynesian (NK) model and in the medium-scale DSGE model developed by the Federal Reserve Bank of New York (FRBNY). As an application, we consider various policy rules and study their performance relative to the optimal commitment equilibrium. Previously suggested policy rules – such as price level targeting and nominal GDP targeting – do not perform well when the price level does not fall by a large amount, as observed during the Great Recession, because they do not entail sufficiently strong commitment to a low future interest rate (that is, an adequate make-up strategy). This also applies to a policy rule we term ====, which arguably approximates the policy regime of the Federal Reserve as presented by Powell (2020). To address this shortcoming, we propose two new policy rules – what we call ==== and ==== – that are more robust. Had either of these policies been in place in 2008 and believed to be credible, our model simulation suggests the Federal Reserve would have reduced the output contraction (relative to trend) by about 80-90 percent. The comparable number for the average inflation targeting rule is 25 percent (Table 3).====Several strategies have been proposed to deal with inequality constraints in DSGE models. Eggertsson and Woodford (2003) exploit a particular structure for exogenous disturbances: the posited shock process implies that the economy unexpectedly moves to a crisis state and then reverts back to the steady state with a fixed probability. Once back to the steady state, it stays there forever (that is, the steady state is an absorbing state). The idea behind the approach is intuitive: instead of treating a single dynamical system that contains both a set of equality constraints and a set of occasionally binding inequality constraints, we split the system into several parts we call ====, each of which contains equality constraints exclusively. Once the system is cast in this form, we can apply perturbation methods since each equation is differentiable.====An application to the ELB scenario should make the point clear. We distinguish among four regimes, each of them corresponding to a different combination of the status of the inequality constraint (for example, ELB binding or not) and the exogenous Markov disturbance (crisis or steady state). For the regimes in which the ELB is not binding, we treat the model as if the ELB was not present. In the other two regimes, when the ELB constraint is binding, the equilibrium conditions are characterised by an ==== constraint (for example, ====). Since all four dynamical systems are described by a set of equations, each can be solved using perturbation techniques.====The assumptions about the shock structure allow us to solve the model recursively in regimes. Starting from the last regime, in which the ELB is not relevant, we work backwards to the period when the shock hits the system and we obtain a piecewise solution. Since outcomes in later regimes influence behaviour in earlier ones through expectations, the strategy is not based on a simple merger of separate models.====Our approach has two key advantages: first, its relative simplicity allows us to handle models with many state variables; second, unlike local-solution techniques, our strategy allows us to assume the basic stochastic structure, making it attractive for simple estimations.==== Note that it is not just the interest-rate constraint, but any model with a constraint that is temporarily binding can be solved using our toolkit.====As a new feature, the toolkit uses an algorithm that generalises the solution method in Eggertsson and Woodford (2003). In particular, it allows for the case of a regime in which the two-state Markov process is in the crisis state but the ELB is not binding. This feature is of particular importance when we analyse policy rules, as a common property of policy rules is that they imply an inertial response of the interest rate. An example is a Taylor-type rule with lagged terms for the nominal interest rates. Rules of this kind often do not imply an immediate reduction of the interest rate to the ELB once the two-state Markov disturbance switches to the crisis state. The new feature is thus a meaningful addition and facilitates the analysis of different types of policy rules in the presence of an ELB, which is the main application in this paper.====The idea of attacking the problem by constructing a piecewise solution is not new, nor is the idea of a toolkit for applying the solution. In fact, Guerrieri and Iacoviello (2015), henceforth ====, provide a toolkit for solving dynamic models with occasionally binding constraints in a similar fashion. The main difference is that we do not assume perfect foresight – that is a deterministic setting. This feature also differentiates our approach from several other strategies, such as the ==== (EP) algorithm. To achieve this, we rely on the specific shock structure implied by a two-state Markov process with an absorbing state. Expectations about the future path of variables are a crucial component of models related to the ELB (for example, uncertainty about whether the economy will hit the ELB and uncertain timing of lift-off), and hence allowing for uncertainty is a useful feature of the toolkit.====Adding a two-state Markov process with an absorbing state usually implies the following timing for the models analysed with the toolkit: initially, a shock hits the economy and the response of the central bank might be to immediately lower the interest rate to zero. In every period the disturbance reverts to its initial absorbing condition with some probability. There is often a transition period, lasting from the point when the shock reverts to its initial level until all other variables of the model return to their steady-state values. One benefit of our setup is that one can separately calibrate the expected time during which the constraint is binding from the actual realised time. Empirical evidence on the Great Recession, for instance the ==== financial forecasts (Aspen Publishers 2008-12), suggests that market participants were expecting the ELB to be binding for a much shorter time than turned out to be the case. We account for this evidence, and we analyse several questions related to it, such as what the output gains would have been had the Federal Reserve adopted alternative policy regimes to that in place during the financial crisis of 2008.====The expected duration of the ELB episode is not necessarily exogenously determined simply by the transition probability of the shock; in the case of a central bank that has commitment power, the duration of the binding ELB will typically be ==== than the persistence of the disturbance in its crisis state. The periods in which the inequality constraint is binding therefore do ==== with the periods in which the shock is in the low state. This means that the duration at the ELB is endogenously determined in the model, as it depends on the optimal decisions taken by the monetary authority, which are a function of, among other things, the realisation of the shock. This is a key challenge in solving the model and is discussed in detail in the paper.====Our main application is monetary policy when the interest rate reaches the ELB. Since the standard policy tool of affecting nominal interest rates is not available at that point, influencing expectations about their future path becomes the main lever through which the monetary authority can affect ==== variables. In this environment, policy rules that are able to mimic some form of commitment from the central bank are believed to perform relatively well. For example, Eggertsson and Woodford (2003), who predict strong deflation, argue that rules that commit to bringing the price level back to pre-crisis levels and to inflate in the future are very effective. A key finding is that price level targeting and nominal GDP targeting do not do well if there is little fall in inflation, as was the case during the financial crisis of 2008 in the United States. The policy of price level targeting we consider is arguably equivalent to the policy of average inflation targeting recently adopted by the Federal Reserve when it amended its policy framework in August 2020, if the average is taken over a sufficiently long period. We also consider an average inflation targeting regime for which the average time period is shorter. This policy provides even less stimulus at the ELB.====In addition, we discuss our two proposed rules – the ==== and the ==== – that imply a commitment from the central bank to make up for past deviations from the target price level and output. We study the two rules' performance in the standard NK model and in the NYFRB DSGE model and show that they generally perform better than the standard rules in the literature. We show this in an environment with low inflation and small movement in the price level, as experienced during the Great Recession. Since both rules imply an aggressive reaction to past output misses, they communicate that the longer the crisis is, the more accommodative monetary policy will be. This in turn generates enough stimulus to prevent a large recession in the first place.====Among previously proposed policy rules, the ones that perform best are the superinertial rule described in Rotemberg and Woodford (1999) and the augmented Taylor rule detailed by Reifschneider and Williams (2000). Policy rules that do not perform as well include price level targeting, nominal GDP targeting and average inflation targeting, a result in line with Reifschneider and Wilcox (2019). The key problem with these rules is that they do not prescribe strong-enough stimulus when inflation is not falling.====The most important advantage of the stochastic structure our toolkit embodies is that it allows for a clear distinction between the expected duration of the shock and the realised duration of it, though the two of course coincide under perfect foresight. This allows us to clearly show the advantage of policy commitments, such as those exemplified by our targeting rules, relative to optimal time-dependent policy – a strategy that resembles the interventions of several central banks during the crisis of 2008. Under optimal time-dependence, the duration of the ELB is tied to calendar time. In contrast, the targeting rule we consider implies a duration at the ELB that depends upon economic conditions. We highlight that a properly chosen state-contingent policy rule vastly outperforms optimal time-dependent policy, a distinction that is not as transparent in a deterministic setting.====In an additional application, we utilise our toolkit to contribute to a recent debate on the economic effects of forward guidance policy. We distinguish two cases: in the standard theory, forward guidance creates additional stimulus by a fully credible announcement that the central bank will keep the interest rate at the ELB for additional periods; this is an expansionary policy. The second case is what Campbell et al. (2012) call ==== forward guidance and Nakamura and Steinsson (2018) refer to as ====. Here, the expected duration of the ELB episode lengthens as well, but this time solely because the revelation of information leads agents to update their beliefs about economic fundamentals, which entails a contraction. We show that both scenarios can match the same increase in the expected duration at the ELB but that they lead to vastly different outcomes.====The paper is structured as follows: section 2 outlines how the solution method relates to the literature; section 3 presents the solution algorithm; section 4 provides a few applications in the context previously defined; section 5 applies our toolkit to the medium-scale FRBNY DSGE model; section 6 concludes.",A toolkit for solving models with a lower bound on interest rates of stochastic duration,https://www.sciencedirect.com/science/article/pii/S1094202521000296,22 April 2021,2021,Research Article,119.0
"Du Qingyuan,Eusepi Stefano,Preston Bruce","Monash University, Australia,University of Texas, Austin, United States of America,The University of Melbourne, Australia","Received 31 January 2020, Revised 28 January 2021, Available online 21 April 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2021.03.002,Cited by (3),"This paper proposes a new theory of exchange rate determination. Under arbitrary beliefs, the exchange rate is determined by an equilibrium restriction which we call the generalized no-arbitrage condition. The pricing function predicts endogenous departures from the conventional rational expectations ====, we find learning dynamics help explain the persistence and volatility of exchanges rates but generate counter-factual predictions on international ==== comovement.","Equilibrium exchange rate dynamics in standard macroeconomic models rely on the joint assumptions of no-arbitrage and rational expectations. However, consistent with evidence on the empirical failure of uncovered interest parity (UIP), these models often require exogenous risk premium shocks to characterize certain features of data. In a small open-economy model we introduce a new theory of exchange rate determination which relaxes the assumption of rational expectations but preserves the assumption of no arbitrage. Arbitrary beliefs imply foreign exchange is priced by a ==== which relaxes standard UIP restrictions on the dynamics of the exchange rate and real interest rate differentials. This paper evaluates the quantitative implications for international macroeconomic comovement.====Our approach builds on Frankel and Froot (1987), Devereux and Engel (2002), Gourinchas and Tornell (2004) and Chakraborty and Evans (2008). We propose a theory of distorted exchange rate beliefs in which agents have imperfect knowledge of long-run economic fundamentals. These fundamentals are the long-run equilibrium outcomes implied by the economic model, including the long-run level of the real exchange rate. Because agents use a Kalman filter to form inferences about these unknown objects, long-run beliefs respond to short-run forecast errors, consistent with survey data on expectations. Embedding these beliefs in a general equilibrium model generates extrapolation bias as an equilibrium outcome. The degree of bias is endogenous to monetary policy and domestic and foreign economic disturbances.====This choice of information friction reflects a growing body of empirical evidence that suggests: i) economic decision makers over-weight more recent data when making projections about the future [Fuster et al. (2010), Bordalo et al. (2018), Landier et al. (2018)]; and ii) economic forecaster's long-term expectations are strongly correlated with short-run forecast errors [Crump et al. (2015) and Carvalho et al. (2015)]. Furthermore, Eusepi et al. (2019) show that a closed-economy version of the model provides a good fit of the US economy and outperforms the rational expectations alternative.====We show this belief structure has novel predictions for exchange rate determination. Applying results from Eusepi et al. (2020) to a small open-economy model, we derive an arbitrage condition for Canadian holdings of domestic and foreign bonds under arbitrary beliefs. The generalized no-arbitrage condition relaxes well-known restrictions of uncovered interest parity. This pricing function of foreign exchange is the sum of the uncovered interest parity condition plus an additional set of conditional expectations of both the real return differential between Canadian and US bonds, and future real exchange rates. In this sense, non-rational beliefs predict the exchange rate to be the sum of fundamentals — the exchange rate that would prevail under uncovered interest parity — plus premia which are belief driven and endogenous to the model. This sets the model apart from a broad class of analyses that capture deviations from the UIP solely with exogenous premium shocks such as Justiniano and Preston (2010a) and Itskhoki and Mukhin (2019).====Aside from introducing learning dynamics, and an auxiliary assumption required to price multiple assets under arbitrary beliefs, our model has the same structure as Justiniano and Preston (2010a).==== Because our model has a linear state-space representation we use Bayesian likelihood-based methods to conduct inference using a sample that starts after the collapse of Bretton Woods exchange rate system. Using a standard set of times series plus US survey data to discipline expectations formation, we estimate three versions of the model: i) one with rational expectations; ii) one with distorted beliefs and the generalized no-arbitrage condition; and iii) one with distorted beliefs restricted to satisfy conventional uncovered interest parity. Together these models permit insight on the role of exchange rate determination and beliefs in mediating international shocks.====While the estimated model produces different predictions to the standard rational expectations setup, we show that, in stark contrast to the closed-economy model using only US data, the information friction is rejected by the data. In other words, the data favor a version of the model where the informational friction is ==== and long-run beliefs are relatively insensitive to short-term forecast errors — the learning gain is close to zero. In fact, rational expectations dramatically outperforms learning.====The deterioration in statistical fit is not due to our theory of exchange rate determination. To illustrate, we consider a version of the model in which the exchange rate is priced under UIP and solely determined by the sum of expected interest rate differentials between home and foreign countries into the indefinite future. Expectations formation, however, remains subject to information frictions. This mirrors the assumptions in Eusepi and Preston (2018) where rational expectations pricing restrictions between two different assets are embedded in a model with imperfect information and learning. The estimated open-economy model with these asset pricing assumptions performs even worse.====The empirical shortcomings of the model stem from its inability to ==== explain the observed international co-movement between Canada and the US and the dynamic behavior of the exchange rate. We conduct a series of experiments, including re-estimating the model after calibrating the learning gain to alternative values, and assuming country-specific learning gains. The experiments provide the following conclusions. First, the introduction of learning generates a trade-off between some improvement in explaining key properties of the exchange rate, such as its high persistence, and a substantial reduction in the international comovement in economic activity.====Second, learning induces ==== in the exchange rate. This property manifests in deviations from UIP that are too large when compared to the data. This volatility gets transmitted to domestic interest rates through the generalized no-arbitrage condition, and to domestic income and consumption through the wealth effects from exchange rate and terms of trade variation. Since these effects are determined by the steady-state properties of the model, the only way to sever the transmission of exchange rate volatility to the real economy is to shut down the learning mechanism.====Third, variance decompositions reveal that the learning and rational expectations models account for macroeconomic dynamics in different ways which underscore the ‘disconnect’ between exchange rate dynamics and the real economy. The rational expectations model uses Canadian markup and monetary shocks to explain the bulk of exchange rate fluctuations. US non-monetary shocks and shocks to Canada's natural output then play an important role in explaining Canadian output fluctuations, with US shocks promoting international comovement in output. Contrariwise, the learning model harnesses US shocks, and in particular monetary shocks, to explain a sizable fraction of fluctuations in the exchange rate, but very little variation in Canadian output, producing little international comovement. And despite their importance in explaining key US variables, monetary policy shocks induce counterfactual movements in the exchange rate. This echos previous results in the literature that in this class of model monetary policy shocks have difficulty explaining exchange rate dynamics — see Steinsson (2008). We are left with a clear dichotomy: we can account for real exchange rate dynamics with Canadian shocks, and Canadian output with US shocks; or the converse. Which it is will matter greatly for policy design.====Our paper relates to a long literature on the behavior of the exchange rate and the international transmission of business cycles. Standard general equilibrium models struggle to explain the joint behavior of exchange rates and macroeconomic variables. Chari et al. (2000), Bergin and Feenstra (2001), Devereux and Engel (2002), Corsetti et al. (2008), Steinsson (2008) and more recently Itskhoki and Mukhin (2019), document various model failures and possible remedies. Justiniano and Preston (2010a) raise further questions, finding that when confronted with a large number of time series, likelihood-based estimation of standard micro-founded open-economy models require risk-premium shocks to explain exchange rate dynamics, but sacrifice explaining international macroeconomic comovement. That empirical work is part of an extensive literature that started with important papers by Lubik and Schorfheide (2005b), Lubik and Schorfheide (2005a) and Adolfson et al. (2007). The paper also contributes to a growing literature on estimated DSGE models with bounded rationality which show promise in explaining the behavior of asset prices and key macroeconomic variables in closed economies [Slobodyan and Wouters (2012a), Sinha (2016), Eusepi and Preston (2018), Eusepi et al. (2019) and Adam et al. (2017)]. For related open-economy empirical work see also Jääskelä and McKibbin (2010).====We structure the paper as follows. Section 2 provides an overview of the model. Section 3 explores implications of arbitrary beliefs for asset pricing, deriving the generalized no-arbitrage condition which determines the exchange rate. Section 4 provides detail on the specific belief assumption used in the empirical work. Section 5 provides estimates and basic implications. Section 6 documents model predictions about excess volatility and international comovement. Section 7 considers a more general belief structure and shows the open economy model distorts econometric inference on the US block of the model — despite being exogenous from an economic perspective. Section 8 concludes.",Non-rational beliefs in an open economy,https://www.sciencedirect.com/science/article/pii/S1094202521000260,21 April 2021,2021,Research Article,120.0
"Bassetto Marco,McGranahan Leslie","Federal Reserve Bank of Minneapolis, 90 Hennepin Avenue, 55401, Minneapolis, MN, USA,Federal Reserve Bank of Chicago, 230 S. LaSalle St., 60604, Chicago, IL, USA","Received 15 January 2020, Revised 25 February 2021, Available online 17 March 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2021.02.014,Cited by (2),"We investigate the relationship between public capital spending and population movements at the state level. We document that gross mobility is positively correlated with capital spending across states and over time within states. We introduce an explicit, dynamic quantitative model of government spending determination, where gross mobility and population growth generate departures from ==== by shifting some costs and benefits of public projects to future state residents. The model is able to account for a large fraction of the correlation in the data.","Every recession experienced by the United States highlights the plight of states whose finances are bound by constitutional restrictions on their indebtedness. Almost all states are in principle prevented from borrowing to cover their ordinary budget. By contrast, most of them are permitted to issue bonds to pay for capital improvements. This rule, commonly known as “the golden rule,” has long been rationalized with the fact that capital improvements benefit generations to come, who should be called to share the burden.==== Current expenditures presumably benefit only the current residents who should therefore bear all the costs. Of course, such a rationale only holds in an economy that is far from Ricardian equivalence, for otherwise government debt would have no effects on allocations and welfare (Barro, 1974).====In this paper, we study the quantitative implications of departures from Ricardian equivalence for public spending at the state level in the United States, focusing especially on capital spending. The degree by which cost/benefit mismatches lead to inefficient spending depends on how far away the demographic structure is from Ricardian equivalence. In the extreme, in a world of no mobility and no population growth, the set of voters would be fixed and the distribution of costs and benefits over time should be irrelevant. The more the population churns across states, the greater are the misaligned intertemporal incentives faced by voters. We thus begin our analysis by documenting the relationship between population dynamics and spending in state-level public spending panel data. We highlight two findings:====To go beyond the qualitative pattern and assess whether these relationships are quantitatively meaningful, we develop a dynamic stochastic political-economy model of public spending determination. Our model builds upon Bassetto with Sargent (2006), where voters choose the provision of public goods anticipating that their future costs (and benefits) will be shared with incomers and will not be borne (and enjoyed) by a household once it emigrates. While Bassetto with Sargent analyzes steady states, we are interested in dynamics and responses to population shocks. This requires developing a richer framework, where public spending evolves over time, and where current voters face decisions knowing that future mobility and population growth are time varying and uncertain.====The premise of this framework is that voters discount future costs and benefits of their policy choices more heavily than efficiency would dictate: this occurs because they neglect costs and benefits that will accrue to future residents of the state that are not yet present. This effect will be larger, the faster a state's population is growing or the more turnover among residents. In the absence of any restriction on government indebtedness we would expect states to run large deficits, thus transferring costs to future residents. However, as mentioned above, the manifestation of the golden rule in the U.S. states prevents voters from financing non-capital expenses with debt. If strictly enforced, the golden rule fully restores incentives to provide the correct amount of nondurable public goods: this is because both costs and benefits accrue immediately to the voters. However, in the case of public capital, both benefits (of current investment) and costs (of current debt issuance) spill over into the future. These costs and benefits could align if debt issuance were structured so that the debt was paid off at the same time as the benefits accrued, but in practice the two need not be aligned correctly. In particular, whether costs or benefits will be shifted further into the future depends on many parameters, but especially on the duration of capital, the maturity of debt, the fraction of debt financing, and the persistence of the shocks. The shorter the duration of capital, the longer the maturity of debt, and the larger the fraction of debt financing, the more costs are shifted to the future relative to benefits, to the point that voters might be induced to overspend. The persistence of the shocks interacts with the durability of capital in determining the time profile of investment. If capital is more durable than shocks are, a shock that lead voters to demand more capital in the current period would lead them to ramp up investment quickly, but would then ==== investment in the future even as the shock has not fully faded away.====In deriving implications from the theory, accounting properly for implementation delays is important. In practice there is a delay between the time of the deliberation and the realization of the costs and benefits. In particular, gathering bids, signing contracts, and completing construction takes time. We are able to derive implications that are independent of the precise structure of implementation lags. We interpret the magnitudes of the relationships that we find in the data using our model's sharp quantitative predictions about the degree of departure from Ricardian equivalence when states adhere to the golden rule. The positive relationship between gross mobility and capital spending is interpreted by the model as evidence of ====: when future costs exceed the benefits, a more mobile population is more prone to ====. The magnitude of the empirically measured link between mobility and spending is economically important. The model accounts well for the pattern that emerges in the data when looking at time-series variation within states. When cross-sectional evidence is included, the pattern from the data is somewhat stronger, leaving room for other factors to play a role.====Our final exercise estimates the fraction of debt financing that would be needed in the model to match the response of public investment to mobility and population growth in the data. We find that the implied amount is in the order of 90-110% borrowing for each dollar spent in public capital. We view this as further suggestive evidence that the model can capture well the quantitative features of the data.====A number of papers in recent years have studied fiscal policy and government debt through the lens of models that explicitly link economic and political decisions; a few examples of this pursuit are Sleet (2004), Klein et al. (2008), Azzimonti, 2011, Azzimonti, 2015, Battaglini and Coate, 2007, Battaglini and Coate, 2008, Barseghyan et al. (2013), Azzimonti et al. (2016), Song et al. (2012), and Bachmann and Bai (2013).==== These models are motivated by stylized facts, or derive simply qualitative predictions. Our paper is an attempt to push further and to develop a model of government spending that has quantitative implications that can be directly linked to data.====In our simple environment, mobility and the voters' location are exogenous to capital spending. We view this as a reasonable approximation for interstate mobility. Kennan and Walker (2011) estimate that a $10,000 subsidy would induce about 2% more people to move across states every year. Public investment is approximately 1.2% of Gross State Product, so its value per capita is modest in comparison.==== Moreover, to the extent that public investment is financed through debt, only the discrepancy between future costs and benefits would affect migration, leading to even smaller endogenous responses. This distinguishes us from a large literature that, starting from Oates (1969) has studied endogenous mobility at the ==== level, where location choice is more elastic, since a local move does not necessarily require a change in jobs or personal networks.====With exogenous mobility, our model abstracts from the possibility that amenities and debt may be capitalized into property prices. When mobility is endogenous, this is an important factor. Barseghyan and Coate, 2016, Barseghyan and Coate, 2017, Barseghyan and Coate, 2019, Barseghyan and Coate, 2020a, Barseghyan and Coate, 2020b use a dynamic model of political economy similar to ours to analyze how the choice of tax instruments and the details of who is making decisions at what stage matters for the efficiency of public investment in this context. Since their emphasis is on the theory, they abstract from a quantitative analysis and from aggregate shocks that are a main object of interest to us.==== A sizeable empirical literature has analyzed the extent by which amenities are capitalized into property prices at the local level. In regards to the specific question of debt, some important contributions are Eichenberger and Stadelmann (2010), Banzhaf and Oates (2013), Stadelmann and Eichenberger (2014), and MacKay (2014).==== The biggest unit of analysis in these studies is a U.S. county, and most of them focus on much smaller jurisdictions. The rationale for this is that capitalization needs both an elastic demand for housing by prospective residents and an inelastic supply of houses. At the state level, demand is less elastic due to the smaller endogenous mobility, and land available for new development is plentiful.==== For this reason, we do not think that this omission is material for our results. Moreover, to the extent that capitalization did occur, we would expect not to find evidence that population churning and spending are related.====In the remainder of the paper, Section 2 describes our empirical analysis. Section 3 presents the model through which we interpret the data; it derives the equilibrium relationship between government spending, population growth, and mobility, provides intuition on its nature, and discusses ways in which this relationship can be informative about the parameters of the model. Section 4 compares the empirical results to the predictions of the model, while section 5 speculates on directions for future research.","Mobility, population growth, and public capital spending in the United States",https://www.sciencedirect.com/science/article/pii/S1094202521000181,17 March 2021,2021,Research Article,121.0
"Aruoba S. Borağan,Cuba-Borda Pablo,Higa-Flores Kenji,Schorfheide Frank,Villalvazo Sergio","Department of Economics, University of Maryland, College Park, MD 20742, United States of America,Division of International Finance, Federal Reserve Board, 20th Street & Constitution Ave., NW, Washington, DC 20551, United States of America,Department of Economics, University of Pennsylvania, 133 S. 36th Street, Philadelphia, PA 19104, United States of America,CEPR, United Kingdom,NBER, PIER, United States of America","Received 15 January 2020, Revised 9 October 2020, Available online 21 January 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2020.12.003,Cited by (11),We develop an algorithm to construct approximate decision rules that are piecewise-linear and continuous for ,"Dynamic stochastic general equilibrium (DSGE) models with financial frictions are widely used in central banks, by regulators, and in academia to study the effects of monetary and macroprudential policies and the propagation of shocks in the macro economy. The most recent vintage of these models involves occasionally-binding constraints arising from financial frictions and the effective lower bound (ELB) on nominal interest rates. In order for these models to be usable for a quantitative analysis, they need to be solved numerically, and their parameters need to be estimated based on historical data.====Two types of solution approaches for models with occasionally-binding constraints have been used in the literature. The first group of solution algorithms can be broadly classified as global methods. Agents' decision rules (or value functions associated with optimization problems) are represented by a family of flexible functions—for example, Chebyshev polynomials—or by a discrete mapping on a finite state-space domain. The flexible functions are parameterized by coefficients that are chosen such that the resulting decision rules (approximately) satisfy the model's equilibrium conditions and solve the underlying intertemporal optimization problems. Examples of this approach include Christiano and Fisher (2000), Adam and Billi (2007), Fernández-Villaverde et al. (2015), Maliar and Maliar (2015), Nakata (2016), Gust et al. (2017), Aruoba et al. (2018), Mendoza and Villalvazo (2020), and Atkinson et al. (2020).====The second type of solution approaches are variants of the extended perfect-foresight path (EPFP) method that build on Fair and Taylor (1983). These algorithms rely on the assumption that, after ==== periods, the system reverts back to the steady state in which the constraint, say, is non-binding. With an initial guess about whether the constraint is binding in periods ====, ====, it is possible to solve the dynamic system for the values of the endogenous variables. One can then compare the initial guess about the duration of the binding regime to the backward solution and iterate until consistency is achieved. Because the computations are based on the initial state, the previously described steps need to be repeated for every ==== in a multi-period simulation. Variants of this approach have been used in Eggertsson and Woodford (2003), Christiano et al. (2015), Guerrieri and Iacoviello (2015), Kulish et al. (2017), Holden (2019), and Boehl (2019). The Guerrieri and Iacoviello (2015) paper is accompanied by a popular model solution toolbox called OccBin that implements a variant of the EFPF approach. We will refer to OccBin in various instances throughout our paper.====Given the model solution, one then constructs a state-space representation for an estimable empirical model. The solution itself generates the state transition equations. A set of measurement equations can then be specified that links the state variables with the observables. Because the model solution is nonlinear, so is the state-space representation. Thus, a nonlinear filter is required to compute the likelihood function. For instance, in the context of DSGE models with an ELB constraint, Gust et al. (2017) and Aruoba et al. (2018) use a particle filter in combination with a global solution to construct likelihood functions. Guerrieri and Iacoviello (2017) use an EPFP solution for a model in which the number of observables equals the number of structural shocks and combine it with an inversion filter that essentially solves for the innovations as a function of the observables conditional on an initial state.====Against this backdrop, the contribution of our paper is to construct an alternative model solution that (i) is able to capture an important aspect of the nonlinear decision rule generated by an occasionally-binding constraint, (ii) can be solved quickly, and (iii) allows us to derive an accurate and fast filter for the evaluation of the likelihood function that exploits the structure of the solution. The procedure is efficient and can be run on a desktop computer in a reasonable amount of time. For instance, estimating the small-scale New Keynesian model in our empirical application using data from 1984 to 2018 takes about 18 hours on a single core.====The basic idea of the proposed solution method is to approximate agents' decision rules globally by piecewise-linear functions that are continuous but have a kink along the locus of the state space in which a constraint becomes binding. The coefficients of the decision rules are determined to ensure that the model's equilibrium conditions are (approximately) satisfied. The equilibrium conditions typically take the form of nonlinear expectational difference equations. We require that the (potentially transformed) state variables enter the occasionally-binding constraint linearly. For the remaining equilibrium conditions a (log)linearization is optional. In determining the decision rule coefficients, we take into account that the constraint could either be binding or non-binding in the next period. Thus, we are capturing precautionary behavior. Importantly, the decision rule coefficients only have to be computed once (as opposed to for each period ==== separately as in the EPFP approach). Compared with higher-order Chebyshev polynomials, the piecewise-linearity and continuity at the kink drastically reduce the number of coefficients that need to be determined and hence simplify computations.====Our solution method is motivated by the observation that more densely parameterized nonlinear decision rules look approximately piecewise-linear in a number of DSGE models. For instance, in Aruoba et al. (2018) we considered a New Keynesian DSGE model and stitched together higher-order Chebyshev polynomials along the locus in the state space where the ELB constraint becomes binding. We found that the decision rules on both sides of the kink are approximately linear. In the Online Appendix we solve a consumption-savings model with an occasionally-binding borrowing constraint and demonstrate that a global solution technique produces approximately piecewise-linear decision rules. Moreover, in Section 3 we solve a simplified version of the New Keynesian DSGE model with an ELB constraint and show that the piecewise-linear structure is exact.====To solve the nonlinear filtering problem, we develop a conditionally optimal particle filter (COPF). A particle filter is a stochastic algorithm that approximates the distribution of a vector of hidden states ==== conditional on the sequence of observations ==== available in time ==== by a swarm of ==== particle values and weights ====. Because of its stochastic structure, repeated runs of the filter generate a distribution of likelihood values. An important property of the particle filter is that the average likelihood across repeated runs is equal to the exact likelihood (unbiasedness). The tuning of the particle filter determines the precision of the approximation. A key step in the specification of the algorithm is the mutation of time ==== particle values into time ==== particle values. We show how, in the case of a piecewise-linear DSGE model solution, the mutation step can be executed optimally, conditional on the stage ==== particle values.====In a sequence of numerical illustrations based on a small-scale New Keynesian DSGE model with an ELB constraint, we document important properties of our solution algorithm and the COPF likelihood approximation. We show that, compared to a naive bootstrap particle filter (BSPF), which mutates particle values by simulating the model solution forward, our COPF drastically reduces the variance of the likelihood approximation holding the runtime fixed. In practice, this allows users to run the COPF with far fewer particles than the BSPF, which in turn speeds up the computations. When we embed the more accurate COPF into a random walk Metropolis–Hastings (RWMH) algorithm, we are able to significantly reduce the persistence of the resulting Markov chain and therefore improve the accuracy of Monte Carlo approximations of moments of the posterior distribution.====A key feature of our paper is that it integrates model solution, likelihood approximation, and Bayesian estimation. There are a few papers that assess the interplay of existing model solution and likelihood evaluation techniques in Monte Carlo experiments. The ones most closely related to our work are Cuba-Borda et al. (2019) and Atkinson et al. (2020).==== Cuba-Borda et al. (2019) take a simple consumption-savings model subject to a borrowing constraint and illustrate that less accurate solution methods affect inference even when the inversion filter is available. They also show that, as one increases the measurement error variance in the BSPF, the likelihood misspecification becomes more problematic, making it harder to retrieve the parameter values that govern the data generating process (DGP). In their setting, measurement error and solution approximation error make it difficult for the econometrician to identify the model regime that generates the data, and this incorrect classification of regimes leads to a bias in parameter estimates. In our empirical application, one of the observed time series allows us to exactly identify the regime, and we modify the COPF to capture this feature.====Atkinson et al. (2020) compare the performance of a fully nonlinear solution and a variant of the BSPF for estimation, with the approximated solution using OccBin and the inversion filter. They simulate data from a DSGE model that includes more frictions and shocks than the model used for estimation. As such, their estimated model is misspecified with respect to the DGP. Their results show that the nonlinear approach performs slightly better than the OccBin approach, but the differences are small. Moreover, relative to the pseudo-true parameters, the estimates from both approaches show biases in some key parameters, such as the degree of price rigidities. Since the OccBin-inversion filter approach can be scaled up easily and is faster, they argue that building a bigger and less misspecified model using this approach may be preferable. Similarly, our method offers scalability, even without multicore processing or distributed computing, and allows for more general model structures and state-space representations than the inversion filter.====Based on U.S. data from 1984 to 2018 on output growth, inflation, interest rates, and the government-spending to GDP ratio, we estimate a small-scale DSGE model using our proposed piecewise-linear and continuous (PLC) solution in combination with the COPF. From the estimated model, we compute dollar-for-dollar government spending multipliers associated with the increase in government spending that was part of the 2009 American Recovery and Reinvestment Act (ARRA). The counterfactual output levels are computed by lowering the exogenous government spending process in the model by an amount that is commensurable to the ARRA intervention, setting expansionary policy shocks during that period to zero, and keeping all other exogenous processes at their historical levels. We find that the ex-post multiplier associated with the combined fiscal and monetary policy intervention in 2009 and 2010, when the United States was at the ELB, was around one over a one- to two-year horizon.====The remainder of the paper is organized as follows. Section 2 describes the small-scale New Keynesian DSGE model with ELB constraint used in the subsequent analysis. In Section 3 we solve a simplified version of the New Keynesian model and show that the resulting decision rules are piecewise-linear and continuous. We also provide a comparison to the OccBin solution. In Section 4, we describe how to impose continuity on piecewise-linear decision rules and derive a canonical form for the DSGE model solution. Section 5 discusses how the decision rule coefficients are determined to approximately satisfy the model's equilibrium conditions. The COPF is derived in Section 6. Section 7 presents some numerical experiments to document the accuracy of the likelihood approximation through the COPF, and Section 8 contains the empirical analysis. Finally, Section 9 concludes. Derivations and further implementation details are provided in the Online Appendix. The Appendix also contains a section that shows how to solve a consumption-savings model with an occasionally-binding borrowing constraint using the techniques proposed in this paper and compares our PLC solution to an “exact” solution and a solution constructed with OccBin.",Piecewise-linear approximations and filtering for DSGE models with occasionally-binding constraints,https://www.sciencedirect.com/science/article/pii/S1094202520301149,21 January 2021,2021,Research Article,122.0
"Fernández-Villaverde Jesús,Sanches Daniel,Schilling Linda,Uhlig Harald","University of Pennsylvania, United States of America,NBER, United States of America,CEPR, United Kingdom of Great Britain and Northern Ireland,Federal Reserve Bank of Philadelphia, United States of America,École Polytechnique, CREST, France,University of Chicago, United States of America","Received 15 January 2020, Revised 19 November 2020, Available online 29 December 2020, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2020.12.004,Cited by (66),", and the central bank arises as a deposit monopolist, attracting all deposits away from the commercial banking sector. This monopoly might endanger maturity transformation.","We investigate the implications for the financial architecture of introducing a central bank digital currency, or CBDC. In particular, we focus on the effects of a CBDC on financial intermediation and maturity transformation. We show how the central bank must offer contracts on par with those of commercial banks to make the CBDC sufficiently attractive as an alternative to commercial bank deposits. While this achieves social efficiency, we also point out a sinister counterpart. We assume that the central bank cannot invest in long-term projects on its own, but has to rely on investment banks instead. In turn, this reliance renders central bank CBDC deposits safer and, thus, more attractive than deposit contracts at commercial banks, endangering the commercial banking sector.====Recent advances in cryptographic and distributed ledger techniques (von zur Gathen, 2015; Narayanan et al., 2016) have opened the door to the widespread use of digital currencies. While the lead in introducing these currencies came from private initiatives such as Bitcoin, Ethereum, and Libra, researchers and policymakers have explored the possibility that central banks can also issue their own digital currencies, aptly called a CBDC.====The introduction of a CBDC can represent an important innovation in money and banking history. Besides its potential role in eliminating physical cash, a CBDC will allow the central bank to engage in large-scale intermediation by competing with private financial intermediaries for deposits (and, likely, engaging in some form of lending of those deposits). In other words, a CBDC amounts to giving consumers the possibility of holding a bank account with the central bank directly.====Defenders of a CBDC have been rather explicit about this implication. For instance, Barrdear and Kumhof (2016, p. 7) state: “By CBDC, we refer to a central bank granting universal, electronic, 24====7, national-currency-denominated and interest-bearing access to its balance sheet.” In this paper, we will use these authors' definition as the working concept of a CBDC.====Similarly, Bordo and Levin (2017) propose that either “an account-based CBDC could be implemented via accounts held directly at the central bank” [p. 7], or that a “CBDC could be provided to the public via specially designated accounts at supervised commercial banks, which would hold the corresponding amount of funds in segregated reserve accounts at the central bank” [p. 8], an option often referred to as a synthetic CBDC. In this formulation, the differences between a CBDC held directly at the central bank or a commercial bank with segregated reserves are mostly inconsequential for our analysis, for they have the same implications for financial intermediation.====Thus, we could be at a juncture where changes in technology –namely, the introduction of digital currencies– may justify a fundamental shift in the architecture of a financial system, a central bank “open to all.” As argued by Friedman and Schwartz (1986) in a classic paper, external forces –in this case, technological innovations– might shape how we think about the role of government in setting up monetary and financial institutions and make us opt for alternative arrangements.====These considerations are already relevant to monetary policy. In June 2018, Swiss voters turned down the sovereign-money (====) initiative that would have given the central bank a monopoly on issuing demand deposits, an idea motivated in part by the possibility of a Swiss CBDC. Despite the ==== initiative being soundly defeated at the ballot box, similar designs are bound to be widely discussed during the coming years.====A CBDC is frequently promoted as a vehicle for financial inclusion of a previously unbanked population as well as a safer alternative to commercial bank accounts. With that, the central bank needs to make the CBDC sufficiently attractive relative to interest-bearing and risk-sharing commercial bank deposit contracts. Inevitably, this means that the central bank will find itself confronted with traditional banking functions such as maturity transformation and financial intermediation. The purpose of this paper is to think these issues through to the very end and answer the key questions that arise.====What effects will the introduction of a CBDC and the opening of central bank facilities have on financial intermediation? Will a CBDC impair the role of the financial system in allocating funds to productive projects? Or can we reorganize the financial system in such a way that a CBDC will still allow the right flow of funds between savers and investors? Will a CBDC make bank runs disappear and stabilize the financial system?====To answer these questions, we build on the classic model by Diamond and Dybvig (1983), augmented by the presence of a central bank. We select this model because it emphasizes the role of banks in maturity transformation: banks finance long-term projects with demand deposits, which may be withdrawn at a shorter horizon. Exploring how a CBDC will interact with maturity transformation is a first-order consideration that has not been thoroughly examined by the literature, often more interested in questions such as the consequences of a CBDC for interest rates, tax evasion, or anonymity (for a review of that literature, see Beniak, 2019). Furthermore, the model by Diamond and Dybvig (1983) allows us to explore how the propensity for bank runs might change with the introduction of a CBDC and to compare those results with previous findings in the literature.====More concretely, we consider a framework in which the CBDC takes the form of demand deposit accounts of the public at the central bank. Like a commercial bank, the central bank holds assets funded by these liabilities, but in contrast to commercial banks, we assume that the central bank cannot invest in high-return long-term projects. This constraint might be due to the central bank not having a good technology to screen, monitor, and liquidate productive projects. The central bank can, instead, rely on investment banks to engage in wholesale loans. We derive an interesting equivalence result that shows that the set of allocations achieved with private financial intermediation (which, in the Diamond-Dybvig framework, can deliver an ==== first-best allocation) can also be achieved with a CBDC, provided competition with commercial banks is allowed.====This equivalence result vindicates some of the views of proponents of a CBDC: the socially optimal amount of maturity transformation can still be produced in an economy where the central bank has been opened to all. But our equivalence result has a sinister counterpart. If the competition from commercial banks is impaired (for example, through some fiscal subsidization of central bank deposits), the central bank has to be careful in its choices to avoid creating havoc with maturity transformation.====We can think about our equivalence result as belonging to the tradition of public finance Modigliani-Miller theorems (Sargent, 2009, ch. 8). These theorems highlight how the same allocation can often be implemented through very different government policies. In our case, the socially optimal allocation can be implemented with and without a CBDC. But these Modigliani-Miller theorems also emphasize how strict are the conditions under which such results work. In the presence of a CBDC, we are likely to find powerful political-economic forces that will break the equivalence conditions. For instance, we can expect directives or subsidies to provide preferences for investment in some class of projects over others that are favored by the electoral process.====The fragility of the equivalence result is even clearer when we add to the analysis the consideration of commercial and central bank runs. Under bank runs, the central bank can still offer the socially optimal deposit contract, but the conditions for runs in commercial and central banks are different. The intuition is as follows. The wholesale loan of the central bank to the investment bank is not callable. This implies that the central bank's indirect investment in the long asset is protected from early liquidation. This protection either deters runs on the central bank or makes them less likely than runs on the commercial banking sector. Consumers internalize this feature and exclusively deposit with the central bank. That is, due to the rigidity of the central bank's contract with the investment banks, the central bank becomes the monopoly provider of deposits.====Importantly, this monopoly power allows the central bank to deviate from offering the socially optimal deposit contract. If the central bank exercises this market power, it will deviate from offering the socially optimal deposit contract, and the economy will not have the first-best amount of maturity transformation.====Commercial banks do not have access to the same rigid contract because of two fundamental aspects of public law in nearly all legal systems: the seniority of the debt to the central bank and the protection it enjoys against forced liquidation. Once the central bank starts dealing with investment banks, it will displace commercial banks from this market because, thanks to its seniority, it will obtain a higher expected rate of return. Also, the protection against forced liquidation will allow the central bank to use the investment banks and deny withdrawal requests that exceed the level of its short assets. A central bank can default but not go bankrupt. These two legal features make the nature of the central bank's assets and liabilities essentially different from a commercial bank's assets and liabilities. This element of CBDCs is an overlooked but key factor behind the consequences of this new money.====Our paper is related to several important branches of monetary economics. In terms of money and banking theory, we are closest to Diamond and Dybvig (1983), and all the subsequent literature. Our main equivalence result resembles some aspects of Brunnermeier and Niepelt (2019), who show the equivalence of private and public monies in quite a different environment without maturity transformation. By contrast, we highlight, in particular, the equivalence of financial arrangements in terms of maturity transformation, a novel result in the literature, and derive our results by studying the incentives of individual depositors.====There is also an emerging literature on CBDC. Beyond the many papers cited above, Andolfatto (2020), Böser and Gersbach (2019), Brunnermeier and Niepelt (2019), Chiu et al. (2019), Niepelt (2020), and Keister and Sanches (2019) discuss related issues to ours regarding the interaction between the commercial banking sector and a CBDC. A distinctive feature of our paper is that we pay much attention to allocations under bank runs and analyze how a CBDC can and cannot prevent these runs.====Finally, notice that the debate on CBDC belongs to a broader debate on narrow banking (Pennacchi, 2012). This literature has been concerned with how alternative forms of financial intermediation may unravel banks' deposit markets (von Thadden, 1998). For example, Jacklin (1987), among others, shows that a profit-sharing contract between a bank and depositors would prevent runs and monopolize the market.====The rest of the paper is organized as follows. Section 2 frames the current discussion within the historical background. Section 3 introduces our model of a central bank open to all. Section 4 defines and characterizes the equilibrium of the economy. Section 5 analyzes bank runs. Section 6 presents some discussion of the robustness of our results. Section 7 concludes.",Central bank digital currency: Central banking for all?,https://www.sciencedirect.com/science/article/pii/S1094202520301150,29 December 2020,2020,Research Article,123.0
"Campbell Jeffrey R.,Weber Jacob P.","Department of Economics, University of Notre Dame, United States of America,Department of Econometrics and Operations Research, Tilburg University, The Netherlands,Department of Economics, University of California Berkeley, United States of America","Received 15 January 2020, Revised 21 September 2020, Available online 13 December 2020, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2020.11.005,Cited by (1),"New Keynesian economies with active ==== rules gain equilibrium determinacy from the central bank's incredible off-equilibrium-path promises (====). We suppose instead that the central bank sets ==== ==== and occasionally has the discretion to change them. With empirically-reasonable frequencies of central bank reoptimization, the ==== game has a unique symmetric Markov-perfect equilibrium wherein forward guidance influences current outcomes without displaying a forward guidance puzzle.","This paper replaces the interest rate rule in the three-equation New Keynesian (NK) model with benevolent central bankers who can commit to interest rate ==== for random terms in office. If central bankers turn over frequently enough, then the economy has a unique symmetric Markov Perfect Equilibrium (MPE) even though each one of them commits to a completely passive interest rate rule. Simple intuition explains this result: central bankers optimize in the future, private agents optimally respond to the resulting interest rates when they are set, and the private sector takes these future outcomes ==== when making current consumption and pricing decisions. This basic logic of Markov-perfect Nash equilibrium reduces the scope for self-fulfilling prophecies to influence current choices.====Our model builds upon the quasi-commitment framework of Roberds (1987), which Schaumburg and Tambalotti (2007), Bodenstein et al. (2012) and Debortoli and Nunes (2014) have previously applied to the NK model. In it, the central banker commits to a deterministic path for interest rates while facing a constant probability of being replaced by a successor who will herself reoptimize. The future interest rates along this path constitute the central banker's forward guidance.====No major central bank provides complete and perfectly credible details of its policy actions for the indefinite future. In the discussion of Campbell et al. (2012), Donald Kohn asserted that “no central bank to date has adopted an Odyssean commitment strategy,” and Laurence Meyer stated that 98 of the 100 market participants he interviewed understood forward-looking statements of the Federal Reserve to be forecasts rather than commitments (Romer and Wolfers, 2012). Therefore, the benchmark from which an empirically-useful consideration of forward guidance should deviate is closer to complete discretion than to perfect commitment. On the other hand, monetary policy committees adjust their policies slowly (Blinder, 1998), so their choices and forward guidance embody some commitment. Indeed, slow adjustment and long periods of inaction are natural features of any institutional setting where decisions have to be made by consensus. Riboni and Ruge-Murcia (2010) demonstrate how such “status quo bias” arises naturally when monetary policy committee members have heterogeneous goals, and they argue that such models explain the actual behavior of central banks well. Ruge-Murcia and Riboni (2017) empirically validate this prediction with a natural experiment created by changes to the Bank of Israel's governance structure. Thus committee gridlock effectively endows a central bank with some degree of commitment, even when individual members vote with discretion (Riboni, 2010).====We interpret our model's central banker replacement as a metaphor for considerations which could lead committees of central bankers to renege on their prior commitments, and quasi-commitment represents this commitment “technology” in an analytically tractable way. Our implementation of quasi-commitment abstracts from central bankers' freedom to respond to non-contractable but relevant variables, as discussed by Kocherlakota (2016). Nevertheless we believe that the quasi-commitment framework represents monetary policy formation more realistically than the standard interest rate rule, which also abstracts from many observable variables used by central bankers in practice. While such rules describe observed central bank behavior conditional upon very limited information sets well, their embodiment within the NK model imposes the incredible off-equilibrium-path promises detailed by Cochrane (2011).====Our work is related to many other examinations of equilibrium multiplicity in the NK model. Adão et al. (2011) gain equilibrium uniqueness with a passive interest rate rule which exactly cancels out forward looking terms in the IS curve; unlike those authors, we have central bankers choosing deterministic paths for interest rates. Adão et al. (2014) deliver determinacy by introducing long-term debt and having the central bank manipulate bond term premiums. Our framework incorporates no deviation from the expectations theory of the term structure. Both King and Wolman (2004) and Armenter (2008) establish multiplicity of MPEs in nonlinear NK models with an inflation bias, while Siu (2008) shows that these results are sensitive to the specification of firms' nominal pricing technology. Those authors studied settings where past private choices of high nominal prices amplify the policymaker's inflation bias. Therefore, the policymaker can facilitate the realization of the private sector's self-fulfilling inflation expectations. The standard NK model with a microfounded quadratic loss function lacks this channel. Finally, Blake and Kirsanova (2012) showed that linearized NK models with endogenous state variables, such as the stock of government debt, can have multiple MPEs. Our model's only endogenous state variables are the interest-rate commitments of incumbent central bankers. Empirically-relevant NK models include many endogenous state variables, such as capital and past wage and price inflation. Just as the Taylor principle is only suggestive of equilibrium determinacy in such environments (Coibion and Gorodnichenko, 2011), our results provide a useful baseline for the examination of DSGE models with quasi-commitment (Debortoli and Lakdawala, 2016).====Another approach to eliminating equilibrium indeterminacy and solving the forward guidance puzzle introduces discounting into the IS curve of the standard model. Definitionally, this reduces the elasticities of current output with respect to future output and inflation. Many authors have introduced modifications to the standard model to introduce such discounting, including Del Negro et al. (2015); Fisher (2015); McKay et al. (2017); Michaillat and Saez (2019); and Gabaix (2020). Campbell and Weber (2019) reviews these modifications in detail. Quasi-commitment yields equilibrium determinacy through a distinct channel. Instead of making expectations of future output and inflation less important for the present, ==== Simple intuition explains this result: central bankers optimize in the future, private agents optimally respond to the resulting interest rates when they are set, and the private sector takes these future outcomes ==== when making current consumption and pricing decisions. This basic logic of Markov-perfect Nash equilibrium reduces the scope for self-fulfilling prophecies to influence current choices. Similarly, quasi-commitment solves the forward guidance puzzle almost by construction by reducing the power of promises, as demonstrated by Haberis et al. (2014). Though our channel introduces no discounting, as the IS curve remains unchanged, we employ some of the insights of this literature in the discussion of our results.====An older game-theoretic approach to the time-consistency problem in monetary policy characterizes all subgame perfect equilibria of the game between an infinitely-lived monetary authority and the private sector (Chari and Kehoe, 1990; Stokey, 1991; Chang, 1998; Phelan and Stacchetti, 2001). Chari et al. (1998) label those equilibria with outcomes inside the policy frontier “expectation traps,” and they interpret regime switches in U.S. macroeconomic performance as movements between “bad” and “good” equilibria. This approach to solving the time-consistency problem conceives of commitment as a characteristic of the solution concept: the best subgame perfect equilibrium. In contrast, we model commitment more heuristically as a technologically-given probability of the central bank keeping its promises. Therefore, we can compare economies with different amounts of central-bank commitment; which seems to us a precondition for quantitative empirical applications.====To demonstrate our framework's quantitative potential, we calculate an upper bound on the average duration of central bank commitment consistent with equilibrium uniqueness using the parameter values estimated by Galí and Gertler (1999) and other values employed by Eggertsson and Woodford (2003) and Schaumburg and Tambalotti (2007). This upper bound varies between three and eleven quarters, so the model can have both equilibrium uniqueness and nontrivial implications for forward guidance.==== Campbell and Weber (2019) note that the standard NK model's forward guidance employs an unspecified ability to coordinate expectations on one of many equilibria. Because our model has only one equilibrium given any interest rate path, its forward guidance does not depend on such “open mouth operations.”====In the next section, we lay the foundations for our analysis by reviewing the Ramsey planning formulation of the central bank's problem. We then introduce quasi-commitment by temporarily ==== that the current central banker's successor returns inflation and the output gap to zero with some fixed probability in each time period, as in Schaumburg and Tambalotti (2007). In Section 3, we relax that assumption by explicitly modeling the game associated with quasi-commitment. In it, an initial central banker chooses a path of interest rates while aware that she may be succeeded in each period by another central banker who will reoptimize. We show that if the probability of reoptimization is high enough, then the resulting unique symmetric Markov-perfect equilibrium coincides with the Ramsey planning allocation under quasi-commitment. Section 4 demonstrates that the quasi-commitment framework with a unique equilibrium lacks a forward guidance puzzle. Section 5 characterizes optimal forward guidance under quasi-commitment facing a transitory cost-push shock. Specifically, we note that ==== to implement price level targeting continues to be optimal even though private agents rationally expect only partial achievement of that goal in equilibrium. Section 6 contains concluding remarks.",Discretion rather than rules: Equilibrium uniqueness and forward guidance with inconsistent optimal plans,https://www.sciencedirect.com/science/article/pii/S1094202520301083,13 December 2020,2020,Research Article,124.0
"Choi Jason,Foerster Andrew","University of Wisconsin–Madison, Department of Economics, 1180 Observatory Drive, Madison, WI 53706, United States of America,Federal Reserve Bank of San Francisco, 101 Market St, San Francisco, CA 94105, United States of America","Received 6 April 2019, Revised 5 November 2020, Available online 2 December 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.11.007,Cited by (0),"An economy that switches between high and low growth regimes creates incentives for the monetary authority to change the parameters in its simple rule. As lower growth tends to produce lower real interest rates, the monetary authority has an incentive to increase the ","The slow growth after the financial crisis and recession of 2008-2009 prompted a renewed debate about objectives and conduct of monetary policy due to structural changes that have taken place. In particular, slower growth contributed to lowering the real interest rate, leading to questions about how, if at all, the systematic conduct of monetary policy should respond to this shift. Much of the debate has called for a reassessment of inflation targets in light of a lower real interest rate.====The emphasis on changing fundamental policy parameters such as the inflation target reflects the widespread view that lower growth is in fact due to a persistent structural change rather than temporary factors that can be addressed by monetary policy without an overhaul of the policy framework. At the same time, while the period of low growth is persistent, the US economy has experienced repeated and lengthy periods of high or low growth (Fernald, 2012; Foerster and Matthes, 2020). This fact suggests that at some point in the future, the economy may return to a period of higher growth, only to switch back to a period of lower growth again at a later date.====Periods of higher or lower growth affect the conduct of monetary policy through direct effects on output and inflation, but also indirectly through the connection between growth and the real interest rate. If growth is persistently low, then an associated low real interest rate provides some justification for increasing the inflation target: a higher inflation target is intended to increase the nominal interest rate in order to avoid hitting the zero lower bound, at which time central banks may have to use unconventional policy such as forward guidance or quantitative easing. However, adjusting the inflation target due to structural change requires two important considerations.====First, moving the inflation target is only one of possibly multiple adjustments to the conduct of monetary policy that can be made. Rather than focusing solely on the inflation target, it makes sense to also consider how strongly the nominal rate should react to current conditions versus have inertia in policy, as this systematic feature may have trade-offs relative to moving the target. Higher degrees of inertia increase the memory of the interest rate process, and thereby become more like a price-level target where past deviations of inflation are compensated for rather than ignored. On the other hand, very high levels of inertia lead to interest rates behaving more like a peg where they do not respond to inflation, increasing macroeconomic volatility.====Second, another issue for changing monetary policy due to structural change is that the economy tends to shift between periods of high and low growth. These repeated regime changes may not occur very frequently, but are not impossible. A monetary authority should account for the possibility of future shifts in the growth rate when setting policy. In addition, if the monetary authority is changing how it reacts as the economy undergoes structural shifts, then households and firms will respond to these policy shifts, leading to feedback effects.====This paper studies optimal simple rules for monetary policy when the structural economy experiences regime shifts. Using a standard New Keynesian model modified so that growth rate switches between high and low growth regimes, it assesses the implications for setting the inflation target and degree of interest rate inertia when monetary policy follows a Taylor rule and is subject to the zero lower bound. The simple rule framework considered has the nominal rate responding to deviations of inflation from the target and fluctuations in the output gap, subject to a degree of inertia in setting the nominal rate. The switching in the growth rate process tends to generate a low real interest rate in one of the regimes, which affects inflation and output dynamics. This change in dynamics causes the monetary authority to need to adjust its policy parameters to avoid the zero bound. The non-linearity resulting from the zero lower bound plays a key role. If nominal rates were not bounded below at zero, a decrease in the trend growth rate would simply imply a decrease in the real interest rate target by a proportional amount; the optimal policy parameters would remain unaffected. However, with a binding zero bound, the monetary authority is unable to provide additional accommodation when armed only with an interest rate rule. This inability of the monetary authority to operate further lower rates induces adjustment in the optimal policy parameters.====The focus on the inflation target and inertia is because these two options present a trade-off for the monetary authority. A higher inflation target raises the level of the nominal rate when the real rate is low, which tends to keep the nominal rate positive, but comes at the cost of higher inflation. On the other hand, making interest rates more inertial and hence dampening their responsiveness to current conditions can keep interest rates stable at positive values at the cost of injecting more volatility into inflation and output. Higher degrees of inertia can also generate lower-for-longer nominal rate dynamics that provide additional stimulus when at the zero lower bound (Nakata and Schmidt, 2019). An optimizing monetary authority that follows a rule will set both of these policy parameters in tandem depending on the growth rate. Focusing on only one, for instance the inflation target, ignores a key degree of freedom available in setting monetary policy. More specifically, comparing optimal rules between a high and low growth rate economy, the optimal rule for low growth has a slightly higher inflation target and more inertia; ignoring the possibility of an increase in inertia would cause the monetary authority to set an even higher inflation target.====In addition, taking repeated switches between high and low growth into account is a key consideration for setting policy. The monetary authority can generate better welfare by internalizing expectation effects. These effects include the direct effects that switching between high and low growth regimes has on monetary policy through household and firm behavior, but also the fact that if the monetary rule changes across regimes, households and firms will internalize the possibility of future policy changes. In other words, communication of a switching policy has effects beyond the current rule set in place. As a result, the optimal inflation target and degree of inertia are similar but not exactly the same as the optimal parameters if each regime occurred in isolation without switching behavior.====One possible caveat to switching monetary policy rules due to structural change is that implementing the correct rule at the correct time may be difficult. For example, Hachem and Wu (2017) use heterogeneous inflation expectations that are updated through social dynamics to study why abrupt changes in inflation targets may not be successfully achieved. In this paper's framework, the difficulty may arise due to the monetary authority misidentifying the growth regime and using the incorrect rule, or due to the authority making a mistake and simply failing to adopt the proper rule. Such an outcome could be costly from a welfare perspective. Under these circumstances, setting an optimal constant rule that is not susceptible to such errors could be preferred if the incidence of implementing the correct rule is relatively high. This result then suggests that providing flexibility for a monetary policy rule can generate gains, but if errors occur with high enough frequency the constant rule is preferred.====The framework studied in this paper focuses on optimality within a class of simple rules for monetary policy, given that simple rules are ubiquitous in both theoretical and applied research. Much of the literature on optimal monetary policy with simple rules assumes a constant economic environment and constant rules over time. Schmitt-Grohe and Uribe (2007) characterize optimal simple rules in an economy without instability in the structural economy, and show these rules nearly replicate welfare achieved by a Ramsey planner. Nakov (2008) studies simple rules in the presence of the zero lower bound, and finds their performance is largely unaffected by the bound. In the context of regime switches in the structural economy, this paper shows simple rules that switch alongside the structural economy welfare can dominate fixed rules.====More recently, there has been considerable discussion about whether to change inflation targets in light of lower growth and real interest rates. For example, Rogoff (2008), Blanchard et al. (2010), and Ball (2013) all call for higher inflation targets in the United States. Billi (2011), Coibion et al. (2012), Dordal-i-Carreras et al. (2016) and Blanco (2018) consider the optimal inflation target in the presence of the zero lower bound on the nominal interest rate. In these cases, the monetary authority sets a constant inflation target that weighs the costs of higher inflation against the chance of hitting the zero lower bound. The optimal simple rules considered in this paper allow this trade-off to be regime-dependent, which enables the authority to set a higher inflation target in regimes where hitting the zero bound is relatively more likely. The focus in this paper on the trade-off between inflation targets and inertia also suggests that optimizing over only the target can produce more drastic changes in inflation targets than is necessary.====Recent experience with extended zero lower bound episodes in the United States and across the world led to much research studying the dynamics of economies at or near the bound. In this extensive literature, this paper relates most closely to Nakata and Schmidt (2019), who study how including an interest rate smoothing objective for a discretionary monetary authority can improve outcomes by keeping the nominal rate lower for longer. These dynamics affect the inertia versus inflation target trade-off considered in this paper. In addition, Bianchi et al. (2019) study an asymmetric rule to improve outcomes that come from the zero lower bound constraint. The presence of deflationary spirals plays an important role in that analysis; these spirals are situations where monetary policy cannot stabilize inflation at the bound, leading to self-fulfilling lower inflation. The analysis in this paper finds theses spirals arise for certain choices of the target or inertia, restricting the parameters that an optimal monetary policy rule can take.====Papers studying switching in monetary policy rules often have non-optimal switches that are independent of any underlying changes in the economy. Davig and Leeper (2007) and Bianchi (2013) consider switches in the coefficients dictating how the monetary authority responds to deviations from its targets. Schorfheide (2005) and Liu et al. (2011) allow for switches in the inflation target, and Foerster (2016) considers both types. However, in each of these frameworks, changes in the monetary policy rule occur randomly and without regard to the state of the private economy. In contrast, this paper motivates regime switching in the policy rule as an optimal response to switches in the private economy.====In the case where papers consider optimal policy with regime switching in the private economy, the monetary authority may face a reduced-form representation of the structural economy as in Blake and Zampolli (2011). On the other hand, Debortoli and Nunes (2014) interpret regime switching in monetary policy as coming from explicit changes in the authority's loss function. Davig (2016) shows how regime switches in price-setting behavior in the structural economy map into switches in the loss function when the authority operates with discretion. In contrast to these frameworks, this paper considers optimal simple rules, and how changes in the structural economy affect the optimal choice of policy parameters.====A caveat to the analysis in the paper is that by focusing solely on the setting of the nominal rate via a simple rule, it ignores explicit consideration of unconventional policy. A number of papers study the economic effects of quantitative easing (Hamilton and Wu, 2012; Gertler and Karadi, 2013; Greenwood and Vayanos, 2014) or forward guidance (Negro et al., 2012; McKay et al., 2016). Sims and Wu (2019, July) build a model that compares the effects of forward guidance and quantitative easing, and Wu and Xia (2016) develop a measure that summarizes multiple forms of unconventional accommodation. While this paper does not explicitly consider these additional channels for monetary policy, it does consider the implications of different simple rules for the long-term interest rate when at the zero bound. This rate can be viewed as a proxy for when unconventional policy might be more effective. In addition, the fact that higher inertia produces lower-for-longer dynamics at the zero lower bound mimics forms of forward guidance.====The remainder of the paper proceeds as follows: Section 2 presents the model, Section 3 shows results for optimal policy rules without regime switching, Section 4 considers optimal policy rules in the presence of regime switches, Section 5 studies the effects of monetary policy errors, and Section 6 concludes.",Optimal monetary policy regime switches,https://www.sciencedirect.com/science/article/pii/S1094202520301101,2 December 2020,2020,Research Article,125.0
"Chen Kaiji,Higgins Patrick,Zha Tao","Emory University, United States of America,Federal Reserve Bank of Atlanta, United States of America,NBER, United States of America","Received 14 May 2018, Revised 24 November 2020, Available online 26 November 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.11.008,Cited by (3),. The model estimation reveals that an exogenous shock to credit supply drives cyclical lending standards and accounts for a significant portion of fluctuations in bank loans and aggregate output.,"Credit shocks to the banking sector have long been recognized as an important driving force behind the output fluctuation (Bernanke, 1983). The observed series of lending standards set by bank loan officers, perhaps the best direct gauge of credit conditions across countries, has been used as evidence to motivate a building of macroeconomic models with exogenous credit shocks for understanding the Great Recession (Perri and Quadrini, 2018). In this paper, we provide an extensive analysis of the cyclical fluctuation of lending standards using the micro data merged from three separate sources: the quarterly Consolidated Report of Condition and Income (Call Report) for banks and the analogous FR Y-9C report for bank holding companies, the Federal Reserve Board's Senior Loan Officer Opinion Survey (SLOOS) on Bank Lending Practices, and the Center for Research in Security Prices (CRSP). We connect our data analysis to an introduction of informational frictions in the banking sector into a dynamic stochastic general equilibrium (DSGE) model.====Our data analysis consists of two parts. First, we exploit the micro data to construct the aggregate series of lending standards, as a measure of the weighted fraction of banks that tighten their lending standards. This constructed series of lending standards allows for potential influences of various aggregate shocks (supply and demand) on the loan market. Second, we explore the possible reasons why a bank tightens or loosens the standards at both the bank level and the aggregate level. By combining the Call Report, SLOOS, and CRSP, we document that balance sheet positions of banks measured by either their book or market values, while relevant for an individual bank to tighten its credit supply, were not a primary reason for ==== of banks to tighten their lending standards during the Great Recession. Instead, a more uncertain macroeconomic outlook was the main reason.====We avail ourselves of this data analysis to discipline our DSGE model by abstracting from considerations of the bank's capital position while emphasizing informational frictions in the banking sector. This abstraction is based on the fact that only a very small fraction of banks viewed their current or expected capital positions as an important economic reason for tightening their credit supply during the Great Recession and even for these banks both book and market values of their capitalizations were as healthy as other banks. The model builds on Townsend (1979), Williamson (1987), and Greenwood et al. (2010) but with an intermediation process in which the degree of informational asymmetry between lender and borrower determines how many banks would engage in costly state verification. In our model, a shock to bank credit supply is equivalent to a productivity shock to the monitoring technology, which affects the bank's probability of discovering misreports by borrowers. The informational frictions in our model manifest the moral hazard problem present in the lending market. More severe informational frictions reduce the bank's probability of detecting misreports by borrowers and force an individual bank to conduct more frequent verification, which results in an increase of the fraction of banks that engage in costly state verification.====A fraction of banks engaging in state verification in the model is linked to a fraction of banks changing their lending standards in the data. Negative shocks to bank credit supply, originated from moral hazard in the lending market, exacerbate informational frictions in the model. A shock to credit supply in our model generates a countercyclical movement of the supply of bank loans as observed in the data. Other economic shocks, such as a technology shock to production and a risk shock to the borrower's delinquency, shift the demand for bank credit, which moves the frequency of state verification and bank loans (output) in the same direction. Our estimated model reveals that credit supply shocks drive the cyclical fluctuation of bank lending standards and explain over 40% of the short-run fluctuations in bank loans and aggregate output. And these impacts are persistent over the four-year horizon. The persistent effects on aggregate conditions such as bank loans and output imply that negative shocks to credit supply produce a more uncertain economic outlook, which is the main reason for the tightening of lending standards as observed in the data.====Our paper relates to the literature on the role of financial factors in the business cycle. The bulk of the recent literature focuses on the role of borrowers' net worth or corporate bond spreads in propagating shocks originating in nonfinancial sectors of the economy (Bernanke et al., 1999; Gilchrist and Zakrajšek, 2012; Christiano et al., 2014, for example). A notable exception is Jermann and Quadrini (2012), who shift attention back to the role of shocks that originate directly in the financial sector (the so-called “financial shocks”) as a source of the business cycle fluctuation. In Jermann and Quadrini (2012), however, financial shocks stem from disruptions in the pledgeability of firms' assets. Several other papers have studied sources of banks' problems and the role of credit supply shocks with three approaches. The first approach focuses on banks' incentive problems and the effect of changes in banks' net worth or their ability to absorb disruptions hitting their liabilities (Gertler and Kiyotaki, 2010; Gertler and Karadi, 2011; Christiano and Ikeda, 2013; Quadrini, 2017). The second approach examines the impact of banks' relaxation of credit constraints on the boom of house prices (Justiniano et al., 2017). The third approach emphasizes how banks' liquidity mismatch opens up the possibility of bank runs (Diamond and Dybvig, 1983; Gertler and Kiyotaki, 2015, for example).====Our paper is motivated by different sources of the micro data showing that the decision made by a majority of banks to tighten their credit supply during the Great Recession was due to an uncertain or less favorable macroeconomic outlook, rather than the book or market values of banks' balance sheet positions.==== This finding is consistent with the empirical evidence provided by Begenau et al. (2019) (BBMV hereafter) on the role of commercial banks' capitalization in the business cycle.====In another related paper, Bassett et al. (2014) (BCDZ hereafter) construct a diffusion index of “exogenous” lending standards. They apply this exogenous series to their vector autoregression (VAR) analysis. As various economic shocks influence changes in bank lending standards, however, it is conceptually difficult, if not impossible, to construct purely exogenous lending standards via reduced-form econometric procedures. It would also be conceptually difficult to connect such an exogenous series to any shock in a general equilibrium model without an explicit linkage between observed lending standards and the theoretical model.====For these reasons, we depart from BCDZ's approach and construct an aggregate series of bank lending standards that are not exogenous but affected by variations in macroeconomic conditions. Our DSGE model allows us to answer which structural shock drives the movement of lending standards. We place a special emphasis on how model and data should be synthesized in the context of credit conditions reflected in bank lending standards. We use the rich micro data information to discipline how certain credit frictions should be introduced into a structural model for the purpose of explaining the cyclical fluctuation of bank lending standards.====The rest of the paper is organized as follows. Section 2 discusses how the micro and macro data are constructed and analyzed. Section 3 presents a DSGE model with informational frictions in the banking sector and builds the linkage between the model and the observed series of lending standards. Section 4 discusses the estimation strategy and analyzes the empirical results. Section 5 offers concluding remarks.",Cyclical lending standards: A structural analysis,https://www.sciencedirect.com/science/article/pii/S1094202520301113,26 November 2020,2020,Research Article,126.0
"Çakır Melek Nida,Plante Michael,Yücel Mine K.","Federal Reserve Bank of Kansas City, 1 Memorial Dr, Kansas City, MO 64198, United States of America,Federal Reserve Bank of Dallas, 2200 N Pearl St, Dallas, TX 75201, United States of America","Received 24 May 2019, Revised 12 November 2020, Available online 23 November 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.11.006,Cited by (5),"We examine the implications of the U.S. shale oil boom for the U.S. economy, trade balances, and the global oil market. Using comprehensive data on different types of crude oil, and a two-country ==== model with heterogeneous oil and refined products, we show that the shale boom boosted U.S. real GDP by a little more than 1 percent and improved the oil trade balance as a share of GDP by about 1 percentage point from 2010 to 2015. The boom led to a decline in oil and fuel prices, and a dramatic fall in U.S. light oil imports. In addition, we find that the crude oil export ban, a policy in effect until the end of 2015, was a binding constraint, and would likely have remained a binding constraint thereafter had the policy not been removed.","Technological advances in horizontal drilling and hydraulic fracturing have led to an unprecedented increase in U.S. oil production. Often referred to as the shale or fracking revolution, the boom in U.S. oil production has renewed interest in the long-standing question on the link between resource booms and economic performance. Several recent papers have focused on the local or regional implications of the U.S. shale boom, suggesting positive economic effects (see, for example, Feyrer et al., 2017; Allcott and Keniston, 2018). However, little is known about the implications of this boom for the U.S. aggregate economy and trade. In this paper, we study the importance and implications of the U.S. oil boom for the U.S. economy, trade balances, and the global oil market. We do so using a dynamic stochastic general equilibrium model of the world economy that takes into account unique characteristics of the U.S. experience: a large increase in production of a certain type of crude oil with an oil export ban in place.====Oil has often not been explicitly modeled in many leading macro and international trade models, partly because the share of oil in aggregate production is small and primary commodities overall account for a modest fraction of global trade. However, recent research has shown that shocks to economically small sectors, such as oil and gas, that feature complementarities with other inputs of production can have disproportionate aggregate effects (Baqaee and Farhi, 2019) and that trade in primary commodities is important with notably large gains from trade compared to gains from trade in standard models (Farrokhi, 2020; Fally and Sayre, 2018).====The relatively few general equilibrium models that do feature oil generally assume that it is a homogeneous good. This is a strong assumption since the characteristics of oil can differ across several dimensions, one of which is density. A key feature of the recent U.S. oil boom is that oil produced from shale deposits via the application of horizontal drilling and hydraulic fracturing is predominantly of one type: light crude. Different types of crude oil are imperfect substitutes for each other in the refining process and refining sectors tend to specialize in processing certain types of oil. The U.S. refining sector is specialized in processing heavier crude oils relative to the rest of the world. This mismatch of increased supply of light oil and existing refining capacity for heavier oil in the U.S. has important implications for the use and trade of various types of crude oil. The importance of this mismatch was potentially magnified by the U.S. export ban on crude oil, a policy in effect until the end of 2015.====In assessing the implications of the U.S. light oil boom quantitatively, we make two contributions to the literature. First, we introduce two previously unexamined sources of heterogeneity into a general equilibrium model with endogenous oil prices. The first source of heterogeneity arises from the different types of oil produced that are imperfect substitutes into the refining process. The second stems from the difference between refineries in the U.S. and the rest of the world (ROW). Our model also features an occasionally binding export ban on U.S. crude oil.====Our second contribution is to assemble a comprehensive data set that contains information on crude oil quality in order to build our model on solid microeconomic foundations. These data inform the building of our model in two ways. First, we use simulated method of moments (SMM) to estimate a number of model parameters to match moments from oil-related and macro data. Of particular note, we estimate three key parameters related to the refining sector: the elasticities of substitution across different oil types and the elasticity of substitution between oil and other factors of production. Second, we carefully calibrate other model parameters targeting a set of first moments for oil-related and macro variables. One key point to highlight is the importance of examining detailed oil data and introducing heterogeneity in crude oil types and in refining technology. If we were to only use aggregate data and pool different types of crude oil into one single oil sector, we would not be able to assess the implications of the shale oil boom for trade in different types of oil, relative prices of oil, and specialization of the refining sector. We also show these heterogeneities are key to properly understand the impacts of the crude export ban.====An essential initial step for our analysis is to document how some important oil market variables have changed during the U.S. shale oil boom. Using various sources, we gather data on production and prices of different types of crude oil as well as trade flows and refiner use of different types of oil. We document that from 2010 to 2015 U.S. light oil production more than tripled, while production increases outside the U.S. were from medium and heavy crudes. In addition, the U.S. refiners' use of light oil increased substantially from 2010 to 2015. Meanwhile, their medium crude use declined and heavy crude use increased. We document dramatic shifts in the quantity and types of oil being imported as well: U.S. light oil imports dropped sharply, medium oil imports declined and heavy oil imports increased since the shale boom. These facts help motivate the features of our model.====Our two-country (U.S. and ROW) general equilibrium model with heterogeneities and an export ban also has the following features. In addition to oil and refined products (fuel), both countries produce a non-oil good.==== The non-oil good is used for consumption and investment, which is costly to adjust, and as an input in the production of oil. Oil is only used to produce fuel, while fuel is consumed by households and also used as an input to produce the non-oil good. An internationally traded bond allows for the possibility of trade imbalances. To simplify, we abstract from distinguishing between traded and non-traded goods, a common feature in many commodity and resource papers that discuss the Dutch disease effect. The reasons are that, contrary to even other advanced (small) open economies that are net commodity exporters, where the commodity sector can be relatively large, the share of oil and gas in U.S. GDP is relatively small, having not exceeded 3 percent since the early 1980s. The U.S. also remains a net importer of crude oil.====We model the shale boom as a series of positive technology shocks that replicate the increase in U.S. light crude production from 2010 to 2015 and then illustrate the general equilibrium outcomes. Our main findings are three. First, we find that the shale oil boom had important impacts on several U.S. macroeconomic variables, notably on real GDP and trade balances. According to our model, the boom boosted U.S. real GDP by a little over 1 percent from 2010 to 2015, which accounts for about one tenth of actual GDP growth over this period. This suggests that the boom has contributed to the recovery from the Great Recession. There is also major improvement in the U.S. oil trade balance, by about one percentage point (as a share of GDP), in line with the data.====Second, our model can match several important aspects of U.S. oil market data during the boom despite relying on a single shock, a light oil technology shock. This includes the sharp drop in U.S. imports of light crude oil, the increased use of light oil by U.S. refiners, and the drop in both the use and imports of medium crude oil by U.S. refiners. On the other hand, we find a counterfactual decline in U.S. refiner use and imports of heavy crude oil. However, we show that the model can explain the changes in the heavy crude data successfully if we add a second shock, a ROW heavy oil supply shock, into the model.====Third, we find that the U.S. crude export ban was a binding constraint, particularly in 2014 and 2015, and that it primarily distorted the upstream and downstream oil sectors and petroleum trade, with negligible impacts on macroeconomic aggregates. We find the ban artificially depressed U.S. light crude oil prices, inflated light crude oil prices outside the U.S., and distorted the relative price of light crude to other types of crude oil. Oil producers, with the exception of light crude producers outside the U.S., were negatively impacted by these price distortions. We find U.S. refiners benefited from those price distortions, as they provided a cost-advantage, leading them to over-process light crude oil and take market share from refiners elsewhere. However, the ban had little impact on the global fuel supply or fuel prices as there was no ban on trade in refined products. Impacts on households, both in the U.S. and ROW, were negligible. Finally, we show that taking into account the heterogeneity in crude quality, and properly modeling and calibrating the refinery sector are key to examine the effects of the ban.",Resource booms and the macroeconomy: The case of U.S. shale oil,https://www.sciencedirect.com/science/article/pii/S1094202520301095,23 November 2020,2020,Research Article,127.0
"Crucini Mario J.,Vu Nam T.","Vanderbilt University, and NBER, United States of America,Miami University, United States of America","Received 30 July 2019, Revised 30 October 2020, Available online 17 November 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.11.002,Cited by (0), of county-level wage income dynamics where each county is subject to a common shock (with county-specific factor loading) and an idiosyncratic shock. We then ask if counties that experienced larger negative wage income shocks during the Great Contraction subsequently received more transfers per capita in the form of grants-in-aid. The fact that the negative business cycle shocks pre-date the passage of the ARRA and subsequent disbursements allows identification of the risk-pooling channel of the grants before fiscal multiplier effects confound these two channels. We find statistically significant and economically large risk-pooling effects.,"The American Recovery and Reinvestment Act (ARRA) was signed into law by then-President Barack Obama on February 17, 2009. As a discretionary peacetime fiscal measure, the total appropriation was the largest in American history. The impetus for the stimulus was, of course, the collapse of the stock market and a rapidly deteriorating macroeconomic situation in the United States and abroad, at the onset of the Great Recession. Unfortunately, the appropriate policy prescription and dosage were impossible to determine with any reasonable degree of accuracy given the lack of consensus on the size of the fiscal multiplier, the economics profession was left in the unenviable position of having to assess the policy ex-post. This evaluation is ongoing and is proving to be a productive and challenging research area.====Much of the literature to date has focused on the question that macroeconomic models are best equipped to answer: How large is the expected change in GDP associated with an exogenous increase in federal government consumption? Empirical studies of the fiscal multiplier have a long intellectual history, recent work includes Feyrer and Sacerdote (2011), Wilson (2012), Chodorow-Reich et al. (2012), Nakamura and Steinsson (2014), and Dupor et al. (2018). A second branch of the post-ARRA literature has attempted to relate the stimulus to job creation (see, for example, Goodman and Mance (2011), Conley and Dupor (2013), Bohn (2013), and Dupor (2014)).==== Finally, a handful of papers deal with the subtle interactions of federal stimulus, and fiscal spending and taxation at the state and local levels (see, for example, Johnson (2009), Cogan and Taylor (2010), and Leduc and Wilson (2017)).====Each of these studies has improved our understanding of fiscal policy and the role that specific parts of the ARRA stimulus package may have played in altering the paths of aggregate GDP and employment relative to a counterfactual without it. Largely absent from the literature is a systematic analysis of the heterogeneous impact of the Great Recession across locations within the country and the role the ARRA may have had in mitigating or exacerbating geographic income inequality arising from the Great Recession.====Our work aims to help fill this gap in the literature by focusing on the second and fifth of the five stated purposes of the ARRA: (1) to preserve and create jobs and promote economic recovery; (2) ====; (3) to provide investments needed to increase economic efficiency by spurring technological advances in science and health; (4) to invest in transportation, environmental protection, and other infrastructure that will provide long-term economic benefits and (5) ====.====To address the success or failure of the legislation along the dimension of changes in income inequality, arising from the Great Recession, requires that we move from an analysis of the macroeconomic impact to the microeconomic impact – from a focus on the time series movements in national income per capita to movements of county-level income relative to the national aggregate. Risk-sharing theory coupled with the theory of fiscal federalism provides the relevant benchmark for understanding these types of policy choices.====Assessing the effectiveness of the ARRA in mitigating income inequality is empirically demanding, requiring microeconomic panel data on both income and ARRA disbursements. These considerations led us to the ====, which ensures a nationally comprehensive accounting of wage income with a frequency of observation conducive to business cycle analysis. As a consequence, the focus is on location-specific, not individual-specific, risks. Individual-specific risks will be averaged out as individual wages are aggregated to the county level. The basic implication of risk-sharing in this setting, then, is that more ARRA funds should be disbursed to counties that experienced larger income shocks during the contractionary phase of the Great Recession.====A necessary, but not sufficient condition for the ARRA to achieve this goal is that disbursements vary across counties. Establishing this fact is challenging because the ARRA was a complex mix of tax changes, grants to state governments, and grants to other private and public institutions. Our focus on counties rather than states is intended to preserve as much of the cross-sectional income variance as possible to identify the risk-sharing channel. Strict adherence in linking ARRA disbursements to county-level economic distress requires that each line-item disbursement has an associated zip code. This leads us to focus mostly on grants, which are the lion's share of the discretionary, non-tax, component of the ARRA.====We have two sets of novel results. The first has to do with the properties of county-level business cycles relative to the aggregate U.S. business cycle. The second has to do with the relationship between discretionary transfers through grants-in-aid and the unexpected variation in county-level wage income.====A novel feature of our econometric model of county wage income dynamics is that it allows for two sources of business cycle heterogeneity: 1) differences in the variance of the county-specific shock to wage income; and 2) differences in the factor-loading on a common macro-shock to wage income. Notice that the common, macroeconomic, shock generates perfect collinearity of business cycles across counties, but some counties may have amplified or muted business cycles relative to the national average, based on their factor-loading parameter.====Turning to details, the half-life of a shock to wage income for the median county is reasonably short, 2.6 quarters, but ranges from 1.7 to 4 quarters at the ==== and ==== percentiles of the cross-county distribution of persistence estimates. The county-specific shocks have a median standard deviation of 2.97% with the corresponding range from 2.22% to 4.21%. Not surprisingly, the macro-shock has a standard deviation that is much smaller than that of the median county-specific shock, 1.3%. The estimated county-specific factor loading ranges from 0.6 to 1.3 for the interquartile range, indicating that some counties have dampened responses to the macroeconomic shock while others' are amplified. Notice that these macro-factor loadings, when combined with the standard deviation of the macro-shock, imply a range of variation at the county level, from 0.78% to 1.69%. While interesting in their own right, these county-level income processes are essential when assessing the potential for monetary and fiscal policies to mitigate or exacerbate geographical income inequality at business cycle frequencies.====This paper conducts an evaluation of the discretionary component of the ARRA in helping counties most affected by the Great Recession. We ask if counties experiencing either larger county-specific shocks during the contraction phase of the Great Recession or had greater sensitivity to the negative macro-shock, received relatively more grant-in-aid through the ARRA.====Estimating a cross-county fiscal policy function, it is found that 7% of the macroeconomic component of private wages shocks is mitigated by the grants-in-aid and no significant offset is found for the county-specific component of private wages. For public wages, the results are reversed; 13.5% of the county-specific shocks are offset while there is no offset of the macroeconomic component of public wages. We also show that aggregating the macroeconomic (common) shocks with the county-specific shocks or aggregating private and public wages leads to misleading inference about the role of the ARRA in offsetting the contractionary shocks of the Great Recession.====Our paper is related to the risk-sharing literature dealing with fiscal policy, most notably, Asdrubali et al. (1996). Their study focuses on the role of various automatic stabilizers in mitigating the variance of income growth across U.S. states and finds that 13 percent of the unconditional variance of gross state product output growth is smoothed by the federal government. However, because they focus on automatic stabilizers, they do not estimate unexpected changes to income but use raw growth rates instead.====This distinction is important: the discretionary appropriations in the ARRA were explicitly designed to overcome the perceived deficiencies of automatic stabilizers in the context of an extreme business cycle downturn. As such, unlike existing stabilizers, the stimulus was unprecedented in size and known to be a temporary measure. There was no expectation of a subsequent round of stimulus. In an interesting recent contribution, Oh and Reis (2012) study the ARRA with a focus on the redistributive components of the policy. Their focus is on the implication of incomplete markets (and other frictions, such as sticky prices) on the size of the aggregate multiplier, rather than the covariance between county-specific transfers and county-level income shocks, which is our focus.====The paper proceeds in the following way. Section 2 describes our data sources. Section 3 presents a detailed picture of the time series and cross-county variation in ARRA grants. Section 4 reports the estimates of county-level wage income dynamics. Section 5 discusses the theory of fiscal federalism and reports the estimation of the fiscal policy function relating grants-in-aid disbursements to county-level and macroeconomic wage income shocks experienced during the Great Recession. Section 6 conducts robustness analysis, such as the sensitivity of the policy function to the exclusion of the counties where state capitals are located. Section 7 concludes.",Did the American Recovery and Reinvestment Act help counties most affected by the Great Recession,https://www.sciencedirect.com/science/article/pii/S1094202520301046,17 November 2020,2020,Research Article,128.0
"Kirpalani Rishabh,Phelan Christopher","University of Minnesota, United States of America,Federal Reserve Bank of Minneapolis, United States of America,University of Wisconsin–Madison, United States of America","Received 16 May 2020, Revised 8 November 2020, Available online 13 November 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.11.004,Cited by (15),"We develop a simple dynamic economic model of epidemic transmission designed to be consistent with widely used biological models of the transmission of epidemics, while incorporating economic benefits and costs as well. Our main finding is that if the technology for tracking infected individuals is sufficiently good, targeted testing and isolation policies deliver large welfare gains relative to optimal policies when these tools are not available. Much of this welfare gain comes from isolating infected individuals rather than testing them. When the tracking technology is not very good, the gains from targeted testing and isolation are small. The message of our analysis is that the returns to improving tracking technologies are very large.","Pandemics force households, firms, and governments to make cruel choices among unhappy alternatives. Much economic activity is enhanced by close person-to-person contact. Unfortunately, this kind of contact typically allows viruses to be more easily transmitted from person to person. Much of the economic literature on epidemics studies the trade-offs between the losses to economic activity associated with limiting contact and the gains from reduced transmission of the virus, including reduced healthcare costs, lower strains on hospitals, and fewer deaths. The response to the coronavirus epidemic in most Western countries has been to limit contacts by limiting economic activity. Some countries—most notably, South Korea, Singapore, Hong Kong, and Taiwan—have limited economic activity to a lesser extent and have supplemented the modest limitations with aggressive policies of targeted testing, contact tracing, and isolation.====In this paper, we develop a version of a fairly standard macroeconomic model of epidemics, incorporate testing and isolation policies into it, and ask to what extent testing policies of targeted testing and isolation can achieve better outcomes. In the version of our model calibrated to the tracking technology available in South Korea, we find that a policy of targeted testing and isolation yields substantial welfare gains. If testing and isolation policies are optimally designed, economic activity must be curtailed to a much more limited extent, and the number of deaths is substantially smaller than if testing and isolation policies are not available. We argue that relative to no testing, untargeted testing without using a tracking technology yields only modest benefits. We also argue that even if testing resources are not available, targeted isolation without testing yields about two-thirds of the welfare gains from a targeted testing and isolation policy.====We then calibrate a version of our model to the tracking technologies available in Germany and Australia and find that targeted testing and isolation policies yield at best modest benefits relative to a policy of untargeted testing. The main reason for the difference is that the fraction of infected people who are successfully tracked is much smaller in our Germany and Australia experiments than in our South Korea experiment. Our analysis implies that the returns to tracking, contact tracing, and the like are very large.====The simple dynamic economic model of epidemic transmission developed here is designed to be consistent with widely used SIR (Susceptible, Infected, Recovered) biological models of the transmission of epidemics (see Atkeson (2020b) for a primer), while incorporating economic benefits and costs as well. We choose a formulation that makes it possible to analyze the benefits and costs of various policies. The main stand we take is that social proximity has benefits by allowing for economic activity to take place. We have in mind that certain types of production activities require groups of people to work in close proximity to one other. The obvious example of such an activity is assembly line production. In other activities, individuals derive value from social proximity in consumption. Examples of such activities are watching live performances of plays or rock concerts. While substitutes are available for production and consumption with social proximity (artisanal production as opposed to assembly line production, or televised rock concerts versus live ones), it is often the case that it is cheaper to provide a given good or service with high levels of contact than with low levels of contact.==== Social proximity has costs when such proximity allows viruses to be relatively easily transmitted.====The standard SIR model in epidemiology has three types of agents: susceptible (or not yet infected) agents, infected agents, and recovered agents (who may be alive or dead). To allow for testing, we extend the model to allow for two types of infected agents: those known to be infected and those not known to be infected. In our economic model, agents engage in a variety of economic activities. Each economic activity is associated with a given number of “meetings” with other agents. These activities are combined to produce a final output good. The virus is transmitted with an exogenous activity-specific probability in a meeting between an infected and susceptible agent. We assume that activities with low transmission probabilities also have low economic value.====The planner seeks to maximize the present discounted utility of consumption net of costs of treating infected agents and of death costs. In our model, absent any testing, the planner excludes from any activity agents who are known to be infected and allocates some of the agents whose types are not known to the activity with the lowest probability of transmission. Since the lowest probability of transmission activity has the lowest economic value, this policy tends to reduce output, but it saves lives. The quantitative version of our model generates output declines and death reductions broadly similar to those in Eichenbaum et al. (2020b) and Glover et al. (2020).====We measure the welfare gains from optimal policy as the permanent percentage increase in consumption that would give the planner the same utility as under no policy. For reference, we note that the loss in welfare in the no-intervention economy relative to the no-pandemic economy is 6.66%. We show that the optimal policy yields a welfare gain of roughly 0.6% relative to no intervention. We then introduce a costly testing technology.==== The planner can choose to test a fraction of the population whose types are not known. We assume the test perfectly reveals whether an agent is infected. We show that optimal policy with this type of untargeted testing yields a welfare gain of 0.7% relative to no intervention. That is, untargeted testing delivers gains of only 0.1% relative to welfare under optimal policy with no testing.====We allow for targeted testing by assuming that each agent whose type is not known is associated with a signal that he is infected. We think of this signal as combining information from a variety of sources. One example is contact tracing, which involves tracing people an infected person has come into contact with, persons whom these contacts contacted, and so on. This signal is informative in the sense that the probability of receiving the signal is higher for infected agents than for susceptible agents. We assume the signal is not perfectly revealing in that the probability an infected person receives the signal is strictly less than one. Out of these agents with the signal, the planner chooses the fraction to test. We choose the signal probabilities to be consistent with data from South Korea. That data suggest that 38% of infected people and 0.44% of susceptible people are associated with the signal. We show that the welfare gain from optimal policy with targeted testing relative to no intervention is roughly 3% of consumption forever. That is, targeted testing allows for a dramatic gain in welfare relative to a no-intervention policy.====In our model, targeted testing allows the planner to very precisely target some agents in order to isolate them. We separate out the effects of isolation from the informational gains to testing by considering a version of the model in which the planner cannot test after receiving the signal.==== The planner simply chooses the fraction of individuals with a signal to isolate. We show that under optimal isolation, the welfare gains are 2% of consumption forever. That is, roughly two-thirds of the gains from optimal testing can be realized by forgoing testing and simply isolating agents suspected of being infected.====We then consider a calibration of our tracing technology that is intended to be consistent with the German and Australian experience. It turns out that in these countries the tracing technology was substantially less effective than in South Korea. Viewed through the lens of our model, the fraction of infected people who emit the signal is much smaller in Germany or Australia than it is in South Korea. We find that calibrated to the German or Australian experience, testing and isolation strategies are only marginally effective at improving outcomes relative to optimal policy without these strategies.====We also conduct a variety of sensitivity exercises by varying the probability of receiving the signal. We show that if 60% of infected agents and 3% of susceptible agents are associated with the signal, the welfare gains relative to no intervention are about 5.5%. That is, the welfare loss from the pandemic is only about 1% of consumption. We also show that if a relatively small fraction of infected agents receives the signal, then the welfare gains are also smaller.====Our findings with respect to South Korea, Germany, Australia and our sensitivity analyses make clear that the returns to improving tracing technologies are extremely large. In this sense, our findings suggest that the policies advocated by Romer and Garber (2020, March), Romer (2020), and Holtemöller (2020) are most useful when supplemented with aggressive and accurate contact tracing.",The hammer and the scalpel: On the economics of indiscriminate versus targeted isolation policies during pandemics,https://www.sciencedirect.com/science/article/pii/S1094202520301071,13 November 2020,2020,Research Article,129.0
"Braun Christine,Nusbaum Charlie,Rupert Peter","Department of Economics, University of Warwick, United Kingdom of Great Britain and Northern Ireland,Department of Economics, UC Santa Barbara, United States of America","Received 15 July 2019, Revised 29 October 2020, Available online 10 November 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.11.001,Cited by (1),.,"Between 1964 and 2000 the increase in female labor force participation of married women has led to a more than doubling in the fraction of families with both spouses in the labor force, from 36% to 75%. Over the same period there has been a 30% increase in the female to male median wage ratio among married couples. How might such changes affect the willingness of married households to migrate across counties for new work opportunities?====In this paper we document that the intercounty migration rate between 1964 and 2000 of single households increased from 5.4% to 9.1% while the migration rate of married couples declined from 5.7% to 5.0%. After controlling for changes in other demographic characteristics such as age, education, and race, for example, we show that the downward trend for married couples persists whereas the upward migration trend for single households becomes flat. This suggests that the labor market experience of married women may be key in explaining the observed decline in couples' migration.====We estimate how much of the discrepancy in these migration patterns can be accounted for by the above mentioned forces. We find that, in total, the rise of dual labor force households can account for 55% of the decline in migration, whereas rising relative wages of women can account for 16% of the decline. These two mechanisms together can account for 65% of the total decline, implying negative complementarities between the two effects. Moreover, we find that approximately 10% of the rise in dual labor force households is induced by the rising job prospects of women. Accounting for this indirect effect of rising wages results in a wage effect that can account for 20% of the decline in migration. These results are both qualitatively and quantitatively similar in two extensions wherein we include an endogenous participation margin and exogenous, non-job related moves. Consistent with Cooke (2011) and Kaplan and Schulhofer-Wohl (2017) who suggest alternative mechanisms as the primary source of declines in migration–namely the Great Recession and increases in technology–for the post 2000 period, we show that female labor force participation among couples instead declined after 2000. Hence, our choice of focusing on the 1964-2000 time period for the effect on migration decisions.====In order to disentangle the composition effect, i.e. more dual searching households, from the wage effect, we construct a two location model with labor market frictions and allow both individuals to receive local and foreign offers while unemployed and employed, interpreting the acceptance of any foreign offer as a move to a new location. Once a move has taken place, only the spouse receiving the foreign offer remains employed. The interaction between mobility and on-the-job search has several implications on the reservation wages of individuals. First, if both spouses are employed, the foreign wage offers for which a household is willing to move is increasing in both spouses current wages, highlighting the fact that increased wages strengthens location ties. Second, if one spouse is employed, the local reservation wage of the unemployed spouse is everywhere decreasing in the employed spouse's wage. This results from increasing location ties as one spouse climbs the job ladder. That is, as the employed spouse's wage increases, both will be less likely to receive acceptable foreign offers, thereby making local offers more attractive and decreasing the reservation wage for the second spouse. The changing location ties of dual searching households, and in particular the strong location ties present when both spouses are employed, is the driving force of our mechanism.====We provide evidence of our mechanisms using household level data from the Current Population Survey (CPS) to test the effect of the joint decision process on migration. We find that households with both members in the labor force are 10% less likely to move. Furthermore, we show that among all households that moved, the relative probability that a dual searching household moves for job related reasons than for other reasons is 26% lower than for couples with only a single searcher. These results are consistent with work by Mincer (1978) and Costa and Kahn (2000) who show that the co-location problem faced by couples has a significant impact on migration decisions.====We also show that ignoring the co-location problem and these changing location ties has implications for estimates of lifetime earnings inequality. Ignoring this additional consideration of spouses within households can bias these estimates by as much as 20% for men, 36% for women, and 23% across all married individuals. This bias has increased for men and decreased for women as increasing wages for women cause fewer male-driven moves for high foreign wage offers and, therefore, more men to optimally enter the state of unemployment as a result of similar female-driven moves throughout their career. This results in dual searching men making choices that differ from their single searching counterparts to a greater degree, implying that the standard search model is no longer a good approximation for these men. The reverse is true for dual searching women.====Our model is similar to that developed in Guler et al. (2012). We add on-the-job search and gender specific offer distributions. Flabbi and Mabli (2018), Rendon and García-Pérez (2018), and Marcassa (2014) also extend the Guler et al. (2012) model to investigate the implications of dual searching households for estimates of lifetime earnings inequality, labor market policy reforms, and spousal unemployment duration, respectively. Karahan and Rhee (2017) argue that aging populations can explain almost half of the historical decline in aggregate migration. Kennan and Walker (2011) studies the effect of expected income on migration decisions. However, none of these papers address the dual-searcher household problem.====Molloy et al. (2017) argue that there is insufficient variation in the fraction of dual searching households to explain this long run decline, however, there is disagreement on this point. Cooke (2013), for example, instead shows that the fraction of dual worker households increased by roughly 15 percentage points from 1980-2010. We show empirically that the more than doubling of such households plays a major role in the migration decline in our sample. Taskin (2016) and Foged (2016) study how migration decisions of couples may change as relative wages change and find that migration is U-shaped in the wife's share of total family income. Neither, however, are able to decompose historical migration trends. Finally, a recent working paper by Guler and Taskin (2018) uses a similar model with marriage and divorce to investigate migration trends. Our paper differs in that we focus on married couples, as other demographic characteristics cannot explain the trend for this group. We allow search on and off the job to differ, and include an empirically representative mix of single and dual searching married couples.====The remainder of the paper proceeds as follows. Section 2 presents the CPS data used in our econometric analysis and highlights the key demographic trends underlying our mechanism. Section 3 outlines our model and derives the migration rates of dual and single searching households. Section 4 presents the results of our calibration and Section 5 conducts our quantitative experiment. Section 6 concludes.",Labor market dynamics and the migration behavior of married couples,https://www.sciencedirect.com/science/article/pii/S1094202520301058,10 November 2020,2020,Research Article,130.0
Gross Till,"Department of Economics, Carleton University, 1125 Colonel By Dr., Ottawa, ON K1S 5B6, Canada","Received 4 May 2018, Revised 15 October 2020, Available online 22 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.010,Cited by (5),". Labour taxes, government consumption, and infrastructure are all declining functions of the share of government revenues which are transferred. Capital taxes are surprisingly increasing in transfers in the short run, but unaffected in the long run. In the short run, capital taxes are too low and infrastructure spending is too high from a global welfare perspective in the absence of transfers, whereas both are at the efficient level in the long run. The dampened capital-tax and infrastructure competition during the transition means that economic efficiency and thus welfare paradoxically increase for low levels of transfers, even though there are no redistributive gains from transfers.","An important issue in federal countries – such as the United States, Canada, and Germany – or supra-national entities like the European Union is how the “taxation” of its members, which are tax authorities themselves, affects efficiency and their incentives to tax and spend. In each of the aforementioned examples, the composing jurisdictions (the states, provinces, Länder, or EU member countries) set some taxes on their own, but transfer part of their revenues to each other, either directly or indirectly through the federal/supra-national level. For instance, in Canada there are equalization payments between provinces; in Germany there is the Länderfinanzausgleich; in the European Union each member state contributes a share of its gross national income and value-added tax revenues and receives in turn funding for agriculture, cohesion policies, research, and other projects. Moreover, in the aftermath of the recent European financial crisis, there have been calls for large automatic transfers between governments of the European Union. Such transfers could potentially have sizable benefits in terms of insurance or macroeconomic stabilization.==== However, such a taxation of governments is likely to cause distortions, similar to the ones created when these same governments tax the private sector. This paper seeks to answer two related questions: First, how are governments' incentives to set fiscal policy affected by a transfer union, where every constituency has to pay a share of its tax revenues into a pool which then disburses the funds back to its members? Second, what are the rough efficiency costs of such inter-governmental transfers?====I adopt an optimal taxation approach, where each government optimally chooses its fiscal policy – capital and labour taxes, infrastructure, government consumption, and debt – taking into account how this influences the decisions of private agents.==== I consider a one-shot game between governments, where each is able to commit perfectly.==== Since the focus is on an international (or inter-jurisdictional) context with several interacting governments, optimal fiscal policy considers also the impact on foreign private agents' behavior and is contingent on the belief of foreign policy. I analyze equilibria where these beliefs are consistent with the actual foreign strategy. Capital is assumed to be mobile, whereas labour is not. Capital is accumulated through agents' optimal savings decisions. I consider a symmetric two-country economy, in which there are no distributional benefits from transfers in order to focus on the distortions for public policy (similar to a conventional representative-agent tax model in which tax revenues are rebated back in a lump-sum fashion). The model easily extends to any number of jurisdictions and almost any type of asymmetry, though, and most analytical (and concerning the number of countries, also numerical) results remain qualitatively unchanged.====Casual intuition would suggest that in the presence of intergovernmental transfers which are based on actual tax revenues (as in the German Länderfinanzausgleich), not some form of fiscal capacity (as in Canada), there is a positive externality emanating from taxes and productive public capital (which I use synonymously for infrastructure).==== The benefits accruing to the home government decrease and the benefits to the other government increase in revenue sharing, so one would thus presume that taxes and infrastructure spending (and hence also government consumption) are a decreasing function of transfers. As Baretti et al. (2002, p. 632) argue, transfers are effectively a tax on tax revenues and “Conventional economic theory suggests that [...] a state's level of [...] tax revenue will be lower the higher the marginal tax rate on its tax revenue.” The welfare effects should then be negative.====This intuition is partially borne out in my model: Labour taxes and government consumption are lower the higher the transfers, and this constitutes an inefficiency. The reason is precisely that labour tax revenues are being taxed from the perspective of each country, while the benefits to the other country are not taken into account. The transfers from the other country on the other hand depend on the foreign labour tax rate, which is taken as given from the home country's perspective. With regard to government consumption, the marginal cost of public funds is increasing in transfers (because of the taxation of tax revenues) while the benefits remain the same, which implies that government consumption is decreasing in transfers.====Concerning capital taxes and infrastructure, this intuition is misleading, though. The initial capital stock is an endowment and should be taxed as much as possible in a closed economy (say at 100%), since taxing it is non-distortionary. [The same logic applies to the short run more generally, when the government sets capital taxes to 100%, which still taxes the initial capital stock indirectly.] In an open economy, each government has an incentive to lower capital taxes to attract capital from abroad, since the marginal social product of capital is higher than the private return to capital for foreign investors, creating a positive intratemporal externality for capital taxes. This leads to inefficient tax competition and a race to the bottom, which is a well-established result in the large literature on this subject.==== Transfers diminish the benefits of attracting capital, as only a fraction of the wedge between capital's marginal product and the investor's private net return flows to the home government, while the rest goes abroad. Transfers thus reduce capital-tax competition.====In the absence of transfers, even though the marginal products of capital and infrastructure are equal to each other at all times (as in first best), this implies surprisingly an overprovision of infrastructure from a global perspective in the short run. Given that capital taxes are suboptimal in the short run, the marginal product of infrastructure should be higher than that of capital; in the long run, with efficient capital taxes, infrastructure provision is efficient, too. Intuitively, one could thus speak of “infrastructure competition” in the short run of an open economy without transfers; infrastructure attracts capital from abroad and, due to the non-cooperative nature of the game, governments do not take this negative externality on the other country into account. Since transfers increase the cost of public funds and governments thus provide less infrastructure, transfers therefore reduce infrastructure competition. For higher levels of transfers (and in the long run), infrastructure is less than what would be efficient, though.====Besides the positive ==== externality of capital taxes an additional ==== negative externality exists, as Gross et al. (2017) show. Capital taxes reduce the incentive to save and thereby the global capital-stock. Since the capital stock is shared by all countries, this effect is not fully internalized. Similarly, higher infrastructure spending increases the returns to capital and therefore encourages savings and expands the global capital stock. As I have argued above, the benefits of attracting capital are diminished by transfers, so the negative intertemporal externality of capital taxes is exacerbated by transfers; the costs of infrastructure increase in transfers, enlarging the positive intertemporal externality of infrastructure.====In steady state, capital taxes and infrastructure are at their efficient levels without transfers, when the intratemporal and intertemporal externality cancel each other out. In the long run lower capital taxes or higher infrastructure no longer attract capital from abroad, but instead create their own supply by encouraging savings. With perfect commitment and a complete tax system, long-run capital taxes are used to implement the efficient (first-best) capital allocation and not to raise revenues.==== The long-run capital taxes in the present paper also implement the efficient capital allocation and since revenues are purely incidental, the fact that some of these revenues are transferred abroad is of no relevance. Transfers hence do not affect optimal capital taxes in steady state. However, for infrastructure the costs are in terms of public funds, which are higher with transfers – the long-run infrastructure allocation is therefore distorted downwards by transfers.====Considering welfare, it follows that relatively low transfers paradoxically increase welfare in the short run, due to reduced capital-tax and infrastructure competition. Higher transfers decrease welfare, though, since infrastructure spending and government consumption are too low. Since capital-tax and infrastructure competition are absent in the long run, transfers unambiguously reduce efficiency in the long run.====In terms of policy, the results suggest that (relatively small) transfers between jurisdictions, which might be beneficial in terms of macroeconomic stabilization, or for redistribution and insurance purposes, will have only small detrimental effects on efficiency or might even improve it. However, it is important to bear in mind that the analysis in this paper abstracts from political-economy considerations, limited commitment, government default, and many other important real-world aspects. Moreover, all the gains stem from the short run, whereas in the long run transfers decrease efficiency.====The remainder of the paper is organized as follows: In the rest of this section I discuss the related literature, in section 2 I present the economic environment and lay out the structure of the game between governments and agents. I analytically derive optimal policies in section 3, whereas section 4 contains the presentation of the numerical results. I discuss the intuition of both the analytical and numerical results in section 5. I perform sensitivity analysis regarding parameter robustness and modeling assumptions in section 6. The last section concludes and proposes avenues for future research. In the appendix I provide some proofs and show the results for the sensitivity analysis.====  Boadway (2004) provides an overview on transfers, stressing that equalizing transfers mostly occur when there is a federal government on top of the regional governments. It is thus strongly related to the theory of decentralization, also see Boadway (2001). Boadway distinguishes between gross equalization (with transfers from the federal level to the regions) and net equalization (with inter-regional direct transfers). The transfers in this paper can thus be classified as a net equalization scheme, since I do not include a federal level.====Transfers either take the form of revenue equalization (as in this paper) or tax-base equalization. Static models of ==== have also found that transfers dampen tax competition: Smart (1998) and Büttner (2006) show that distortionary taxes may actually increase as a function of transfers and Köthenbürger (2002) and Bucovetsky and Smart (2006) argue that this may improve efficiency under tax competition.====Static models of ==== generally find the opposite: As Köthenbürger (2002, p. 402) states, a revenue-based transfer scheme “tends to reinforce the effects of tax competition.” He studies a model where governments choose capital taxes and government consumption in a standard model of tax competition.==== Hindriks et al. (2008) find that without transfers there is undertaxation and underprovision of infrastructure and higher transfers leave taxes unchanged, but decrease infrastructure investment, leading to a welfare improvement. They assume that governments choose public infrastructure and capital taxes sequentially. Relatedly, Köthenbürger (2011) emphasizes that optimizing over taxes or expenditures for local governments may lead to different results. In their theoretical model, Baretti et al. (2002) abstract from tax competition, so that output is produced from immobile labour alone. They show that transfers generally lead to lower effective taxes.====The literature on equalizing transfers does not, to the best of my knowledge, extend to a fully dynamic environment. The dissertation by Kim (2014) considers a dynamic economy with transfers similar to the one presented in this paper but does not show any analytical results. Moreover, the numerical implementation does not take into account the transition. Gong and Zou (2002) have a government maximize steady-state welfare subject only to steady-state constraints. There are two state governments and one federal government, but there is no capital mobility and no interaction between state governments. Ogawa and Yakita (2009) study a growth model with interregional transfers, but restrict fiscal policy to labour taxes. Local governments do not take into account how their taxes affect human and physical capital accumulation. Cyrenne and Pandey (2015) analyze a growth model where the government's single choice is the time-invariant share of tax revenues which goes to infrastructure vs. government consumption. They find that higher transfers likely lead to less infrastructure spending and more government consumption, but there is no strategic interaction between governments and no tax competition.====This paper also relates to and builds on the small literature in dynamic optimal taxation (with commitment) in open economies.==== Correia (1996) extended the Chamley-Judd result of zero long-run capital taxes to a small open economy; Gross (2014) does the same for large open economies. Angyridis (2007) studies a stochastic small open economy. Gross et al. (2017) focus on a computational analysis of transition dynamics and cross-border externalities in a two-country model. Gross (2015a) shows that the long-run capital allocations and qualitative capital-tax conclusions derived in closed-economy models also apply to open economies.====The contribution of this paper is thus twofold: First, in comparison to the literature on intergovernmental transfers, this is the first paper to analyze the incentive and welfare consequences of transfers in a fully dynamic optimal taxation framework. The time dimension is important, because the results differ markedly in the short and long run. Moreover, I consider a relatively comprehensive set of fiscal-policy instruments, with debt, labour and capital taxes, and productive and unproductive government spending. As far as I am aware, the transfers literature has so far only considered subsets of this. Second, with regards to the works on optimal dynamic taxation in open economies (without transfers), this paper analyzes the case of endogenous government consumption and infrastructure, and establishes that there is infrastructure competition in the short, but not in the long run. In the sensitivity analysis, I also show how labour mobility can be included in dynamic tax competition models and that it does not play a quantitatively significant role – at least under the assumptions made here. Furthermore, this paper is the first to compare different equilibrium concepts in a dynamic open economy-framework numerically, and, conceptually, makes an argument why the benchmark concept is the preferred equilibrium notion.====There is relatively little empirical evidence on the effect of transfers on fiscal policy. Baretti et al. (2002) find that transfers tend to reduce tax revenues of states in Germany. In my model tax revenues are decreasing in transfers. Even though capital tax competition is dampened in the short run, the negative impact of transfers on labour taxes outweighs this and the model is thus consistent with these empirical findings. The work by Smart (2007) suggests an increase in tax revenues through transfers for Canada, but this is a system of capacity-based transfers. Büttner (2006) and Egger et al. (2010) find that fiscal equalization leads to higher business tax rates of municipalities in Germany, in consonance with the short-run results of my model, but fiscal equalization in this context seems to be largely in terms of capacity.==== For US and German states, the results by Potrafke and Reischmann (2015, p.975) “indicate that intergovernmental transfers have implicitly subsidized debts,” in line with my findings that transfers lead to a higher path of government debt at any time period.==== I am not aware of any research which studies the consequences of transfers on fiscal policy over time, so one cannot evaluate the dynamic component of the model. Overall, however, the (scant) evidence regarding the impact of (revenue-based) transfers on fiscal policy is in consonance with the results in this paper. As mentioned above, the model necessarily abstracts from some practically relevant issues, though. For instance, a benevolent government with perfect commitment is useful as a benchmark, but I am personally not convinced that governments actually do behave in this way. While governments may or may not act as in my model, it is still useful for policy purposes, as it provides an idea what governments ought to do. The focus of this work is on understanding how the incentives to tax and spend are affected by transfers, rather than empirical predictions.",Dynamic optimal fiscal policy in a transfer union,https://www.sciencedirect.com/science/article/pii/S1094202520301034,22 October 2020,2020,Research Article,131.0
"Dauchy Estelle,Navarro-Sanchez Francisco,Seegert Nathan","Campaign for Tobacco Free Kids, 1400 I (Eye) Street NW, Suite 1200, Washington, DC 20005, United States of America,University of Utah, 1655 Campus Center Drive, Salt Lake City, UT 84108, United States of America","Received 25 April 2019, Revised 2 October 2020, Available online 21 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.004,Cited by (0),"This paper fills an important gap in our understanding of the role of the US tax system in changing household welfare and inequality. It deconstructs the mechanisms by which the federal income tax system operates to affect the transmission of income shocks to consumption, and therefore, consumption inequality. To this end, it links micro and macro models of the distribution of income and consumption to changes in the federal income tax system. We find important changes in the types of income shocks to which consumers are insured or not, as well as the extent to which tax policy contributes to insuring consumers from these shocks. Importantly, we find that without decomposing the tax system into the three mechanisms outlined in the model, economists might have erroneously inferred that the role of tax policy as a mechanism for consumption insurance did not change over time. We also find that tax policy changes have disproportional effects across socioeconomic groups, and that this contributes to increasing inequality.","; ====, ====). Answering these questions is exceptionally timely, as they contribute to a broader debate about rising income and consumption inequality in the United States, and have been central in political debates.==== This debate includes academic studies focused on identifying the nature of these income shocks and evaluating their impacts on income inequality.====The extent to which tax policy affects the transmission of income shocks to consumption and inequality remains an open question. On one hand, the evidence in ==== suggests a limited role for tax policy. Specifically, they find that while tax policy dampens income shocks, a substantial portion of permanent income shocks is transmitted to consumption, and the transmission of these shocks has remained steady over time – even through major tax reforms, such as the Tax Reform Act of 1986. On the other hand, ==== finds that tax policy changes in the US substantially increased income inequality, using micro-data of US tax revenue to simulate changes between the pre- and post-tax incomes of taxpayers. To reconcile these two pieces of evidence, we combine these methods into a comprehensive model that deconstructs the effects of tax policy changes on the transmission of income shocks to consumption.====To get a more general perspective and gain traction on these important questions, we extend the literature in several ways. First, we expand the typical period analyzed to over four decades, covering 1968–2010, which enables us to cover a large number of minor and major tax policy changes. Second, to incorporate these additional years, we update the model that links transmission parameters with moments of income and consumption conditions. Third, we combine micro-simulations of changes in individual tax liabilities due to changes in tax policy, with a macro-labor model to directly link tax policy changes to the transmission of consumption from income shocks.====We find large changes in the transmission of permanent and transitory income shocks, which would not have been observable in the smaller period typically studied. In particular, we find that households have become more insured against permanent income shocks, but less insured against transitory income shocks over time. We also find that these changes were non-linear across income and education groups, revealing increased inequality in the transmission of income shocks to consumption.====We also find that there is more insurance to permanent income shocks than previously thought. This is in part due to the fact that the period typically studied (1979-1992) is an exceptional period with less insurance than the periods before or after it. We also show that previous estimates of this transmission might be limited by modeling choices and underlying assumptions, some of which might defy the data. Following pioneering work by ====, our model captures several key features of the data; for example, that some individuals are credit-constrained. We show that this generalization of the model leads to lower estimates of the transmission of permanent income shocks to consumption and, thus, greater insurance. Together, we find that 26% of permanent income shocks are transmitted to consumption, which is substantially lower than the 64% found by ====.====Additionally, we find that tax policy plays a major role in explaining the changes in the transmission of income shocks to consumption. These findings are the result of an important innovation in our model to describe tax policy changes. We decompose tax policy changes in three parts: ==== tax policy, ==== tax policy, and ==== changes in tax policy. Active tax changes measure the changes between pre- and post-tax incomes that are entirely due to legislative acts that affect tax rates or the tax code, such as the Tax Reform Act of 1986. These changes are the mechanical effects of tax policy found in studies that use micro-simulations of tax liability (====). In practice, the change in insurance due to active tax policy changes is measured in the simulation by holding income fixed to a base year and allowing the tax schedule to change as observed. Our simulations of active tax policy changes show a dramatic decrease in the amount of insurance the federal income tax provides in 1987, corresponding exactly with the Tax Reform Act of 1986.====Passive tax policy changes measure the changes in the difference between pre- and post-tax incomes that are entirely due to exogenous changes in the distribution of income across individuals and how this distribution interacts with the tax code; for instance, tax brackets. In practice, they are measured by keeping the tax code fixed to a base year while allowing the income distribution to change as observed. To further ensure that passive changes are not affected by the tax system, we remove changes in observed income due to labor responses to changes in taxation. The resulting measure of passive tax policy changes is more likely to represent exogenous changes in income distribution (e.g., technological growth). Behavioral changes represent all other changes including individual responses to active and passive tax changes, ====, aversion to risk, and individuals' perceptions of the tax system. To measure them, we perform a Oaxaca-Blinder decomposition.==== tax policy change where income tax rates remain at 15 and 25 percent, but incomes increase by 20 percent to $12,000 and $72,000 for the two income groups, respectively. Suppose that this increase in income is entirely due to exogenous changes (e.g., technological growth), and keeping constant taxpayers' behavioral responses to changes in income distribution. After-tax income inequality decreases to 98.5. In this scenario, even though the tax system did not change, its non-linearity contributed to a decrease in after-tax inequality.====We find large and opposite changes in active and passive tax policy from 1968 to 2010 that obfuscate the impact of tax policy in studies that do not separate these components. In particular, active tax policy changes increased the transmission – on average, across taxpayers – of both permanent and transitory shocks to consumption, by 35 and 47 percent, respectively. By contrast, passive changes in tax policy and behavioral changes operated in the opposite direction, sometimes more than undoing this increase.==== These findings reveal that decomposing into three distinctive parts the mechanism by which the tax system affects the transmission of income shocks to consumption is critical in order to adequately quantify the impact of tax policy changes. In particular, without the decomposition, researchers might have erroneously estimated that tax policy did not affect the transmission of income shocks to consumption, or that the insurance provided by the tax system did not change.====To better understand the role of tax policy on consumption inequality, we extend this analysis by considering how these estimates change by income quartile and educational attainment. In general, we find similar patterns across socioeconomic groups. Differences in magnitudes and non-monotonic differences, however, highlight the fact that tax policy has had a heterogeneous impact across the population and has contributed to increasing consumption inequality. In particular, the bottom income quartile and the least-educated groups are the only groups for which the transmission of permanent income shocks has hardly decreased over time, and their transmission is significantly larger than the other groups. Meanwhile, the transmission of transitory income shocks has increased proportionately more for the bottom and top income quartiles. We also find noticeable differences in the role of tax policy on different socioeconomic groups. For instance, active tax policy increased the transmission of permanent income shocks for all but the top income group.====Our findings suggest important considerations for future tax policy. On one hand, tax policy changes can substantially influence the transmission of income shocks to consumption. On the other hand, the income tax system impacts the insurance of consumption to income shocks in non-linear ways across population groups. It affects inequality in unintended ways due to the complexity of the interactions between the direct and indirect effects of tax policy.====In many ways, our paper responds to the call by ==== to extend existing empirical and theoretical models of insurance by further studying the relative role of various insurance mechanisms beyond self-insurance through borrowing/saving. We specifically focus on the complexity of the insurance mechanism that occurs through the federal income tax system and how this mechanism has changed over time. We believe that this analysis is an important contribution to our understanding of how income taxation affects household welfare.====This paper is structured as follows: Section ==== provides an overview of methods in the literature, and how we combine and extend these methods; Section ==== provides a detailed description of the theory and empirical approaches; Section ==== describes the data and the imputation strategy; Section ==== presents the results; and Section ==== concludes.====The following is the Supplementary material related to this article.",Taxation and inequality: Active and passive channels,https://www.sciencedirect.com/science/article/pii/S1094202520300971,21 October 2020,2020,Research Article,132.0
"Manuelli Rodolfo E.,Yurdagul Emircan","Washington University in Saint Louis and Federal Reserve Bank of Saint Louis, United States of America,Universidad Carlos III de Madrid, Department of Economics, Calle Madrid, 126, Getafe, 28903, Spain","Received 27 February 2019, Revised 25 September 2020, Available online 20 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.002,Cited by (2),"We develop a ==== model to study the impact of HIV/AIDS on human capital accumulation decisions and on aggregate output. We calibrate the model targeting a sample of Sub-Saharan African countries, separately for 1995-2004 and 2010-2017. Our results show that the scale-up of ART treatments since mid-2000s, and the additional reductions in the transmission rate have had a significant impact on output. Counterfactual exercises suggest significant output gains from further increasing ART treatment and decreasing infectiousness. The output gains are substantially larger than the costs. We find that treatment and policies that reduce infection (e.g. condom use) are substitutes.","What is the potential impact on output of policies that reduce the incidence of HIV/AIDS? The empirical literature that measures the contribution of improved health to economic performance has not produced, so far, definitive results. Studies that regress output on some measure of health tend to find large economic returns from health improvements (e.g. Sachs (2003); Audibert et al. (2011)). Since, by its very nature, that approach is subject to the possibility of significant biases, it is important to contrast the findings with the predictions of more detailed models. The results from this approach are mixed. Some macro models that allow for changes in population but model health improvements in a very stylized manner find that the economic impact of improving “health” is either relatively modest (e.g. see Bloom and Mahal (1997), Well (2007), and Ashraf et al. (2008)) or directly negative (i.e. Acemoglu and Johnson (2007)). The findings of micro based studies (see the summary in Bleakley (2010)) suggest that, in the case of some diseases (e.g. malaria and hookworm), the economic benefits of eradication are substantial.====At the macro level there is a large literature that supports the view that cross-country differences in human capital are an important source of variation in output per worker.==== In this paper we study a general equilibrium model of human capital accumulation in which individuals face three sources of uncertainty. The first source is death which arrives with a state contingent probability. The second source is transitioning from a healthy state to being infected with HIV/AIDS state. The third source is whether the infected individuals receive treatment upon infection. Infection without treatment is characterized by lower life expectancy and higher morbidity. Individuals take this uncertainty as given. Following Ben-Porath (1967) they decide how much human capital to accumulate to maximize the life-time income. Individuals choose the length of schooling, the time invested in on-the-job learning, and investments in human capital in terms of goods both during schooling and on-the-job. Changes in the disease environment (e.g. in the probability of receiving ART treatment) have effects at the individual level (changes in human capital accumulation) as well as aggregate effects (changes in the distribution of the population that affect endogenous infection rates and aggregate human capital).====We calibrate the model using a sample of 25 Sub-Saharan African countries for two different periods: 1995-2004, which is the pre-ART period, and 2010-2017. This specification allows us to study the impact of ART treatment on health and economic outcomes. We find that the HIV-related changes contribute significantly to the output per worker growth in our sample, and can partially account for the observed growth rates in the data. The model predicts that if all that changed between the two periods was the HIV environment, the average growth rate of output per worker in our sample would have been 12 percent.====Motivated by these findings we conduct two experiments. We explore the consequences of scaling up the ART treatment, and of reducing the degree of infectiousness in person-to-person interactions. Given the non-linearity of the model we explore different levels of intervention. We find that, for each country, the impact measured in elasticity form is roughly constant, that is independent of scale, in both types of interventions.====Our results suggest that the output gains from these experiments can be significant. Increasing treatment probabilities by 50 percent leads to output per worker gains around 5-7 percent for five countries, with an average gain of 1.8 percent in our sample. At the other end, reducing the infectiousness rate by 50 percent would generate output gains of 10-20 percent for three countries, and an average gain of 4.5 percent in the sample. Nevertheless, we find large variation in the predicted impact of both sets of experiments on aggregate output.====We provide some preliminary estimates of the costs of interventions that would correspond to our experiments. In the case of treatment we estimate the costs of ramping up ART treatment. In the case of policies that can potentially decrease the rate of infection we consider the costs of promoting condom use. We find that the costs of all our interventions are relatively small, with cost - gain ratios below 8 percent in all experiments. The average cost-gain ratio is below 2 percent for all interventions.====Finally, we study the determinants of the output elasticities with respect to the interventions. We find that initial conditions of the disease environment matter for the returns from these experiments. We find that policies that reduce infectiousness and policies that increase treatment are substitutes in terms of their impact on output per worker.====Our work is related to a number of papers that study the impact of HIV/AIDS on human capital accumulation and aggregate performance. Corrigan et al. (2005) study a two period OLG model of the impact of a parent's death —which they identify as AIDS related— on the human capital of the children and trace the aggregate impact. Bell et al. (2006) study a similar model but they allow parents to choose investment in child rearing and education. The model is used to simulate the impact of different policies on the South African economy and the effects are significant. More recently, Ferreira et al. (2011) develop an OLG model that also captures some aspects of the impact of HIV/AIDS on human capital accumulation. There are some important differences between those papers and our work. First, the assumption of two or three period lived individuals necessitates making extreme assumptions to calibrate the models as one period can be either 20 or thirty years. Second, there are significant differences in the human capital accumulation function as we do not assume an externality but parents' investment in their children's education follows standard cost benefit analysis. Moreover we provide a more general formulation along the lines of the Ben-Porath model that allows us to derive implications for schooling that are absent in those models. Finally, we conduct a number of experiments that require a rich life-time framework beyond the reach of a simple two or three period OLG model.====Overall we view our work as complementing the existing body of research by providing a more realistic life cycle model that allows us to derive (and check) the implications of different interventions on variables other than those AIDS related (e.g. schooling). Importantly, we assume that individuals take the features of the disease environment as given. The risk of infection for healthy adults is endogenous, as it depends on the number of healthy as well as the infectious adults. However, this risk is homogeneous among the healthy group, and we capture the transmission rates, conditional on the distribution of health status within the economy, with a parameter in reduced form. In a recent paper, Greenwood et al. (2019) study the optimal risk taking by modeling the specifics of sexual behavior, though abstract from human capital accumulation. In turn, we see our results also complementary to their work.====In Section 2 we describe the model. Section 3 describes the model calibration and discusses the model implications for a few non-targeted features of interest. Section 4 gives an accounting exercise for the observed changes between 1995-2004 and 2010-2017 in our sample. Section 5 contains the results of our experiments and Section 6 offers some concluding comments.","AIDS, human capital and development",https://www.sciencedirect.com/science/article/pii/S1094202520300958,20 October 2020,2020,Research Article,133.0
"Chiu Jonathan,Molico Miguel","Bank of Canada, 234 Wellington Avenue, Ottawa, ON K1A 0G9, Canada","Received 9 September 2018, Revised 13 October 2020, Available online 20 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.009,Cited by (0),"We study the short-run effects of ==== to solve the model. We find that a one-time expansionary monetary policy shock has persistent positive effects on output, prices, and welfare, even in the absence of nominal rigidities. Furthermore, the effects of positive and negative monetary shocks are typically asymmetric. Negative (contractionary) shocks have bigger effects than positive (expansionary) shocks. In addition, in an economy with larger shocks, the responses tend to be disproportionately larger than those in an economy with smaller shocks. Finally, the effectiveness of monetary shocks depends on the steady-state level of ","It is a common belief that monetary policy has real and persistent short-run effects. However, the channels through which it operates are still not well understood. Most literature has focused on the role of nominal rigidities in explaining these effects in the context of representative agent models. More recently, some work has focused on the redistributive channel of monetary policy in heterogeneous agents models with uninsurable idiosyncratic risk. In this paper, we study the short-run effects of monetary policy in a search-theoretic monetary model in which agents are subject to idiosyncratic liquidity shocks as well as aggregate monetary shocks. Namely, we analyze the role of the endogenous non-degenerate distribution of liquidity, liquidity constraints, and decentralized trade, for the transmission and propagation of monetary policy shocks.====Starting from the seminal work of Kiyotaki and Wright (1989), a large body of research has been dedicated to develop micro-founded monetary models with decentralized trade to study many theoretical and policy questions. In this class of models, the same frictions that give a role for money will limit the ability of agents to insure against idiosyncratic shocks, typically leading to a non-degenerate distribution of money.==== Monetary policy can have long-run and short-run real redistributive effects in these economies. Most of the studies have focused on long-run (steady-state) analysis. For example, Deviatov and Wallace (2001) and Molico (2006) show that redistributive expansionary monetary policy can increase output and welfare.==== However, due to technical difficulties, the short-run implications of such models have not been fully explored. The key reason is that the distribution responds endogenously to aggregate shocks and it is technically challenging to keep track of its evolution. The distribution is particularly important in this environment because, unlike in models of centralized trade, agents in a search-theoretic monetary model are subject to random pairwise exchanges and they need to know, not only one market price, but the whole distribution of prices to make their consumption, saving and portfolio decisions. For this reason, the short-run dynamics of search-theoretic models are still under-examined. Some exceptions, discussed in section 2, are Rocheteau et al. (2018, 2019) and Jin and Zhu (2019).====In this paper, we extend Molico (2006) by incorporating aggregate monetary policy shocks and study the short-run redistributive effects and the internal propagation mechanism of a random-matching model of money. In the model economy, agents are subject to pairwise trading and idiosyncratic uncertainty with regards to their trading opportunities. Heterogeneous idiosyncratic trading histories imply a non-degenerate distribution of money holdings which becomes a state variable summarizing the past histories. Monetary policy, modeled as lump-sum transfers, has redistributive and persistent effects on output and prices – aggregate shocks propagate and diffuse gradually as the money distribution adjusts over time.====As is well known, solving models with heterogeneous agents and aggregate uncertainty constitutes a technical challenge since the set of state variables contains the cross-sectional distribution of agent's characteristics, which is a time-varying infinite dimensional object in the presence of aggregate uncertainty. The bilateral random matching and decentralized trade nature of a search model makes it particularly difficult, and computationally prohibitively costly, to apply the standard approach used in the Heterogeneous Agents New-Keynesian (HANK) models literature to compute the stationary distribution and the transitional dynamics.==== We contribute to the literature by developing numerical algorithm in the spirit of Algan, Allais and Den Haan to analyze the short-run distributional effects of monetary policy in random-matching models. In a search-theoretic model of money, the assumption of random bilateral matching implies that the agents must form an expectation regarding the potential gains from trade in order to solve their problems. We follow Algan et al. (2008) and parameterize the cross-sectional distribution using a polynomial that matches a set of moments of the distribution. This allows us to compute such expectation using quadrature technique without introducing a significant approximation error. Furthermore, we reduce the number of moments that we keep track as state variables by introducing the notion of reference moments as in Reiter (2002).====We solve for the recursive stochastic equilibrium of the model to study the dynamic responses to money growth shocks. In particular, we analyze the impulse response of the distribution of money, output, velocity of money, prices and welfare to a one-time money growth shock. Furthermore, we study how these responses depend on the direction (positive versus negative) and magnitude of the shocks (small versus large). Finally, we also study how the effect of money growth shocks depend on the level of trend inflation and the degree of search frictions.====We find that a one-time expansionary monetary policy shock has persistent positive effects on output, prices, and welfare, even in the absence of nominal rigidities. Furthermore, the effects of positive and negative monetary shocks are typically asymmetric. Negative (contractionary) shocks have bigger effects on output than positive (expansionary) shocks. In addition, in an economy with larger shocks, the responses tend to be disproportionately larger than those in an economy with smaller shocks. Finally, the effectiveness of monetary shocks depends on the steady-state level of inflation. The higher the average level of inflation (money growth), the bigger the impact effect of a shock of a given size but the smaller its cumulative effect. These results are consistent with existing empirical evidence.====Several explanations have been put forward to explain the empirical evidence on the asymmetric effects of monetary policy (see Morgan (1993)). The two most prominent ones have focused on the role of credit rationing and of downward price rigidities. Our paper highlights an alternative channel. In our model, the real asymmetric effects of monetary policy arise from the interaction between the endogenous non-degenerate distribution of liquidity and the endogenous trading decisions of the agents, in particular, their willingness to spend. Agents face idiosyncratic trading risk, and the accumulation of money balances is the only form of insurance available. As such, as their liquidity decreases they will be increasingly less willing to spend. By moving poor agents away from their liquidity constraints, a positive money growth shock has an expansionary effect, while a negative shock generates the opposite effect. Agents' response, however, is non-linear. Specifically, an individual responds more strongly to a tightening of the liquidity constraint, than to a loosening of it. Furthermore, the strength and the degree of asymmetry of the aggregate response depend on the endogenous fraction of agents with a tight liquidity constraint. This fraction, in turn, depends on the endogenous decision of the agents, influenced by exogenous factors such as the level of trend inflation and the degree of search frictions. Also, the persistence of these effects depends not only on exogenous search frictions, but also on the endogenous spending pattern chosen by the agents. Our results highlight the fact that explicitly modeling the frictions that give rise to money and the exchange process is not only important for the long-run analysis, but also important for understanding the short-run effects of monetary policy.====The rest of the paper is organized as follows. Section 2 discusses how our model and findings relate to the New-Monetarist and HANK models literature. Section 3 describes the model environment. Section 4 defines a recursive equilibrium and provides some partial analytical characterization. Section 5 defines the approximate economy and briefly discusses in the numerical algorithm used to compute the recursive equilibria. In section 6, we parameterize, characterize the equilibrium and analyze the effects of monetary policy shocks. Section 7 concludes the paper.",Short-run dynamics in a search-theoretic model of monetary exchange,https://www.sciencedirect.com/science/article/pii/S1094202520301022,20 October 2020,2020,Research Article,134.0
"Glitz Albrecht,Vejlin Rune","Universitat Pompeu Fabra, Barcelona GSE and IPEG, Spain,Department of Economics and Business Economics, Aarhus University, Denmark","Received 2 April 2019, Revised 6 October 2020, Available online 15 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.007,Cited by (3),"In this paper, we study the role of coworker referrals for labor market outcomes. Using comprehensive Danish administrative data covering the period 1980 to 2005, we first document a strong tendency of workers to follow their former coworkers into the same establishments and provide evidence that these mobility patterns are likely driven by coworker referrals. Treating the presence of a former coworker in an establishment at the time of hiring as a proxy for a referral, we then show that referred workers initially earn 4.6 percent higher wages and are 2.3 percentage points less likely to leave their employers than workers hired through the external market. Consistent with a theoretical framework characterized by higher initial uncertainty in the external market but the possibility of subsequent learning about match-specific productivity, we show that these initial differences gradually decline as tenure increases. We structurally estimate a stylized model using indirect inference and find that the noise of the initial signal about a worker's productivity is 14.5 percent lower in the referral market than in the external market, and that firms learn about their workers' true match-specific productivity with a probability of 48.4 percent per year. Counterfactual simulations show that average wages are lower in the absence of a referral market, primarily because of lower average match productivity in the external market.","Survey evidence from many countries in the world consistently shows that between a third and half of all jobs are found through social contacts. One important mechanism through which such contacts can facilitate the job search process is by means of a referral. Referrals provide employers looking to fill a vacancy with information about potential applicants that they otherwise would not have. Workers hired as the result of a referral are therefore likely to be better matched to their employers than workers hired through more formal channels.====In this paper, we study the role of referrals for workers' labor market outcomes. To motivate our empirical analysis, we present a stylized theoretical search model that is characterized by initial uncertainty and subsequent learning about workers' match-specific productivity. Firms can hire workers either through the referral market or through the external market. When workers and firms meet, they observe a noisy signal of their true match-specific productivity which is assumed to be less precise in the external market than in the referral market. As a result of this lower uncertainty, workers hired through a referral are initially better matched to their employers, which is reflected in higher starting wages and an initially lower separation probability. Due to learning and the successive dissolution of bad matches, these initial wage and turnover differences decline as tenure in the firm increases.====In the first empirical part of the paper, we then test the dynamic predictions of our theoretical model using comprehensive Danish administrative data covering the period 1980 to 2005. Since actual referrals are not observed in this type of data, we use a proxy that is based on the observation that, through coworker-based referrals, workers have a tendency to start their new jobs in firms in which a former coworker is already present. We first document that this type of mobility pattern is indeed linked to the prior personal interaction of workers and cannot be explained by random mobility or similarity in observable skills between workers and their former colleagues. Building on this result, we use an indicator for starting a new job in a firm with a former coworker present as a proxy for having obtained the job through a referral, and estimate non-parametric convergence profiles for both wages and job turnover probabilities as a function of tenure.====Our empirical analysis yields a number of results. First, we find that the probability of starting a new job in a firm with a former coworker already present is significantly higher than would be expected under a random allocation of workers to firms, both unconditional and conditional on workers' observable skills. This finding lends support to our assumption that the presence of a former coworker in the new firm of a worker can serve as a proxy for having obtained the job through a referral. Our main estimation results then show that, in the first year after being hired, workers recruited through the referral market earn 4.6 percent higher wages and are 2.3 percentage points less likely to leave their employer than workers hired through the external market. As predicted in the presence of learning about match-specific productivity, these differences gradually decline over time and stabilize in the long run, with most of the learning taking place within the first six years of the employment spell. We then show that the main mechanism driving these convergence patterns is the more pronounced weeding-out process of bad matches among the group of workers hired through the external market. In contrast, the contribution of differential wage growth among workers who remain with their hiring firms in the long run is relatively limited.====Distinguishing workers with different levels of education reveals that the initial wage gains are more pronounced for high-skilled workers, which suggests that the initial uncertainty about the skills of these types of workers is larger, possibly because of the more complex jobs they tend to fill. This hypothesis is also consistent with the slower speed of learning we find in our structural estimation for high-skilled workers in comparison to low-skilled workers. Corresponding results for workers belonging to different age groups do not show major differences in initial wages and turnover probabilities but suggest that young workers hired through the referral market have on average a lower productivity than their counterparts hired through the external market.====Analyzing the relationship between referral provider and referral receiver, we show that referrals from coworkers with whom prior interaction was more intensive tend to amplify the initial wage and job stability gains, consistent with the idea that a higher contact intensity in the past improves the quality of information the referral provider is able to pass on when making a referral. We also document that referrals from older and more highly educated coworkers are particularly valuable in the job search process.====In the second part of the paper, we estimate the structural parameters of our model by indirect inference, matching the tenure-specific empirical moments from the reduced-form estimations to their simulated model counterparts. The estimates reveal that the uncertainty about match-specific productivity in the referral market is around 14.5 percent lower than in the external market, indicating that referrals provide additional information to hiring employers. Furthermore, our findings show that learning takes place at a relatively fast pace, with a 48.4 percent probability that a worker's true productivity is revealed in any given year. We estimate that average match-specific productivity in the referral market is 14.0 percent higher than in the external market suggesting that referrals allow firms to tap into a better pool of job applicants. To conclude, we use the model to perform counterfactual simulations, which show that average wages would be 10.4 percent lower in the absence of a referral market, primarily because of the lower average match-specific productivity in the external market and, to a lesser extent, differences in the level of uncertainty.====While matching the overall convergence patterns well, the baseline model does not provide a good fit for the difference in wages between referred and externally hired workers at high levels of tenure, which is the primary statistic identifying the average difference in the underlying productivity distributions. We therefore estimate an alternative model, which puts more weight on these long-run differences in wages. Using this model, which fits this feature of the data well, we estimate that the average match-specific productivity of workers in the referral market is about 7.6 percent higher than in the external market (compared to 14.0 percent in the baseline model) and that average wages would be 3.9 percent lower if average productivity and uncertainty in the referral market were to be the same as in the external market. While this is still a sizeable effect, the decline in magnitude relative to the baseline estimate of 10.4 percent indicates that the latter is most likely an upper bound.====The particular focus on referrals by former coworkers in our analysis is motivated by the important role these types of referrals seem to play in the labor market (see, e.g. Granovetter, 1995). In a recent study, Eliason et al. (2019) exploit rich Swedish administrative data to provide the first comparative evidence on the relative importance of different types of social networks for individual labor market outcomes, documenting a prominent role for former coworkers that is only surpassed by that of immediate family members. Presumably, the usefulness of coworkers as providers of referrals (as in Montgomery, 1991, Simon and Warner, 1992, Galenianos, 2013) or sources of information about available job opportunities (as in Calvó-Armengol and Jackson, 2004, Wahba and Zenou, 2005, Boucher and Goussé, 2019) is due to the fact that, by having worked together in the past, coworkers tend to possess better knowledge about their social contacts' respective skills and are more aware of suitable job openings than other types of social contacts such as former classmates, friends or neighbors, who often lack the professional interaction on the job and the attachment to the relevant labor market segment (see Antoninis, 2006).==== In recent years, a number of studies have therefore analyzed in more detail this type of social connection and its role for labor market outcomes (see, for instance, Cingano and Rosolia, 2012, Hensvik and Skans, 2016, Glitz, 2017, Saygin et al., 2019).====Contrary to much of this literature, which has focused on the role of coworker-based networks in increasing job offer arrival rates (Cingano and Rosolia, 2012, Glitz, 2017, Saygin et al., 2019), we study the role of ==== referrals in reducing information frictions in the hiring process. Our empirical work is guided by a Jovanovic-type learning model, initially adapted to the referral context by Simon and Warner (1992) and then substantially extended by Dustmann et al. (2016), in which initial uncertainty coupled with subsequent learning about match-specific productivity is generating distinct patterns of wages and job turnover as a function of tenure. Our empirical analysis is closely related to the work by Burks et al. (2015), Brown et al. (2016) and Dustmann et al. (2016) who study the main predictions of the same learning model but follow different strategies than ours to identify jobs that have been obtained through a referral. The main analysis in Dustmann et al. (2016) relates the share of ==== in a firm located in a large metropolitan area at the time of hiring to a worker's subsequent labor market outcomes and provides both a theoretical and empirical link between this share and the likelihood of having obtained the job through a referral, allowing the estimated effects to be translated into referral effects. In addition, they also provide some evidence using actual referrals by linking the responses from a recent survey to the German administrative data. However, the number of observations in this linked data set is relatively small so that the scope of the analysis based on actual referrals remains relatively limited. Both Brown et al. (2016) and Burks et al. (2015) use data on actual referrals from a single or small number of large firms to test the predictions of the learning-based model against other types of labor market referral models.====Our analysis contributes to this literature in a number of ways. First, we are the first to systematically analyze the role of coworker-based referrals within the learning-based model framework of Simon and Warner (1992) and Dustmann et al. (2016). Given the importance of coworker referrals in many labor markets around the world, it is important to understand how they affect workers' outcomes and whether their role differs from that of other types of referrals. Second, thanks to population-wide data from Denmark, we can test the predictions of the model in a more comprehensive way than existing studies, for example regarding the precise shape of the convergence profiles and the role of different mechanisms in generating these profiles. Third, in contrast to existing studies, our analysis is the first to cover an entire national labor market rather than specific individual firms or segments of the labor market. Such representativeness is vital when it comes to evaluating the economy-wide gains from referrals given their heterogeneous impacts across different types of workers, firms and industries. Fourth, we document for the first time that workers encounter former coworkers in their new jobs at a rate higher than chance, providing an empirical basis for such movements to be interpreted as the result of referrals. Finally, we are the first to perform a full structural estimation of the model parameters of interest, shedding light on important features such as the speed of learning, average match-specific productivity and the degree of uncertainty about workers' skills in different markets. Since, contrary to many other network dimensions, coworker relationships are nowadays readily observable in most administrative data sets, our analysis can provide a foundation for future work on this topic, both theoretical and empirical.====The paper is structured as follows. In the next section, we present the main features of our theoretical framework. We then discuss our econometric model in Section 3. In Section 4, we provide information about the data used in the empirical analysis and summarize their main features. In Section 5, we show evidence that speaks to the appropriateness of using the presence of former coworkers as a proxy for a referral. In Section 6, we present our main empirical results. Finally, in Section 7, we describe the calibration and summarize the key findings from our structural model estimation. Section 8 concludes the paper.",Learning through coworker referrals,https://www.sciencedirect.com/science/article/pii/S1094202520300983,15 October 2020,2020,Research Article,135.0
"Rocheteau Guillaume,Hu Tai-Wei,Lebeau Lucie,In Younghwan","University of California, Irvine, United States of America,University of Bristol, United Kingdom of Great Britain and Northern Ireland,Korea Advanced Institute of Science and Technology, Republic of Korea","Received 8 July 2020, Revised 12 October 2020, Available online 15 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.008,Cited by (5),"We introduce a new approach to bargaining, with strategic and axiomatic foundations, into models of decentralized asset markets. According to this approach, which encompasses the ","Modern monetary theory and financial economics formalize asset trades in the context of decentralized markets with explicit game-theoretic foundations (e.g., Duffie et al., 2005; Lagos and Wright, 2005). These models replace the elusive Walrasian auctioneer by a market structure with two core components: a technology to form pairwise meetings and a strategic or axiomatic mechanism to determine prices and trade sizes. This paper focuses on the latter: the negotiation of asset prices and trade sizes in pairwise meetings.====Going back to Diamond (1982), the search-theoretic literature has placed stark restrictions on individual asset inventories, typically ====. As a result, in versions of the model with bargaining (e.g., Shi, 1995; Trejos and Wright, 1995; Duffie et al., 2005), the only item to negotiate in pairwise meetings — the ==== of the negotiation — is the price of an indivisible asset in terms of a divisible commodity.==== Recent incarnations of the model (surveyed in Lagos et al., 2017) allow for unrestricted portfolios of divisible assets, ==== with ====. A key conceptual difference when ==== is that the agenda of the negotiation is not unique. Any ordered partition of ==== constitutes an agenda, where the elements of this partition correspond to items to be negotiated sequentially. For instance, agents can sell their whole portfolio at once, as a large block, or they can partition their portfolio into bundles of varying compositions and sizes to be added to the negotiation table one after another.====The possibility of negotiating asset sales according to different agendas raises several questions regarding trading strategies and price formation in decentralized asset markets. Do agendas matter for asset prices and trade sizes when agents have perfect foresight and information is complete? What is the optimal strategy of the asset owner to partition his portfolio, e.g., should the portfolio be negotiated as a whole or divided into smaller bundles? What is the relation between the bargaining problem with an agenda and the bargaining solution of Nash (1950)?====Our contribution is to introduce a new and generalized approach to bargaining over portfolios of assets in models of decentralized asset markets with the notion of agenda at the forefront, under both strategic and axiomatic foundations. The paper is composed of two parts. The first part provides a detailed description of bargaining games with an agenda and derives a series of methodological results that will be useful to incorporate these bargaining games into a general market structure. The second part focuses on the general equilibrium and derives some implications of the agenda of the negotiation for asset prices, allocations, and welfare.====We start with a simple agenda that partitions a portfolio of homogeneous assets into ==== bundles of equal sizes. This agenda is a natural extension of the negotiation in Shi (1995) and Trejos and Wright (1995), where the indivisible asset is now interpreted as a bundle of divisible assets. The extensive-form bargaining game, called the alternating-ultimatum-offer game, is composed of ==== rounds. In each round, one asset bundle is up for negotiation. One player makes an ultimatum offer, and the identity of the proposer alternates across rounds. Agents are forward-looking and can anticipate the outcomes of future rounds. In contrast to the Rubinstein game, our game is nonstationary, since the amount of assets left for negotiation decreases over time, and it admits a unique subgame-perfect equilibrium (SPE) characterized by a system of difference equations with initial condition allowing us to compute the terminal allocation in closed-form for all ====.====The limit as ==== goes to infinity is called the ====. It gives a simple and intuitive relationship between asset prices and trade sizes, and it has properties distinct from the Nash (1950) solution that make it tractable for general equilibrium analysis, including monotonicity and concavity of trade surpluses with respect to trade size. Moreover, it coincides with the axiomatic ordinal solution of O'Neill et al. (2004) where an agenda is defined as a collection of Pareto frontiers indexed by time.====In order to relate our approach to the Nash solution, commonly used in the asset market literature (e.g., Duffie et al., 2005; Lagos and Wright, 2005), we extend our ====-round game by assuming that in each round agents play an alternating-offer game with risk of breakdown, as in Rubinstein (1982). The equilibrium allocation of this ====-round game is obtained by applying the Nash solution consecutively ==== times, where the solution in one iteration becomes the disagreement point of the next iteration. We characterize the outcome in closed form for all ==== and show it coincides with the Nash solution and the gradual solution in the two limiting cases ==== and ====, respectively. We endogenize the agenda of the negotiation by letting asset owners choose ==== to maximize their surplus from trade. The optimal choice is ====, i.e., it is optimal for the owner to add assets on the bargaining table gradually, one infinitesimal unit at a time.====The second part of the paper incorporates bargaining solutions with an agenda into a general equilibrium model of decentralized asset markets with endogenous portfolios along the lines of Lagos and Wright (2005) and Lagos and Zhang (2020). The equilibrium under Nash bargaining (====) features asset misallocation: a fraction of the asset supply ends up being held by agents with no liquidity needs. In contrast, under gradual bargaining (====), the first best is implemented as long as the asset supply is sufficiently abundant. In the case of fiat money, the optimal policy, the Friedman rule, generates the first best under gradual bargaining for all bargaining powers whereas it fails to do so under generalized Nash bargaining as long as producers have some bargaining power. Using the same calibrated parameter values as in Lagos and Wright (2005), going from ==== to ==== increases output and consumption at the optimal policy by 76%. Even a moderate increase from ==== to ==== raises output by 39%.====This finding is especially stark in a monetary version of the model where agents trade short-lived assets that they value according to linear preferences, e.g., as in the model of OTC market of Lagos and Zhang (2020). We allow agents to choose how much of their short-lived assets to bring into a match. Under Nash bargaining, the OTC market shuts down for all interest rates and the equilibrium achieves its worst allocation. This result is a direct consequence of the non-monotonicity of agents' surpluses with respect to the quantity of goods or assets that they bring to the negotiation table. Under gradual bargaining, the OTC market is active and the equilibrium achieves first best for all interest rates below a positive threshold.====Finally, we extend our environment to allow for any arbitrary number of assets. All assets, except fiat money, generate the same stream of dividends. The notion of agenda allows us to introduce a new asset characteristic – negotiability – defined as the inverse of the amount of time required for the sale of each unit of the asset to be finalized, e.g., each asset added to the negotiation table needs to be authenticated and ownership rights take time to transfer.==== Our model generates an endogenous pecking order: assets that are more negotiable are put on the negotiating table before the less negotiable ones. In equilibrium, the most negotiable assets have lower rates of return and higher velocities. Hence, our model explains rate-of-return differences of seemingly identical assets. As the time horizon of the negotiation becomes arbitrarily large, differences in rates of return vanish but differences in velocities persist. We discuss the potential of our model to address two puzzles in monetary theory, the rate-of-return dominance puzzle and the indeterminacy of the exchange between two fiat currencies.",Gradual bargaining in decentralized asset markets,https://www.sciencedirect.com/science/article/pii/S1094202520301010,15 October 2020,2020,Research Article,136.0
Gorn Alexey,"University of Liverpool Management School, Chatham Street, Liverpool, L69 7ZH, United Kingdom","Received 26 September 2018, Revised 9 October 2020, Available online 14 October 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.10.006,Cited by (2),"This study relates the increase in the U.S. top wages to the increasing prominence of headhunters. Headhunters improve the matching between firms and employees via two channels: screening of candidates and passive on-the-job search. I incorporate headhunters in the labor market framework of random search with two-sided heterogeneity. The calibrated model shows that headhunters can account for 32% of the increase in the top 10% wage share in the U.S. from 1970 to 2010, with 19% due to improvements in matching between workers and firms. I provide supporting micro evidence for CEO compensation, as well as cross-country evidence on headhunter hires/fees and top income growth.","Wages at the top of the distribution have been rising sharply in the United States since the early 1970s. Top 10% wage share==== increased from 25.7% in 1970 to 34.5% in 2010 in the U.S. (Piketty (2014)). One of the reasons for the rise of top wages and wage dispersion is improved matching between firms and employees in the top positions==== (Song et al. (2019)). Why did the matching improve? The conventional view attributes the improvement in matching to skill-biased technological change which raises incentives for firms and workers to be better matched. However, while this explains the rise of the upper-middle class, it cannot explain the sharp rise of top wages. Song et al. (2019) show a strong non-linearity in the sorting pattern====: a disproportional shift of high-skilled workers to high-paying firms in comparison to medium-skilled workers to medium-paying firms, and this cannot be generated by technological progress in models of labor markets.====In this paper, I show that the improved matching at the top is due to decreasing search frictions in the labor market for top positions. I develop a model where frictions are reduced by the increasing role of headhunters, or executive search firms. Headhunters started to gain market share in the U.S. in the 1970s and now assist in filling more than half of the positions in the top wage segment. They enhance matching for two reasons. First, they provide more suitable candidates for the firm because they can screen the candidates better. Second, they induce passive on-the-job search as they contact potential candidates directly, creating opportunities for new matches without active search from employed workers.==== As a result, the headhunters restrict the pool of potential candidates to only the high-skilled workers while, at the same time, expand the pool of potential candidates to a larger number of those high-skilled workers. These two features result in better matching between high-skilled workers and firms, a higher surplus and, therefore, a higher wage in such matches. Because headhunters operate mostly on the top wage segment, such improvements in matching do not happen (or happen to a lower degree) over the rest of the distribution. Therefore, the presence of headhunters generates a strong non-linearity in sorting improvement that leads, in turn, to soaring of top wages compared to the rest of the distribution. The decrease in the search frictions also changes the outside options of workers and firms affected by the headhunters. The changes in the outside options affect the wage bargaining outcome and increase the top wages even further at the same time compressing the wages at the bottom.====To quantify the contribution of better matching induced by headhunters to the increase in top wages, I develop a labor market model along the lines of Pissarides (1985) augmented with heterogeneous workers and firms.==== I introduce the headhunter industry by adding a new channel for matching workers and firms. Firms with an open position can either post a vacancy as in the standard model or hire through a headhunter. The difference for the firm is that it cannot screen workers coming through vacancies, while the headhunter guarantees a minimal skill level of the worker with whom the firm is matched. Consider now the worker's side. Low-skilled workers have access to the standard channel and they can search from both unemployment and employment. Every worker searching through the standard channel has to pay a per-period search cost and, therefore, search “actively”. For high-skilled workers, instead, on top of the active search, there is also a possibility of “passive” search. A worker is searching passively if she agrees to consider an offer when a headhunter calls. Screening and passive search match exactly the two main features of the headhunter industry. To study the third feature of headhunters, the role in wage setting, I use period-by-period wage bargaining for wage determination. I then isolate the effects of matching on wage distribution by closing the wage bargaining channel.====Having set up the model, I apply the following calibration strategy. First, I calibrate the model without headhunters to match moments of the wage distribution and aggregate labor market moments in the U.S. in the 1970s. The key calibrated parameters include those characterizing the exogenous distributions of workers over skills and firms over productivity. The U.S. labor market in the 1970s is well approximated by the model with no, or limited, role for headhunters. Having fixed the parameters not related to the headhunters, I then introduce the headhunter channel to the model and calibrate the related parameters to target the moments of the headhunter industry in the 2010s as well as the change in the joint distribution of top workers and firms between the 1980s and 2000s.==== At the same time, I introduce skill-biased technological change to match the increase in the 90/50 wage ratio from 1970 to 2010. I do it by increasing the degree of complementarity in the production function.==== Having both mechanisms in the model allows me to evaluate the relative contribution of each mechanism to the change occurring on different parts of the wage distribution.====My calibration strategy answers the question how would the distribution of wages (and, therefore, also the top wages) have changed between the 1970s and 2010s if skill-biased technological change and headhunters had been the only factors raising top wage inequality. To assess the relative contribution of the two factors, I then shut down one channel at a time: the skill-biased technological change or the headhunter channel. I also give a chance to skill-biased technological change to explain all the increase in wage inequality without the headhunters. To do that, I change the increase in the degree of complementarity to match the increase in the 90/50 wage ratio and assess how the model fits other moments. To further study the mechanisms I then exploit the richness of the model and compare several statistics related to the wage distribution with and without the headhunters. Importantly, I perform experiments in line with Song et al. (2019) and compare results from the model-generated data to the U.S. data. This allows me to see whether the improvement in matching in the model has similar features to the observed improvement.====The main quantitative result of the paper is that the rise of headhunters accounts for 32% of the increase in the top 10% wage shares in the U.S. from the 1970s to 2010s. Out of 32%, the improvement in the matching between workers and firms accounts for 19% and the change in wages due to the improvements in outside options accounts for the other 13%. Skill-biased technological change contributes to another 23% of the top 10% wage share increase, and the interaction between the two factors raises the top 10% share by 3%.==== The sharp increase of top wages in the model is mainly due to improved matching after the introduction of headhunters. Comparing joint distributions of worker-firm matches in the two steady states reveals a pattern similar to empirical results of Song et al. (2019),==== where most types of firms lose the highest-skilled workers and where the highest-paying firms gain those workers disproportionately. The headhunter channel generates the strong non-linearity in the change in assortative matching observed in the data, with a disproportionate improvement in matching for highest-skilled workers. I am not aware of other theoretical models able to generate such non-linearity. If I allow the model to match the increase in the 90/50 wage ratio without the headhunter channel, the model explains a smaller share of the increase in the top 10% wage share but overshoots the 50/10 wage ratio. This happens exactly because of the absence of a strong non-linearity of the skill-biased technological change. An increase in complementarity shifts the whole distribution of wages to the right. The non-linearity generated by the headhunters allows shifting the right tail of the distribution farther apart from the rest of the distribution without changing the shape of the distribution in the middle.====The model relates to other theoretical models showing the importance of assortative matching for wage distribution. Bagger and Lentz (2019) is the closest study. They show that on-the-job search is a crucial mechanism to generate assortative matching in a Diamond-Mortensen-Pissarides model with two-sided heterogeneity. Bagger and Lentz (2019) consider only active on-the-job search. Uren and Virag (2011), instead, show that skill requirements are important to generate an increase in between-group inequality (increased differences between wages of workers with different skill level). Skill requirements play a similar role as does the screening by headhunters. Uren and Virag (2011) study the overall shape of the wage distribution, while this paper focuses on headhunters and the top part of the distribution.====This paper presents two blocks of independent empirical evidence supporting the mechanism. First, it uses micro-level data on CEO compensation of listed companies in the U.S. to study the effects of a change of the CEO on CEO compensation in the company. The main result is that firms pay significantly more to new CEOs comparing to the previous ones, and this difference is higher during the periods when headhunters are used more intensively and in the states where there are less legal obstacles to the activity of headhunters.==== These results fit the prediction of the model that headhunters improve matching between firms and CEOs and, therefore, increase wages at the top. Second, the paper uses cross-country differences in the use of headhunters in Europe in 1997 to show that in countries where headhunters were used to a larger extent the top income shares increased by more in the following years. This evidence is in line with the prediction of the model that the more the high-productive firms use headhunters, the better is the improvement of matching at the top, and the higher are the top wages.====The remainder of the paper is organized as follows. Section 2 presents the theoretical model. Section 3 presents the quantitative results. Section 4 discusses the available empirical evidence about headhunters and headhunter industry. Section 5 presents suggestive empirical evidence. Section 6 concludes.",The role of headhunters in wage inequality: It's all about matching,https://www.sciencedirect.com/science/article/pii/S1094202520300995,14 October 2020,2020,Research Article,137.0
"Jungherr Joachim,Schott Immo","University of Bonn, Germany,Université de Montréal, Canada,CIREQ, Canada","Received 20 August 2018, Revised 7 October 2020, Available online 14 October 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.10.005,Cited by (4),"We introduce long-term debt and a maturity choice into a dynamic model of production, firm financing, and costly default. Long-term debt saves roll-over costs but increases future leverage and default rates because of a commitment problem. The model generates rich distributions of maturity choices, leverage ratios, and credit spreads across firms. It explains why larger and older firms borrow at longer maturities, have higher leverage, and ==== credit spreads. Firms' maturity choice matters for policy: A financial reform which increases investment and output in a standard model of short-term debt can have the opposite effect in a model with short-term debt and long-term debt.","This paper starts out from a simple observation. Empirically, most firm debt is long-term. About 67% of the average U.S. corporation's total stock of debt does not mature within the next year. This fact is missing from most macroeconomic models. The standard assumption is that all firm debt is short-term, i.e. all debt issued in period ==== fully matures in period ====. In this paper, we introduce long-term debt and a maturity choice into a dynamic model of production, firm financing, and costly default. We find that the model replicates important cross-sectional facts on firm financing and debt maturity. Moreover, we show that firms' maturity choice matters: Policy recommendations based on a model of endogenous debt maturity can be the opposite of those generated by a standard model of short-term debt.====In this paper, we study a model of heterogeneous firms which finance productive capital with equity, short-term debt, and long-term debt. Each period, firms choose investment, leverage, and debt maturity. Long-term debt saves roll-over costs but increases future leverage and default rates because of a commitment problem: Outstanding long-term debt distorts firms' incentives to issue additional debt (====) and to invest (====). These effects are particularly large if a firm's default risk is high.====We then take this model to the data. Combining balance sheet information on US publicly listed firms with data on credit ratings and bond spreads, we document large and systematic heterogeneity in firms' investment, financing, and maturity choices. Smaller and younger firms have lower shares of long-term debt. Their leverage ratios are low and credit spreads on their debt are high. As firms grow in size and age, their share of long-term debt increases. Larger and older firms have higher leverage ratios and pay lower credit spreads.====Our model replicates these patterns. The key mechanism is that the costs of higher debt maturity (i.e. the negative effects of debt dilution and debt overhang) are particularly large for firms with high default risk. Because larger and older firms are more profitable, their default risk is low. This reduces the cost of borrowing at long maturities and allows them to increase leverage and pay lower credit spreads compared to smaller and younger firms.====We then use the model to study two counterfactuals. A first experiment quantifies the costs of debt dilution and debt overhang in distorting firms' investment, borrowing, and maturity choices. We find that debt dilution and debt overhang increase leverage and the default rate, while reducing debt maturity and investment. In a second experiment, we show that it is important to take firms' maturity choice into account in model-based policy evaluations. A financial reform which lowers default costs increases investment, output, and consumption in a standard model of short-term debt. We show that the same reform can have the opposite effect in a model of endogenous debt maturity because it gives rise to an increase in debt dilution and debt overhang.====Our paper provides a quantitative analysis of endogenous debt maturity in a dynamic model of production, firm financing, and costly default. It contributes to a large literature that studies the role of financial frictions in shaping firm dynamics. Existing work typically assumes that all firm debt is short-term (e.g. Cooley and Quadrini, 2001; Hennessy and Whited, 2005; Gilchrist et al., 2014; Khan et al., 2016; Ottonello and Winberry, 2018; Arellano et al., 2019). From an empirical point of view, the disregard of long-term debt is problematic. At issuance, the average term to maturity is three to four years for bank loans, and more than eight years for corporate bonds (Adrian et al., 2012).====Computational difficulties are the main reason why risky long-term debt is usually absent from dynamic macroeconomic models. Optimal firm behavior depends on the price of long-term debt, which itself depends on firm behavior, both today and in the future. A firm that cannot commit to future actions must take into account how today's choices will affect future firm behavior. In this paper, we compute the global solution to this fixed point problem. This allows us to study how firms optimally adjust their debt structure over time and how these choices shape the firm distribution.====Gomes et al. (2016) consider long-term debt but assume exogenous debt maturity. Our results suggest that models which take firms' maturity choice into account can contribute to our understanding of firm dynamics and the impact of policy reforms.====There is a long tradition in corporate finance of modeling firms' maturity choice. In Leland and Toft (1996), Leland (1998), Diamond and He (2014), DeMarzo and He (2016), and Dangl and Zechner (2016), firms are not allowed to adjust the maturity structure of their debt over time. In Brunnermeier and Oehmke (2013) and He and Milbradt (2016), firms are allowed to dynamically adjust maturity but the total value of debt is fixed. We contribute to this theoretical literature by providing a quantitative analysis which allows for the dynamic adjustment of both the level and the maturity of debt.====In the sovereign debt literature, a substantial body of work studies the maturity structure of government debt (e.g. Debortoli et al., 2017; Faraglia et al., 2018). The interaction of government debt maturity and default risk is studied by Arellano and Ramanarayanan (2012), Chatterjee and Eyigungor (2012), Hatchondo et al. (2016), and Aguiar et al. (2019). While many insights from this literature carry over to models of firm financing, endogenous output and investment play an important additional role for firms' maturity choice.====In Section 2 we develop a dynamic model of production, leverage, and debt maturity. Section 3 evaluates the quantitative performance of the model. In Section 4 we use the model to conduct two counterfactual experiments. Concluding remarks follow.",Optimal debt maturity and firm investment,https://www.sciencedirect.com/science/article/pii/S1094202520301009,14 October 2020,2020,Research Article,138.0
"Bilbiie Florin O.,Ragot Xavier","University of Lausanne, Switzerland,CEPR, United Kingdom of Great Britain and Northern Ireland,Sciences Po, CNRS, and OFCE, France","Received 13 January 2020, Revised 2 October 2020, Available online 14 October 2020, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2020.10.003,Cited by (16),A ,"Price stability has been the central banks' mantra in virtually every country since the 1990s. This is perhaps one of the greatest successes in terms of impact of academic research on policymaking: the reason for this focus is that in standard models price stability is either the exact social optimum, in benchmark textbook frameworks (e.g. Woodford, 2003; Gali, 2008), or very close to it in quantitative models (e.g., Justiniano et al., 2013).====In this paper we show that price stability is no longer optimal with household heterogeneneity, in particular when households self-insure against idiosyncratic risk using scarcely available liquid assets. We build a tractable general equilibrium New Keynesian (NK) model with heterogeneous households where aggregate demand depends on ====, defined as monetary assets used for self-insurance in the spirit of the seminal Bewley model. We identify a novel channel that we label the liquidity-insurance motive of optimal policy. This changes the standard stabilization objectives (of inflation and real activity) by introducing a linear term in the otherwise standard quadratic objective function of the central bank, capturing the lack of consumption insurance. Quantitatively, this implies optimal deviations from price stability in response to shocks that in standard sticky-price models generate no such trade-off. Yet we show that in a calibrated version of our model, costs of price stability are “small” even for calibrations with high enough “inequality” (imperfect insurance) implying such a linear welfare cost. However, the costs are much larger when this novel distortion co-exists with the standard NK “markup distortion”, which also does not imply large deviations from price stability by itself.====It is therefore the ==== of these two distortions that is of the essence for generating significant deviations from price stability. The reason is that the presence of monopolistic rents, what the NK literature called the “markup distortion”, has distributional consequences in our framework with imperfect insurance. These interact non-trivially with the liquidity-insurance motive to shape the central bank's optimal policy of liquidity provision. By using inflation, the central bank can ==== move output towards its efficient level (mitigating the markup distortion) ==== provide insurance through liquidity. The combined benefit of deviating from price stability is thus large when both these distortions are large: inflation allows “killing two birds with one stone”.====Despite this central role for providing liquidity-insurance, the benefit of doing so through direct equal transfers to all households, referred to as “helicopter drops” (HD) in the literature==== is surprisingly small compared to open-market operations (OM) for a variety of calibrated versions of our quantitative model. This is so even though the latter means of money creation, by directing the transfer to those who do participate in financial markets, does the opposite of what is called for by the liquidity-insurance motive.====In our model, heterogeneous households are subject to liquidity constraints, and liquidity is used to self-insure against uninsurable risk: financial markets are incomplete ==== Bewley, and participation is limited (infrequent) in the Baumol-Tobin tradition. In equilibrium, aggregate demand depends on ====, which we define as the nominal asset used by households to self-insure; we call it “money”, but it can be any asset whose return is affected by monetary policy. Liquidity is thus used in equilibrium as long as there is a need for insurance, or ====, understood following the Bewely-Huggett-Aiyagari (heterogeneous-agent) literature as the endogenous outcome of uninsurable shocks combined with households' ability to self-insure. We thus focus on the notion of liquidity that has a long tradition, going back at least to Friedman's (1969) analysis of the redistributive effect of monetary policy and to Bewley's (1983) formalization of that analysis.====Within the recent incomplete-market, heterogeneous-agent New Keynesian “HANK” (Kaplan et al. (2018)) literature, our contribution is a tractable monetary framework capturing the key insights of the Baumol-Tobin theory of money.==== The simplicity of our ==== incomplete-market NK model may be of independent interest to some researchers.====We consider—like many others—that monetary policy is the relevant tool at business cycle (quarterly) frequency to improve the distorted market outcome. Thus, we analyze the residual trade-offs for monetary policy after the (imperfect) use of any fiscal tools, without considering time-varying fiscal tools as a policy instrument. But we do let fiscal policy do much in our model: in the baseline, it takes care of the monopolistic distortion by sales subsidies, which it finances by an implicit redistribution of profit income. Indeed, liquidity is an equilibrium phenomenon because imperfect insurance subsists as fiscal policy does not undo inequality perfectly; the amount of liquidity demanded is thus an indirect metric of the lack of insurance left after fiscal policy. The degree of imperfect insurance in the model is in fact the main determinant of optimal inflation, and we calibrate it to match the (data) fall in consumption at unemployment, which takes into account any fiscal transfers.====We study Ramsey-optimal monetary policy in this framework, and unveil a to the best of our knowledge novel channel that we call the ====, for short: with imperfect insurance (inequality) there is a rationale for providing liquidity, whose inflationary consequences' costs are generically dominated by its insurance benefits. In other words, the trade-off faced by a central bank changes: providing insurance through liquidity is consistent with its standard objectives of stabilizing inflation and aggregate demand, but our novel channel implies that inflation stabilization take a back seat.====We illustrate this analytically by providing a second-order approximation to the aggregate welfare function à la Woodford (2003, Ch. 6). There is scope for a planner to provide consumption insurance, an objective that is costly to achieve through inflation when prices are sticky (and absent a full set of fiscal instruments). This trade-off operates in the long-run, as in any monetary model, making deflation optimal by shrinking nominal liquidity, as prescribed by the Friedman rule and its incomplete-market variants. But more importantly, and unlike other monetary sticky-price frameworks, the trade-off ==== operates in the short run in response to shocks: insofar as there is long-run inequality making the liquidity-insurance motive operative, optimal policy requires volatile inflation. What is more, this inflation volatility matters for welfare: a central bank that stabilizes inflation, albeit around an optimal long-run target, incurs a large welfare cost: consumers would pay (around 0.1 to 0.5 percent of consumption) to live in the economy with volatile inflation. Such deviations and welfare effects are larger than those encountered in existing monetary models with nominal rigidities.====Inflation volatility is beneficial in our economy because it dampens the consumption volatility of constrained households without much affecting the unconstrained, who can self-insure. The optimal policy consists of providing liquidity, which insures the constrained, and inflating away some of its value, in order to give the unconstrained the right intertemporal incentives to hold this liquidity for precautionary purposes.====Since the optimal policy consists of providing liquidity to insure in face of aggregate shocks, it is only natural that more direct ways of injecting this liquidity (such as helicopter drops HD) are preferable to indirect ways (such as open market operations OM). The former consist of injecting liquidity during the period so that it reaches all but especially constrained unit-MPC households. While the latter (OM) consists of exchanging liquidity for other assets and transferring the proceeds only later through the consolidated budget, thus depriving the central bank of a within-the-period transfer. In the latter case, optimal policy thus needs to rely more on the Pigou effect, or on a distortionary tax: using (costly) inflation to influence the value of real balances of constrained households.==== We provide a rigorous welfare comparison of the two policy arrangements by calculating Ramsey-optimal policy for each and find that, for a same change in government liabilities, implementing optimal policy through HD is indeed preferable to the most favorable OM (whereby liquidity is transferred to households after one period only), but only slightly so. Thus despite there being incomplete markets and liquidity constraints of a rather extreme form, in our quantitative model the welfare benefit of implementing monetary policy through direct transfers like HD rather than open-market operations is surprisingly small.==== Our paper is at the intersection of three literatures on optimal monetary policy: in models with sticky prices, with money, and with heterogeneity and incomplete markets. Our model integrates two streams of monetary economics that evolved divergently over the past decades: New Keynesian (NK) models with nominal rigidities, and microfounded models of money demand with flexible prices====; in particular, we connect two ==== focusing on heterogeneity, market incompleteness and limited participation. One stream consists of ==== in the Bewley and Baumol-Tobin tradition. In our model, money is used to self-insure against idiosyncratic shocks as in Bewley models, but only for non-participating agents as in the Baumol-Tobin literature. Early key contributions with ==== include Bewley (1983), Scheinkman and Weiss (1986), Lucas (1990), and Kehoe et al. (1992).==== Drawing on this literature, two assumptions are key to deliver our model's tractability. First, households participating in financial markets have a high income and join a family where risk is pooled; this is an extension of Lucas (1990), also used by i.a. Challe et al. (2017) and Heathcote and Perri (2018). Second, a family head chooses the allocations of all households, including those not participating in financial markets who have a low income, under liquidity constraints. In the equilibrium that we focus on, non-participating households consume all their liquid wealth and there are only two wealth states instead of a whole distribution of wealth as in the fully-fledged Bewley model. This delivers Euler equations that preserve self-insurance through liquidity or money demand, while capturing heterogeneity in a simplified manner. This setup is reminiscent of the “large household” approach to tractability (of the distribution of money balances) introduced by Merz (1995) and Shi (1997) in a search framework.====The other stream of literature that our paper is related to studies ====; an early, 2000s literature introduced “hand-to-mouth” consumers, or limited participation in asset markets, to study aggregate demand and monetary policy. Bilbiie (2008) is an early example of such a model focusing on monetary policy, where a subset of agents are (employed) hand-to-mouth and have unit MPC.==== Compared to that class of “TANK” (two-agent NK) models, we allow for temporarily-binding credit constraints and self-insurance. Quantitative HANK models also feature liquidity constraints and self-insurance in a way consistent with microeconomic heterogeneity, replicating the distributions of wealth and MPCs. Such studies of monetary policy include Kaplan et al. (2018), Auclert (2018), and McKay et al. (2016).==== builds on the seminal paper of Lucas and Stokey (1983), extended to sticky prices by Khan et al. (2003), Adao et al. (2003), Woodford (2003, Ch. 6), Benigno and Woodford (2005, 2012), and Schmitt-Grohe and Uribe (2004, 2007). Compared to these papers, our framework gives rise to significant optimal deviations from price stability over the cycle (i.e. not only in the long run), in response to shocks that in these standard frameworks do not generate such deviations. A welfare-maximizing central bank optimally tolerates inflation volatility due to providing liquidity for insurance (even though, as we shall see, inflation is unconditionally “bad” for constrained households because it reduces the real value of their money balances). Renouncing this volatility by adopting a policy of constant deflation at the optimal asymptotic rate has a large welfare cost in our model, whereas it is innocuous in the NK models with money demand.====Several papers study ====. Bilbiie (2008) derived optimal policy in a TANK model, where a fraction of households are hand-to-mouth; Curdia and Woodford (2016) and Nistico (2016) do the same in models with infrequent participation and borrowers and savers. The setup of these last two papers shares similarities to ours, in particular concerning the “infrequent participation” structure that draws on an earlier monetary theory literature; but in the domain of optimal policy, these studies focus on the case where there is perfect insurance in steady state, thus abstracting from the liquidity-insurance, or inequality channel that gives rise to the novel trade-off we emphasize.====Several more recent and independent papers deal broadly with the same topic but differ substantially and in several key respects: assumptions about the environment, solution techniques, results, and economic intuition and mechanisms. Differently from Bhandari et al. (2018), we consider an economy with two assets to analyze the role of liquidity injections when liquidity constraints bind occasionally; this is also different relative to Challe (2019) and Nuno and Thomas (2018). The key mechanism we focus on is self-insurance through liquidity in an economy with two types of assets liquid and illiquid, and limited participation. In both Bhandari et al. and Nuno and Thomas a main channel, absent in our paper, is instead “Fisherian”: inflation redistributes from savers to borrowers by reducing the value of debt. In Challe's model with uninsurable idiosyncratic risk given by endogenous unemployment (but no equilibrium trade and no endogenous liquidity), there are no deviations from price stability under optimal policy.====What distinguishes our framework is the introduction of limited participation in financial markets as a microfoundation for liquidity-money. We focus on and isolate a novel trade-off between liquidity-insurance and standard stabilization, illustrated through a “loss function”. The Ramsey problem that we solve is simple and transparent and implies that imperfect long-run consumption insurance is essential (in conjunction with the presence of monopolistic rents, which also have a distributional dimension) to motivate large optimal deviations from price stability. We also conduct a meaningful welfare comparison of HD and OM as ways to provide liquidity.====Lastly, this paper is complementary to ====. Bilbiie (2018, 2020) builds a tractable-HANK model with liquid bonds and illiquid stocks and uses it as a reduced projection of key summary statistics of rich HANK models, disentangling between inequality and risk, and analyzes monetary and fiscal policy transmission, multipliers, and forward guidance—including in liquidity traps; the latter paper derives optimal monetary policy in a cashless model where the reasons to deviate from price stability analyzed here are absent altogether. Le Grand and Ragot (2018) use a flexible-price, one asset model to derive a general method (using the “history space”) for calculating Ramsey fiscal policy in heterogeneous-agent models. A companion note Bilbiie and Ragot (2019) analyzes the effect of liquidity-inducing helicopter drops on aggregate demand and inequality.",Optimal monetary policy and liquidity with heterogeneous households,https://www.sciencedirect.com/science/article/pii/S109420252030096X,14 October 2020,2020,Research Article,139.0
Schroth Josef,"Bank of Canada, 234 Wellington St, Ottawa, ON K1A 0H9, Canada","Received 1 December 2019, Revised 29 September 2020, Available online 6 October 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.010,Cited by (3),"This paper studies distributional consequences of equity injections that are funded with financial-sector levies. In the model economy, financial intermediation supports both productive investment and individual consumption smoothing, subject to an equity requirement. When intermediary equity is low, then an equity injection involves a trade-off between increasing credit supply immediately and distortive levies that reduce credit supply in the future. I find that equity injections redistribute from poor to wealthy households, even though average welfare increases. While wealthy savers benefit greatly from an increased return on savings, poor households suffer from lower future wages.","Financial crises lead to policy interventions that can carry significant fiscal costs (Laeven and Valencia, 2013). While policy makers prefer to follow the prescription in Bagehot (1873) that public funds should only be provided to institutions that are expected to repay them, it is difficult to determine ex ante an institution's ability to repay. The resulting tension has been highlighted by the debate on whether the collapse of Lehman Brothers—an inflection point of the 2007–09 global financial crisis—should have been prevented.==== Recent financial regulation reform alleviates this tension by formulating an amended version of Bagehot's prescription. It requires that public funds that cannot be recovered from individual institutions should be recovered by imposing levies on the broader financial sector.==== However, levies that are imposed to recoup bailout funds may distort financial sector actions—with various potential repercussions throughout the economy. As a result, it is not obvious who are the net beneficiaries, and net payers, of a financial-sector funded bailout.====This paper studies the distributional consequences of financial-sector funded bailouts. To this end, I build a model in which banks intermediate between producers and workers. While producers require bank loans to fund physical capital investment, workers demand bank loans and bank bonds to smooth consumption over time. However, because of moral hazard concerns banks can only attract external funding if their shareholder value is sufficiently high relative to the amount of loans they supply. During a financial crisis, when bank value is low, banks are therefore forced to reduce loan supply. I use the model economy to study the implications of a policy that injects equity into banks—to increase their value and thus their loan supply—and recoups the initial injection by taxing banks over time. The policy reduces the cost of bank loans during a financial crisis at the cost of raising it somewhat in future periods as banks pass on the tax to borrowers.====The main result of the paper is that equity injections that are funded by a tax on bank lending redistribute from poor to wealthy households in the economy, even though average welfare increases. The equity injection prevents a disruption of bank intermediation activity and thus avoids a scarcity of both bank loans and bank bonds. As a result, wages and the return on savings are stabilized in the short-run which benefits both poor and wealthy households. In the long-run, however, wages are depressed because the tax on lending increases the funding cost of entrepreneurs and corporate firms which decreases their demand for labor. The cost of the equity injection is long-term and especially affects poor households because they rely primarily on labor income. On net, wealthy households benefit greatly from an increased supply of bank bonds while poor households are somewhat worse off.",On the distributional effects of bank bailouts,https://www.sciencedirect.com/science/article/pii/S1094202520300922,6 October 2020,2020,Research Article,140.0
McKiernan Kathleen,"Department of Economics, Vanderbilt University, PMB 351819, 2301 Vanderbilt Place, Nashville, TN 37235, United States of America","Received 25 March 2019, Revised 29 September 2020, Available online 6 October 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.10.001,Cited by (4),"As populations age, countries across the globe are dealing with the issue of how to fund retirement consumption for their workers. The design of Social Security programs is more difficult when the country also exhibits an informal economy where workers avoid the taxation of the government and are not entitled to its benefits. In this paper, I study the example of Chile–a country that transitioned from a pay-as-you-go Social Security system to a system of private, individual retirement accounts in 1981 and also exhibits a significant informal sector–in order to quantify the transitional welfare impact of Social Security privatization when workers have the option to evade the public system through informality. I construct an OLG model which allows households to split working time between a taxed formal sector, an untaxed informal sector, and home production. I find large long-run welfare gains of roughly 10 and 15 percent for low and high-productivity workers, respectively. However, these gains come at the expense of losses for two groups: low-productivity workers who are retired at the time of the reform and high-productivity workers within 5 years of retirement at the time of the reform. The presence of informality leads to conflicting mechanisms: (1) the elasticity channel decreases welfare gains from reform: including informality as imperfectly substitutable with formality in utility decreases labor supply elasticity and renders the pay-as-you-go ==== less distortionary, and (2) the market wage channel increases welfare gains from reform: the privatization of the Social Security system causes wage growth which informal workers can receive without facing the distortions of any remaining taxation in the formal sector. Quantitative results indicate the elasticity channel is stronger in the case of Chile, and the inclusion of informality decreases the long-run welfare gains from privatizing Social Security.","As world populations age, many governments are dealing with the issue of how to provide retirement income for citizens. There is ample research that pay-as-you-go (PAYG) Social Security programs are unsustainable, especially in the presence of these aging populations. As the ratio of workers to retirees falls, the burden of the government to provide retirement consumption for the population increases. Previous work has shown there are long-run welfare gains from reforming from a pay-as-you-go Social Security program to a system of private, individual retirement accounts. However, this work also shows that these long-run gains are often financed by welfare losses for workers who live through the transition between the programs. As the economy transitions between the programs, the government loses payroll tax income from new workers. At the same time, it must also compensate older workers and current retirees for prior contributions to the pay-as-you-go system.====This transition between PAYG and individual accounts and its welfare impact is influenced by how agents augment their savings, consumption, and–of particular importance for this study–labor choices in response to the policy change. The presence of outside options to working in the market is key in understanding the labor market response of workers. Recent estimates indicate that over 60 percent of the world's employed population works in the informal economy, and some areas of the globe may experience informality rates of nearly 90 percent.==== Due to the large size of this sector in many countries, understanding how policy changes interact with the size and behavior of this sector is important to understand possible impacts of such reforms.====Changes in the taxation and benefits associated with policy reforms change the incentives for formal, informal and home sector work. In particular, as taxes increase or the benefits that a worker is eligible for based on formal work decrease, the incentives for households to avoid regulation and work informally or at home increase. Therefore, as taxes and transfers adjust throughout a transition between a PAYG pension program and an individual account system, the sizes of the informal economy and the home production sector change. These changes have a direct impact on the size of the tax base, the transfers workers are entitled to, and, thus, the government budget constraint. Therefore, movements in the size of the various labor sectors in response to a policy change can affect the long-run impact of the reform.====In 1981, Chile became the first country to amend its PAYG Social Security system and begin the transition to a mandatory savings program consisting of private, individual retirement accounts. In the years that followed, this change has inspired other countries in Latin America, as well as countries throughout the world, to make changes to their own retirement systems. In the aftermath of the Chilean reform, ten other countries within Latin America took on reformed programs inspired by the new Chilean system.==== Additionally, countries all across the globe have reformed their Social Security systems==== to include individual retirement accounts.==== Many other countries, including the United States, however, have not moved away from their PAYG systems. For this reason, Social Security in Chile stands as an example for pension systems across the globe.==== Additionally, the Chilean economy exhibits a significant informal sector. Therefore, Chile provides a case study of not only Social Security privatization, but also of the impact of such a reform in the presence of outside options to formality.====To address this question, I build an overlapping generations model including three key features. First, I include age and productivity heterogeneity to study how the reform impacts welfare for different age groups and income levels. Second, I model the government policy after the specifics of the Chilean reform to isolate the impact of various aspects of this reform. Finally, and unique in my analysis, I include a household decision to split working time between a taxed formal sector, an untaxed informal sector, and home production to focus on how movements between sectors impact the outcomes of the reform.====Formality, informality, and home production have distinct roles within the model. The formal sector of the market allows workers to receive a market wage and a retirement pension based upon their formal earnings. However, workers must also pay taxes on the income earned for work in the formal sector. Home production, which previous literature has shown to be important in household decisions, gives households an outside option to working in the market. Home production produces a non-tradable good which is substitutable with market goods. Workers can use home production to avoid taxation while continuing to consume. However, the worker forgoes the wage they would have received for market work. Informality fills a gap between the formal sector and the home production sector. The informal sector gives workers a way to earn a market wage while also avoiding the taxation of the formal sector.====The way in which formal and informal labor are modeled is crucial for this work. I highlight two methods through which informal work affects households: through government policies and through utility. First, households pay taxes on income earned formally. Only this formal working income is used to calculate the pension in the PAYG system. As the household spends more time working informally, its taxes and average taxable earnings decrease. Therefore, the pension the household is eligible for also decreases. Second, I highlight non-pecuniary aspects of formal and informal work by modeling these sectors as imperfectly substitutable in household utility. Because formality and informality are substitutes, there is movement between the sectors as the labor tax–the relative price of formal work–moves. However, these sectors are imperfectly substitutable so there will always be strictly positive hours in both sectors. Fear of punishment if caught working informally, the need to work formally to gain access to credit markets or social programs, lack of access to formal sector jobs, and other non-pecuniary differences between the sectors are captured in this inability of labor to respond to only the taxation of the formal sector. An additional benefit of this modeling choice is the Chilean micro-data can be used to measure the relationship between formal and informal work.====Using micro-data for the Chilean economy, I estimate the parameters that discipline the relationship between home production and market labor as well as those that discipline the relationship between formal and informal labor supply. In addition to using this micro-data, I ensure that the model is consistent with various macroeconomic aggregates and use the government policy to construct historical series for the fiscal variables. By using this data, I can quantitatively measure the welfare impact of the transition between the pay-as-you-go program and the privatized system of individual accounts.====In order to measure the welfare impact, I simulate two demographics transition paths for the economy: one in which the reform occurs as it did historically, and a second as if the economy remained in the PAYG system from 1980. The results of this experiment show that long-run welfare gains are present due to the reform. Manual laborers, who pay lower contribution rates and receive less generous PAYG pensions, experience gains of nearly 10 percent of lifetime consumption and employees and managers, who pay higher contribution rates and receive larger pensions prior to the reform, experience gains of roughly 15 percent. However, generations that live through the transition between the two programs experience welfare losses up to around 10 percent of remaining lifetime consumption. Manual laborers who are retired at the time of the reform experience losses due to falling government transfers; employees and managers who are nearing retirement at the time of reform experience losses as the transitional policy pays them a smaller pension than they would have received under the pay-as-you-go program.====The inclusion of home production and informality together lead to higher long-run welfare gains. However, these gains can be further decomposed into the portion caused by the inclusion of home production and the portion caused by the option of informality. Similar to other research, I find that including only home production increases aggregate welfare gains as home production increases the elasticity of the labor supply. This leads the Social Security payroll tax to be more distortionary than in an economy without the option of home production. Therefore, the economy experiences larger long-run welfare gains when the payroll tax is removed through the reform.====Informality, on the other hand, causes two mechanisms which have conflicting impacts on long-run welfare gains. First, including the option of informality as an imperfect substitute in the utility of the household decreases the elasticity of the labor supply as the ability to move between the sectors is limited by the degree of substitutability between them. Therefore, in contrast to home production, including informality renders the PAYG payroll tax less distortionary and decreases long-run welfare gains. Second, informality gives households an opportunity to receive a market wage without facing any additional labor income taxation in the formal sector. Since the transition between pension systems leads to increased wages, including informality increases long-run welfare gains from reform. Quantitatively, the first mechanism dominates; in an economy including informality, privatization of the pension system leads to smaller welfare gains than the same reform in an economy with only formality.====The paper precedes as follows. Section 2 introduces the related literature. Section 3 describes the model. Section 4 discusses the background of the Chilean pension system before and after the reform. Section 5 details how parameters are estimated from the data. Section 6 presents the transitional results. Section 7 discusses how including informality and home production impact the long-run results. Section 8 concludes.",Social Security reform in the presence of informality,https://www.sciencedirect.com/science/article/pii/S1094202520300946,6 October 2020,2020,Research Article,141.0
Wilemme Guillaume,"University of Leicester, University Road, Leicester LE1 7RH, United Kingdom","Received 8 June 2018, Revised 29 September 2020, Available online 2 October 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.011,Cited by (1),"This paper characterises efficiency-enhancing ==== when job search activities generate excessive mismatching between firms and workers. Workers can direct and modulate their search efforts towards particular firms before matching and bargaining over wages. A composition externality arises because workers do not internalise the consequences of mismatching on the entry decision of firms, resulting in lower average sectoral productivity and suboptimal ====. The optimal policy ==== low-surplus matches and subsidises high-surplus matches. The model is calibrated on occupational mismatching in the United States. In addition to the pre-existing flat tax, the corrective taxes subsidise each $1 increase in productivity by $0.45 to $0.87 on average.","This paper characterises efficiency-enhancing taxes when job search activities generate excessive mismatching between heterogeneous workers on the one hand, and heterogeneous jobs that firms open endogenously on the other hand.====The frictional nature of the labour market has generated new results in public finance research. The way in which search frictions shape wages and unemployment matters in the design of public policies, such as redistribution taxes (Hungerbühler et al., 2006; Lehmann et al., 2011; Golosov et al., 2013; Bagger et al., 2018), or unemployment insurance (Cremer et al., 1995; Geromichalos, 2015; Boadway and Cuff, 2018; Landais et al., 2018). However, most of this research abstracts from the inherent inefficiency of frictional markets through specific assumptions. In the presence of search frictions, laissez-faire equilibria are generally inefficient because agents generate positive and negative externalities from searching and matching. To preserve efficient equilibria, the aforementioned literature relies on specific assumptions, such as homogenous agents or perfect market segmentation. In these cases, there is no Pigouvian motive for public intervention, and optimal policies only serve an equity motive.====This paper investigates the Pigouvian motive of optimal taxation in a labour market with two-sided heterogeneity, partial market segmentation and unobserved search efforts. At the laissez-faire equilibrium, firms and workers are inefficiently mismatched because jobseekers are not selective enough. Absent an equity motive, the efficiency-enhancing tax scheme has anti-redistributive characteristics. Ignoring this inefficiency can, therefore, understate the deadweight loss of redistributive policies.====The model builds on the framework of Diamond-Mortensen-Pissarides (see Pissarides, 2000, for an overview) with firm and worker heterogeneity. The search and matching technology is modelled with two main assumptions. First, the labour market is partitioned by firm type. Each firm type defines a segment of the labour market where matching is random. Workers' search decisions are informed by firm types. Wages are Nash-bargained after matching. Thus, the two-sector model of Uren (2006) and the circle model of Decreuse (2008) are generalised to an environment with two-sided heterogeneity. Second, workers can search simultaneously on several segments and they can modulate the intensity of their search accordingly. The search technology has decreasing marginal returns within each segment of the labour market, as in the directed search model of Decreuse and Zylberberg (2011).====The economic mechanism in this article relies on a general equilibrium effect, which connects workers' search strategies to firms' decisions to create jobs. A jobseeker is incentivised to search for jobs that may not be suitable so that they leave unemployment faster. A jobseeker has incentives to search for jobs that are not their best fit in order to leave unemployment quickly. Jobseekers' selectivity, thus, determines the extent of mismatching in the labour market. From the firms' perspective, job mismatching decreases profits because jobs are, on average, less productive. Consequently, more mismatching leads to fewer job openings. By not internalising the firms' job creation costs, workers are not selective enough and thus, job quality is diminished. This ==== adds to the standard search externalities of the Diamond-Mortensen-Pissarides framework. The laissez-faire equilibrium is always inefficient. This inefficiency, already described by Uren (2006) and Decreuse (2008), is considered in a richer environment.====The contribution of this research is to characterise a system of taxes that restores the efficiency of the decentralised equilibrium in a general framework with two-sided heterogeneity and search technology. Optimal taxes describe the departure from efficient allocation. In this article, Proposition 1 proves the existence of pay-off profiles that decentralise efficient job creation and search efforts. Proposition 2 shows that the laissez-faire equilibrium with Nash-bargained pay-offs is inefficient in general. Proposition 3 defines an optimal taxation scheme that decentralises efficient allocation. The optimal tax scheme implements redistribution from low-surplus to high-surplus jobs within each segment of the labour market. Workers are, thus, provided with incentives to be more selective.====Optimal taxes have two salient features. First, the government can decentralise the efficient first-best allocation ==== observing multi-dimensional search strategies. A proportional tax on wages, a proportional subsidy on match surplus and a lump-sum tax correct two inefficiencies in the labour market. The proportional tax on wages maintains the Hosios-Pissarides condition (Hosios, 1990; Pissarides, 2000), which states that match surplus is split according to the search externalities created by each side of the market (Boone and Bovenberg, 2002). The proportional subsidy on match surplus makes good matches more profitable to workers. As a consequence, jobseekers are more selective in their applications as they internalise the composition externality.====Second, the optimal tax scheme depends on a limited number of variables, namely bargaining power, matching elasticity, and match surpluses. For specific modellings of heterogeneity, redistribution from a low-surplus to a high-surplus job is equivalent to redistribution from a low-productivity to a high-productivity job, in what I define as anti-redistribution. In the circle model, the optimal policy is always anti-redistributive (Proposition 4). The optimal tax scheme also extends to endogenous on-the-job reallocation. An unemployed worker who starts a job generates more surplus than an employed worker who changes jobs. Therefore, the optimal policy taxes more job-to-job transitions than unemployment-to-employment transitions.====The model is calibrated on occupational mismatching in the United States (US), with a flat tax on wages as a proxy for the US tax scheme. The net output at the equilibrium with the flat tax is 20% to 40% lower than its optimal value, depending on the bargaining power of workers. The higher the bargaining power, the lower the efficiency loss because workers internalise a larger part of the composition externality. In the simulated model, optimal taxes are negatively correlated with job productivities, suggesting that the efficiency-enhancing tax scheme is anti-redistributive. Everything else equal, if a given employed worker produced $1 more per period, they would receive on average an additional tax deduction or subsidy of $0.03 to $0.44 depending on the specification. Without the flat tax, the net output loss of the laissez-faire equilibrium ranges from 0.5% to 12%. Occupational mismatching is defined by productivity losses when workers are not employed in their most suitable occupation. This concept is different from occupational mismatching, defined as the shortcomings and excesses of jobseekers by occupation, developed by Sahin et al. (2014).====This paper relates to the public finance literature that features search frictions, with an article by Bagger et al. (2018) being the most relevant. In their model, heterogeneous workers search off and on-the-job, and the only reason for the government to introduce taxes is equity. By taxing productive workers, the government distorts search efforts and generates inefficiency. They do not consider job vacancy creation and, therefore, abstract from the composition externality. I complement their approach by accounting for endogenous job vacancy creation while ignoring the equity motive of taxation.====The effects of labour market policies in a frictional environment have been studied by Pissarides, 1985, Pissarides, 1998, Lockwood and Manning (1993), and Boone and Bovenberg (2002). This article features job mismatching as an additional dimension, while preserving a simple tax result. Shimer and Smith (2001) and Blázquez and Jansen (2008) also introduce taxes in the presence of equilibrium mismatching. They consider, respectively, taxes on search intensities and taxes paid by unemployed workers, mainly for illustrative purposes. Instead, my focus on optimal taxes discusses the implications in terms of redistribution.====The composition externality that results in inefficient laissez-faire equilibria has been identified in various environments featuring heterogeneity and Nash bargaining. The composition externality always leads agents to not ‘aim high enough’ in equilibrium. The literature can be broadly classified into three categories. The first category considers job-acceptance strategies and the (undirected) search efforts of heterogeneous agents in a random search framework (Lockwood, 1986; Shimer and Smith, 2001; Albrecht and Vroman, 2002; Blázquez and Jansen, 2008). The second category focuses on ex-ante technological or human capital investments before matching (Acemoglu and Shimer, 1999; Acemoglu, 2001; Amine and Santos, 2008). The common ground between these two categories is that search is random in a unified labour market. A worker has the same odds of finding a good match or a bad match. This article belongs to a third category, in which search is directed towards different segments of the labour market (Moscarini, 2001; Charlot and Decreuse, 2005; Uren, 2006; Decreuse, 2008; Estache and Foucart, 2018). In this article, both the quantitative and qualitative margins of search activities are modelled.====Recent research on online job searches provides supporting evidence for the modelling of search activities in this article. Belot et al. (2018) conduct an experiment and find that online jobseekers, exposed to tailored advice, adjust the ‘occupational breadth’ of their search. The occupational search margin is, thus, a possible interpretation of search decisions in this article. Marinescu and Wolthoff (2020) show that workers pay more attention to job titles than to wages when they direct their search. Analogously, in my setting, workers direct their search towards different submarkets, defined by firm types, not by wages. Theoretically, this modelling differs from the competitive search framework of Moen (1997) and Menzio and Shi (2011). In a competitive search framework, firms can open new submarkets when they post new wages. In my setting, such wage competition is precluded because the number of submarkets is exogenous.====The first section of this paper defines the theoretical framework, proves the inefficiency of the laissez-faire equilibrium, and characterises the optimal tax scheme. In the second section, the model is extended to account for job-to-job mobility and calibrated on occupational mismatching in the US. Finally, a conclusion is provided.",Optimal taxation to correct job mismatching,https://www.sciencedirect.com/science/article/pii/S1094202520300934,2 October 2020,2020,Research Article,142.0
"Di Nola Alessandro,Kocharkov Georgi,Scholl Almuth,Tkhir Anna-Mariia","University of Konstanz, Germany,Deutsche Bundesbank, Germany,Goethe University Frankfurt, Germany","Received 23 January 2019, Revised 22 September 2020, Available online 25 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.009,Cited by (10)," in terms of income, ====, self-employment, and misreporting. Tax evasion alleviates ==== and leads to a larger self-employment sector but reduces the average size and productivity of self-employed businesses. Tax evasion generates positive welfare effects for the self-employed at the expense of the workers.","Tax evasion of individual income is substantial in the United States. The Internal Revenue Service (IRS) estimates that the lost federal tax revenue due to underreported individual income is $197 billion in 2001, which is 18 percent of the actual individual income tax liability (U.S. Department of the Treasury 2006).==== Tax evasion is concentrated among self-employed businesses. While only 1 percent of wages and salaries is misreported, this figure rises to 57 percent for self-employed income (Johns and Slemrod 2010). Self-employed businesses constitute an important component of the U.S. economy. They account for 39 percent of the assets and 21 percent of the income in the economy.====What are the aggregate consequences of tax evasion in the self-employment sector in the U.S.? Does evading taxes by self-employed businesses matter for aggregate outcomes, inequality, and welfare? What are the channels through which such effects operate? What are the implications for tax enforcement policies?====To answer these questions we develop a dynamic general equilibrium model with incomplete markets and occupational choice and study the impact of tax evasion on aggregate outcomes, the distribution of wealth, and welfare. In our model environment, infinitely-lived households face idiosyncratic and persistent shocks to their labor productivity and their talent of running a self-employed business. They pay progressive income taxes, make consumption and saving decisions, and choose between being a worker or self-employed each period. Workers supply their labor services to the labor market and cannot evade taxes. Self-employed households invest in capital, hire labor, and use a decreasing returns to scale technology to produce the consumption good. They are credit-constrained and face a borrowing limit proportional to the amount of their net wealth. Self-employed households may hide a share of their business income, however, tax evasion is costly. When misreporting their business income, they are confronted with the probability of being detected by the tax authorities and punished by a proportional fine on the evaded taxes. Therefore, they optimally determine the size of their firms, taking into account that detection becomes more likely as their business grows. In addition to the self-employment sector, there is a corporate sector in which firms operate with a constant returns to scale technology and use labor and capital competitively to produce the consumption good.====We calibrate the model to the U.S. economy at the start of the 2000s using the Panel Survey of Income Dynamics (PSID). Importantly, the parameters related to tax evasion are set to match the average misreporting rate of income as well as the cross-sectional misreporting rates conditional on the level of income. The model replicates important quantitative features of the U.S. economy in terms of income, wealth, and self-employment. The model successfully matches the size distribution as well as the average leverage of self-employed businesses. The overall excellent fit of the model with respect to this broad set of empirical facts for the U.S. economy gives us confidence to use the model for a quantitative analysis.====We study the impact of tax evasion by comparing our benchmark economy with a counterfactual economy in which taxes are perfectly enforced. The optimal decision rules highlight three important channels through which tax evasion affects aggregate outcomes. ==== The ====: tax evasion acts like a subsidy and stimulates asset accumulation, allowing higher investment in business capital. ==== The ====: the opportunity to evade taxes induces less talented agents to run self-employed businesses. ==== The ====: self-employed agents have incentives to keep their businesses small in order to stay under the radar of the tax authorities and to reduce the probability of being audited.====The quantitative analysis of the stationary equilibrium suggests that tax evasion by self-employed businesses matters for aggregate and distributional outcomes. In our application to the U.S. economy, the opportunity to evade taxes increases the number of self-employed businesses but reduces the average productivity of the self-employment sector. Moreover, tax evasion increases the share of small businesses, which is crucial for replicating the empirical self-employed firm size distribution. Furthermore, the economy with tax evasion is characterized by higher aggregate savings and larger aggregate output than the counterfactual economy with perfect tax enforcement. While in the aggregate, wealth inequality increases, self-employed households and workers are affected differently by tax evasion. Within the group of the self-employed, wealth inequality is reduced, whereas it is raised within the group of workers.====Our finding that tax evasion raises output in the aggregate is not trivial since tax evasion distorts the self-employed firm size and induces less able agents to self-select into self-employment. However, at the same time, tax evasion reduces the distortionary impact of income taxation and alleviates the financial frictions imposed on self-employed households via the credit constraint. Our results highlight that in the aggregate the ==== dominates the ==== and the ====. Importantly, the ==== is quantitatively stronger in economies in which self-employed businesses face tighter borrowing limits.====Next, we study the welfare implications of tax evasion. To this end, we calculate the welfare effects of eliminating tax evasion by adopting a perfect tax enforcement technology. Our analysis suggests that the elimination of tax evasion generates an aggregate welfare loss of about 1.7 percent, measured in consumption equivalence units. However, perfect tax enforcement raises tax revenues by around 1.9 percent of GDP, which corresponds to the empirical estimate of the U.S. underreported tax gap of 2 percent of GDP (U.S. Department of the Treasury (2009)). If these additional tax revenues are redistributed to all households via lump-sum transfers or tax cuts, perfect tax enforcement still adversely affects aggregate productivity and the welfare of the self-employed, but workers substantially benefit. In this case, the aggregate welfare effect turns positive and amounts to around 1.9 percent, measured in consumption equivalent units. Importantly, poor self-employed households in the lowest decile of the wealth distribution suffer most from perfect tax enforcement because tax evasion allows them to relax their credit constraint. If the additional tax revenues are used for tax cuts targeted to the self-employed to alleviate their financial constraints, then perfect tax enforcement generates higher aggregate productivity and an overall aggregate welfare gain of about 1 percent. Not only the self-employed benefit from the tax cut but also workers, albeit their gains are significantly lower.====Against the backdrop that taxes are imperfectly enforceable, we focus on the penalty for tax evasion as a tax enforcement policy instrument. In the U.S., the fine for tax fraud amounts to 75 percent of the missing taxes. To analyze how a rise in the penalty affects aggregate outcomes and welfare, we employ our benchmark economy and vary the fine between 25 and 400 percent. Our quantitative findings suggest that raising the penalty from 75 to 125 percent reduces misreporting and sharply increases tax revenues. If these additional tax revenues are redistributed to all households via lump-sum transfers or tax cuts, both workers and self-employed households experience welfare gains between 0.2 and 0.3 percent, measured in consumption equivalence units. Penalties beyond 125 percent of the missing taxes reduce the share of self-employed, depress aggregate output and dampen the increase in tax revenues with adverse welfare effects for the self-employed.====The rest of the paper is organized as follows. The next subsection discusses the related literature. In Section 2, we provide further details on tax evasion in the U.S. Section 3 presents the model. Section 4 explains the calibration procedure and shows the model fit. In Section 5, we present and discuss how tax evasion affects aggregate outcomes, inequality, and welfare. Moreover, we analyze the interaction between credit constraints and tax evasion. Section 6 studies the impact of tax enforcement policy on aggregate outcomes and welfare. The last section concludes.",The aggregate consequences of tax evasion,https://www.sciencedirect.com/science/article/pii/S1094202520300910,25 September 2020,2020,Research Article,143.0
Siena Daniele,"Banque de France, 041-1422 DEMFI-POMONE, 31, rue Croix des Petits Champs, 75049 Paris cedex 01, France","Received 13 April 2018, Revised 20 September 2020, Available online 25 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.006,Cited by (9),", caused euro area periphery imbalances. Quantitatively, anticipated shocks are important drivers of CA and real exchange rate fluctuations.","Before the Great Recession, euro area countries (EA) experienced large and persistent current account imbalances. The financial crisis and the following Sovereign Debt Crisis revealed that those countries running large deficits were also the ones experiencing stronger economic contractions.==== This reanimated a still unsettled debate on what are the causes of these imbalances. Blanchard and Giavazzi (2002), during the build up phase, looked at the current account movements in Ireland, Portugal and Greece and assigned to the current and expected productivity differentials (in favor of debtor countries) the root of these movements. Today, many competing explanations emphasize different macroeconomic sources and there is no yet an agreement on a unique narrative. Interestingly, however, all these studies agree that imbalances were related to the construction of the European Monetary Union (EMU) and start their analysis as of 1 January 1999, the birth of the euro.====Nevertheless, current account movements started to materialize earlier, between 1995-1996 (Fig. 1(a)). This fact, which has been surprisingly overlooked, motivates this paper and its focus on the role of expectations as sources of current account movements in the euro area. The aim of this paper is twofold: first, to uncover the common causes of the current account imbalances experienced within the euro area periphery before the Great Recession, accounting for the 1996-1999 period; second, to evaluate the plausibility of different narratives and quantify their contribution to current account and real exchange rate fluctuations, using an estimated open economy model and empirical evidence.====Given the importance of the timing, it is essential to revise shortly some salient facts on the build up of the euro area. In June 1989, the European Council decided that the first stage of the economic and monetary union would begin on 1 July 1990. The negotiations outcome was the Maastricht Treaty signed on 7 February 1992, which came into force on 1 November 1993. The years around the treaty (1992-1993) have been tormented for the euro project, with currency crises (Italy and UK) and high uncertainty on people's consensus (e.g. rejection by referendum in Denmark and difficulties in ratification in France). In April 1994 none of the EU Member States satisfied the criteria to be eligible for the monetary union. Nevertheless, in June 1995 it was decided that the year 1999 would be the starting date of the common currency and in December 1995 European leaders in Madrid decided to name of the new European currency ====. This is when agents started to believe in the European Monetary Union project and began to act accordingly. Only Greece, among the countries who wanted to enter in 1999, had to postpone the entrance in 2001 as it didn't satisfy the necessary criteria. For Greece, this meant a sudden reversal of expectations and explains why, given my interest on the role of anticipation, Greece will be analyzed separately. The focus of the paper will be on Ireland, Portugal and Spain grouped jointly (henceforth IPS) or taken in isolation.====Three facts are at the core of my analysis. (i) First, diverging current account balances have characterized the EA up to the Great Recession. Since 1996, IPS started to increase current account deficits while other countries, such as Germany and Austria, began to raise surpluses (Fig. 1(a)). Greece, because of the unexpected delay in joining the EMU, saw an ‘S-pattern’ of current account balances. It started by accumulating deficit contemporaneously to IPS, then experienced a sudden re-balance when it became clear that it would not enter in the EMU in 1999, and finally increased persistently its deficit. Interestingly, CA balances within the EA evolved in two phases. An initial one, between 1995 and 2000, where it increased by 179% and a second one, between 2003 and 2007, where it augmented by 59% (Fig. 1(e)). (ii) Second, in periods of increasing deficits, GIPS (Greece, Ireland, Portugal and Spain) were growing above historical trend, augmenting investment and experiencing a persistent real exchange rate appreciation with respect to the rest of the EA (Fig. 1(b) and 1(d)).==== (iii) Around 1996 (1998 for Greece), the long term borrowing cost premium that IPS had to pay with respect to the EA core countries started a remarkable decrease, characterized again by two phases.==== Phase one, from 1995 to 1999, with an almost linear decrease in the spread from 3.9 percent to 0.2 and phase two, from 2002 to 2005, with a smooth decrease from 0.2 to 0.03 (Fig. 1(c)).====The group of euro area periphery countries often includes Italy. However, the Italian economy didn't behave as other periphery countries in the early stage of the EMU. In particular, two out of the three common macroeconomic features of GIPS did not materialize in Italy. First, Italy has been a net lender to the rest of the world up to the year 2000 and reached only a maximum CA deficit to GDP of 1.5% in 2006. Second, and more importantly, periods of decreasing current accounts were not, as in GIPS, periods of appreciating real exchange rate and output growing above trend. Given the aim to investigate the existence of a common source of euro area periphery imbalances, I exclude Italy from the analysis.====Motivated by the fact that the current account balance (defined as the change in net foreign assets) captures the ==== feature of international trade and that EA imbalances started to widen before the actual introduction of the euro, I investigate the role of anticipated shocks. While these shocks have been extensively studied as drivers of domestic business cycles, only few researches focused on the international setting and none on EA imbalances.==== This paper contributes to this literature by assessing, qualitatively and quantitatively, the impact of anticipated shocks as sources of current account imbalances. Related to my study, Hoffmann et al. (2017) investigate if productivity shocks (modeled as noise shocks) can explain the build-up of US current account imbalances.==== Focusing on the EA, my paper differs in four aspects: (i) it jointly (and crucially) analyzes the current account with the real exchange rate and GDP; (ii) it considers a broad variety of competing explanations, not only productivity; (iii) it estimates fundamental parameters for the international transmission of shocks, as the trade elasticity and shock persistence (see Corsetti et al. (2008)); (iv) it ==== the contribution of anticipated shocks, as news shocks, for current account and real exchange rate fluctuations.====Using a structural open economy estimated model, I take the road started by Blanchard and Giavazzi (2002) and Blanchard (2007) of analyzing the imbalances within the EA. The main idea is that current account imbalances are different depending on their sources (e.g. Giavazzi and Spaventa (2011) and Eichengreen (2010)) but are observationally equivalent if looked separately from international prices and GDP components. I therefore construct a theoretical set-up that allows me to evaluate different narratives, starting with growth differentials (“catching-up”), and I focus on those that can explain ==== the observed dynamics of the current account, the real exchange rate and GDP. More specifically, I lay out a New Keynesian DSGE small open economy model in a monetary union with two sectors (tradable and non tradable), that combines different features of open economy general equilibrium models.==== I include unanticipated, one and two-year anticipated innovations for each shock and I estimate it on GIPS data with Bayesian techniques (see An and Schorfheide (2007)). I then analyze the importance of productivity (sector-specific and common labor augmenting), preferences, investment, labor supply, markup, monetary policy and yield spread shocks (anticipated and not, for all structural disturbances), using impulse response functions and variance decompositions. Even if my model is intentionally kept standard and lacks possible amplification mechanisms (e.g. financial frictions, borrowing constraints, non-Ricardian households, etc) I believe it captures well, through the multitude of stylized exogenous fluctuations, the majority of the possible narratives behind imbalances.==== A prior predictive analysis, along the lines of Geweke (2010) and Leeper et al. (2017), shows in fact that all possible narratives are plausible concurrent explanations given the range of prior considered. It will therefore be to the empirical estimation to select and quantify the plausible narratives, assessing if changes in expectations indeed played a role.====The existing literature has emphasized many different macro factors that could be important determinants of EA imbalances. After the catching-up view of Blanchard and Giavazzi (2002), Fagan and Gaspar (2007) and Schmitz and von Hagen (2011) supported the idea that was financial integration that facilitated capital flowing towards country with lower income per capita. Others, however, challenged this narrative by showing that demand shifts toward non-tradable goods (and not competitiveness====) were behind these imbalances (Gaulier and Vicard (2012)). Generally, if the GIPS borrowed to increase production in the non-tradable sector they have likely violated their inter-temporal budget constraint (Giavazzi and Spaventa (2011)). Sodsriwiboon and Jaumotte (2010) showed that indeed periphery countries borrowed more than what supported by their economic fundamentals, potentially because of inflation differentials (Polito and Wickens (2014)) or differential in inflation expectations (Bonam and Goy (2019)). In Spain, loosening credit constraints for households and risk-premium shocks explained an important fraction (25%) of net exports, according to in 't Veld et al. (2014). Zemanek et al. (2009) and Berger and Nitsch (2014) show how capital flew towards countries with higher domestic distortions while Gopinath et al. (2017) and Monacelli et al. (2018) show how GIPS capital inflows might themselves be responsible for generating distortions.====Financial frictions might have been channeling, in heterogeneous ways, common shocks across the euro area. Jaccard and Smets (2019), by noticing that current account imbalances arose among well synchronized economies, show how a common productivity shock can produce trade deficits in countries with larger financial frictions and lower contract enforcement, generating pro-cyclical imbalances.==== The role of financial frictions in intermediating external flows is highlighted by Hale and Obstfeld (2016), who find a link between EMU banks' lending to GIPS and their borrowing from financial centers, supporting the idea of a preferential channel of EMU core countries in lending to GIPS.==== On a similar logic, de Ferra (2020) shows how a change in the institutional European framework, generating subsidies for member countries in holding euro-denominated assets, contributed to current account imbalances (up to 40% of the change in the current account balance observed in Germany and Spain) and to the severity of the sovereign debt crisis. Indeed, if real distortions interacted with financial friction in countries without monetary policy independence, this could explain the subsequent heterogeneous propagation of financial shocks in EA countries (see Gilchrist et al. (2018) and Gilchrist and Mojon (2018)).====Two main results emerge from my analysis. First, unanticipated and anticipated reductions in international borrowing costs, and not “catching-up”, have been the main drivers of the EA periphery imbalances, explaining from a third to half of current account movements. Second, quantitatively, overall anticipated shocks explain a large part of business cycle fluctuations, especially of international variables such as the current account (47 percent) and the real exchange rate (45 percent). Results are robust for Greece, Ireland, Portugal and Spain estimated both jointly or separately, and for a multitude of estimated models (with and without anticipated shocks and government spending====) and observable variables considered (with and without consumption and spread series). Identification crucially comes from the joint behavior of macro variables: increasing current account deficits with both increasing consumption and investment (in the tradable and non-tradable sector), growing output and real exchange rate appreciation.====The estimation of the models assigns to ====, ==== and ==== anticipated movements in yield spreads a crucial role. These were indeed three characteristics of the experienced yield convergence in EA periphery. In section 4.2 and 4.3 I present two results as supportive evidence: first, an important part of the yield spread contraction between 1996-2007 was not driven by macro fundamentals (following De Grauwe and Ji (2013) and Perego (2020) methodology) and that a significant fraction of the unexplained movements was correlated with agent's expectations (i.e. inflation forecast, confidence indicator and trust in institutions); second, the smooth convergence of the IPS long-term yield spread is consistent with a simple expectation hypothesis of the yield structure where agents anticipate a ==== total convergence of rates across EMU countries. This evidence is in line with the existing literature that identify four groups of possible drivers of yield spreads suppression: 1. Fall in policy risk (i.e. delegation to a credible monetary authority (Barro and Gordon (1983), Swanson (2008)) and decreased political risk (Hale and Obstfeld, 2016)); 2. Harmonization of collateral treatment (Buiter and Sibert, 2005) and financial regulations (Kalemli-Ozcan et al., 2010); 3. Fall in exchange rate risk and transaction costs (Martin and Rey, 2004) and (Hale and Spiegel, 2012); 4. Fiscal policy convergence (Swanson, 2008) and (Catao et al., 2017). These plausible causes, with a necessary remark for fiscal convergence, are in large part exogenous to GIPS macroeconomic conditions. As for fiscal convergence, Swanson (2008), Catao et al. (2017) and my analysis in section 4.3 show that fiscal budget variables, and fundamentals more generally, can hardly explain more than a small fraction of spreads contraction.====The paper is organized as follows. Section 2 describes the economic environment while section 3 illustrates the Bayesian estimation of the model (in a common block and country-by-country). Section 4 investigates how structural shocks explain the current account imbalances and examines the importance of anticipated shocks for current account and real exchange rate fluctuations. Estimation results are presented for GIPS and for different specifications of the model. Section 5 concludes.",The euro area periphery and imbalances: Is it an Anticipation Story?,https://www.sciencedirect.com/science/article/pii/S1094202520300909,25 September 2020,2020,Research Article,144.0
Izumi Ryuichiro,"Department of Economics, Wesleyan University, Middletown, CT, USA","Received 9 December 2019, Revised 18 September 2020, Available online 23 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.007,Cited by (1),"What are the effects of banks holding opaque, complex assets? Should regulators require bank assets to be more transparent? I study these questions in a model of financial intermediation where opacity determines how long the realized value of an asset remains unknown. By allowing a bank to sell assets before the realization is known, opacity provides insurance to the bank's depositors. However, higher opacity also increases depositors' incentives to join a bank run. In choosing the level of opacity, therefore, a bank faces a trade-off between providing insurance and increasing fragility. If depositors can accurately observe the level of opacity, banks will choose the socially-efficient level. If depositors are unable to observe this choice, however, banks will have an incentive to become overly opaque and regulation to limit opacity can improve welfare.","Just over a decade ago, the economy was in the midst of the global financial crisis. An important feature of this crisis was widespread ==== in which depositors and other creditors withdrew funds from a variety of shadow banking arrangements.==== One such arrangement was Asset-Backed Commercial Paper (ABCP) conduits, some of which invested funds into complex assets whose value was difficult to assess in a timely manner. This type of ==== is blamed for causing or at least exacerbating the global financial crisis. The Dodd-Frank Wall Street Reform and Consumer Protection Act was introduced in 2010 “to promote the financial stability of the United States by improving accountability and transparency in the financial system.” Subsequently, new rules were adopted that impose stronger prudential standards on financial firms that use derivatives and prohibit commercial banks from sponsoring and investing in hedge funds. It is, however, said that banks have been historically and purposefully opaque. This opacity enables banks to issue ==== liabilities by keeping asset qualities unknown and isolating the valuation of liabilities from the risk of assets.==== Doing so allows bank liabilities to be a stable medium of exchange and store of value. This role of opacity is an important feature not only of traditional commercial banks but also of shadow banks.==== For example, an ABCP conduit issues information insensitive liabilities in the form of commercial paper backed by Asset-Backed Securities, Mortgage-Backed Securities, or derivatives that may be highly complex and risky. The disparity between these two views raises a fundamental question: should the banking system be transparent or opaque?====This paper addresses the question by constructing a version of the Diamond and Dybvig (1983) model of financial intermediation that illustrates the costs and benefits of opacity in a unified framework. In particular, I study an environment with financial markets and fundamental uncertainty as in Allen and Gale (1998) and with limited commitment as in Ennis and Keister (2009). I add the ability of a bank to make its assets opaque in the sense that it will take time to discern the true value of the assets. Until the true state is known, the bank's assets will trade in financial markets based on their expected payoff. By choosing the level of opacity, the bank determines how many of its depositors will be paid while its assets remain information insensitive in this sense. The bank's assets mature in the long-term and yield the realized return, which implies that the bank's repayments to its creditors in the long-term are necessarily contingent on the realized return. Opacity, therefore, can make the bank's liabilities information insensitive only in the short-term. This fact, in turn, affects depositors' decisions on when to withdraw. I show that while opacity is a way to provide insurance to the bank's depositors, it may at the same time worsen financial fragility. I use the model to derive the optimal level of opacity and discuss the conditions under which regulation that limits opacity is desirable.====In practice, opacity is interpreted as the complexity of a bank's assets. Derivatives and asset-backed securities tend to be complex and hard to assess, and even financial firms themselves may have difficulty assessing their asset qualities.==== For this reason, I assume ====: neither the bank nor depositors and outside investors have information on the asset quality during the information insensitive period.==== Assets can be structured with various degrees of complexity and, hence, I assume the choice of opacity is a continuous variable.====I begin my analysis by showing that opacity generates a risk-sharing opportunity in the spirit of the classic Hirshleifer (1971) effect. The market price of the bank's assets depends on their expected return until the realized return is known and, hence, opacity provides depositors with insurance against fundamental uncertainty. In other words, opacity allows the bank to transfer the asset-return risk from risk-averse depositors to risk-neutral investors. A higher level of opacity insures more depositors from uncertainty in the short-term. The repayments made to depositors who wait to withdraw are made using matured assets and, hence, are necessarily exposed to uncertainty. My first contribution is to discover a novel mechanism through which this type of insurance raises the possibility of a self-fulfilling bank run. The insurance offered by opacity is available only to depositors who withdraw before the asset returns are known. This fact gives depositors an incentive to withdraw early, before the information is revealed, which increases fragility.====My second contribution is to derive the optimal level of opacity. In choosing a level of opacity, the bank faces a trade-off between providing insurance and increasing its susceptibility to a run. The optimal level of opacity depends on the extent to which outside investors discount future consumption and on the volatility of asset returns. When asset returns are more volatile, for example, the insurance is more beneficial and a higher level of opacity is optimal. My third contribution is to show that when the choice of opacity is unobservable by depositors, regulating opacity can improve the allocation of resources and financial stability. Depositors may have difficulty evaluating the details of complex structures of derivatives or asset-backed securities. I find that, in this case, the bank will choose the highest possible level of opacity. As a result, the associated expected utility of depositors will be lower, and fragility will be higher than is optimal. Introducing a regulatory limit on opacity can then improve welfare. This analysis provides a novel justification for regulating opacity.==== My paper contributes to a growing literature on opacity and financial stability. While various mechanisms of financial stability and fragility have been proposed in the literature, beginning with the seminal works of Bryant (1980) and Diamond and Dybvig (1983), my paper is the first to study how opacity itself makes depositors more likely to panic and thereby show that higher opacity is always worse for financial stability. Existing papers on opacity in theoretical models of bank runs or roll-over risk conclude that opacity enhances or has mixed effects on financial stability. The key differences in these studies are the assumption of asymmetric information and the focus on information-driven bank runs. Jacklin and Bhattacharya (1988) augment Diamond and Dybvig (1983) to incorporate risky investments and study bank runs triggered by providing information about asset returns in the intermediate period. Parlatore (2015) builds a global game model of bank runs based on Goldstein and Pauzner (2005) and shows that transparency increases the economy's vulnerability to bank runs. She interprets the precision of private signals about fundamentals as opacity. In her environment, transparency means precise information about the fundamental state, which enhances the strategic complementarity of depositors' withdrawal decisions. She shows that a lower precision, or opacity, reduces the risk of bank runs by removing possible coordination incentives. Bouvard et al. (2015) and Ahnert and Nelson (2016) also study the roll-over behavior of a bank's creditors in a global game and show that transparency reduces the likelihood of bank runs at solvent banks during crises but increases fragility during normal times. Chen and Hasan (2006) study transparency and the contagion of bank runs. They build a version of the Diamond-Dybvig model with two banks in which investment returns are random but correlated. Their results show that transparency may increase the chance of contagion when a run occurs at a bank. In contrast with all of these papers, my paper studies an environment with symmetric information and it is the ==== that increases the run susceptibility.====My analysis also contributes to a growing literature on information and risk-sharing in financial intermediation. A popular idea in this literature is that less information can enhance risk-sharing. My paper shares the idea that opacity improves risk-sharing with Kaplan (2006) and Dang et al. (2017). Kaplan (2006) extends Diamond and Dybvig (1983) to include risky investment and compares two types of deposit contracts: the middle-period repayments are contingent (transparent) or non-contingent (opaque) on the realization. In contrast, I measure opacity by the time it takes to verify asset returns and, hence, the degree of opacity is continuous. He studies the optimal banking contract and shows that a non-contingent contract generates risk-sharing effects. The idea that less information can improve welfare originated in Hirshleifer (1971). While Kaplan (2006) assumes that policymakers can prevent bank runs costlessly with commitment, my paper studies an environment with limited commitment and, hence, runs may occur in equilibrium. Dang et al. (2017) study the effects of opacity on roll-over behavior in a model of financial intermediation. They show that a bank can provide a fixed amount of goods, also known as safe liquidity, independent of the realized return of its assets if the returns are unobservable, or opaque. My paper shares the idea of the Hirshleifer effect with these papers but goes further by showing how this type of risk-sharing mechanism not only affects the allocation of resources but also increases financial fragility.====The idea that opacity enhances financial stability is often studied with the risk-taking behavior of banks. Jungherr (2018) characterizes the optimal level of opacity in an environment where opacity reduces the risk of bank runs but encourages banks to take an excess risk. He shows that when asset returns are correlated, banks choose higher opacity than the socially optimal level in order to hide information about their portfolio. Cordella and Yeyati (1998), Hyytinen and Takalo (2002) and Moreno and Takalo (2016) also show that transparency may enhance the bank's risk-taking and increases the chance of a bank failure. Shapiro and Skeie (2015) study the optimal disclosure about bailout policies in resolving a bank. A higher willingness to bail out reduces run incentives of depositors but leads to riskier behavior of the bank. In my model, the bank does not have a portfolio choice, and the bank's risk-taking is not the source of fragility.====Adverse selection is another growing idea in studying opacity, together with runs. Faria-e Castro et al. (2017) model disclosure by combining the ideas of bank runs, competitive financial markets as in Allen and Gale (1998) and the Bayesian persuasion approach. My paper also combines the idea of bank runs and competitive financial markets, but supposes symmetric information. They study how asymmetric information drives adverse selection in financial markets, but disclosure negatively affects runs or roll-over risk, characterizing the optimal use of disclosure. My paper also introduces a financial market in which a bank can trade its assets, but one of the key assumptions of the paper is that the assets will be traded at a discounted pooling price.====Opacity is also part of recent discussions of disclosing stress test results or bank balance sheet information. Goldstein and Sapra (2014) review this literature and show that opacity is preferred in a majority of studies. Goldstein and Leitner (2018) emphasize the Hirshleifer effect to characterize optimal information disclosure. Alvarez and Barlevy (2015) study mandatory disclosure of bank balance sheets in an environment where banks are interconnected. They show that the mandatory disclosure of a bank's balance sheet may reassure not only its creditors but also other banks' creditors by reducing concerns of contagion, but it loses an opportunity of risk-sharing. Monnet and Quintin (2017) study how much information investors should receive. The investors can liquidate their investment by selling it to other investors or scrapping them. They show that more information improves investors' scrapping decisions, but raises the risk of liquidation losses. These works suppose that a regulator or bank has more information about bank's assets, whereas my paper studies an environment of symmetric information.====The rest of the paper is organized as follows: Section 2 introduces the model environment and the definition of equilibrium and financial fragility. Section 3 derives the equilibrium condition for a bank run and analyzes the effect of increasing opacity on financial fragility. Section 4 characterizes the optimal level of opacity subject to the trade-off between risk-sharing and fragility. I study the case where the choice of opacity is unobservable in Section 5 and then conclude.",Opacity: Insurance and fragility,https://www.sciencedirect.com/science/article/pii/S1094202520300892,23 September 2020,2020,Research Article,145.0
Stokey Nancy L.,"Kenneth C. Griffin Department of Economics, University of Chicago, United States of America","Received 25 June 2020, Revised 15 September 2020, Available online 23 September 2020, Version of Record 29 September 2021.",https://doi.org/10.1016/j.red.2020.09.008,Cited by (5),"The importance of new technologies derives from the fact that they spread across many different users and uses, as well as different geographic regions. The ==== of technological improvements, across producers within a country and across international borders, is critical for long run growth. This paper looks at some evidence on adoption patterns in the U.S. for specific innovations, reviews some evidence on the diffusion of new technologies across international boundaries, and looks at two theoretical frameworks for studying the two types of evidence. One focuses on the dynamics of adoption costs, the other on input costs.","Sustained long-run growth requires the adoption of new technologies.==== Thus, innovation, whether it is costly R&D or serendipitous discovery, is fundamental for growth and has—deservedly—been well studied. But the importance of most new technologies derives from the fact that they spread across many different users and uses, as well as different geographic regions. Thus, the ==== of technological improvements, across producers within a country and across international borders, is arguably as critical as innovation for long run growth. Technology diffusion is the focus here.====Good data on diffusion are not readily available. Indeed, for many innovations, there are none at all. This paper looks at the evidence on adoption patterns and rates in the U.S. for several specific innovations where good micro data have permitted detailed studies. It then reviews some of the evidence on the diffusion of new technologies across international boundaries, where data is even more limited. No attempt is made to review all the work on technology adoption.====The discussion is selective and focuses on the role of cost reduction. Specifically, two aspects of cost are considered. The first involves the dynamics induced by changes in the fixed cost of adoption. Adoption takes time, and economic motives govern who adopts a new technology and how quickly they adopt it. As use of an innovation increases, its quality typically improves and its cost of adoption falls. Consequently, early adoption by some users facilitates later adoption by a broader set of users. The dynamics of adoption costs are important for explaining diffusion across users in a single environment.====The second aspect of cost involves relative input prices. Many new technologies are, by design, labor-saving and capital-using, so their attractiveness depends on the relative prices of capital and labor inputs. Wage rates vary enormously across countries, while the cost of capital varies much less. Hence relative wage rates are important for explaining diffusion across countries.====Direct evidence on adoption patterns across countries is rarely collected, but indirect evidence is sometimes available. Many technologies are ‘embodied’ in new capital goods, specific to them. This fact is useful, since good data are available on investments in tangible capital. Moreover, technologies that are embodied in capital goods have a unique method for international diffusion: the capital goods themselves are highly traded. Thus, equipment imports are a channel by which one country—either developed or developing—can acquire technology from abroad. Indeed, for developing countries a large fraction of their total investment in producer equipment consists of imported goods coming from advanced countries. Moreover, there are good data on imports of equipment types, by fairly narrowly defined sector, as well as by source country.====Before proceeding, two limits on the scope of this study should be noted. First, only ==== technologies are considered here. New consumer goods are also important for welfare, but their diffusion is explained by a different set of factors. For the same reason, there will be no discussion of adoption of high-yield varieties (HYV's) in India, sub-Saharan Africa and elsewhere.====Finally, note that the focus here is on diffusion of ====, as opposed to ====. The former are adopted by producers—industrial firms or agricultural enterprises—and then utilized by that producer's workforce, be it one individual or a large group. Thus, the adoption decision is at the producer level, and the technology is a non-rival input across that producer's workers. In contrast, ideas are the property of individuals: an individual can utilize only ideas that he/she has adopted. Ideas are surely a more fundamental concept: all technological innovations begin with an idea.==== But technologies are—perhaps—easier to measure.====The rest of the paper is organized as follows. Section 2 reviews several detailed studies of the diffusion of particular technologies across producers in the U.S. Section 3 looks at the evidence on cross-country diffusion, including evidence on productivity in agriculture. Section 4 looks at two simple models of technology diffusion across producers that are compatible with much of the evidence. The first is suitable for looking at diffusion within a single country, the second for looking at cross-country diffusion. Section 5 concludes.",Technology diffusion,https://www.sciencedirect.com/science/article/pii/S1094202520300880,23 September 2020,2020,Research Article,146.0
Schiman Stefan,"Austrian Institute of Economic Research (WIFO), Arsenal 20, A-1030 Vienna, Austria","Received 16 April 2019, Revised 14 September 2020, Available online 17 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.005,Cited by (4),. Beyond these findings the paper addresses empirical regularities of labor supply shocks that are at odds with theoretical predictions.,"According to the canonical search and matching model of the labor market the Beveridge Curve shifts due to reallocation shocks or, transitorily, due to aggregate activity shocks (Elsby et al., 2015).==== Reallocation shocks are typically thought of as exogenous variations in labor market matching, either due to sectoral shifts in job creation and job destruction (structural change) or changes in matching efficiency (Abraham and Katz, 1986; Lilien, 1982). As a result, unemployment and vacancies move simultaneously in the same direction and shift the Beveridge Curve closer to or further away from the origin.==== Aggregate activity shocks, on the other hand, are usually associated with movements along the Beveridge Curve. These movements may be accompanied by transitory outward shifts if vacancies respond faster to business cycle fluctuations than unemployment (Hansen, 1970). Christiano et al. (2015) show that such transitory deviations from an otherwise stable relationship can account for the movement of the US Beveridge Curve after the Great Recession.====In addition to these well-known effects, exogenous changes in the supply of labor are another independent factor that affects the position of the Beveridge Curve. The role of labor supply shocks was first addressed by Blanchard and Diamond (1989). Based on the empirical observation that the unconditional correlation between employment and the labor force is relatively high in the US, they conclude that causality runs not only from employment to the labor force (through the effects of aggregate activity shocks and reallocation shocks on participation), but that causality must run in both directions.==== They postulate a positive conditional correlation of employment and unemployment in response to labor supply shocks. With this identifying assumption they find that higher labor supply leads to increased matching and, hence, on impact to a reduction of posted vacancies. Improved matching stimulates new hires (labor demand), such that vacancies increase in the medium run and unemployment declines. Taken together, labor supply shocks trigger a counterclockwise movement in the Beveridge space.====However, Blanchard and Diamond (1989) find labor supply shocks to be hardly empirically relevant for movements of the postwar US Beveridge Curve. Since then, exogenous variations in labor supply have played at best a subordinated role in the study of Beveridge Curve dynamics. This paper reemphasizes labor supply shocks and shows that they can have significant effects, in particular with respect to job-related migration.====I demonstrate this with the example of Austria, a small country in central Europe, whose Beveridge Curve has shifted substantially outwards in recent years (Fig. 1). This shift was preceded by a liberalization of the access to its labor market for a large group of foreign workers whose home countries had joined the European Union (see Section 2). I estimate structural vector autoregressions and identify various shocks proposed in the literature.====Historical decompositions show that the bulk of the observed shift is indeed attributable to labor supply shocks. Their superiority as compared to other competing sources of Beveridge Curve movements rests on two developments. First, the staggered increase of unemployment and vacancies only fits the pattern of labor supply shocks. It is in contrast to that of aggregate activity shocks which would require that vacancies move first. Reallocation shocks, meanwhile, usually involve a more simultaneous movement of unemployment and vacancies. Secondly, the increase in unemployment went along with an increase in aggregate employment. This is an exclusive property of labor supply shocks, as these variables would correlate negatively in response to reallocation shocks and aggregate activity shocks.====The paper provides several further insights. In a regional analysis, labor supply shocks are found to be distributed unequally across regions: The closer the region is to the migrants' home countries, i.e. the more likely it is that workers will be able to commute or at least make short-term stays at home, the more intensive the shocks will be. I also find that foreign workers displace domestic workers in the short run, but that this displacement effect is only transitory and that domestic employment returns to its pre-shock level in the medium run. Furthermore, I substantiate previous empirical evidence that labor supply shocks have inflationary effects. This suggests that they entail stronger (labor) demand repercussions than predicted by theoretical models. I discuss the consequences of this result for sign restrictions and conclude that caution is needed when labor supply shocks are identified without resorting to a proper restriction of unemployment.====  The paper draws from and contributes to a strand of the literature that uses vector autoregressions (VARs) identified by sign restrictions to analyze labor market dynamics. Fujita (2011), Cairó et al. (2019) and Fontaine (2019) identify shocks along the US Beveridge Curve to study the response of various labor market variables. Hairault and Zhutova (2018) investigate the relative contributions of the job-finding rate and the separation rate to unemployment fluctuations in France and the US in response to multiple structural shocks. Benati and Lubik (2014) combine sign and long-run restrictions to identify transitory and permanent shocks and investigate time variation in the US Beveridge Curve during the postwar period. Mumtaz and Zanetti (2012) use a sign restriction identification scheme to study the dynamic responses of labor input to technology shocks.====With respect to labor supply shocks, an important contribution has been made by Foroni et al. (2018). These authors derive sign restrictions from a fully fledged New-Keynesian macro model with search and matching frictions and apply them to VARs estimated on US data. They find that as labor supply shocks raise the number of job seekers, competition for vacancies intensifies. This makes it more difficult for existing job seekers (unemployed) to be matched to a vacancy, bolstering the stock of unemployment. On the other hand, higher supply of labor and a potentially richer variety of skills enables firms to fill their vacancies faster, bolstering the stock of employment and dampening the stock of vacancies. The labor supply shock dampens wages and stimulates labor demand. This lifts employment further, it fosters vacancy postings and dampens the heightened level of unemployment. Again, labor supply shocks result in a counterclockwise outward movement of the Beveridge Curve.====There is also a nascent VAR literature on migration-specific labor supply shocks; important contributions include Kiguchi and Mountford (2019) on the US, Furlanetto and Robstad (2019) on Norway and Smith and Thoenissen (2019) on New Zealand. This is the first paper that explicitly addresses Beveridge Curve dynamics of labor supply shocks. In doing so, it draws on the case of Austria which experienced an acceleration of immigration due to the enlargement of the European Union. The analysis of regional effects is another novel contribution to the literature. Furthermore, the paper addresses the inflationary effect of labor supply shocks and discusses its consequences for identification.====  The analysis proceeds as follows: Section 2 provides some background information on the institutional context of the labor supply shock under investigation. Section 3 sets out the empirical model and the identification assumptions. Section 4 presents the results for impulse responses, Beveridge Curve counterfactuals, a specification that distinguishes between domestic and foreign workers and a regional analysis. It also places the findings on the Beveridge Curve in a broader context of labor market developments in Austria. Section 5 introduces an extended model and Section 6 concludes.",Labor supply shocks and the Beveridge Curve — Empirical evidence from EU enlargement,https://www.sciencedirect.com/science/article/pii/S1094202520300818,17 September 2020,2020,Research Article,147.0
"Borri Nicola,Reichlin Pietro","LUISS, Italy,LUISS & CEPR, Italy","Received 18 November 2019, Revised 9 September 2020, Available online 16 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.003,Cited by (3),"We consider ==== in a model with wealth-poor and wealth-rich households, where ","There is mounting consensus, among scholars and commentators, that shifting taxation from labor to capital may be an optimal response to the increase in wealth–to–income ratios and wealth inequality that has been documented for many advanced economies over the last decades (Piketty and Saez, 2003; Piketty and Zucman, 2014; Saez and Zucman, 2016; Fagereng et al., 2016; Piketty et al., 2017). This policy is mostly motivated by distributional objectives and it is sometime credited as having small efficiency costs (Piketty et al., 2015; Saez and Zucman, 2019). In this paper we investigate the long-run social welfare effects of such tax reform under full commitment by considering a simple model where households accumulate different levels of wealth; the latter consisting in business capital, housing, and financial assets; and the government has access to a limited set of tax rates (a flat tax on wages, housing rents and wealth, the latter being possibly contingent on the types of wealth and on the households' net asset position). We show that an optimal tax structure implies heterogeneous tax rates/subsidies on housing wealth and no tax on financial and business capital.====In our model labor supply is inelastic and households can be lenders or borrowers, homeowners or renters. Wealth heterogeneity is based on the assumption that households have different time discount rates and face borrowing constraints, so that some households end up having zero net wealth. In this set up, the steady state distribution of wealth is perfectly polarized between a set of ==== and ==== households, although all of them may work and own some housing in different quantities. The only relevant difference between the two sets of households is that, due to an indivisibility, the wealth-poor sort themselves between renters, with zero home ownership, and homeowners, with the value of their home perfectly matched by mortgages. The supply side of the economy includes two produced goods; a perishable consumption good (also called ====); and residential construction. The latter generates an evolving stock of housing subject to physical depreciation. Technologies employ labor and capital, although the housing sector also needs some flow of new land available for construction every period. All the revenues from the sale of land permits go to the government, either because it is the only land owner or because, despite land being privately owned, these revenues are fully taxed by the government.====Within this set up, we study the optimal tax problem assuming that the planner can choose among a limited set of linear tax rates: an income tax, a tax on business capital, a tax (or subsidy) on housing capital and a tax (or subsidy) on rental rates. Importantly, we assume that income taxes are non individual contingent whereas wealth taxes may be contingent on the households' net asset position, ====, on whether the household has positive or negative financial wealth net of mortgage debt. Within this limited menu of taxes, we show that the Chamley-Judd zero steady state tax on financial and business capital survives (Chamley, 1986; Judd, 1985), whereas housing wealth is taxed at a non zero rate. In particular, we show that it is optimal to impose a positive tax on the rich households' housing wealth, and a subsidy on the user cost of housing (or rent) faced by poor households. For poor homeowners, this can be implemented as a negative tax on housing wealth or imputed rents.====Using a sufficient statistics approach we provide a characterization of the optimal tax rates (and subsidies) on housing in terms of price elasticities in two distinctive cases: the case of quasilinear and CES utility. In the former case, income effects disappear and a standard result obtains: the housing wealth tax on the rich and the housing subsidy to the poor depend inversely on the housing price elasticity. For the case of a CES utility, instead, price elasticities are not the only determinants. Both housing taxes and subsidies are decreasing in the elasticity of substitution between consumption and housing services and, most importantly, the size of the rich households' capital income as a share of their consumption affects negatively the housing tax and positively the housing subsidy. More generally, the behavior of the housing tax rates is related to the “general equilibrium elasticities” of consumption and housing services====: the larger is the former relative to the latter, the larger are the efficiency gains from shifting taxation to housing wealth. In our model, the elasticity of consumption is decreasing, and the elasticity of housing increasing, in the households' capital income as a share of consumption. Hence, other things equal, the higher is the share of income coming from wealth, the lower is the optimal housing wealth tax. Since the poor have zero wealth, a rising aggregate wealth-to-income ratio has no or little effects on housing subsidies but it may have a strong effect on the rich households' housing tax rates.====Based on this finding, we use a Cobb-Douglas representation of preferences and technology to evaluate numerically the impact on optimal tax rates of a rising aggregate wealth-to-income ratio generated by two alternative mechanisms: an increase in public debt or a fall in the real interest rate. We show that the behavior of the optimal tax rates changes dramatically according to which of the two mechanisms is in place. If wealth rises because of a rising public debt, then the optimal income tax rises; the housing subsidy is flat at around 2%; and the housing tax on the rich households falls substantially. When, instead, aggregate wealth rises as a consequence of a falling real rate, then the optimal income tax falls (by a small amount); the housing tax on the rich households rises strongly; and the housing subsidy falls by approximately one percentage point. In both scenarios, the housing subsidies are small, while the housing tax rates are large (between 30 to 60%).====We additionally find that the tax on income falls with wealth when the interest rate drops, compared to the first scenario, because the gross wage rises substantially, thereby generating a larger tax base. Note that, under our parametrization of preferences and technologies, the second mechanism (a drop in the interest rate) goes along with a stronger re-adjustment of all equilibrium variables, and, in particular, it generates a higher stock of capital (to compensate for the lower marginal productivity), a higher housing wealth (and prices), and a higher level of the poor households' mortgage debt.==== These different patterns are consistent with the role of capital income in affecting the general equilibrium elasticities that come out from the planning optimum. Namely, if a rising wealth-to-income ratio is obtained through a larger public debt that leaves the real interest rate unaltered, then rich households' capital income rises, so that the general equilibrium elasticity of housing grows relative to consumption and, then, it is optimal to decrease the housing wealth tax. If, on the other hand, a rising wealth-to-income is obtained through a falling real rate, then it is possible (as it happens in our simulations) that capital income falls relative to wages, so that the optimal housing wealth tax rises. This suggests that the way wealth taxes should respond to rising wealth and wealth inequality is far from obvious.====Finally, we provide an estimate of how the economy responds to an ==== shift of taxation from labor to wealth. In particular, using again the Cobb-Douglas specification of preferences and technologies, we simulate the impact of introducing of a 1% tax on net wealth starting from zero (====, from the case in which only labor is taxed) and we show that this increases net wages modestly (by 1-3%), but has a strong positive effect on the user cost of housing faced by poor households (====, the effective price of housing services). We estimate that an income-equivalent welfare loss of this policy for poor households is around 5%, and these numbers are not substantially affected when the aggregate wealth-to-income ratio increases. The basic intuition is that, at steady states, the net of tax interest rate is given by the rich households' rate of time preference (====, it is invariant to the capital tax rate), so that the burden of the capital tax is shifted on the poor households, who face a higher user cost of housing and a higher cost of debt.====Our results depend on some strong assumptions. First, an inelastic labor supply makes the model biased towards the idea that wealth should not be taxed, so that a positive taxation on housing should be fairly robust.==== Second, deriving the wealth distribution from different subjective discount factors and debt limits has some limitations, although it is a very standard practice in neoclassical growth theory and, in some way, necessary to produce the stronger observed polarization in wealth than in income which is not easily reproducible in models with homogeneous preferences (Jones, 2015). Third, by concentrating the analysis on steady states we miss the analysis of the transition from low to higher tax rates, which is motivated by the need to focus on long-run phenomena.====Following the seminal contributions by Chamley (1986) and Judd (1985), the literature on optimal taxation has provided various arguments why wealth should be taxed, even in the long-run and under commitment, ranging from life-cycle considerations, precautionary savings and imperfect information. More recently, Piketty and Saez (2013) and Saez and Stantcheva (2018) have advanced the idea that the optimality of positive capital tax rates may emerge due to the ==== of the long-run supply of capital.==== In turn, finite values for the elasticity of long-run wealth are obtained by assuming that the latter (or the services it generates) enters the individuals' utility function. In particular, Piketty and Saez (2013) consider a life-cycle model where households derive utility from bequests and Saez and Stantcheva (2018) assume that wealth enters the households utility function directly for various reasons, among which are “social status”, “power”, “philanthropy”. In our model housing is both a store of value and an asset that generates utility services, whereas the supply of financial and business capital retains the property of being infinitely elastic in the long run. This explains why, in our model, taxing housing wealth may be optimal, while taxing financial wealth is not. In fact, housing taxation has been advocated in several studies, especially as a way to avoid a sub-optimal tax discrimination between factor inputs and sources of wealth, and many authors have highlighted the existence of substantial welfare gains from increased housing taxation, due to the failure to tax implicit rental income and because of mortgage interest deductibility characterizing existing tax codes in most advanced economies (see Poterba (1984), Gahvari (1984), Berkovec and Fullerton (1992), Auerbach and Hines (2002), Gervais (2002) and Mirrlees et al. (2011)). These distortions imply that housing investment crowds out business capital and generates excessive levels of home ownership. Furthermore, a heavier taxation of housing wealth may reduce inequality in economies where, because of capital market imperfections and indivisibilities, rental housing is concentrated among poor households (although Gervais (2002) finds that the distributional effects of eliminating housing tax incentives are quantitatively small). Our contribution differs from this literature because we are specifically interested in (differentiated) wealth taxation and the way it should evolve in response to increasing wealth inequality, instead of examining the welfare gains from reducing fiscal incentives on housing. Whereas the case for housing taxation is usually based on the unavailability of non distorting taxes, in our model housing taxes (and subsidies) survive despite the fact that labor taxes are non distortionary. The papers most related to ours are Alpanda and Zubairy (2016) and Bonnet et al. (2020). Alpanda and Zubairy (2016) consider a model with patient and impatient households, borrowers and lenders, homeowners and renters, and build a dynamic general-equilibrium model to study the transitional and steady-state effects of a large menu of exogenous taxes (mortgage interest deductions, taxation of imputed rental income, property tax rates and a reduction in depreciation allowance). Our model shares a similar environment and studies the optimal taxation with a smaller menu of taxes. Bonnet et al. (2020) consider an economy with heterogeneous wealth composition (business capital, housing and land) and heterogeneous households (capitalists/landlords and workers/tenants). Differently from our model, they assume that poor households have no wealth (in particular, no land and housing wealth) and obtain housing services by renting from rich households. In their model, capital should not be taxed and the first best allocation can be implemented by levying a tax on land. The optimality of a land tax follows from the planner's preference for redistribution and the fact that land is a fixed factor (====, a land tax is non-distortionary).====The remainder of this paper is organized as follows: section 2 presents the model; section 3 considers the optimal taxation problem; section 4 presents quantitative results for the optima tax structure and for the introduction of an exogenous general wealth tax; finally, section 5 presents our conclusions.",Optimal taxation with home ownership and wealth inequality,https://www.sciencedirect.com/science/article/pii/S109420252030079X,16 September 2020,2020,Research Article,148.0
"Barlevy Gadi,Fisher Jonas D.M.","Federal Reserve Bank of Chicago, United States of America","Received 9 January 2020, Revised 26 August 2020, Available online 15 September 2020, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2020.09.001,Cited by (4),Borrowers in U.S. cities where ,"It is now well established that the U.S. housing boom of the 2000s was associated with a substantial increase in nontraditional mortgages.==== Many of these mortgages featured backloaded payments in which borrowers were asked to make low payments early on and then substantially higher payments later. The most popular mortgage of this type was the interest-only (IO) mortgage which obliges the borrower to pay just their interest obligation for some initial predetermined period – usually 3, 5, or 10 years – and only then require borrowers to pay both interest and principal. As we show in this paper, IOs were not just more popular when house prices rose, but they were a key feature of cities with rapid house price growth. The share of IOs is tightly correlated with the rate of house price growth in a city even after controlling for other mortgage characteristics such as the share of subprime loans, the share of privately securitized loans, and the share of mortgages with high leverage. At the same time, there is no consensus on why IOs surged in popularity, and whether this growth was a cause for concern. Among policymakers, the rise of these mortgages in markets with rapid house price growth raised alarms. In a GAO report to Congress, Williams (2006) questioned the sustainability of these mortgages, arguing that borrowers may not have been aware of all their inherent risks and that some borrowers relied on them to purchase homes for investment purposes and were more likely to default if faced with financial distress. The report suggests policymakers should consider regulating the use of these products. In contrast, economic research has highlighted various benefits of such mortgages, questioning the need to restrict them. Cocco (2013) finds in United Kingdom data between 2000 and 2008 that IO loans were primarily used by borrowers whose income was predictably set to grow and who could use such loans to relax their liquidity constraints. Chiang and Sa-Aadu (2014) argue that IO mortgages can benefit more mobile households. Dokko et al. (2019) find that IOs and other non-traditional mortgages facilitated home purchases in areas where housing became expensive. Similarly, Bäckman and Lutz (2017) argue that affordability was behind the surge in IOs in Denmark. LaCour-Little and Yang (2010) and Brueckner et al. (2016) argue that expected future house price growth led lenders to offer mortgages with backloaded payments that borrowers prefer but would be too expensive in the absence of expected house price growth. If mortgages with backloaded payments confer the benefits identified in these papers, interfering with the ability to enter such contracts could make agents worse off. Whether the rise in IO use demands a policy response depends on why such mortgages were popular in cities with rapid house price growth. To address this question, we construct a theoretical framework that features endogenous house prices and mortgage choice. We use this model to guide our empirical exploration of the various explanations for the appeal of IOs. Our evidence suggests these explanations can account for much of the variation in IO use across cities. At the same time, we find that they cannot account for the popularity of IOs in cities with rapid house price appreciation. Essentially, cities where house prices took off most quickly did not feature systematically higher expected income growth, more mobile populations, less affordable housing, or evidence of a preference for backloading, but borrowers in these cities did show a systematic penchant for using IOs. Since the residual variation in IOs after controlling for these factors remains strongly correlated with house price appreciation, we turn to a different and novel explanation for the concentration of IOs in cities with rapid price appreciation. In our model, if mortgages are not subject to recourse, meaning a lender cannot seize the borrower's income in case of default, agents may have an incentive to speculate when house price growth is risky, profiting if house prices rise but defaulting if they fall. Demand by speculators can push house prices above the expected present value of housing services of the marginal house. This overvaluation would encourage IO use. Speculators find IOs attractive since they can shift more risk to lenders, but lenders also like them because they encourage borrowers to sell overvalued properties earlier than they would otherwise, exposing themselves to less risk. Consistent with this, we find that for the cities with high price growth, IOs were more popular in non-recourse states. We also find that the borrowers who used IOs behaved in a manner consistent with speculation: They were more likely to sell their property when house prices rose and more likely to default when house prices fell than those with traditional mortgages. To the extent that the rise of IOs in cities with rapid house price growth is driven by speculation, this would suggest a role for policy. Speculators in our model take on too much leverage because they fail to internalize the costs of default that their lenders incur, so there is scope for intervention. At the same time, our analysis suggests IOs might be a symptom rather than a source of the underlying problem, and restricting IOs may be inappropriate. First, we do find that much of the variation in IO use across cities can be explained by factors that imply borrowers benefit from IOs, suggesting restricting these contracts, even if only when we see a rise in their use in booming cities, would likely harm many borrowers. Second, in our model, it is not the introduction of IOs that causes speculation, but the arrival of opportunities for speculation that lead agents to use IOs. Preventing agents from using backloaded mortgages would not discourage speculation, and agents would have an incentive to speculate even with traditional mortgages. A more effective intervention would grant lenders recourse or impose down payment requirements to directly discourage speculation. Our analysis also suggests that the rise in backloaded mortgages may be a useful beacon for alerting policymakers to the presence of speculation, and so an additional reason not to prevent their use is that it would destroy information useful to policymakers. However, our model also cautions against blindly focusing on the overall use in IOs as a measure of speculation, since we find they appeal to borrowers for other reasons. Sullivan (2018) reports a recent resurgence in IOs and cites it as a cause for concern. But if this latest rise in IOs were concentrated in recourse states, as extending our data to recent years suggests, this increase may not be a sign of rising speculation but a more benign indicator.",Why were interest-only mortgages so popular during the U.S. housing boom?,https://www.sciencedirect.com/science/article/pii/S1094202520300776,15 September 2020,2020,Research Article,149.0
"Karabarbounis Marios,Macnamara Patrick","Federal Reserve Bank of Richmond, United States of America,University of Manchester, United Kingdom of Great Britain and Northern Ireland","Received 11 January 2019, Revised 7 September 2020, Available online 15 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.09.002,Cited by (10),"We analyze misallocation of capital in a model where firms face different types of financial constraints. Private firms borrow subject to a collateral constraint while public firms issue long-term bonds subject to default risk. We estimate our model using employment and ==== reflecting the overall distribution of firms in conjunction with firm-level data on credit spreads that we target for the set of public firms. In our model, a productive private firm is unable to grow fast if its collateral is limited. But a productive public firm can overcome its financial constraints because it faces low borrowing costs in the debt market, a relationship we also verify in the data. As a result, financial frictions for private firms disrupt investment behavior to a greater degree and generate a larger misallocation of resources relative to financial frictions for public firms.","During the past decade, there has been a considerable amount of research analyzing the effect of financial frictions on misallocation of capital and aggregate total factor productivity (see for example, Buera and Shin, 2011; Khan and Thomas, 2013; Midrigan and Xu, 2014; Moll, 2014). According to this literature, firms have limited access to borrowing in capital markets and therefore have difficulty quickly reaching their optimal scale of production. The typical way these borrowing constraints are modeled is in terms of restrictions to the amount of collateralized assets. But while collateral constraints describe well the behavior of typically smaller private firms that rely on bank loans, they are less descriptive of larger firms with access to public bond and equity markets.==== The natural question that arises is: what is the impact of financial frictions on aggregate total factor productivity (TFP) in a model that captures more realistically the financing constraints of both private and public firms?====In this paper, we measure misallocation of capital in a model with two types of firms. First, private firms issue one-period bonds subject to a collateral constraint and can access the external equity market at a relatively high cost. Second, public firms borrow by issuing long-term defaultable bonds in the debt market and can issue external equity at a relatively low cost. Firms are born as private entities and can transition to public status by paying a one-time fixed cost. We integrate our life-cycle model with private and public firms into an otherwise standard model of firm dynamics with idiosyncratic productivity shocks, capital adjustment costs, and firm entry and exit.====We find that financial frictions have a larger impact on private firms relative to public firms. Although standard intuition suggests that private and public firms differ in size, we argue that our results hinge on another key difference between the two types of firms. A productive private firm is unable to grow fast if its collateral is limited. But a productive public firm (even a small one) can borrow cheaply in the bond market because lenders take into account the high stream of expected profits. As a consequence, the severity of financial frictions depends on the nature of financial constraints.====We estimate our model using statistics reflecting the overall U.S. distribution of firms (both young and mature) over leverage and employment in conjunction with a broader set of financial moments that characterize the behavior of larger public firms. These financial moments are constructed using micro-level data on bond issuances and credit spreads from Thomson Reuters Bond Security Master Data and financial data from Compustat.====Our model closely replicates two important features of the credit market: first, the sizable dispersion in the cross-sectional distribution of credit spreads; second, the negative relationship between credit spreads and firm productivity. The necessary model element to replicate these empirical patterns is long-term financing. With long-term bonds, credit spreads are determined based on the whole sequence of default probabilities until the bond matures. The large heterogeneity in outcomes over long horizons generates a large dispersion in default probabilities across firms and, hence, credit spreads. In contrast, with one-period bonds, credit spreads are priced only based on the next period's probability of default. This results in a distribution that is too concentrated around the risk-free rate.====In our main quantitative experiment, we eliminate financial frictions and analyze the change in macroeconomic aggregates overall, as well as separately, for private and public firms. When we shut down financial frictions, capital increases overall by 33%. However, private firms increase their capital by 55% while public firms only by 7%. More importantly, when we eliminate financial frictions only for public firms, TFP increases (due to a more efficient allocation of resources) by 1.9%. In contrast, when we eliminate financial frictions only for private firms, TFP increases by 3.5%. Therefore, financial frictions are more severe for private firms that face collateral constraints relative to public firms that can issue long-term bonds.====Our paper's primary contribution is to analyze the impact of financial frictions on misallocation in a model that captures more realistically the financing constraints of both private and public firms.==== Moll (2014) analyzes the role of persistence in productivity shocks and self-financing in a model where all firms are constrained by means of their collateralized assets. Midrigan and Xu (2014) analyze an economy where financial frictions disrupt the transition from a relatively unproductive, traditional sector into a modern, productive sector as well as generate dispersion of capital within the productive sector. Both sectors face the same type of financial constraint in the form of a collateral constraint. In our model, firms start their life cycle with a collateral constraint but have the choice to use the public debt market as a source of financing. We show that the effect of financial frictions is modest among public firms since they have cheaper access to capital markets. Gilchrist et al. (2013) calculate misallocation based on the credit-spread distribution of U.S. public firms using a static model. We also calculate misallocation using information on credit spreads, but we estimate a dynamic model with long-term bonds.====Khan and Thomas (2013) model financial frictions and capital adjustment costs jointly. In their paper, capital adjustment costs propagate financial shocks by preventing young firms from quickly reaching their optimal scale. Moreover, Khan et al. (2016) study the effect of financial shocks in an economy with default risk. Jo and Senga (2019) analyze the aggregate implications of targeted credit subsidies in a model with heterogeneous firms, collateral constraints, and endogenous entry and exit. A major difference with the aforementioned studies is that our paper allows for long-term bonds and shows that it is a necessary feature to match the cross-sectional distribution of credit spreads.====Another strand of the literature studies the heterogeneity in firms' financial positions and its effect on aggregate outcomes. Crouzet and Mehrotra (2018) use detailed micro data and show that it is only the very largest firms that are less sensitive to cyclical fluctuations. Jeenas (2018) studies the effect of monetary policy on investment as a function of the firms' leverage and liquidity. Similarly, Ottonello and Winberry (2019) explore the effect of monetary policy on investment as a function of firms' financial positions. They show that firms with low default risk are the most responsive to monetary shocks. Bustamante (2019) analyzes empirically and quantitatively the effect of long-term debt on investment decisions as well as the role of monetary policy. Our addition to this literature is to demonstrate that heterogeneity in financial constraints (manifesting in terms of collateral constraints or long-term default risk) is an important aspect to consider when analyzing misallocation of capital and aggregate productivity.====In addition, our work is related to several recent papers that analyze either multiple types of firms or multiple types of debt financing. Zetlin-Jones and Shourideh (2017) measure the use of external finance by private and public firms and, as a result, their response to financial shocks. In their model, both private and public firms share the same type of financial constraint (in the form of a collateral constraint). Moreover, Dyrda and Pugsley (2018) analyze the organizational form of firms and build a model with C corporations and pass-through companies. Finally, Crouzet (2018) models heterogeneous firms with a variety of choices of debt instruments: bank loans versus market debt. The emphasis in that paper is on the aggregate and cross-sectional composition of financing while in our paper it is on the cross-sectional behavior of credit spreads.====Our paper is also related to the growing literature on long-term financing. Hatchondo and Martinez (2009) and Chatterjee and Eyigungor (2012) were the first papers to introduce long-term borrowing in a tractable model of sovereign default. Gordon and Guerron-Quintana (2018) introduce capital in a sovereign default model with long-duration bonds and calibrate the model to cross-country moments. Gomes et al. (2016) focus on the role of monetary policy and inflation in a model with long-duration bonds and investment. Crouzet (2017), Sánchez et al. (2018), and Jungherr and Schott (2020) use models with an endogenous maturity choice. We use a simpler setup with bonds maturing probabilistically in order to structurally estimate our model using firm-level data.====The paper is organized as follows. Section 2 describes the model. Section 3 describes the empirical analysis, and Section 4 describes the structural estimation exercise. Section 5 reports the main results. Section 6 explores how some of the key features of our model contribute to our findings. Finally, Section 7 concludes.",Misallocation and financial frictions: The role of long-term financing,https://www.sciencedirect.com/science/article/pii/S1094202520300788,15 September 2020,2020,Research Article,150.0
"Obstfeld Maurice,Rogoff Kenneth","University of California, Berkeley, United States of America,Harvard University, United States of America","Received 7 July 2020, Revised 26 August 2020, Available online 1 September 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.08.004,Cited by (2)," (====, ==== (====, ====), however, argues that this result is wrong, and that fractional currency backing is a Maginot line that is insufficient to rule out hyperinflation. We show here why, in fact, his analysis involves a subtle change in model specification that adds a distinct monetary fragility to our model. Our baseline analysis uses a canonical money-in-the-utility-function setup due to ==== (====, ====), but following ====, we show the same results go through in an overlapping-generations model of money.","We begin by reprising our argument that by backing currency, a government can foreclose the possibility of hyperinflationary equilibria. We will use the slightly simpler version of the model in ==== (====, ====), also employed in ====Individuals receive ==== units of the perishable consumption good each period. Let ==== denote an individual's consumption rate at time ====, ==== her nominal money holdings, and ==== her subjective discount factor. The infinitely lived representative consumer maximizes====where ==== is the price level at time ====, subject to====with ==== denoting transfers from the government that the individual takes to be exogenous. We assume that ==== and ==== are increasing and strictly concave, with the usual smoothness and Inada properties. The above assumption of separability between the utility from consumption and the derived utility from holding cash balances is quite important, as ==== (====, ====) emphasize: otherwise, even though there may not exist divergent speculative paths, there can still be multiple stable equilibrium paths converging to the same long-run monetary equilibrium (as shown in ====), a fundamental point to which we return later. The consumer's first-order necessary conditions imply==== for ====.==== illustrates how, unless the price level starts out exactly at the unique stationary equilibrium level ==== (that is, where the price level reaches infinity at time ====) if the prior path of real balances ==== ends at the point ==== where====Even though money becomes worthless at ====, it still gives just enough marginal benefit at time ==== to compensate for the (small) loss of consumption the individual must forgo to hold on to it, which is why ==== can remain finite even when everyone knows ==== is imminent.====We went on to show, however, that speculative price bubbles can be ruled out if the government gives even a very small backing ==== to the currency, sufficient to cap the price level at ==== through the simple arbitrage argument that money can never trade at a price below the value at which the government is willing to redeem it.==== (Whether the government will back the currency and the value of backing can both be uncertain.) It is immediately obvious from ==== that such a price ceiling (a floor on the value of money) implies that all the aforementioned equilibrium hyperinflationary paths must unravel backward, because the price level cannot go to infinity as the equilibrium paths would require.====It should be obvious that if there were no contingent backing initially in place and the economy embarked on a speculative hyperinflation, the government could crush it by subsequently promising a floor on the value of money, however low. This last scenario is, however, different from the one we have modeled, in which having a contingent guarantee in place ex ante will prevent speculative hyperinflationary equilibria even from forming.",Revisiting speculative hyperinflations in monetary models,https://www.sciencedirect.com/science/article/pii/S1094202520300764,1 September 2020,2020,Research Article,151.0
"Barro Robert J.,Jin Tao","Harvard University, USA,Tsinghua University, China","Received 1 October 2018, Revised 8 August 2020, Available online 20 August 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.08.002,Cited by (15),", around 6. Most of the explanation for the equity premium derives from RE, although LRR makes a moderate contribution. However, LRR helps in fitting the Sharpe ratio. Generating good matches to the equity premium and Sharpe ratio simultaneously is still challenging."," proposed rare macroeconomic disasters, particularly potential events akin to the U.S. Great Depression, as a possible way to explain the “equity-premium puzzle” of ====. The Rietz idea was reinvigorated by ==== and ====, who modeled macroeconomic disasters as short-run cumulative declines in real per capita GDP or consumption of magnitude greater than a threshold size, such as 10%. Using the observed frequency and size distribution of these disasters for 36 countries, ==== found that a coefficient of relative risk aversion, ====, around 3.5 was needed to match the observed average equity premium of about 7% (on levered equity). ==== modified the analysis to gauge the size distribution of disasters with a fitted power law, rather than the observed histogram. This analysis estimated the required ==== to be around 3, with a 95% confidence interval of 2 to 4.====, henceforth NSBU, modified the baseline rare-disasters model in several respects: (1) the extended model incorporated the recoveries (sustained periods of unusually high economic growth) that typically follow disasters;====(2) disasters were modeled as unfolding in a stochastic manner over multiple years, rather than unrealistically occurring as a jump over a single “period;” and (3) the timing of disasters was allowed to be correlated across countries, as is apparent for world wars and global depressions.====, of about 6.4 was required to match the observed long-term average equity premium. Although the NSBU model improved on the baseline rare-disasters models in various ways, the increase in the required ==== was a negative in the sense that a value of 6.4 may be unrealistically high. The main reason for the change was the allowance for recoveries from disasters; that is, disasters had a smaller impact on asset pricing than previously thought because they were not fully permanent. In the present formulation, we improve in several respects on the NSBU specification of rare events.====The notion of rare macroeconomic events has been employed by researchers to explain a variety of phenomena in asset and foreign-exchange markets, as surveyed in ====. Examples of this literature are ====, ==== (====, ====), ====, ====, ====, ====, and ====.====, henceforth BY, introduced the idea of long-run risks. The central notion is that small but persistent shocks to expected growth rates and to the volatility of shocks to growth rates are important for explaining various asset-market phenomena, including the high average equity premium and the high volatility of stock returns. The main results in BY and in the updated study by ==== required a coefficient of relative risk aversion, ====, around 10, much higher than the values needed in the rare-disasters literature. (BY assumed an intertemporal elasticity of substitution of 1.5 and also assumed substantial leverage in the relation between dividends and consumption.) In our study, we incorporate the long-run risks framework of BY, along with an updated specification for rare macroeconomic events.====The idea of long-run risks has been applied to many aspects of asset and foreign-exchange markets. This literature includes ====; ====; ====; ====; ====; ====; ====; and ====. ==== provide a critical empirical evaluation of the long-run-risks model.====There is a large literature investigating separately the implications of rare events, RE, and long-run risks, LRR. However, our view is that—despite the order-of-magnitude increase in the required numerical analysis—it is important to assess the two core ideas, RE and LRR, in a simultaneous manner.==== This study reports the findings from this joint analysis.",Rare events and long-run risks,https://www.sciencedirect.com/science/article/pii/S1094202520300740,20 August 2020,2020,Research Article,152.0
Stokey Nancy L.,"University of Chicago, United States of America,NBER, United States of America","Received 4 February 2019, Revised 30 July 2020, Available online 17 August 2020, Version of Record 10 March 2021.",https://doi.org/10.1016/j.red.2020.08.001,Cited by (3),"A model is developed in which two complementary forms of investment contribute to growth—technology and skill acquisition, and growth takes two forms—TFP and variety growth. The rate of TFP growth depends more heavily on the parameters governing skill accumulation, while variety growth depends, roughly, on the difference between the parameters governing technology and skill accumulation. Conditions for the existence of a BGP are established, and the effects of various parameters are characterized. In an example, subsidies to skill acquisition (technology acquisition) are powerful tools for stimulating TFP growth (variety growth). Investment incentives off the BGP are also explored.","This paper develops a model in which two factors contribute to growth: investments in technology by heterogeneous firms and investments in human capital by heterogeneous workers. Income growth in turn takes two forms: growth in the quantity produced of each differentiated good and growth in the number of goods available. Call these two forms total factor productivity (TFP) growth and variety growth.====Both types of investment affect both forms of growth, but the contributions are not symmetric. Improvements in the parameters governing investment in skill raise the rate of TFP growth and reduce the rate of variety growth. Improvements in the parameters governing investment in technology raise the rate of variety growth, while the effect on TFP growth is positive, zero, or negative as the elasticity of intertemporal substitution (EIS) is greater than, equal to or less than unity.====This asymmetry appears despite the fact that skill and technology are modeled as symmetric in many respects, and on balanced growth paths (BGPs) the rate of TFP growth is also the (common) growth rate of technology and human capital. But the factors are fundamentally different in two respects. First, human capital is a rival input while technology is nonrival. That is, an increase in a worker's human capital affects only his own productivity, while an improvement in a firm's technology can be exploited by all its workers.====In addition, the two factors differ in the way entry occurs. Growth in the size of the workforce is exogenous. Entry by new firms is endogenous, and entering firms must invest to obtain technologies for new goods. Thus, the expected profitability of a new product affects the incentives of entrants, and the entry rate is governed by a zero-profit condition.====Analyzing both types of investment together is important because there is strategic complementarity in the incentives to invest. Incumbent workers invest in skill to increase their wages. But without continued improvement in the set of technologies used by firms, the returns to workers' investments would decline and, eventually, be too small to justify further investment. Similarly, incumbent firms invest in better technologies to increase their profits, but without continued improvement in the skill distribution of the workforce, their returns would eventually be too small to justify further investments. Sustained growth requires continued investment in both factors, and the contribution of this paper is to characterize the interplay between the two types of investment. It also suggests why empirical work based on regression analysis is likely to fail: if two factors are highly complementary, a linear framework will have difficulty picking up their effects.====The rest of the paper is organized as follows. Related literature is discussed in section 2. Section 3 sets out the production technologies and characterizes the (static) production equilibrium. The production function for differentiated intermediates has two inputs, technology and human capital, and it is log-supermodular. Hence the competitive equilibrium features positively assortative matching between technology and skill. Proposition 1 establishes the existence, uniqueness and efficiency of a production equilibrium, describing the allocation of labor across technologies and the resulting prices, wages, output levels, and profits. Lemma 2, Lemma 3 establish some homogeneity properties. The first main result, Proposition 4, shows that if the technology and skill distributions are Pareto, with locations that are appropriately aligned, then the equilibrium allocation of skill to technology is linear, and the wage, price, output, and profit functions are isoelastic.====Section 4 treats dynamics: the investment decisions of incumbent firms, new entrants, and workers; the evolution of the technology and skill distributions; and the interest rate and consumption growth. Section 5 provides formal definitions of a competitive equilibrium and a balanced growth path. A balanced growth path features stationary, nondegenerate distributions of relative technology and relative human capital, with both growing at a common, constant rate.====Section 6 specializes to the case where technology and skill have Pareto distributions, showing that the isoelastic forms for the profit and wage functions are inherited by the value functions for producers and workers. This fact leads to a tractable set of conditions describing investment and the evolution of the technology and skill distributions on a BGP. The second main result, Proposition 5, provides conditions that ensure the existence of a BGP.====Section 7 looks at the effects of various parameters and policies. Proposition 6, the third main result, describes the effects of parameter changes on TFP and variety growth. Because investments of both types have important positive external effects, the competitive equilibrium investment rates are presumably too low, and the effects of subsidies to investment are described in Corollary 1. In a roughly calibrated example, such subsidies are very powerful.====Section 8 examines the incentives to invest off the BGP. Proposition 8 shows, in a simple setting, that the incentive increases (decreases) for the lagging (leading) factor, suggesting that growth in one factor alone cannot be sustained in the long run.====Section 9 concludes. Proofs and technical derivations and arguments are gathered in Appendices.",Technology and skill: Twin engines of growth,https://www.sciencedirect.com/science/article/pii/S1094202520300661,17 August 2020,2020,Research Article,153.0
"Bárány Zsófia L.,Siegel Christian","Sciences Po, France,CEPR and ROA, United Kingdom,University of Kent (School of Economics and MaGHiC), CEPR, ROA and GLO, United Kingdom","Received 25 July 2019, Revised 22 July 2020, Available online 3 August 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.07.007,Cited by (6),"We study the origins of labor productivity growth and its differences across sectors. In our model, sectors employ workers of different occupations and various forms of capital, none of which are perfect substitutes, and technology evolves at the sector-factor cell level. Using the model we infer technologies from US data over 1960-2017. We find that sectoral differences in labor productivity growth are largely due to sectoral differences in the growth rate of routine labor augmenting technologies. Neither capital accumulation nor the occupational employment structure within sectors explains much of the sectoral differences in labor productivity growth.","The fact that labor productivity growth is different across sectors is well known. Average annual labor productivity growth between 1960 and 2017 in the US, for instance, was 2.5% in the goods sector, much higher than the 1.5% in low-skilled and the 0.7% in high-skilled services. However, it is not clear what the origins of these differences are. Several explanations are possible, such as differences in technological progress across sectors or across production factors, or differential trends in capital deepening or in the use of other inputs. We study the drivers of sectoral labor productivity growth in a production-side framework. What sets our framework apart from the literature is that (i) we consider various types of occupational labor as distinct production factors, (ii) technological change is sector-and-factor specific, and (iii) we infer the evolution of the sector-and-factor specific technologies over time directly from the data. Essentially, we perform model-based sectoral growth accounting.====In our model we consider different occupations as distinct production factors for a variety of reasons.==== First, given that occupations entail very different tasks, they are most likely not perfect substitutes. This implies that using the simple summation of hours worked within a sector might not capture labor's true contribution to a sector's output. The second reason is that occupations are likely to use different technologies, which might grow at different rates. Third, the effects of new technologies and of the accumulation of (different types of) capital on the various occupations might depend on the tasks performed by that occupation, in particular on their routine content and cognitive requirements. In our analysis we therefore differentiate between manual, routine and abstract labor. We also distinguish between Information and Communication Technology (ICT) capital and non-ICT capital, allowing for different substitutability with the various types of occupational labor. Altogether we thus have five factors of production: three types of occupational labor, and two types of capital.====The second key feature of our framework is that we allow technologies to evolve in a very flexible way, at the sector-factor level. It is common to think about sector-specific technological change and factor-augmenting technologies. However, on the one hand, technological change can occur differentially within sectors across factors of production, even within labor and within capital factors. For example in the automobile manufacturing industry various tasks performed by routine workers (on the production line) have been largely automated, without a direct impact on the productivity of manual workers (e.g. cleaners and janitors) or of abstract workers (e.g. accountants or managers). Thus technologies augmenting the different occupations in this industry did not evolve in the same way. On the other hand, technological change can occur differentially across sectors for a given factor. Production line workers, as we discussed above, have seen large changes in technologies, with many tasks automated. Workers in material moving occupations in the shipping industry used to lift heavy objects by hand or by hand-operated equipment. Now these occupations require operating cranes and forklifts to move containers around. Sales workers in real estate nowadays can communicate more information and more easily with clients, but still need to do viewings in person. These examples show how different the evolution of technology of routine occupations can be across sectors. To accommodate for all of these possible cases, we allow technological change to be sector-and-factor specific. Note as we infer from the data these sector-and-factor specific technologies, the data could suggest any pattern. For example technology could improve at the same rate either for all factors within a sector (i.e. sector-specific productivity improvements), or for a given factor across all sectors (i.e. factor-biased technological change).====In order to infer factor-augmenting technologies we need to make assumptions about the structure of production.==== Similarly to Katz and Murphy (1992) and Krusell et al. (2000) we assume a (nested) CES production function.==== Taking values for the substitution elasticities from the literature, we infer sector-specific factor-augmenting technologies in each period from firm optimality conditions. This approach is similar to Caselli (2005), Caselli and Coleman (2006), and Buera et al. (2018). In order to infer technologies we need data on the occupations of workers within each sector, which we take from the US Census and American Community Survey (ACS). Further, we combine data from the U.S. Bureau of Economic Analysis (BEA) and EU KLEMS 2017 to obtain value added, labor income shares, prices, employment and capital of different types by sector.====Our results show that allowing for technologies to be sector-and-factor specific is crucial. Technologies have evolved at very differential rates, both across factors within each sector and across sectors for a given factor (occupation or type of capital), echoing the general conclusions of Caselli (2016).==== In particular, technologies augmenting routine occupations have been growing the fastest in all sectors, but at very different rates across sectors: at 5.6% per year in goods, at 2.9% in low-skilled services and at 1.3% in high-skilled services.====Through a series of counterfactual simulations, we study the role of technological change and of inputs in labor productivity growth. We find that the single most important driver of sectoral labor productivity growth differences is sector-specific routine labor augmenting technological change. Without this type of technological change, labor productivity growth would have been almost equalized across sectors. Specifically, sector-specific routine labor augmenting technological change explains at least 59 percent of labor productivity growth in low-skilled services, 74 percent in goods and 21 percent in high-skilled services.====As we found such an important role for sector-specific routine labor augmenting technological change, we want to assess to what extent this can be assigned to overall routine labor augmenting technology growth or to sectoral components in technological progress. Therefore, we conduct a factor model decomposition of the growth rates of all sector-specific occupational labor-augmenting technologies (or sector-occupation specific technologies for short). With this, we establish that these growth rates are well described as the sum of sector-specific and occupation-specific components, and both types of components are required.==== Moreover, we show through counterfactuals that the occupation-specific components do not explain sectoral differences in labor productivity growth. This suggests that sector heterogeneity in routine labor augmenting technological change is the key driver of sectoral differences in labor productivity growth rates.====Further counterfactuals allow us to evaluate the role of various other channels proposed in the literature for sectoral productivity growth differences. As suggested by Acemoglu and Guerrieri (2008), differential capital intensities and capital accumulation could be driving the faster productivity growth in the goods sector. While we find that capital accumulation contributes to labor productivity growth (without it growth would have been 39 percent lower on average), it does not generate the sectoral differences in labor productivity growth observed in the data. Instead, we confirm the finding of Herrendorf et al. (2015), that differences in labor-augmenting technological progress across sectors are crucial. They do not differentiate labor by occupation, and as such cannot identify the sources of the sectoral differences in labor-augmenting technological change. In principle, these could be driven by differences in sectoral intensities of occupational employment and technological change specific to occupations, as suggested by Duernecker and Herrendorf (2016) and by Lee and Shin (2017). However, in our framework we find that differences in occupational employment structure across sectors do not contribute much to sectoral labor productivity growth differences. Instead our results indicate that the sectoral differences in the growth rate of routine labor augmenting technologies themselves are crucial.====We conduct our analysis at the sectoral level, and it also has implications for aggregate productivity growth, as emphasized in Duarte and Restuccia (2010), Duernecker et al. (2017), and Duarte and Restuccia (2020). We find that in our context, i.e. taking into account five factors of production and their technologies, technological change is much more important than input use for labor productivity growth also in the aggregate. A novel finding of our paper is that the contribution of routine labor augmenting technological change is large and increasing over time. In its absence aggregate growth would have been lower by about a third between 1960-1990, and there would have been hardly any growth over 1990-2017.====The paper proceeds as follows: section 2 shows the facts about sectoral production on which we base our analysis. Section 3 introduces the production-side framework used to infer technologies and explains its implementation. In section 4 we analyze the role of inputs and technologies in labor productivity growth through counterfactuals. In section 5 we verify that our results are very robust to alternative substitution elasticities, to a different nesting of the production function, to controlling for workers' human capital, and to different treatments of the data. The final section concludes.",Engines of sectoral labor productivity growth,https://www.sciencedirect.com/science/article/pii/S109420252030065X,3 August 2020,2020,Research Article,154.0
"Chassamboulli Andri,Gomes Pedro","Department of Economics, University of Cyprus, CY-1678 Nicosia, Cyprus,Department of Economics, Mathematics and Statistics, Birkbeck, University of London, Malet Street, WC1E 7HX London, United Kingdom,Department of Economics, Universidad Carlos III de Madrid, Calle Madrid 126, 28903 Getafe, Spain","Received 20 March 2020, Revised 21 July 2020, Available online 30 July 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.07.006,Cited by (9),"We set up a model with search and matching frictions to understand the effects of employment and wage policies, as well as nepotism in hiring in the public sector, on unemployment and rent seeking. Conditional on inefficiently high public-sector wages, more nepotism in public-sector hiring lowers the unemployment rate because it limits the size of queues for public-sector jobs. Wage and ==== impose an endogenous constraint on the number of workers the government can hire through connections.","Governments hire workers to produce public goods, but they do not face the same competitive forces as private firms. As a result, governments use their employment and wage policies to accomplish a multitude of goals: to attain budgetary targets (Gyourko and Tracy, 1989); to implement a macroeconomic stabilization policy (Keynes, 1936); to redistribute resources (Alesina et al., 2000); or to satisfy interest groups for electoral gains (Gelb et al., 1991). This paper builds on the observation that, in several countries, government hiring practices are sometimes based on nepotism.====We define nepotism as the restriction that some jobs in the public sector are reserved for a subset of workers that have political or personal connections. By having access to this subset of jobs, some workers can use their connections to “jump the queue” and find jobs in the public sector faster. One dimension that is common to all countries is political appointments. Whenever there is a change in government, there is a subsequent turnover of jobs. The report Government at a Glance by OECD (2017) highlights the cross-country differences in staff turnover following a change of government. In countries such as Germany and the UK, there is little turnover, mainly in advisory posts. In countries such as Greece and Spain, the turnover extends to layers of senior and middle management. A second dimension is the influence that politicians or civil servants use to hire friends or family members. Besides vast anecdotal evidence of such practices, it is also backed by survey evidence.==== In Section 2 we analyse data from the ==== and the ==== and find that these practices are present in the public sector, more than in the private sector, and that they vary widely across European countries. In particular, they are more prevailing in countries where the public-private wage differential is larger.====Our objective is to study the interaction between public-sector policies, nepotism and unemployment. First, we want to understand the effects of nepotism in the public sector on unemployment. We find a silver lining to nepotistic hiring. Although it is inefficient and is absent in the first-best equilibrium, conditional on inefficiently high public-sector wages, more nepotism lowers the unemployment rate by shortening the queues for these jobs and increasing employment in the private sector. Second, we want to understand how employment and wage policies influence incentives to use political and personal connections to get a job. We show that nepotism only exists if public(-sector) wages are too high and that it can be restricted if the government sets an efficient wage.====Given the amount of anecdotal and survey evidence of such practices, it is perhaps surprising that research documenting evidence of nepotism or cronyism in the public sector is limited. Scoppa (2009) finds that the probability of working in the public sector in Italy is 44 percent higher for individuals who have a parent also working there. Durante et al. (2011) find a higher concentration of last names in universities in Italy relative to the overall population, and that this concentration increased in regions with low civic capital, after a reform decentralizing the university hiring choices in 1998. Martins (2010) finds that in Portugal, between 1980 and 2008, over the months preceding an election, appointments in state-owned firms increased significantly compared to private-sector firms. Hiring also increased after elections, but only if a new government took office.====On top of these papers that provide suggestive evidence of nepotism and cronyism in the public sector, two recent papers by Fafchamps and Labonne (2017) and Colonnelli et al. (2018) have a better identification strategy. Fafchamps and Labonne (2017) find that, following the 2007 and 2010 municipal elections in Philippines, individuals who shared one or more family names with a local elected official were more likely to be employed in better-paying occupations, compared to individuals with the loosing candidates' family names. The magnitude of the effect is consistent with preferential treatment of relatives as managers in the public sector. Colonnelli et al. (2018) apply a regression discontinuity design in close electoral races in Brazil to matched employer-employee data on the universe of public employees. They find that politically connected individuals enjoy easier access to public-sector jobs, but are less competent. Despite these empirical efforts to identify nepotism, given the nature of this activity, it is difficult to empirically measure its aggregate effects.====We study the conditions that allow for nepotism in hiring in the public sector, and its consequences, from a theoretical angle. In Section 3 we set up a search model in which workers can search for jobs in either the private or the public sector. Employment and wages in the private sector are determined through the usual channels of free entry and Nash bargaining. This ensures that job-finding rates reflect nothing but match surplus so that identical workers have equal chances of finding a job. In the public sector, by contrast, employment and wages are exogenous. We account for the possibility of nepotism or cronyism by assuming that job seekers can use their personal relationships and connections to find a public-sector job. We assume that prior to entering the labour market, workers can pay a cost to get “connections” that is drawn from an exogenous distribution across workers. In our setting, nepotism means that the government reserves some of its jobs for workers with those connections. Under such practices, in equilibrium, workers with connections can more easily find public-sector jobs than similar workers that do not have connections.====This paper contributes to the recent labour market search literature that analyzes the role and effects of public-sector employment and wages. Burdett (2012) includes the public sector in a job-ladder framework where firms post wages. Bradley et al. (2017) further introduce on-the-job search and transitions between the two sectors to study the effects of public-sector policies on the distribution of private-sector wages. Albrecht et al. (2018) consider heterogeneous human capital and match specific productivity in a Diamond-Mortensen-Pissarides model. Michaillat (2014) shows that the crowding-out effect of public employment is lower during recessions, giving rise to higher government spending multipliers. Navarro and Tejada (2018) analyse the interaction between public employment and the minimum wage. These papers' objective is to determine how employment and wage policies affect private employment and wages, as well as the unemployment rate. They assume that the unemployed randomly search across sectors, and, hence, policies affect the equilibrium only by affecting the outside option of the unemployed and their reservation wage.====Hörner et al. (2007) study the effect of turbulence on unemployment when wages in the public sector are insulated from this volatility. Quadrini and Trigari (2007) analyze the effects of exogenous business cycle rules on unemployment volatility. Gomes (2015) emphasizes the role of the wage policy in achieving the efficient allocation, while Afonso and Gomes (2014) highlight the interactions between private and public wages. Gomes (2018) examines the heterogeneity of public-sector workers in terms of education. These papers assume that the two sectors's labour markets are segmented, and that the unemployed choose which of the sectors to search in, depending on the government's hiring, separation and wage policies.====We add to this literature by considering the choice of finding a public-sector job through connections and by analyzing how government policies affect this rent-seeking activity. Moreover, while we assume segmented markets, in Section 7 we contrast the transmission mechanism and our results with those from a model with random search. We prefer the assumption of segmented markets because it portrays a realistic mechanism of selection into the public sector, documented empirically by Krueger (1988), Nickell and Quintini (2002) or experimentally by Bó et al. (2013), lying at the heart of current policy discussions. High pay attracts many unemployed to queue for public-sector jobs. Conversely, if pay is too low, few unemployed search in the public sector, which then faces recruitment problems.====Our first main finding is perhaps surprising. Conditional on inefficiently high wages, more nepotism in the public sector lowers the unemployment rate. When the value of a public-sector job is higher than that of a private-sector job (because of either high wages or a low separation rate), more of the unemployed queue for these jobs, moving away from the private sector. If most of these jobs are available only through connections, fewer unconnected unemployed are going to queue and will search in the private sector instead. Although it fosters an inefficient rent-seeking activity, nepotism mitigates the adverse effects that high public wages have on employment. The evidence from survey data, shown in Section 2, is consistent with this result. A corollary of this first result, shown in Section 5, is that, although it entails itself a cost, nepotism might reduce the welfare losses of inefficiently high public-sector wages.====Although the mechanism is different, this result echoes those found in papers studying referrals – e.g., Horvath (2014), Galenianos (2014) or Bello and Morchio (2017) – which have focused exclusively on the private sector. These papers argue that social networks can improve the matching process by working as an information channel or increasing the efficiency of search. We argue that hiring through connections works differently in the public sector. In the private sector, free entry of firms ensures that the gains of alternative hiring channels translate into job creation, and wage bargaining guarantees that the surplus generated is shared between firms and workers. On the contrary, we view the public sector as having a fixed number of jobs that are safer and better paid, which induces workers to find alternative ways to get them. The mechanism does not involve a better search technology or better information about vacancies, but the knowledge that some vacancies are reserved for a subset of workers with connections, which shortens the queues for public-sector jobs.====Focusing on the public rather than the private sector allows us to understand how policies affect nepotistic hiring. In our setting, the government can hire through connections, provided that it pays high enough wages to attract enough searchers. In other words, government employment and wage policies impose an endogenous limit on how many workers it can hire through connections. The constrained-efficient allocation can be achieved with an optimal wage that simultaneously limits the queues for public-sector jobs and makes it impossible to hire through connections. This second result is supported by the evidence from the survey data that non-meritocracy in the public sector is associated with higher wage premium. It rationalizes why evidence of nepotism in the public sector is common in Southern European countries, in which public sectors pay substantial premia relative to the private sector, while it is absent in Nordic countries, which tend to pay a negative premium.====Given the common perception that workers hired through political connections are less competent, supported by Colonnelli et al. (2018), we consider in Section 7 an extension where workers have heterogeneous ability. The importance of wage and human resource policies on the quality of public-sector workers has attracted much interested from an empirical micro literature, summarized by Finan et al. (2015). It has also been recently studied theoretically by Geromichalos and Kospentaris (2020). We find that, when the government prioritizes high-ability workers in the recruitment, low-ability workers face lower chances of getting a job, so they have stronger incentives to get connections to jump the queue.====In our model we take the government policies as being exogenous in order to isolate the effects of each of the policies. In Section 6 we provide one possible microeconomic foundation for the government's policy choices. The government's employment, wages and use of nepotism in hiring workers are chosen to maximize an objective function that includes the production of government services, the preferences of a union, a benefit of nepotism, which could reflect general corruption or vote buying and a cost of nepotism in terms of possible media backlash. The simple model of government choices highlights possible interdependencies of policies and generates the different particular cases that we study.",Jumping the queue: Nepotism and public-sector pay,https://www.sciencedirect.com/science/article/pii/S1094202520300636,30 July 2020,2020,Research Article,155.0
Kospentaris Ioannis,"Virginia Commonwealth University, United States of America","Received 25 October 2019, Revised 23 July 2020, Available online 29 July 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.07.008,Cited by (4), on duration dependence are examined through the lens of the model. Subsidized employment raises job-finding rates for all workers but helps short-term more than long-term unemployed workers. Training raises the job-finding rates of the long-term unemployed but lowers job-finding rates for workers with short unemployment spells. The interaction of unobserved heterogeneity with skill loss due to endogenous vacancy creation is crucial for these results.,"A well-documented characteristic of unemployment is ====: long-term unemployed workers have a significantly worse chance of finding a job than newly unemployed workers.==== The two most popular explanations for duration dependence agree that it is caused by a decline in the quality of the unemployment pool over time, but disagree on how this deterioration happens. The ==== explanation states that ex-ante differences in worker quality cause the quality of the unemployment pool to decline via a composition effect. That is, the long-term unemployed are workers of ex-ante poor qualities (such as the inability to work in teams or follow instructions). Alternatively, the ==== explanation states that the quality of ex-ante similar workers declines the longer they remain unemployed, so that unlucky workers become of lower quality ex-post. That is, the long-term unemployed are workers who lose skills (such as knowledge of specific software or presentation skills) during unemployment.====There has been a lot of important empirical research on duration dependence, but it largely assumes away the effect of the quality of the unemployment pool on job creation. As a result, the empirical literature has so far ignored potentially important interactions between unobserved heterogeneity and skill loss as well as how these interactions affect duration dependence. For example, if the level of ex-ante qualities of the unemployed is low, employers create relatively few jobs. Finding a job becomes harder, which amplifies skill loss for unlucky workers and results in even stronger duration dependence. In this example, the interaction of ex-ante with ex-post worker differences creates more duration dependence. Due to their reduced-form nature, the approaches proposed so far must attribute interactions like this to ==== unobserved heterogeneity ==== skill loss. To make progress, the equilibrium response of employers to the unemployment pool quality must be explicitly modeled in a framework with both ex-ante and ex-post worker differences.====In this paper, I use an equilibrium search model to quantify the contributions of unobserved worker heterogeneity and skill loss to duration dependence in unemployment. The model has a crucial numerical property that supports identification: unobserved heterogeneity has a very small impact on reemployment wages. In the model, wages depend almost exclusively on workers' skill levels. I use high-quality estimates of the effects of unemployment duration on reemployment wages and callback rates to leverage this result and distinguish the effects of each type of worker heterogeneity.==== The main result of the analysis is that ex-ante worker differences, due to unobserved qualities, explain roughly 70% of the deterioration of job-finding prospects over the unemployment spell; ex-post worker differences, due to skill loss, explain the remaining 30%.====Worker heterogeneity in the model operates in two ways. First, following Gonzalez and Shi (2010) and Fernández-Blanco et al. (2018), ex-ante unobserved differences among workers are modeled as differences in suitability for jobs. That is, each worker is able to produce positive output only in a fraction of available jobs. Workers can be either of broad or limited suitability; the former can perform a strictly greater share of jobs than the latter. The real-world interpretation of this assumption is that a worker with a low level of the ability to work in teams or follow instructions will be suitable for fewer jobs in the economy than a worker with high levels of these qualities.==== Unobserved heterogeneity results in duration dependence due to dynamic selection: as the unemployment spell evolves, broad-suitability workers find jobs faster, leaving more limited-suitability workers in the unemployment pool. Second, skill loss is modeled as depreciation in workers' on-the-job productivity while in unemployment. Consequently, the long-term unemployed are ex-post different than newly unemployed workers. The real-world interpretation of this assumption is that a worker's productivity at a job for which she is suitable depends only on her skill level, such as software knowledge or presentation skills. Since the level of these skills deteriorates over time, this creates duration dependence for each individual worker.====As previously mentioned, the key result that supports identification is that unobserved heterogeneity has very small effects on reemployment wages in the model. To establish that, I show that the path of reemployment wages over the unemployment spell is practically the same for a wide range of unobserved heterogeneity parameters. To understand why this is the case, keep in mind that wages are tightly linked to the surplus generated by a firm-worker match, as in all search models. Hence, the key result actually states that unobserved heterogeneity has a small effect on how the surplus changes when firms match with workers at different stages of the unemployment spell. The surplus of the match depends on the value of the output generated by the match, as well as the value of unemployment. Unobserved heterogeneity has no effect on the output of the match and a small marginal effect on the value of unemployment over the unemployment spell.====Unobserved heterogeneity has no effect on match output because, following Gonzalez and Shi (2010), I assume that firms can perfectly identify and costlessly disregard unsuitable workers when they evaluate them. Hence, only workers who are found suitable for a job are hired. When firms post wages to attract workers, they recognize that these wages will be paid to suitable workers only. Unobserved heterogeneity makes it more difficult to find a suitable worker, but it does not directly affect the value of output the worker produces when matched to a firm. It is the worker's on-the-job productivity that determines the level of output produced when the worker is found suitable. The worker's on-the-job productivity is assumed to represent her level of skills. Hence, unobserved heterogeneity is not reflected in match output.====Unobserved heterogeneity has a small marginal effect on the value of unemployment over the unemployment spell due to a countervailing force in the model that partially compensates workers with long unemployment spells. The model features directed search; thus workers of different unemployment durations search for jobs in separate submarkets. Free entry implies that firms will be indifferent between posting vacancies in these submarkets. Due to unobserved heterogeneity, the average suitability of workers is greater in the submarkets with the short-term than the long-term unemployed.==== Hence, firms pay higher wages to the short-term unemployed. For firms to be indifferent, though, they must face lower waiting times to fill their vacancies in the submarkets with the short-term than the long-term unemployed. That is, there are fewer vacancies per unemployed worker early in the unemployment spell. Workers with longer spells are partially compensated with more vacancies, which dampens the effect of lower wages on the value of unemployment. Hence, the marginal effect of decreasing average suitability (that is, unobserved heterogeneity) on the value of unemployment is quantitatively small.====To leverage this result, I exploit data on callback rates and reemployment wages over the unemployment spell. Since wages in the model depend almost exclusively on skill loss, using the empirical path of reemployment wages pins down the speed of skill loss in the model. Moreover, firms in the model recognize that the average suitability of workers decreases over the unemployment spell. I show that the parameters of unobserved heterogeneity are tightly linked to the firms' expectations regarding average suitability over the unemployment spell. These expectations capture firms' beliefs regarding unobserved worker qualities at different unemployment spells, similar to callback rates in the data. Using the empirical path of callback rates over unemployment duration pins down the parameters of unobserved heterogeneity in the model.====To quantify the importance of each type of worker heterogeneity for duration dependence, I consider alternative versions of the model. Specifically, I compute the effect of removing each type of worker heterogeneity from the model on job-finding rates of workers with different unemployment spells. The main result is that unobserved heterogeneity accounts for 72% of total duration dependence observed in the first year after a worker became unemployed. Skill loss accounts for the remaining 28%. Importantly, the bulk of the effect of skill loss is concentrated among unemployment workers with spells longer than six months.====The next contribution of the paper is to analyze the effects of labor policies on duration dependence through the lens of the model. Two types of policies are considered: training and subsidized employment programs. The policies are modeled as comparative statics exercises, lowering the speed of skill loss (training) and the cost of vacancy creation (subsidized employment). Three interesting findings stand out. First, different policies affect different outcomes. Subsidized employment has a large effect on job-finding rates and a negligible effect on reemployment wages, while the opposite is true for training. Second, a policy may help different groups of unemployed workers to different degrees. Subsidized employment in the model raises the job-finding rates of workers with unemployment spells up to three months more than the rates of workers with longer unemployment spells. Third, a policy may help some workers but hurt others. Training in the model raises the job-finding rates of workers with spells longer than nine months but lowers the rates of workers unemployed less than five months. These findings recommend caution to policy-makers choosing among different labor market policies to improve the prospects of the unemployed.====One of the main contributions of this paper is to highlight the importance of the ==== of unobserved heterogeneity with skill loss for duration dependence. This interaction is due to the fact that job creation in the model depends on the quality of the unemployment pool, and it can be seen in at least two parts of the paper. First, in the decomposition exercise: the decline in the job-finding rate in the model with both unobserved heterogeneity and skill loss is larger than the sum of the declines in job-finding rates in a model with unobserved heterogeneity only and a model with skill loss only. This could not happen in a partial equilibrium approach in which job creation is not explicitly modeled. Second, in the results of the policy exercises: I show that a model featuring only unobserved heterogeneity would severely overestimate the positive impact of the subsidized employment program. Specifically, the impact of subsidized employment is twice as large in a model with unobserved heterogeneity only compared to the full model.====The paper proceeds as follows. In Section 2, I provide a review of the relevant literature. In Section 3, I describe the model environment, define an equilibrium and analytically establish equilibrium existence and characterization. In Section 4, I present the empirical evidence that inform the model. In Section 5, I describe the calibration procedure and how the data pin down model parameters. In Section 6, I present the quantitative results and provide intuition for the workings of the model. Section 7 concludes the paper. Finally, all proofs can be found in Online Appendix A, extra details for the quantitative analysis of the paper in Online Appendix B, an extension of the model in Online Appendix C, and an alternative calibration strategy in Online Appendix D.",Unobserved heterogeneity and skill loss in a structural model of duration dependence,https://www.sciencedirect.com/science/article/pii/S1094202520300648,29 July 2020,2020,Research Article,156.0
"Furlanetto Francesco,Gelain Paolo,Taheri Sanjani Marzie","Norges Bank, Bankplassen 2, PB 1179 Sentrum, 0107 Oslo, Norway,BI Norwegian Business School, Norway,Federal Reserve Bank of Cleveland, 1455 East 6th Street, Cleveland, OH 44114, USA,International Monetary Fund, European Department, Washington, DC, USA","Received 14 January 2020, Revised 10 July 2020, Available online 28 July 2020, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2020.07.004,Cited by (5),This paper investigates how the presence of pervasive financial frictions and large financial shocks changes the optimal ,"During the last 30 years, the evolution of inflation and, to some extent, of measures of real economic activity has been more and more disconnected from financial variables. On the one hand, Christiano et al. (2010) document that inflation is always relatively low during stock market booms (which, in turn, are often associated with credit booms). On the other hand, financial variables such as stock prices and various measures of credit have experienced large boom-and-bust cycles, with an average duration much longer than that of standard business cycles. In addition, Jermann and Quadrini (2006) and Fuentes Albero (2019) document that the volatility of financial variables at business cycle frequencies has increased substantially since the mid-1980s, while the volatility of inflation and real economic activity measures has diminished markedly. At the same time, a series of influential papers find that shocks originating in the financial sector may play an important role in driving these dynamics, cf. Christiano et al. (2014) (henceforth CMR), Gilchrist and Zakrajsek (2011) and Jermann and Quadrini (2012).====From a policy perspective, many prominent observers, such as the Bank for International Settlements (BIS), have questioned the desirability of monetary policies pursuing price stability in a world in which inflation and real economic activity measures are more and more disconnected from financial variables. According to this view, the boom-and-bust cycle ending with the Great Recession is an iconic example of how the pursuit of price stability may not be sufficient to achieve real economic activity stability in the presence of financial disturbances. In this paper we reevaluate this view and investigate whether the presence of pervasive financial frictions and large financial shocks is, in and of itself, a reason for central banks to abandon their focus on price stability, which they have pursued over the last twenty years. In addition, and related to the previous point, we also investigate whether the presence of financial frictions and financial shocks has an impact on the measurement of real economic activity indicators such as the output gap. This question is interesting because, as shown by Borio et al. (2017), traditional measures of the output gap computed by several policy institutions were in negative territory in the pre-Great Recession period, arguably a period of financial exuberance for the US economy.====To investigate our questions, we need an estimated macroeconomic model in which financial frictions and financial shocks play an important role in interpreting the data. We largely rely on CMR (2014), a state-of-the-art model with financial frictions featuring a financial accelerator mechanism along the lines of Bernanke et al. (1999) and an important role for financial shocks. Those disturbances are modeled as shocks to the net worth of firms, which directly affect the availability of credit for the production sector, and as shocks to the volatility of the cross-sectional idiosyncratic uncertainty (risk shocks), which reflect possible tensions in financial markets (or fluctuations in uncertainty) and include news components. We use the model to compute the optimal equilibrium and evaluate the trade-offs for the monetary policy authority. Such an exercise has not been done in the context of an estimated model with financial frictions and extends the analysis by Justiniano et al. (2013), henceforth JPT, in the context of the standard New Keynesian model. Our research question is admittedly complex and we acknowledge upfront that our framework does not capture important elements such as frictions in the household and banking sectors, and important nonlinearities such as bank runs and sudden stops in credit flows. Nevertheless, the financial accelerator model seems to be a useful and well-known starting point to investigate optimal monetary policy in a quantitative set-up with financial frictions.====Our main result is that nominal stabilization remains a very useful intermediate objective for central banks to pursue on the way to optimal policy, even in a model in which financial frictions are pervasive and financial shocks are dominant. In fact, in our model the optimal monetary policy achieves an almost full stabilization of wage inflation and price inflation, as in the absence of financial frictions and financial shocks. Achieving nominal stabilization, however, does not guarantee a full stabilization of real economic activity around its potential level. Some fluctuations in the output gap are unavoidable and actually desirable (conditional on the interest rate being the only policy instrument available). The magnitude of those fluctuations is far from negligible but still relatively small, at least when compared to historical fluctuations in the output gap obtained under the estimated monetary policy rule. This result relies on the fact that financial shocks do not seem to pose particular challenges to the monetary policy authority: in the end they behave like standard demand shocks and do not generate adverse trade-offs between nominal and real stabilization.====While the presence of financial frictions and financial shocks affects the policy prescriptions only to some extent, it has a large effect on the measurement of the stance in real economic activity. In fact, the output gap derived from our baseline model is more persistent and volatile than the output gap derived by JPT (2013) in the absence of financial frictions, which constitutes our reference for comparison. In particular, we estimate a long cycle for the output gap that was positive from the mid-1990s until the Great Recession, thus over a period characterized by asset price boom-and-bust cycles. A standard New Keynesian model implies instead a negative output gap in the pre-Great Recession period. The main reason for such a different shape for the output gap in the model with financial frictions is that financial shocks absorb explanatory power from labor supply shocks. In fact, neither of the financial shocks propagates in the potential economy, thus behaving like monetary policy shocks. Potential output in the model with financial frictions is therefore substantially different from its counterpart in the standard New Keynesian model.====This paper contributes to the literature on optimal monetary policy in models with financial frictions. Carlstrom et al. (2010), De Fiore and Tristani (2013), Nisticó (2016) and Ravenna and Walsh (2006) evaluate optimal monetary policy in simple small-scale models with financial frictions, where they are able to derive analytical expressions for the model-consistent welfare functions. In a similar set-up, Faia and Monacelli (2007) and Curdia and Woodford (2010) study optimal monetary policy rules. De Fiore et al. (2011) analyze optimal monetary policy in a model in which firms' financial positions are denominated in nominal terms and debt contracts are not state-contingent. Fendoglu (2014) computes the Ramsey monetary policy in a calibrated financial accelerator model. We contribute to this literature by conducting our analysis in an estimated (rather than calibrated) model driven by several disturbances, including two financial shocks.====Since we use an estimated model, we can evaluate the trade-offs faced by the central bank from a quantitative point of view. We can thus extend to the framework with financial frictions a discussion that has so far been confined within the standard New Keynesian model (cf. Blanchard and Galí, 2007). Most central banks perceive a trade-off between stabilizing inflation and a measure of capacity utilization. However, in medium-scale DSGE models the importance of this trade-off largely depends on whether the low-frequency fluctuations in hours worked are attributed to labor supply shocks (JPT, 2013) or to wage mark-up shocks (Debortoli et al., 2019). In the former case, trade-offs between real and nominal stabilization exist but are fairly weak, thus leading to a sort of ==== in which the monetary policy authority is able to stabilize price inflation, wage inflation, and the output gap almost completely.==== In the latter case, trade-offs are substantially larger and the weight on the output gap should be equal to or larger than that of annualized inflation when designing a loss function for the central bank. Our contribution is to measure the policy trade-offs in an environment where frictions are more pervasive. Moreover, our results do not depend on which labor market shock is driving the low-frequency fluctuations in hours, since the labor market shock loses almost all of its explanatory power in favor of financial shocks.====We also contribute to the literature on the behavior of the output gap in structural macroeconomic models. Earlier contributions include Levin et al. (2005), Galí et al. (2007), Edge et al. (2008), Christiano et al. (2011), and Galí et al. (2011). As far as we know, our paper is the first that derives the output gap from an estimated model with financial frictions driven by a large set of shocks.==== The importance of considering financial factors in the computation of the output gap is stressed in Borio et al. (2017) in a reduced-form set-up. Our paper considers the same issues in a structural model.====The rest of the paper is organized as follows. Section 2 describes the model. Section 3 summarizes the details of the Bayesian estimation and the main properties of the estimated model. Section 4 discusses the optimal monetary policy exercise. In Section 5 we investigate further the model-based measure of the output gap and its properties. Finally, we conclude in Section 6.","Output gap, monetary policy trade-offs, and financial frictions",https://www.sciencedirect.com/science/article/pii/S1094202520300594,28 July 2020,2020,Research Article,157.0
"Tse Chung-Yi,Xu Yujing","Faculty of Business and Economics, University of Hong Kong, Hong Kong","Received 30 June 2018, Revised 6 July 2020, Available online 24 July 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.07.003,Cited by (0),"In an OTC market where dealers' inventory capacities differ, dealers trade among themselves to rebalance inventories for facilitating the sale and purchase of the asset to and from their investor clients. In a market where the asset is sold quickly, the small-capacity dealers sell to the large-capacity dealers to help them replenish their inventories. Conversely, in a slow market where it takes a relatively long time for the asset to be sold, the small-capacity dealers buy from the large-capacity dealers to help them free up inventory capacities. The prediction, though counterintuitive, is supported by some available empirical evidence.","Many financial assets, including government and corporate bonds, asset-backed securities, and derivatives, are traded in over-the-counter (OTC) markets instead of in centralized exchanges. Two distinguishing features of OTC markets are that the trades are almost always intermediated by dealers of various kinds and that the dealers do not just trade with investors but also among themselves. Indeed, inter-dealer trades can account for a significant fraction of the overall transactions for a given asset.====An important question on inter-dealer trades in OTC markets that seems to have attracted scant attention is how market conditions help shape the directions of trade among heterogeneous dealers.====One instance suggestive of how the trading directions among dealers are influenced by market conditions can be found in Adrian et al. (2017). Specifically, they find that while large dealers expanded their balance sheets much more than small dealers did in the run-up to the 2008 financial crisis, a few small dealers actually expanded their balance sheets during which large dealers were downsizing theirs post-crisis.==== This difference in balance sheet movements between large and small dealers before and after the 2008 financial crisis could be, to a certain extent, due to large dealers, on the whole, buying from in the up market but selling to small dealers in the down market as equilibrium responses to the changing market environment.====In this paper, we extend the seminal random search models of the OTC market of Duffie et al. (2005) and Lagos and Rocheteau (2009) to study how dealers trade with one another for managing inventory levels for trading with their customers and how the directions of trade among dealers are determined. The point of departure is that, in our model, dealers are heterogeneous in their inventory capacities. The heterogeneity can be due to risk management considerations, portfolio choices, or can result from differences in financing costs.====In particular, in our model, there is a given measure of what we call small dealers, each endowed with one unit of inventory capacity, and a given measure of what we call large dealers, each endowed with two units of inventory capacity. In each period, investors buy from and sell to dealers in an OTC market in which only dealers who are holding at least a unit of the asset in inventory can sell to and only dealers having at least one unit of spare inventory capacity can buy from investors. Once the investor-dealer trades are completed, and only then, a perfectly competitive inter-dealer market opens, through which dealers trade to rebalance their inventory holdings.====Underlying most of the results in the paper is a particular ranking of the marginal benefits of inventory according to which the first unit of inventory is valued higher by a large dealer than by a small dealer, whereas the small dealer values his last and only possible unit more than the large dealer values his last and second unit. To both the small and the large dealers, having the first unit allows the dealer to sell to investors in the next round of trading. To the small, but not the large, dealer, this is at the expense of exhausting one's inventory capacity after which the dealer can no longer buy from any investors. The large dealer similarly would forgo any opportunity to buy from investors when he exhausts his inventory capacity in acquiring his second and last unit of the asset but with no compensating benefit since he already has a unit for sale to investors without adding the second unit to inventory.====In other words, small dealers value a unit of the asset in between large dealers starting out with an empty inventory and large dealers starting out with one unit of inventory. The competitive inter-dealer market in equilibrium would then first allocate the asset to large dealers, then to small dealers if there remain units of the asset yet to be allocated to dealers, and finally to large dealers already holding a unit in inventory in case the asset supply is sufficiently large for all small dealers to already hold a unit in inventory. Such allocations are by means of small dealers selling to large dealers in a market with a small asset supply and small dealers buying from large dealers in a market with a large asset supply. Small dealers then should trade with large dealers only but not among themselves while trades between the two types of dealers tend to flow in one direction only in a given market. In our model then, the direction of trade between a small and a large dealers in a given market is persistent, consistent with the finding in Li and Schürhoff (2019).====A dealer is said to provide immediacy for another dealer if the first dealer sells to (buys from) the second dealer when it takes a long time on average for the second dealer to buy (sell) the asset in the market. In our model, large dealers sell to small dealers when there is a large asset supply or when there are only few dealers in the market buying from investors, during which it should be easy for small dealers themselves to buy the asset from investors. When there is a small asset supply or when there are dealers aplenty competing to buy from investors, at which times it should be hard for an individual dealer to buy from investors, it is small dealers who sell to large dealers. Hence, it is the small dealers who trade to provide immediacy for the large dealers in our model, selling to (buying from) large dealers at times during which it takes a long time for the latter to buy (sell) in the market.====The last implication is consistent with the finding in Adrian et al. (2017) if the booming market pre-crisis is time during which it is relatively easy and the market bust post-crisis relatively hard for dealers to find investor buyers in the market. In the earlier period, large dealers gain inventory from small dealers and expand their balance sheets faster. In the later period, small dealers amass inventory from large dealers and expand their balance sheets relative to those of large dealers.====In many OTC markets, dealers can be distinguished by their degrees of centrality, where more central dealers are those who trade with a greater number of dealers than less central ones (Li and Schürhoff (2019) and Hollifield et al. (2017)). The distinction resembles a core-periphery trading structure in the abstract, where the more central dealers can be thought of as core dealers and the less central ones as peripheral dealers. Given that small dealers in our model on the whole trade only with the large dealers, rather than among themselves, whereas the large dealers trade with all dealers, the two types of dealers behave similarly as the less and more central dealers do, respectively, identified in the empirical studies, with regard to the set of dealers they trade with. Under this interpretation, in our model, it is the less central dealers who provide immediacy for the more central dealers. The prediction, counterintuitive as it seems, suggests an explanation for how those classified as peripheral dealers earn a higher markup when they are the last links of the intermediation chain than when they are the first links, reported in Hollifield et al. (2015) and how the spread between the prices investors pay and receive is higher when they trade with central dealers, reported in Li and Schürhoff (2019).====The equilibrium in our model is constrained efficient in that the allocation of inventories and spare capacities among dealers falling out from inter-dealer trades in equilibrium coincides with the planning optimum. That the two allocations coincide perhaps is not surprising given that dealers trade an indivisible asset in a competitive market. More interestingly, it suggests that for efficiency, small peripheral dealers indeed should trade to provide immediacy for large central dealers.====A further novel result in our model is that the inter-dealer trading volume and the asset supply relation is “M-shaped”. Dealers trade among themselves to rebalance inventory, to which the need is greatest when they find it hardest to acquire inventory or liquidity from investors; i.e., when the asset supply is at the lowest or highest level. But precisely when the asset supply is at the lowest or highest level, dealers who possess inventory to sell or spare capacity to buy can only be few and far between. Increases in the asset supply from lowest level and decreases from the highest level should then give rise to more trades. For intermediate levels of asset supply, dealers buy from and sell to investors both with relative ease, largely alleviating any need for rebalancing inventory. In this way, the trading volume peaks at two levels of asset supply – when it is moderately low and when it is moderately high. In contrast, in one extension in which we consider a frictional inter-dealer market, the trading volume-asset supply relation is bell-shaped, with the volume peaking at an intermediate asset supply.==== From this difference is a readily implementable empirical test of the efficiency of a given inter-dealer market.====  Our framework is adapted from the seminal models of OTC markets in Duffie et al. (2005) and Lagos and Rocheteau (2009). In these models, to simplify, the authors assume that whenever a dealer trades with an investor, the dealer can instantaneously offset the transaction by trading in a perfectly competitive inter-dealer market that opens at all times. Such an environment, in which a dealer trades with another dealer only if and when he meets an investor, is apparently not set up to study inter-dealer trades as the trades cannot possibly exhibit any distinctive structure.====In this paper, we extend the two aforementioned models by assuming that dealers only have periodic access to the inter-dealer market, giving rise to a model in which dealers may choose to hold inventory to facilitate future trades. Dealers in Lagos et al. (2011) and Weill (2011), respectively, also have incentives to hold inventory when there is a negative shock knocking the market off the steady state and when there is a transient selling pressure in a competitive dynamic market.====Our primary contribution is an investigation of how the directions of trades among dealers facing different inventory constraints are determined and the implications thereof on how small peripheral dealers provide immediacy for large central dealers. Our paper then contributes to the literature on how dealers specialize and form a core-periphery trading network in which the trading direction is also persistent. The literature has studied how a dealer becomes a core dealer when his search ability is high (Neklyudov (2019)), when he invests more to raise his contact rate (Farboodi et al. (2018b)) or to improve his bargaining skills (Farboodi et al. (2018a)), or when the dealer specializes in serving clients who trade frequently (Sambalaibat (2018)), how the usual network externality should lead to most dealers choosing to set up costly connections with just a handful of dealers (Wang (2017)), and how information imperfection should result in individual agents specializing in market making and taking on the role of core dealers (Chang and Zhang (2019)).====Dealers' inventory constraints become especially relevant in the aftermath of the 2008 financial crisis. Our paper adds to the growing literature on the roles of such constraints in determining the directions, structure and volume of inter-dealer trades. In particular, Dunne et al. (2015) study the effects of the interaction of inventory constraint and adverse selection on market stability while Cimon and Garriott (2018) show how dealers are adapting to the recent regulations on inventory by shifting to an agency basis of trade. Besides, Choi and Huh (2018) find that customers, not dealers, are increasingly the liquidity providers post-crisis, a finding in line with our prediction that small dealers provide immediacy for large ones.====Small dealers in our model trade to provide immediacy for large dealers as they value a unit of inventory in between large dealers having an empty inventory and large dealers already holding a unit inventory. There is a subtle similarity to how investors having an intermediate valuation of the asset in Hugonnier et al. (2018) and Shen et al. (2018) endogenously become dealers, buying from investors with the lowest valuation and then staying on the market selling to investors with the highest valuation.==== The difference between our model and theirs is that the difference in valuation in ours arises endogenously and that the small dealers in our model, as intermediaries for large dealers, either just sell to or buy from their large dealer customers in a given market.====The rest of the paper is organized as follows. In Section 2, we set up the model and then study the model's equilibrium. We discuss the model's implications on trading directions between small and large dealers and compare those implications against the available empirical evidence in Section 3. In Section 4, we explore two additional implications of the model as pertaining to how dealers' inventories and the inter-dealer trading volume vary with changes in the market environment. Section 5 shows how the equilibrium allocation coincides with the planning optimum. In Section 6, we discuss two extensions of the model and demonstrate how the major results hold in more general settings. Section 7 concludes. All proofs are relegated to the Appendix.",Inter-dealer trades in OTC markets – Who buys and who sells?,https://www.sciencedirect.com/science/article/pii/S1094202520300612,24 July 2020,2020,Research Article,158.0
"Guerra-Salas Juan,Kirchner Markus,Tranamil-Vidal Rodrigo","Central Bank of Chile, Monetary Policy Division, Chile,Central Bank of Chile and Universidad Católica de Temuco, Departamento de Ciencias Económicas y Administrativas, Chile","Received 17 August 2018, Revised 21 July 2020, Available online 24 July 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.07.005,Cited by (3),A labor market specification featuring search frictions and unemployment within an otherwise standard New Keynesian ,"Most New Keynesian DSGE models used for policy analysis and forecasting at central banks and other policy institutions assume that the labor market always clears at a sticky nominal wage (à la Calvo) through variations along the intensive margin of labor supply (i.e., hours), but there is no role for adjustment along the extensive margin (i.e., employment).==== The latter stands in stark contrast to academic research that has emphasized the role of labor market flows based on search and matching theory. According to that literature, search frictions and matching can successfully explain several relevant labor market facts, such as the existence of involuntary unemployment and the dynamics of job creation and job destruction (see Pissarides, 2011),==== and the relative unimportance of hours per worker versus employment in the cyclical fluctuation of total hours worked (see Hansen, 1985, and Trigari, 2009).====Some of that disconnect between labor market research and labor market specifications in practical policy models may be due to the fact that the usefulness of search frictions in medium-scale quantitative DSGE models for monetary policy analysis and forecasting is not yet sufficiently well understood. Hence, in this paper we let search frictions compete with the standard labor market specification in a DSGE framework. In particular, we assess whether and how the inclusion of a search and matching specification à la Diamond (1982), Mortensen (1982) and Pissarides (1985), improves the empirical fit and forecasting performance of an otherwise standard New Keynesian small open economy (NK-SOE) model. Our model with search frictions features both the extensive and intensive margins of labor supply; endogenous separations following Mortensen and Pissarides (1994), Cooley and Quadrini (1999) and den Haan et al. (2000), so that fluctuations of employment in response to shocks are due to changes in the vacancies posted by firms, but also in higher or lower firing; and staggered Nash wage bargaining following Gertler et al. (2008), a Calvo-type process for wage determination by which, each period, only a fraction of firms renegotiates the nominal wage with its workforce. The analysis is conducted using Bayesian techniques and Chilean data. While our paper forms part of several recent studies that have investigated the usefulness of labor market search and matching in macroeconomic models, as we discuss below, we are among the first to analyze the benefits of search frictions in a small open economy context. In addition, only few related studies have examined the relevance of endogenous separations with both margins of labor supply in estimated DSGE models.====The shortcomings of labor market specifications in standard DSGE models, both for closed and open economies, become clear from a brief review. In particular, exogenous labor market shocks are typically found to be important drivers of aggregate dynamics in those models: in the Smets and Wouters (2003) euro area model, labor supply preference shocks are the most important drivers of output while wage markup shocks are responsible for the bulk of variations in real wages; whereas in the Smets and Wouters (2007) U.S. model where there is no separate labor supply shock, wage markup shocks account for most of medium- to long-term fluctuations in output and inflation. In Adolfson et al.'s (2007a) NK-SOE model, labor supply shocks are also among the most important shocks to explain output, wage and inflation dynamics in Sweden. The fact that exogenous labor market shocks are so important in standard DSGE models seems unsatisfactory, not only because their underlying determinants are hard to identify, but also because one might expect that labor market outcomes are to a large extent consequences of more structural shocks such as monetary or fiscal policy shocks or, in open economies, foreign shocks (i.e., shocks to foreign interest rates, foreign demand, commodity prices, etc.). In addition, all of the above models rely on relatively large real wage elasticities of individual hours worked to match fluctuations in total hours, which is known to be at odds with micro evidence (see Chetty et al., 2011).====Due to the above shortcomings, recent model developments using search and matching theory have attempted to improve labor market specifications and generate stronger endogenous propagation properties of DSGE models. For instance, Christiano et al. (2011) describe the labor market in a NK-SOE model using a search and matching framework with variations on both margins of labor supply.==== Their estimation results for Swedish data show that the labor supply shock becomes unimportant in explaining output, the estimated elasticity of individual hours is relatively low, and no wage markup shock is needed. However, the labor supply shock is still the most important shock for both total hours and real wages, and basic foreign shocks are relatively unimportant for aggregate dynamics.==== An earlier study by Krause et al. (2008) based on a closed economy model with search and matching estimated with U.S. data also found a relatively low elasticity of individual hours and a small role for labor supply shocks. However, in this model price markup and (ad hoc) match efficiency shocks are the dominant force in labor market fluctuations.==== Part of the failure of this model to explain the fluctuations of both labor market variables and other macroeconomic variables such as output and inflation through more structural shocks may be due to the absence of endogenous separations, in line with the results of Sedláček (2014). Indeed, many studies have found that endogenous separations are important for understanding labor market flows and their interaction with output and inflation (e.g., Trigari, 2009).====Hence, the success of search frictions in quantitative DSGE models has so far been mixed. Our results shed additional light on this issue. In particular, we find that the data strongly favor the model with search frictions over the standard model, as reflected by a significantly higher marginal data density, as well as a significantly better ability of the model with search frictions to match the majority of the second moments of the data. Furthermore, in the model with search frictions, foreign shocks are far more important drivers of the business cycle than in the standard model. This finding is consistent with the large literature on the drivers of business cycles in small emerging economies such as Chile.==== Finally, the forecasting performance of the model, for labor market variables as well as other key variables such as output and inflation, is also significantly improved by the search frictions. Our paper therefore provides further evidence that search frictions are useful to explain aggregate dynamics in quantitative DSGE models for small open economies.====An additional finding of our analysis is that labor search frictions seem to substitute for rigidities in investment. In the model with search frictions and in the standard model, investment is subject to adjustment costs, as is typical in the quantitative DSGE literature. However, the estimated elasticity of adjustment costs is substantially lower in the model with search frictions. When the model features these frictions, capital frictions are less necessary to fit the data, further highlighting the importance of search and matching in the labor market as a propagation mechanism.====The rest of the paper is organized as follows. Section 2 presents our NK-SOE model with search frictions and matching, while the model with the standard labor market specification is described in Section 3.==== Section 4 describes the calibration and estimation strategy, while Section 5 compares the fit of the model under the two labor market specifications, discusses the role of search frictions in propagating various types of shocks, and provides an analysis of the forecasting performance of the different models. Finally, section 6 concludes.",Search frictions and the business cycle in a small open economy DSGE model,https://www.sciencedirect.com/science/article/pii/S1094202520300624,24 July 2020,2020,Research Article,159.0
Miranda-Pinto Jorge,"University of Queensland, Australia","Received 10 April 2019, Revised 30 June 2020, Available online 9 July 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.07.001,Cited by (19),"This paper shows that GDP growth volatility declines with production network diversification. To account for this evidence, I build a multisector model with CES technologies and a cost of complexity in the bundle of intermediates. Production network diversification decreases volatility when ==== and labor are substitute inputs. U.S. sectoral data suggest that labor and intermediates are substitutes in service sectors. Therefore, a calibrated model that then also matches each country's production network can quantitatively generate the empirical patterns. The model also explains why service-oriented countries are less volatile: service sectors have a more diversified set of suppliers.","The production process, such as that of cell phones in Korea, entails complex connections between firms in different sectors of the economy. Samsung, for example, uses not only chips, plastic, and financial services, but also equipment, water, and gas. Every economy produces different goods and services, which then translate into a particular structure of input-output connections. What features of the production network structure amplify or mitigate sectoral shocks? The current literature highlights the role of the intensity of input-output connections—e.g., the existence of star intermediate input suppliers—while ascribing no role to production network diversification—e.g., the number of non-zero input-output connections. In this paper, I revisit these results, empirically and theoretically.====From an empirical standpoint, this paper shows that production network diversification—as measured by the fraction of non-zero input-output connections or by the diversification of the sectoral intermediate input bundle—is a key driver of countries' sales shares concentration (as measured by the Herfindahl-Hirschman Index (HHI)) and GDP growth volatility. Specifically, in a panel of 48 OECD and non-OECD countries for the period 1970-2014, I document two main facts: in countries with a more-diversified production network, i) sales shares are less concentrated, and ii) GDP growth volatility is lower.====From a more theoretical standpoint, I generalize the canonical economy of Acemoglu et al. (2012) to help explain the cross-country patterns I document. I allow for non-unitary elasticities of substitution in production—in particular, non-unitary elasticity between the bundle of intermediate inputs and the labor input. I then emphasize the role played by the intermediate input bundle diversification embedded in nested CES production technologies with input-output weights. In this environment, not only sectoral supplier importance, but also the diversification of the network, matters for the propagation and amplification of sectoral shocks. Using U.S data, I calibrate the model to match each country's input-output structure and estimate elasticities of substitution in production. To the extent that U.S sectoral technologies also describe sectoral technologies across countries, I use the model to perform several quantitative exercises.====The main result in this paper is as follows. The model economy suggests that a higher diversification of the production network reduces the economy's HHI of sales shares and, therefore, volatility, as long as production technologies display high substitutability between intermediate inputs and labor. Standard CES production technologies with input-output weights embed a ==== in the bundle of intermediates. In particular, firms that source intermediate inputs from a more-diversified set of suppliers display a complexity cost. Therefore, all else equal, when labor and intermediates are substitute inputs, an increase in production diversification leads to an increase in labor demand to overcome the complexity cost. This generates a decline in firms' intermediate input shares which then reduces sectoral interdependence and mitigates the effect of sectoral shocks along the production chain.====Using U.S. sectoral data, I show that the elasticity of substitution between labor and intermediates is larger than one in service sectors. I then use countries' sectoral data on intermediate input shares and intermediate input bundle diversification to show that, as predicted by the model, sectors with more-diversified intermediate input bundles display lower intermediate input shares. This effect is stronger in service sectors, consistent with their higher production flexibility. I also show that service sectors have more-diversified intermediate input bundles than do non-service sectors. This, together with the fact that services have higher production flexibility, can help explain, through the lens of the model, why service sectors display lower intermediate input shares and, therefore, why service-oriented economies are less volatile.====The model is able to qualitatively and quantitatively replicate the fact that production network diversification is associated with lower macroeconomic volatility. The calibrated model, which matches each country's input-output structure, is quantitatively successful in replicating the observed empirical patterns. The model's implied relationship among production network diversification, service share, and volatility is of an order of magnitude similar to that observed in the data.==== This paper contributes to the literature on production networks and aggregate fluctuations in Horvath (1998), Foerster et al. (2011), Acemoglu et al. (2012), Carvalho et al. (2016), Atalay (2017), and Baqaee and Farhi (2019). With respect to these studies, this paper illustrates the importance of a new production network feature, production network diversification.==== In doing so, I emphasize the relevance of deviating from homogeneous and Cobb-Douglas sectoral technologies and of taking into account the production complexity of using a more-diversified intermediate input bundle.==== This paper also provides international evidence of the importance of the input-output network structure in shaping the HHI of sectoral sales and macroeconomic volatility.====This paper also contributes to the literature on sectoral composition and macroeconomic volatility in Moro (2012), Moro (2015), Koren and Tenreyro (2007), Carvalho and Gabaix (2013), di Giovanni and Levchenko (2012), and Gabaix (2011). These papers highlight the importance of sectoral composition and the HHI of sales shares in driving macroeconomic volatility. This paper complements the aforementioned papers by providing empirical and theoretical support for the idea that the HHI of sales shares is shaped by the production network structure, with an emphasis on production network diversification. This paper also complements previous studies by showing that the lower intermediate input share in service sectors can be explained by the high production diversification and high production flexibility observed in services.====Finally, this paper contributes to the literature on production diversification and volatility in Koren and Tenreyro (2013) and Krishna and Levchenko (2013). Different from these papers—in which production diversification shapes volatility via the law of large numbers (LLN), regardless of the details of the production function and in a framework without input-output linkages—this article proposes a different mechanism. A more-diversified network of producers reduces volatility only when the substitutability between intermediates and labor is high, and the mechanism operates through lowering intermediate input shares, which then mitigate the effect of sectoral shocks along the production chain.","Production network structure, service share, and aggregate volatility",https://www.sciencedirect.com/science/article/pii/S1094202520300582,9 July 2020,2020,Research Article,160.0
"Carroll Daniel R.,Dolmas Jim,Young Eric R.","Federal Reserve Bank of Cleveland, United States of America,Federal Reserve Bank of Dallas, United States of America,University of Virginia, United States of America,Zhejiang University, China","Received 9 May 2018, Revised 25 June 2020, Available online 8 July 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.06.016,Cited by (2),We study the political determination of flat tax systems using a workhorse ,"Our goal in this paper is to understand the political selection of flat tax systems. We use the workhorse macroeconomic model of Aiyagari (1994), extended to include an array of fiscal instruments and elastic labor supply. We then ask which tax system would be selected at time zero, conditional on a fixed initial distribution and taking into account the transitional dynamics. The specific systems we permit allow for differential flat tax rates on labor and capital income as well as on consumption, with a time-varying lump-sum transfer. That is, our policy space is inherently multidimensional.====This multidimensionality poses a problem for some political economy mechanisms, especially those that rely on the median voter theorem; in general, Condorcet winners do not exist in more than one dimension. Moreover, we cannot appeal to a single-crossing property because households in our model differ along two dimensions (labor productivity and wealth) and cannot be ordered according to their preferences over taxes.====We present two solutions to this problem. First, following a large literature, we investigate the taxes that survive under probabilistic voting. With probabilistic voting, agents experience a random “non-economic” shock and candidates attempt to maximize the expected number of votes they receive; the resulting equilibrium maximizes a social welfare function, which allows us to easily characterize the outcomes.====However, one may be concerned (and rightly in our opinion) that non-economic shocks put little discipline on outcomes. We show that the model's prediction is quite sensitive to the assumed distribution of these non-economic shocks. For this reason, we also consider a generalized solution concept to the Downsian voting game (pairwise competition) that permits set-valued solutions. Specifically, we consider the uncovered set from McKelvey (1986), which contains the strategies that survive under agenda setting and sophisticated voting (that is, what strategies could be implemented under some assumption about the order of votes). If the uncovered set is a singleton, then that policy is a Condorcet winner.====We find that our benchmark probabilistic voting case and the uncovered set approach deliver similar outcomes. For convenience, since the outcomes are the same, we will refer the outcome of the probabilistic voting process as the Condorcet winner. We will also discuss the outcomes in terms of the policy game; we hope that the reader will not be confused by this switch, as we think it clarifies the political aspects of our study.====We first list our three main results, then discuss the features of the model that are crucial for obtaining them. First, we find a Condorcet winner: at least up to a fairly fine grid approximation, outcomes are unique, and any variation we miss due to the discrete approximation is small enough that it has no economic consequences. Second, the Condorcet winner chooses to set labor income taxes to zero, relying on a combination of capital income and consumption taxes. And third, the Condorcet winner sets lump-sum transfers to zero in the long run, after one period of positive transfers. These results arise as a combination of three factors: the shape of the initial distribution (in terms of income source and the marginal propensity to consume), the mobility of earnings and wealth of agents (how quickly the poor become the middle class), and the patience of agents (how forward-looking their voting behavior is).====Why do we obtain a Condorcet winner? In a model with a small number of types, one can in principle assemble a majority in many different ways. If those majorities can be assembled using types with very different preferred tax systems, then the range of equilibrium outcomes can be large. Thus, an agenda setter, by properly sequencing votes, can obtain very different equilibrium policies. Our model endogenously delivers a large number of types, but the measure of these types is such that effective majorities cannot be constructed that prefer very different outcomes: the winning coalition must include the middle class, who, in general, prefer the same kinds of taxes as the poor. The rich are quite different but insufficiently numerous to win, even though our model somewhat understates the measure of rich agents.====Why does the Condorcet winner have zero labor taxes? Here, patience and mobility play an important role. Our vote is once-and-for-all, meaning that once in place the taxes cannot be changed. Even without commitment, the currently “middle class” are reluctant to tax labor income since that is their primary source of income (labor's share of total income in our model is 0.64, and is higher for most agents and lower for the very wealthy). With commitment, even the poor become reluctant to tax labor income, as they anticipate their wages will rise (due to mean reversion in idiosyncratic productivity) and they place a relatively high weight on future periods (due to the low discounting needed to match the aggregate wealth-income ratio). Only the currently wealthy wish to impose labor taxes as they do not work, but these agents are not sufficiently numerous to win. Furthermore, even the non-working wealthy do not like very high labor taxes, because these taxes reduce the return to capital. If we relax the constraint that labor income taxes cannot be negative, we find that the winning policy generates a rather large subsidy on labor income.====Why does the Condorcet winner choose both capital income and consumption taxes? Both taxes attack the inelastic initial distribution of capital, meaning that they are attractive to the initially poor. The consumption tax, however, is regressive: it affects the initially poor more than the initially wealthy because the marginal propensity to consume is higher for the poor. As a result, voters are reluctant to impose high consumption taxes. In contrast, the capital income tax is progressive, but has negative long-run effects by reducing wages; as with labor income taxation, most voters oppose low wages. The result is a compromise that is reminiscent of the theory of the second best (Lipsey and Lancaster (1956)), where all instruments are used a little rather than one instrument a lot. Again, the lower bound on the labor income tax rate plays a role here: with subsidies, the consumption tax rate blows up and capital taxes move close to zero.====Why does the Condorcet winner choose zero transfers in the long run? Again, patience and mobility are crucial. Similar to labor taxes, transfers are limited by the expectation of the poor that they will be middle class in the near future. The middle class pays more in taxes than they receive in transfers (in general), and therefore, they oppose a transfer; many of the poor agree since they expect to be middle class soon. The wealthy are, of course, also opposed, since they will pay more in taxes than they receive in transfers. Note that zero transfers are chosen despite the presence of substantial market incompleteness and wage risk; as a result, the distribution of wealth changes very little over the transition.==== As with the previous two results, relaxing the prohibition on subsidizing labor income changes the outcomes: now the transfer is quite large.====We conduct experiments designed to illuminate results two, three, and four. First, we examine a version of the model with wage risk shut down: there is still heterogeneity, but it is now time-invariant.==== As shown in Chatterjee (1994) and Krusell and Ríos-Rull (1999), the resulting model has no mobility; if one household is initially wealthier than another, they remain so forever.==== In this case, we find that transfers increase substantially, with the concomitant increase in capital income and consumption taxes, but labor income taxes remain zero. Furthermore, if we impose the condition that income taxation must be uniform – the only available choices tax capital and labor at equal rates – we get complete dependence on consumption taxation. Finally, we study a version where agents vote “myopically”; their economic decisions are made using one discount factor, while their votes are evaluated using a lower one. The result is that transfers again rise, but labor income taxes are still zero. Thus, we conclude that the zero labor tax result is very robust and is driven by the shape of the initial distribution, while the zero transfer result is more fragile and depends critically on mobility and patience.====Since mobility is important for understanding our results, we turn to confronting our model with some facts. Using the PSID we estimate mobility for both earnings and wealth across quintiles over five-year horizons (as in Budría et al. (2002)). Our model overstates the amount of mobility for earnings and understates it for wealth over five-year horizons, but the support for the Condorcet winner is strong enough that a closer fit would be unlikely to change the result. Furthermore, the quintiles for which the deviations are largest are the top and bottom; under equal-weighted voting these groups are not decisive.====We then study how outcomes would differ under “wealth-weighted” voting; as in Bachmann and Bai (2013), we assume that wealthier agents effectively have “more votes” than poorer agents. Here, we find that labor income taxes win: as votes become more concentrated in wealthy agents, consumption and capital income taxes are set to zero and labor income taxes are used to finance government spending. Transfers remain zero. This result cements our intuition that the initial distribution is key for the zero labor tax result (this result is the same if subsidies are permitted).====Obviously our experiments have limitations. Our policy space is heavily restricted, for feasibility reasons: we assume that households vote over time-invariant tax systems only, and they vote only once. To explore whether this commitment is crucial, we introduce a revote: at some point along the transition to the new steady state, we allow households to reconsider their tax system. We find that the Condorcet winner continues to win, and this result includes the new steady state as well. There are two reasons for this result. First, the Condorcet winner enjoys very strong support; in fact, it would survive under standard supermajority requirements. Second, the distribution of wealth does not change significantly along the transition, so agents are almost in the same situation as when they started; as a result, the voting outcome is unchanged. Thus, we believe that commitment may not be critical, and given that abandoning it is very computationally burdensome we do not explore this extension.====We expect things may change if we expand the vote to permit time-varying tax systems. Dyrda and Pedroni (2018) find that the Ramsey optimal tax plan involves capital and labor income taxes that change over the transition to the new steady state, suggesting that more flexibility in the policy space could give us different answers.==== Unfortunately, computational considerations prevent us from exploring this direction at this time (and for the foreseeable future).",The politics of flat taxes,https://www.sciencedirect.com/science/article/pii/S1094202520300570,8 July 2020,2020,Research Article,161.0
"Bidder Rhys M.,Krainer John R.,Shapiro Adam Hale","University of Cambridge, United Kingdom,Federal Reserve Bank of San Francisco and Board of Governors, United States of America","Received 27 February 2019, Revised 26 May 2020, Available online 30 June 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.06.014,Cited by (6),We use variation in banks' loan exposure to ==== effect.,"Banks solve complex optimization problems when choosing their portfolio. Consequently, when a bank suffers a shock its response can be expected to be multifaceted. Nevertheless, many canonical models of bank behavior simplify the portfolio problem to draw out broad intuitions. Perhaps the most prevalent intuition is that when a bank is damaged it will generally scale back its operations (Holmstrom and Tirole (1997)). Empirical studies inspired by this intuition have focused on a particular component of banks' loan books (typically corporate lending) and show that damaged banks cut credit supply.====We investigate banks' responses to credit losses induced by the precipitous decline in oil prices of mid-2014 and show that the standard intuition above is incomplete. We take a cross-balance sheet approach, whereas much of the previous literature examines the ‘bank lending channel’ within a given asset class.==== We find that banks do not uniformly reduce credit supply following a damaging shock. Instead, while some types of credit are contracted, others are ====. In particular, corporate loans and ==== mortgages are reduced, while mortgages to be securitized and shifted off balance sheet are expanded. This pattern of adjustment entails a shift from heavily risk-weighted assets, against which banks must hold capital at a high rate, to lower risk-weighted assets. The effect on total lending, total size of the balance sheet, and the degree of leverage appears ambiguous. What is unambiguous, however, is a pattern of ‘de-risking’. We observe banks making substantial reductions in their average risk weight rather than reducing their overall quantity of investments. We corroborate these empirical findings with survey evidence from the Senior Loan Officer Opinion Survey (SLOOS), which suggests that banks tightened terms of credit for the types of loans they wished to reduce (portfolio loans) and loosened credit for those they wished to attract (securitizable loans). Interestingly, we find little evidence that the de-risking ==== asset classes was mirrored by de-risking ==== an asset class.====The intuition that a bank might de-risk when damaged follows from basic portfolio theory. A shock to net worth may change the bank's effective risk tolerance as it is shifted closer to some type of funding constraint (see Froot and Stein (1998)). In this sense, a portfolio rebalancing reflects an updated solution to the risk-return tradeoff faced by the bank. One way a damaged bank can pull back is in terms of loan quantities—the traditional focus of the bank lending channel literature. De-risking can be thought of as an alternative form of pulling back viewed from the broader portfolio perspective. We show that one can put the more traditional bank lending channel studies in a broader portfolio-level context and obtain important additional insights.====An additional contribution we make is to assess the ultimate impact of the oil shock on borrowers. We find no evidence of an operational ‘credit channel’ - the term typically given to the indirect effect of a shock on borrowers, via their banks. Specifically, borrowers who were more exposed to damaged banks do not appear to have made significant changes to their total loan balances or total assets after the shock. Since our data provide the identity of the borrowers, we can trace borrowers across banks, helping us to figure out ==== the effect is limited. The data reveal that borrowers facing worsening terms from exposed banks simply switched to other less-exposed banks within our sample.====The finding of a limited credit channel is in contrast to much of the literature that suggests such borrowers should be relatively harmed (see Khwaja and Mian (2008), Chodorow-Reich (2014), Acharya et al. (2014), Huber (2018), Murfin (2012) for example). Related work has examined the credit channel during periods of stress for the broader financial system.==== While it is especially important to assess how the banking system functions in such crisis periods (where welfare losses may be more intense and where the role of policy is enhanced) it may also be the case that the strength or even the presence of the credit channel could depend on the crisis context. The availability of alternative financing from public debt and equity markets and other banks is likely very different from the normal period that we analyze. This study helps quantify the extent to which this is the case, providing a benchmark for papers that examine the effect of shocks in crisis periods.====We use granular data from the FR Y-14 filings (Y14) obtained from bank holding companies as part of the Federal Reserve's Comprehensive Capital Analysis and Review (CCAR). This dataset includes a broader array of borrowers and loan types than those typically used in the banking literature, such as the Shared National Credits (SNC) data or Dealscan, and spans a wider range of asset classes. Information about specific loans held on the banks' balance sheets allows us to construct the exposure of the banks' corporate lending to firms in the oil and gas (O&G) sector prior to the sudden decline in oil prices in 2014. Our treatment stems from exploiting variation in this variable across banks, which implicitly induced variation in the impact of the price decline on net worth.====For a variety of loan-types, we isolate the credit supply effects by using borrower fixed effects as in Khwaja and Mian (2008) which strips out possible credit demand effects. Any remaining endogeneity bias would arise from supply factors - specifically, factors associated with the banks' decision to hold O&G loans. Our knowledge of the identities of the borrowers and banks allows us to include a wide range of controls at the bank and bank-borrower level to address these concerns. These observable bank characteristics are not correlated with O&G exposure, indicating that high and low exposed banks appear similar on average. Additionally, we instrument our exposure variable with banks' branch locations from several years prior to the shock (similar to Gilje et al. (2016)) with little change in our results. Overall, our results are robust to a variety of alternative exposure measures, standard error constructions, and permutation exercises.",De-leveraging or de-risking? How banks cope with loss,https://www.sciencedirect.com/science/article/pii/S1094202520300569,30 June 2020,2020,Research Article,162.0
"Epstein Brendan,Finkelstein Shapiro Alan","Department of Economics, University of Massachusetts, Lowell, Falmouth Hall, 201 Riverside St., Lowell, MA 01854, United States of America,Department of Economics, Tufts University, Braker Hall, 8 Upper Campus Road, Medford, MA 02155, United States of America","Received 25 May 2019, Revised 17 June 2020, Available online 29 June 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.06.015,Cited by (4)," increase in the shares of household ==== firm financial participation in EMEs can lead to smoother cyclical fluctuations, particularly in labor markets.","This paper addresses the following question: What are the labor-market and business-cycle implications in emerging economies (EMEs) of transitioning from their low ==== of firm ==== household participation in credit markets and the banking system to the much greater shares that characterize advanced economies (AEs)? Understanding these implications is critical as credit markets are important transmitters of external shocks in EMEs, and EMEs have recently undertaken efforts to steadily bolster firm and household domestic financial participation (GFDR, 2014). Given the recentness of these efforts, there is little empirical evidence that can shed light on their business cycle consequences. Moreover, lessons from similar episodes in AEs need not apply given the distinctive characteristics of EMEs. Thus, we analyze this issue using a quantitative theoretical framework.====Our main results show that the extensive margin of household ==== firm domestic financial participation can be important factors behind several, but not all, defining differences in labor-market and business cycle dynamics between EMEs and AEs. Specifically, greater ==== financial participation in EMEs can exert substantial downward pressure on cyclical volatility though it is not, by itself, powerful enough to completely eliminate the differences in business cycle dynamics with respect to AEs. While greater ==== financial participation alone has negligible volatility-reducing effects, combining it with greater firm participation can significantly narrow the quantitative differences in business cycle dynamics between EMEs and AEs.====There are three well-known business cycle characteristics that distinguish EMEs from AEs. First, in EMEs consumption is more volatile than GDP, while the opposite is true in AEs. Second, in EMEs the trade balance-GDP ratio is countercyclical, while it is acyclical in AEs. Third, EMEs have higher wage volatility and lower unemployment volatility (relative to GDP volatility) compared to AEs. At the same time, it is well known that compared to AEs, EMEs have much lower bank credit-GDP and bank deposits-GDP ratios—two ==== measures of domestic financial participation. Of course, these aggregate measures embody both the extensive and intensive margins of firm and household financial participation. Honing in on the extensive margin, a much less-known but important characteristic of developing economies and EMEs is that the shares of households and firms that participate in the domestic banking system are starkly lower in EMEs compared to AEs (see Section 2). Indeed, less than 50 percent of individuals in EMEs have an account at a financial institution, while virtually every individual in AEs does. In turn, the share of EME firms that have bank loans (which are by definition financially-included) is on average 20 percent, while the average share of AE firms with bank loans is at least three times as large. The strikingly low shares of financially-included firms in EMEs is of particular relevance since these firms account only for a small share of total employment in these economies. This makes labor markets a central element of interest in the analysis of domestic financial participation.====Our baseline small-open-economy (SOE) framework features endogenous firm entry, equilibrium unemployment, and two household and firm categories—financially-included (====) and -excluded (====). ==== households, whose members account for a given share of the population and can work in ==== firms, create firms, and have access to deposits and foreign debt. ==== households, whose members account for the remaining share of the population and can work in ==== firms, only consume labor income and do not participate in credit markets. Compared to ==== firms, ==== firms face higher sunk-entry costs, a fraction of which is financed by credit. The benefit of entering and becoming an ==== firm—and the defining feature of firm financial participation in our framework—is access to a more capital-intensive technology that endogenously delivers higher labor productivity. This feature is in line with existing evidence documenting a positive link between firms' access to credit and their productivity. Endogenous firm entry in both categories gives rise to an endogenous share of firm financial participation.====Our calibrated model replicates the low shares of firm and household financial participation in EMEs and matches key empirical EME business cycle moments. ==== raising the shares of both firm and household financial participation to AE levels reduces the relative volatility of consumption below 1, as is observed in AEs; brings the relative volatility of wages down and closer to the one in AEs; and makes the trade balance acyclical, as is observed in AEs. In contrast, the relative volatility of unemployment decreases non-trivially, which ==== the difference between EMEs and AEs. This last finding suggests that the lessons from AEs regarding financial participation need not apply to EMEs. ==== increasing firm financial participation achieves similar results, but from a quantitative standpoint, greater household participation is needed even though ==== increasing household participation from EME to AE levels hardly changes EME business cycles.====The intuition behind these results traces back to how the average labor-productivity differential between ==== firms and ==== firms changes with greater ==== financial participation ==== versus greater ==== financial participation, and how the number of firms in the economy expands. Specifically, greater ==== financial participation ==== increases capital demand and the number of ==== firms, and bolsters average labor productivity across firms in both categories, with the change in ====-firms' labor productivity being much greater compared to that of ==== firms. Critically, the greater change in ==== firms' labor productivity increases the average value to firms of having a worker, thereby stabilizing firms' hiring and capital demand responses to shocks. This stabilizing effect, which reduces the volatility of hiring, capital demand, wages, employment, labor income, and ultimately household consumption amid shocks, is compounded by having more firms in the economy. These changes also make the trade balance considerably less countercyclical.====The opposite outcome takes place under greater household financial participation ====: A sharp steady-state reallocation of employment towards ==== firms that surpasses the endogenous increase in capital demand implies that ==== firms' average labor productivity changes relatively more compared to that of ==== firms. As such, ==== firms' decisions become more sensitive to shocks, thereby contributing to sharper fluctuations. This last mechanism is compounded by having a greater share of the population exposed to interest rate shocks and explains why greater ==== financial participation ==== does not generate significantly different cyclical fluctuations compared to what EMEs (which have small shares of household financial participation) currently exhibit. However, if greater firm financial participation takes place ==== greater household participation, the resulting greater reallocation of resources available to ==== firms interacts with the greater increase in ==== firms' labor productivity and the number of firms, which explains why a joint increase in financial participation brings EME aggregate dynamics ==== closer to the ones in AEs, and why unemployment volatility becomes considerably lower.====Our work makes three contributions to the literature on EME business cycles and labor market dynamics. First, we illustrate that the ==== of domestic financial participation can have nontrivial effects on business cycle dynamics; it can also be a potential driving force behind differences in labor market and aggregate dynamics between AEs and EMEs (a force that related studies on EMEs have abstracted from). Moreover, the type of extensive margin underlying greater domestic financial participation matters for the likely changes in labor market and aggregate volatility. Second, to the extent that lower aggregate volatility may be a desirable objective in EMEs, our results stress the importance of ==== expansions in firm ==== household domestic financial participation—with greater firm participation being a critical factor—and not in any given extensive margin alone. Third, we highlight the limitations of aggregate indicators of domestic financial participation for understanding the volatility consequences of bolstering domestic financial participation in EMEs: Greater firm and household participation are both associated with greater credit-GDP ratios but, by themselves, have contrasting implications for cyclical dynamics. As such, understanding the underlying sources of changes in these aggregate indicators is critical.====The remainder of this paper is as follows. Section 2 summarizes related literature, places our contributions in context, and summarizes evidence that motivates our framework. Section 3 presents the model. Section 4 discusses the results from our quantitative analysis. Section 5 concludes.",Increasing domestic financial participation: Implications for business cycles and labor markets,https://www.sciencedirect.com/science/article/pii/S1094202520300557,29 June 2020,2020,Research Article,163.0
"Wellschmied Felix,Yurdagul Emircan","Universidad Carlos III de Madrid, Getafe, Spain,IZA, Bonn, Germany","Received 1 November 2018, Revised 4 June 2020, Available online 16 June 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.06.012,Cited by (1),"US entrepreneurs typically work long hours in their firms and these hours form a large part of the firms' labor input. This paper studies the role of endogenous owner hours in shaping the wealth distribution among entrepreneurs. We introduce owners' endogenous labor supply into a model of entrepreneurial choice and financial frictions. The model fits well the levels and the dispersion of wealth among entrepreneurs. Long owner hours incentivize poor, highly productive individuals to be owners and help the most productive owners to accumulate large quantities of wealth. On net, owners working long hours decreases the median owner wealth and increase wealth dispersion among owners. Differently, the ability to work sufficiently short hours incentivizes owners to run low productivity firms with high wealth to income ratios. Finally, alternative calibrations ignoring the endogenous labor supply of owners lead to owners that are much richer than in the data and overstate the effect of financial frictions in the economy.","There is a large variation in wealth levels of US entrepreneurs, with the top 25th percentile owner holding 3 times the average worker wealth and the bottom 25th percentile owner holding one-fifth of the average worker wealth. The entrepreneurship literature typically attributes a part of this variation to the interaction of financial frictions (typically in the form of collateral constraints for factor costs) and firm-level heterogeneity in the history of productivity shocks. In these models, such as Quadrini (2000) and Cagetti and De Nardi (2006), entrepreneurship arises from good business ideas of their owners. Owners have to finance part of their firms' operating costs with their wealth, thus, providing an incentive for owners to accumulate wealth.==== Productivity heterogeneity leads to heterogeneity in desired firm sizes and, thus, in the required wealth to finance operations. Moreover, firms' histories, i.e. the tenure in entrepreneurship and the accumulated productivity shocks, add to the variation in wealth as the longer a firm is already operating and the more profitable it has been the more wealth the owner can accumulate to overcome the financial frictions.====A relatively less explored pattern in entrepreneurship is a high level and a significant dispersion of the labor supply of owners into their own firm. On average, owners work more than 40 hours per week, largely contributing to the total labor input in most firms. Owner hours are the only labor input for 75 percent of the firms in the Survey of Business Owners (SBO), and, even within employers, the mean ratio of owner hours to hired labor is 57 percent. Moreover, owner hours exhibit high dispersion, with an interquartile ratio of 1.7 for the SBO, and 1.8 in the Survey of Income and Program Participation (SIPP).====This paper studies the role of endogenous owner hours in (i) shaping the wealth distribution among entrepreneurs, (ii) explaining the wealth of entrepreneurs relative to workers, and (iii) inferring financial frictions and their impact on the entrepreneurial sector and the owners' wealth distribution. We use an entrepreneurship model with financial frictions and endogenous owner hours to show that owners working sufficiently long hours in their firms makes it attractive to operate firms with little wealth and, at the same time, allows owners to accumulate large quantities of wealth. Accordingly, it expands the left and the right tails of the wealth distribution within entrepreneurs. Moreover, the ability to work sufficiently short hours generates a higher number of low income, yet moderately wealthy, owners. In regards to our second objective, we show that capturing owner hours implies on average less severe financial frictions and more modest wealth levels of owners relative to a model without endogenous owner hours.====In our model, individuals differ in their productivity as entrepreneurs, productivity as workers, their wealth, and disutility of working. They can be either workers or entrepreneurs, and entrepreneurs face financial constraints in the form of a collateral constraint. Workers supply a fixed number of hours. In contrast, entrepreneurs supply their hours flexibly in their firm and also rent capital and hire labor to produce output. Hired labor is not a perfect substitute for owner hours, reflecting the need for supervision and the unique knowledge of an owner in her own firm.====We identify the substitutability between owner hours and hired labor using the average owner hours in intermediate and large firms relative to smaller ones. We calibrate the remaining parameters to match further moments of owner hours, firm size, and labor market outcomes in the US. The model replicates well the non-targeted features of the relationship between owner hours and firm performance. First, the model generates a hump-shaped relationship between firm size and owner hours. The calibrated degree of substitutability between owner hours and hired labor is low enough to make owner hours increase with firm size for smaller firms but high enough to generate reduced owner hours in larger firms, where the marginal disutility of working is too high compared to the diminished effect of their hours on consumption. Second, the model generates a positive correlation of owner hours with contemporaneous and future firm performance. Since financial frictions dampen current firm performance and future growth, long owner hours lead to higher firm output today and faster accumulation of wealth leading to high output in the future. These long owner hours are particularly relevant for highly productive young firms, and the model matches the growth rates observed in the data early in firms' life-cycles.====Importantly, we also establish that the model replicates closely the wealth features of entrepreneurs observed in the data. It matches not only the corresponding dispersion but also the fact that while most entrepreneurs have substantially more wealth than the median worker, the bottom 25% of the entrepreneurial wealth distribution holds almost no wealth. Moreover, despite the median owner income being lower than median worker income, the median owner wealth is higher than the median worker wealth.====We then use our model to study the role of endogenous owner hours in shaping these wealth patterns. In doing so, we distinguish between owners' ability to supply sufficiently long hours and their ability to supply sufficiently short hours. The ability of owners to work significant hours in their firm increases the profitability of businesses. It allows highly productive owners, particularly those with a low disutility of working, to accumulate the wealth to get closer to their optimal size; hence, it expands the right tail of the wealth distribution. Moreover, it makes it feasible to operate firms with little wealth, hence, expanding also the left tail. In fact, a counterfactual experiment where owners cannot work more than 10 hours a week generates a 20 percent decrease in the 95th percentile level of wealth and makes the 25th percentile more than double. Consequently, the interquartile ratio decreases to a half in this counterfactual. Put differently, our results show that owners' ability to exert their hours into the firm makes the two tails of the distribution thicker and increases the wealth dispersion.====Next, we study the role of the ability to work sufficiently short hours, namely shorter than salaried workers, for the owners' wealth distribution. In line with the above, the existence of poor, productive owners and also that of very rich owners is related to upward, not downward, flexibility. In contrast, owners that value the ability to work short hours are the unproductive ones, who are over-represented in intermediate wealth ranges. Many of these owners operate their firm because they have more wealth than the typical worker and, therefore, higher consumption and lower optimal hours. This generates owners with lower income, yet, higher wealth than the typical worker. Hence, a counterfactual imposing a lower bound of 40 hours on entrepreneurs' weekly hours reduces the number of owners with modestly high wealth levels. This pushes apart the two tails of the wealth distribution and increases dispersion measures such as the interquartile ratio that rises from 8.1 in our benchmark to 9.5 in this counterfactual. That is, owners' ability to work sufficiently short hours decreases the wealth dispersion.====This paper also shows how the endogenous labor supply of entrepreneurs as a modeling device changes the inferred strength of financial frictions and the resulting wealth levels among business owners. We recalibrate two versions of our model, one that rules out owners' labor supply in their firm altogether, and one that highlights the importance of flexibility by fixing the labor supply of owners at 40 hours per week. As discussed above, owners' ability to supply significant hours to their firm contributes largely to the value of operating a firm and tilts the distribution towards low wealth firms. Therefore, all else equal, ignoring owners' labor supply decreases the number of operating firms, particularly low wealth ones. As a consequence, such a model's recalibration infers a higher average entrepreneurial ability of individuals to rationalize the number of active entrepreneurs. In turn, the model without owner hours infers that financial frictions are more severe to rationalize that the average firm size is quite small, despite entrepreneurs being highly productive. Consequently, the share of financially constrained firms increases to 67% in the model without owner hours up from 24% in the baseline model. Higher average productivity coupled with more stringent financial frictions implies stronger incentives to accumulate wealth. Hence, owners' wealth levels are too high at all points of the distribution in this alternative model. To be specific, the average owner wealth relative to average worker wealth, which is in line with the data in our model, increases by a factor of 15. The dispersion of wealth among entrepreneurs is similar to the baseline model, however, as we argue above, endogenous owner hours coupled with relatively weak financial frictions explain the data better than strong financial frictions because the model is consistent with the substantial number of low wealth owners. In line with the higher fraction of firms being financially constrained, this model also differs largely in the predicted effects of relaxing financial constraints. If we double the borrowing ability of firms for the operating costs, this alternative model predicts the output in the entrepreneurial sector to almost triple, whereas the predicted increase in our benchmark is 50 percent.====Turning to the recalibrated model with fixed owner hours, we find that this model, similar to our benchmark, captures that most owners have little income. It fails to rationalize, however, that many of these low-income owners are relatively wealthy because it misses selection into entrepreneurship based on wealth. Moreover, the underlying impact of financial frictions on this model are stronger, with 32 percent of firms being financially constrained instead of the 24 percent in our model. This is due to two reasons. First, the upward flexibility of owner hours provides owners with a cushion against financial frictions. Second, similar to the model without owner hours, this recalibrated model implies larger optimal firm sizes and more severe financial constraints. Accordingly, doubling the borrowing ability of entrepreneurs generates a positive impact on the entrepreneurial output that is 25 percent larger than the one in benchmark model.====The labor supply of entrepreneurs to their businesses so far has not received much attention from the macroeconomics literature. Two recent exceptions are Yurdagul (2017) and Allub and Erosa (2019). The former introduces owner hours as shifters of Hicks-neutral productivity to study the value of hours flexibility to entrepreneurs. Meanwhile, Allub and Erosa (2019) study the self-selection of entrepreneurs into employer and non-employer firms where entrepreneurs supply a fixed amount of hours to their firm. The difference in our paper is that first, we allow for endogenous variation in entrepreneurs' labor supply so that we can capture its interaction with financial frictions, propensities to work, and firms' performance. Second, our focus is on the wealth distribution of entrepreneurs whereas these papers' objectives are directly about income differences and flexibility in labor supply (the first paper), or firm dynamics (the second paper).====There is also a strand of literature related to our paper, where the owner may supply managerial input into her firm. Bhattacharya et al. (2013) study the effect of endogenous managerial skill accumulation on firms' life-cycle profiles. In Lee (2019), managerial ability enters the production function in a similar way, and this ability depends on whether the manager is also the owner of the firm or not. In contrast to this line of research, we restrict attention to owners' endogenous labor supply when it comes to modeling their input to the firm and study the effects of this input on entrepreneurs' wealth outcomes.====The paper is organized as follows. Section 2 describes the model. Section 3 explains the calibration and Section 4 discusses the model's predictions for relevant empirical features relating firm performance to owner hours. Section 5 studies the role of owner hours in wealth accumulation of business owners. Section 6 concludes.",Endogenous hours and the wealth of entrepreneurs,https://www.sciencedirect.com/science/article/pii/S1094202520300533,16 June 2020,2020,Research Article,164.0
"Önder Yasin Kürşat,Sunel Enes","Ghent University, Department of Economics, Ghent, Belgium,OECD, Economics Department, 2 Rue André Pascal, 75775 Paris CEDEX 16, France","Received 21 May 2018, Revised 9 May 2020, Available online 20 May 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.red.2020.05.002,Cited by (6),"We investigate the debt sustainability implications of leaving a currency union in the context of the Grexit phenomenon. Using a common currency debt specification that is calibrated to the 2001-2010 episode of the Greek economy, we obtain default in year 2012 as an endogenous outcome in a baseline event analysis. Next, we consider a Grexit scenario in which the government issues both euro and drachma denominated sovereign debt and has the option to inflate away its nominal debt obligations by discretion. In the Grexit economy, lenders charge an ex ante ","This paper tackles the question of whether members of the European Economic and Monetary Union (EMU) who were caught in the euro area debt crisis with excessive sovereign borrowing should have left the eurozone, through the lens of debt sustainability. One major virtue of monetary unions such as the EMU is that monetary policy is delegated to a supranational authority, e.g. the European Central Bank (ECB), which values price stability (as defined in its mandate) more than member countries, although each member sovereign might judge committing to price stability as not the best policy to follow for their citizens. Achieving better commitment to price stability by effectively abandoning national currencies and sovereign monetary policy provides a nominal anchor for the member states and relaxes sovereign borrowing limits (see Aguiar et al., 2014). However, extreme episodes of a sudden reversal in confidence (such as the recent eurozone debt crisis) might impose a nontrivial trade-off on a member state, who holds a large stock of sovereign debt. In this case, two among the (not many) options are either to restructure the debt (which is a form of declaring outright default) at the expense of staying in the currency area or exit the currency union and start issuing debt in a national currency in order to use discretionary inflation to reduce real debt repayments, or even perhaps a combination of both.====Our paper reflects on this trade-off with a formal analysis by investigating the question of which option would be deemed optimal by a benevolent government in the case of Greece during the recent eurozone debt crisis. The revelations by Greek prime minister George Papandreou on (much higher than thought) budget deficit-to-GDP ratios in 2009 triggered a loss of confidence toward Greek debt sustainability. The ensuing panic escalated upon credit rating downgrades in early 2010, which were followed by a surge in country bond spreads. Volatility increased further after an additional credit rating downgrade by Moody's in June 2010. This year was also marked with the heightened debate on whether Greece should actually leave the euro area to gain access to a national currency as a tool to alleviate its problems, as exemplified by Economist (2010), Eichengreen, 2010a, Eichengreen, 2010b, Feldstein, 2010a, Feldstein, 2010b and Krugman (2010).====However, Grexit never actually took place and as the year 2011 progressed, the IMF conveyed warning signals that it was highly likely that Greece would end up having serious debt sustainability problems given the macroeconomic outlook. These expectations were re-affirmed in the debt sustainability analysis in IMF (2011), which highlighted a negative growth trajectory for Greece in years 2011 and 2012. In March 2012, the debt event occurred and Greece and its creditors agreed on a debt exchange that implied a haircut rate of 50% (when bank recapitalization costs are deducted) relative to 2012 GDP.====As a starting point, we first construct a common currency debt (CCD, hereafter) economy specification for Greece. The CCD economy is a fairly standard quantitative sovereign default model in the Eaton and Gersovitz (1981) tradition, which extends the canonical model with real debt contracts in Aguiar and Gopinath (2006) and Arellano (2008) to one that includes long-term debt contracts ==== Hatchondo and Martinez (2009) and Chatterjee and Eyigungor (2012) and partial recovery as in Hatchondo et al. (2016) upon default. In this small open economy, a benevolent sovereign government issues discount euro bonds in order to smooth out endowment fluctuations.==== The sovereign cannot control the supply of the common currency, so it effectively takes the rate of inflation as given under the assumption of the law of one price. Furthermore, we assume that the monetary authority of the currency area fully commits to price stability, effectively making euro bonds a real contract.====In contrast with monetary policy, the sovereign decides on the fiscal policy in the CCD economy and cannot commit to debt repayment. Therefore, while issuing new debt, it faces a bond price schedule which depends on the economy's fundamentals as well as gross debt issuance and reflects a default risk premium charged by international lenders. Foreign lenders are risk neutral and expect to earn as much as the international risk-free rate while investing in euro bonds issued by the sovereign and hence make zero expected profits by competition. In each period, having observed the aggregate endowment realization, the government repays its debt or declares default. In the case of a default, an exogenous haircut rate is applied on outstanding bonds and the economy enters into financial autarky which lasts for a random but finite number of periods.====We calibrate the CCD economy to the 2001-2010 episode of Greece which starts with the adoption of euro as a common currency and ends with an era in which the Grexit debate escalated in academic and policy circles. The CCD specification mirrors the average sovereign debt-to-GDP ratio and euro bond spreads of Greece over German bunds for the reference period. We first replicate the 2012 debt crisis by carrying out an event analysis in which we feed the cyclical deviations of Greek GDP from a HP trend for the 2006-2016 episode to our calibrated CCD economy as exogenous endowment realizations. The event analysis is successful in generating a gradual rise and an abrupt spike in euro bond spreads as the economy evolves between 2009 and 2012. Furthermore, the sovereign optimally defaults in year 2012 under the adverse GDP realization in that year.====We study Grexit by considering a scenario in which the sovereign now operates in a multiple currency debt (MCD, hereafter) regime by the virtue of an unanticipated announcement of abandoning the common currency in year 2010. The MCD economy specification constitutes a second baseline in which the sovereign can borrow both in drachma and euro (now a foreign currency) denominated bonds in international debt markets. Since Grexit never took place in reality, we calibrate the MCD economy by targeting the long-term level and currency structure of sovereign debt as well as the long-term level and cyclicality of inflation in the pre-eurozone episode of the Greek economy.====Two key features of the MCD economy stand out. First, the government now can resort to the discretionary use of currency depreciation (which is equivalent to inflation under the law of one price and the existence of a single tradable good) without commitment to price stability. That is, when government finds repayment optimal, it also sets the inflation rate optimally ====, in a way to reduce real debt burdens of the drachma debt. In this manner, discretionary inflation serves the sovereign as a tool for partial repudiation of debt as discussed by Calvo (1988). To ensure that inflation is bounded from above in equilibrium, we assume that inflating the economy is associated with real output losses that are a variant to Rotemberg (1982)-type menu costs. Under lack of commitment to price stability, the sovereign then trades off the distortions created by inflating the economy with the improvement in debt sustainability, thanks to lower real drachma debt repayments under higher inflation.====Resembling the theoretical exposition by Barro and Gordon (1983) on the use of discretionary inflation, drachma lenders in the MCD economy anticipate that the sovereign would demonstrate an inflationary bias ====, inducing them to charge a risk premium (based on the expected inflation in the subsequent period) to the sovereign ====. This premium indeed, would make it very difficult for the sovereign to sell drachma debt upon Grexit, bringing us to the second key assumption in the MCD economy, which suggests that multiple currency debt of the sovereign shall be held by heterogeneous investors that lend in segmented bond markets. Particularly, euro bond investors under the MCD economy are modeled to be more risk averse toward holding the defector country's foreign currency denominated debt, whereas the drachma debt holders continue to be risk neutral while discounting future flows.==== The aversion premium charged by euro lenders counterbalances the inflation risk premium asked by drachma lenders in the MCD economy, facilitating the Grexit model to feature local currency debt shares that are as high as they used to be in the case of Greece prior to joining the eurozone. As the sovereign is now able to attain an empirically relevant level of drachma debt share in its borrowing portfolio, it can in turn, rely more on discretionary inflation to enjoy fiscal relief. Risk aversion premium asked on hard currency debt additionally captures the economic costs on the defector country as discussed by Eichengreen (2010a) and de Guindos (2018), as it shifts up the borrowing cost of the sovereign in euros in the aftermath of the Grexit.====Since we allow for the co-existence of foreign and local currency bonds in the MCD economy, the tendency to display an inflationary bias creates a perverse incentive for the sovereign to buy back some euro denominated debt, issue unrealistically large levels of drachma debt and hyper-inflate the economy just before outright defaults. This eliminates some of the defaults in simulations and generates occasional consumption booms that go hand-in-hand with hyper-inflation under very adverse macroeconomic fundamentals. To avoid such anomalies, we impose a no-buyback constraint that bars the sovereign from buying back euro debt while issuing drachma bonds in the MCD economy.====We then repeat our event analysis under the MCD model to study the case of Grexit, with the MCD economy having as much euro debt as the CCD economy in the beginning of 2010 and no initial drachma debt. The sovereign tilts the currency structure of its debt (35% in drachma) toward local currency in four years in this case, to enjoy debt relief by inflation and is able to escape outright default. Consequently, the euro bond spread under Grexit attains a maximum level of about 700 basis points lower than the case of the CCD economy. This however, comes at the cost of incurring reduced bond prices on euro debt, reflecting the aversion premium introduced after Grexit and a moderate inflation crisis of about 11% per annum.====Much intriguingly, the substitution of partial default via inflation for outright repudiation of debt under Grexit results in a welfare loss. We demonstrate that these losses are mainly linked with lower revenue generation from (now more costly) euro debt issuance, which reduces consumption substantially in the initial phase of the transition that follows the Grexit. Therefore, we find that repricing of hard currency sovereign debt and real distortions of inflating away the economy dominate the hedging benefits from the discretionary use of inflation from a normative point of view.====After executing the main event analysis for Grexit, we employ a number of quantitative exercises to further explore workings of the two baseline models. First we consider debt-sustainable fundamentals in year 2012, so that the eurozone economy would also have escaped default and compare the transition dynamics produced by the two debt issuance regimes. Second, we explore the way that we introduce heterogeneity in the degree of risk aversion of foreign lenders, define inflation costs and impose the no-buyback constraint upon Grexit in the MCD model. These experiments conclude that lender heterogeneity is instrumental in generating empirically relevant currency structures, procyclicality of inflation can be modeled without having to model a Phillips curve and allowing buybacks of euro debt does not imply hyperinflation crises in our event analysis. Finally, we consider counterfactual timings for the Grexit event, which essentially allows us to input different fundamentals at the time when the sovereign chooses to leave the nominal anchor. None of the counterfactual Grexit timings (that take place in either 2008 or 2012) imply outright default in year 2012.",Inflation-default trade-off without a nominal anchor: The case of Greece,https://www.sciencedirect.com/science/article/pii/S1094202520300338,20 May 2020,2020,Research Article,165.0
"Melosi Leonardo,Primiceri Giorgio E.,Tambalotti Andrea","Federal Reserve Bank of Chicago, United States of America,Northwestern University, United States of America,NBER, United States of America,CEPR, United Kingdom of Great Britain and Northern Ireland,Federal Reserve Bank of New York, United States of America","Received 20 May 2021, Available online 28 May 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.red.2021.05.004,Cited by (0),None,None,Introduction to the special issue in memory of Alejandro Justiniano,https://www.sciencedirect.com/science/article/pii/S1094202521000417,28 May 2021,2021,Research Article,168.0
"Fernández-Villaverde Jesús,Guerrón-Quintana Pablo A.","University of Pennsylvania, United States of America,Boston College, United States of America","Received 18 May 2020, Available online 23 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.005,Cited by (62)," augmented with financial frictions and uncertainty shocks. Fifth, we use the model to illustrate our previous discussions and to show how uncertainty shocks can be expansionary, a useful finding in several contexts.","During the last decade, a vibrant literature has explored the role of uncertainty and its time-variation in driving business cycles.==== From the empirical side, ====From a theoretical perspective, the work by ====, ====, ====, and ==== and assert causality: uncertainty shocks drive part of the business cycle. An extensive literature has followed these pioneering studies and extended the theoretical models in many directions.====This paper reviews this literature on uncertainty shocks and business cycle research. In the interest of space, we will focus on equilibrium models of the business cycle with a representative agent in the tradition of most of the papers in ====This model will be rich enough to incorporate most of the mechanisms outlined in the literature linking uncertainty shocks with business cycles and, yet, simple enough to be understood quickly. For example, we will not add nominal rigidities or extra layers of real rigidities. And, instead of a multitude of shocks, we will have only six shocks –three level and three uncertainty shocks– joined in pairs of one level and one uncertainty shock. The first pair will work as demand shocks. The second pair will work as supply shocks. The third and final pair will work as financial friction shocks. The trinity of demand, supply, and financial shocks will capture the intuition often used by researchers to organize their thinking about business cycles and be easy enough to separate from each other.====We will organize the rest of our exposition as follows. Section ==== will document some basic time-series evidence that justifies the study of uncertainty shocks. Section ==== will review the different mechanisms that relate uncertainty shocks and aggregate variables in equilibrium models of the business cycle. Section ==== will discuss how to model uncertainty shocks. Section ==== will present a real business cycle model with financial frictions and time-varying uncertainty. Sections ==== and ==== will explain the computation and calibration of the model, including several methodological points of interest about how to handle the inherent non-linearities of models with uncertainty shocks. Section ==== will report our quantitative results in terms of moments of the model, the role of uncertainty shocks, its impulse-response functions, and persistence. Section ==== will offer some concluding remarks and propose lines of future research.",Uncertainty shocks and business cycle research,https://www.sciencedirect.com/science/article/pii/S1094202520300454,23 June 2020,2020,Research Article,171.0
"Gertler Mark,Kiyotaki Nobuhiro,Prestipino Andrea","New York University, United States of America,Princeton University, United States of America,Federal Reserve Board, United States of America","Received 15 April 2020, Available online 20 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.004,Cited by (26),. We then consider how the optimal macroprudential policy weighs the benefits of preventing a crisis against the costs of stopping a good boom. We show that countercyclical capital buffers are a critical feature of a successful macroprudential policy.,"We develop a model of banking panics which is consistent with two important features of the data: First, banking crises are usually preceded by credit booms. Second, credit booms often do not result in crises. That is, there are “bad booms” as well as “good booms” in the language of Gorton and Ordonez (2019). We then use the model to study macroprudential policy.====We first describe the main facts we think a model of financial crises should capture. Fig. 1 portrays the link between credit growth and financial crises, using data from Krishnamurthy and Muir (2017). The evidence is based on a panel of annual data of industrialized countries, ranging from 1869 to 2018. The authors use the narrative based classification in Jordà et al. (2011) to determine periods in which a country experienced a financial crisis. The figure then plots the average behavior of output, credit growth and credit spreads, around the time a crisis occurs. In each of the three panels, the crisis occurs at year zero. The upper-right panel shows that credit growth on average steadily increases prior to the crisis before declining afterward, as a number of authors have recently noted, e.g. Schularick and Taylor (2012). The bottom panel shows that prior to a crisis, GDP growth on average increases relative to trend by roughly two percent, but when the crisis hits it experiences a sharp and persistent decline of nearly eight percent. Finally, as support for the notion that the output contractions reflect financial crises, credit spreads increase on average prior to and during the crisis, before eventually going back to a normal level, as shown in the upper-left panel.====Fig. 2, however, makes clear that high credit growth does not always lead to a crisis, nor is it necessary for a crisis to arise. The data in the figure plots annual demeaned credit growth in a country lagged two years (the horizontal axis) versus one year (the vertical axis).==== The red dots are episodes where a country experienced a financial crisis while the blue are instances where a crisis did not occur. If we think of a credit boom as a period in which credit growth is above average for two consecutive years, then the upper right quadrant reflects periods preceded by credit booms. Accordingly, crisis episodes happening after credit booms are all the red dots in the upper right hand quadrant in the figure, while the blue dots in the upper right quadrant are episodes in which a credit boom did not result in a crisis. As the figure shows, more often than not, a credit boom does not result in a financial crisis. Conditional on a credit boom, the probability of a crisis is just 4.9 percent. It is true, however, that a credit boom makes a crisis more likely: conditional on no credit boom, the probability of a crisis drops to 2.8 percent.====Our goal in this paper is to first develop a macroeconomic framework with banking panics that is consistent with the evidence in Fig. 1, Fig. 2, and then to use the model to study regulatory policy. The framework we develop is based on Gertler et al. (2020b), henceforth GKP (2020), which is a standard New Keynesian macro model modified to include banks and banking panics that disrupt real activity. Within that framework, we capture both credit booms preceding crises and the banking collapse and disruption of real activity that follows. In the spirit of Geanakoplos (2010) and Bordalo et al. (2018), the source of the boom is optimistic beliefs by financial intermediaries (or banks in short) about future returns to capital that are eventually disappointed.==== This leads to a buildup of bank credit that is funded by an increase in bank leverage, mostly in the form of short term debt. High levels of debt, in turn, make the system vulnerable to a run by increasing the exposure of banks to negative returns on their assets, so that even small negative shocks can trigger system wide runs that result in deep contractions in economic activity. In this latter regard, the model captures the highly nonlinear dimension of financial crises. We use global methods to solve the model numerically in order to characterize these nonlinearities.====There are several differences from our earlier work. First, while in our earlier paper we used a canonical New Keynesian framework with capital accumulation and focused our study on the Great Recession, here we consider a simple endowment economy but allow for recurrent credit booms that may or may not result in banking crises. This allows us to capture the statistical relationship between credit booms and financial crises described above. The presence of good and bad credit booms sets the stage for our study of macroprudential regulation. By restricting financial intermediation, macroprudential policies can prevent the large credit booms that are the root cause of financial crises. However, since the regulator can't tell apart bad credit booms from good ones, attempts at preventing crises will often end up stifling good booms. By matching the relative frequency of good and bad credit booms in the data, our framework allows us to study quantitatively how the optimal policy weighs the benefits of preventing crises against the costs of stopping good booms. We also analyze the features of optimal regulation and show, for example, that countercyclical capital buffers are a critical feature of a successfully designed macroprudential policy.====One final important modeling difference from our earlier work is that we allow for equity injections into the banking sector. In our earlier work we assumed that bank capital was only accumulated via retained earnings. What this implies is that to meet equity capital requirements, the only margin of adjustment is for banks to reduce assets. Allowing for new equity injections introduces a second margin of adjustment. We assume however that at the margin, equity injections are costly. If they were costless, equity finance would become the sole source of funding for banks, eliminating the possibility of runs or any other type of banking instability.==== However, there is a very large literature in finance that argues that equity finance is costly for banks and stresses the important role of debt finance in contexts where agency problems affect the relationship between bank managers and outside investors.==== Accordingly, in this paper we assume that equity finance comes at a cost. While we do not explicitly model the frictions that underpin this cost, we discipline its impact on banks funding choices by matching the observed average leverage ratio and equity issuance rate of financial firms. In particular, while the data shows that equity issuance rose during the financial crisis, it remained small as a fraction of total equity. In particular, Fig. 4 shows that the average annual equity issuance of financial firms was one percent of trend equity between 1985 and 2007, and peaked at around 2.4% during 2008-2010.==== Accordingly as a check that our parametrization is reasonable, we show that the model implied equity injections after a banking panic are in line with that observed during the recent financial crisis.====Our paper contributes to a large literature that studies the role of financial intermediaries in macroeconomic fluctuations. Much of this literature builds on the conventional financial accelerator model of Bernanke et al. (1998), and Kiyotaki and Moore (1997). While the traditional models were developed to study how procyclical movement in nonfinancial borrowers balance sheets work to amplify and propagate macroeconomic fluctuations, Gertler and Karadi (2011) and Gertler and Kiyotaki (2010) showed how the basic mechanism could be applied to study financial firms as well. One limitation of the original models was that, by studying the local behavior of the economy around a non-stochastic steady state, they could not capture the non linear dimension of financial crises. To address this limitation, a series of papers have tried to capture the nonlinear dimension of financial crises by exploiting occasionally binding financial constraints, e.g. Mendoza (2010), He and Krishnamurthy (2019) and Brunnermeier and Sannikov (2014). While we also allow for occasionally binding constraints, the main source of non-linearity in our paper is the occurrence of a bank run. As in our earlier work, e.g. Gertler and Kiyotaki (2015), Gertler et al. (2016) and Gertler et al. (2020b), we model bank runs as rollover panics following the Calvo (1988) and Cole and Kehoe (2000) models of sovereign debt crises.==== The existence of a bank run equilibrium depends on the health of banks balance sheets. When banks balance sheets are weak, fears of a bank run can become self-fulfilling even in the absence of any negative fundamental shock.==== Bank runs, in turn, force banks to liquidate assets at firesale prices, causing a sudden collapse in bank equity, and a deep and prolonged economic contraction.====We also contribute to the growing literature that studies the role of macroprudential regulation in preventing crises. Beginning with Lorenzoni (2008), a lengthy literature has emerged that examines bank regulation in a macroeconomic setting. The main conceptual motive for regulation in this literature is the presence of a pecuniary externality, where individual banks fail to take account of the impact of their risk exposure on the dynamics of asset prices. This work has been both qualitative, e.g. Angeloni and Faia (2013), Jeanne and Korinek (2019), Chari and Kehoe (2016), and quantitative, e.g., Bianchi and Mendoza (2018), Benigno et al. (2013), and Begenau and Landvoigt (2018)). We differ in two main ways. First, since we allow for endogenous nonlinear financial panics that lead to real economic disasters, the main gain from macroprudential policy in our model is reducing the likelihood of one of these disasters. In our view, avoiding such disasters is the primary objective of macroprudential policy in practice. More formally, the externality in our model is that banks fail to account for the impact of their individual risk exposure on the likelihood of a panic. In addition, by modeling credit booms as well as busts and making the distinction between good and bad credit booms, we are able to characterize the tradeoff between reducing the likelihood of banking crises versus stifling good credit booms.====Section 2 develops the baseline model of banking and banking panics. Section 3 introduces beliefs and then numerically illustrates how the model can generate credit booms and busts, including good booms as well as bad booms. Section 4 then analyzes macroprudential policy. The Appendix provides a detailed development of the model and the nonlinear computational algorithm for solving it.","Credit booms, financial crises, and macroprudential policy",https://www.sciencedirect.com/science/article/pii/S1094202520300491,20 June 2020,2020,Research Article,172.0
McGrattan Ellen R.,"University of Minnesota, and Federal Reserve Bank of Minneapolis, United States of America","Received 29 April 2020, Available online 20 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.007,Cited by (14),". I find that sector-specific shocks and ==== linkages play an important role in accounting for fluctuations and comovements in aggregate and industry-level U.S. data, and I find that at business-cycle frequencies, the model's common component of TFP is not correlated with the standard measures of aggregate TFP used in the ==== literature. Adding financial frictions and stochastic shocks to financing constraints has a negligible impact on the results.","This paper sheds light on a measurement issue that confounds analyses of key macrodata during economic booms and busts. Because firms invest heavily in R&D, software, brands, and other intangible assets, changes in GDP, which does not include all intangible investments, understate the actual changes in total output. As a result, it is possible to observe large changes in hours and investment coincidentally with little change in ==== total factor productivity. In other words, innovation by firms—which is fueled in large part by their intangible investments—may be evident “everywhere but in the productivity statistics.”==== Here, I use theory and recently revised U.S. national accounts to more accurately estimate U.S. total factor productivity (TFP) at both the aggregate and industry levels.====I develop a dynamic multi-sector general equilibrium model and explicitly incorporate intangible investment. Multiple sectors are needed to account for the vast heterogeneity in intangible investment rates across industries. Firms in the model economy have access to two production technologies: one for producing new tangible goods and services, and another for producing new intangible capital goods and services. Tangible capital is assumed to be a rivalrous input, but intangible capital is assumed to be a nonrivalrous input, since knowledge can be used simultaneously in producing consumer goods and services and in creating new ideas. I explicitly model industry linkages that occur through purchases of intermediate inputs and through purchases of new tangible or intangible investment goods.====Business-cycle fluctuations in the baseline model are assumed to be driven by shocks to industry and aggregate TFP, the impact of which depends on details of the industry input- and capital-use linkages. In an extension, I also allow for stochastic financing shocks, as in Jermann and Quadrini (2012), with firms facing a cost of adjusting dividends and using costly external finance to fund new projects. Both versions of the model can potentially rationalize the large labor wedges found by Chari et al. (2007) when applying their business cycle accounting approach to U.S. data with their no-intangible, no-financial-friction prototype model.====To parameterize income and cost shares, I start with the 2007 benchmark input-output table and take advantage of the fact that the Bureau of Economic Analysis (BEA) now includes expenditures on intellectual-property products—software; R&D; mineral exploration; and entertainment, literary, and artistic originals—as part of investment rather than as part of intermediate inputs. Additionally, I reassign several categories of intermediate inputs that are under consideration for future inclusion in the BEA fixed assets, including computer design services, architectural and engineering services, management consulting services, advertising, and marketing research. In the version of the model with financing frictions, I use industry-level data from Compustat to construct time series for ratios of tangible capital to output and debt to output, both of which are needed to derive estimates of the shocks to the enforcement constraints.====Because the model includes intangible capital stocks that cannot be accurately measured, it is not possible to use observations on factor inputs and outputs to directly measure the TFP series, as has been done in earlier work (see, for example, Horvath, 2000). Instead, I use maximum-likelihood methods to estimate stochastic processes for the latent TFPs, which are assumed to have both sector-specific components and a common component. This is done using quarterly data on gross outputs for major industries from the BEA and per capita hours for several intangible-intensive industries from the Bureau of Labor Statistics (BLS). Using observations not used in the estimation, I run external tests of the theory and derive model predictions for the latent TFP and intangible investment series.====A key test of the theory is its predictive performance for fluctuations in aggregate U.S. hours and sectoral comovements in hours for all major industries, data not used to estimate the model parameters. For the baseline model, I find that the model's predicted aggregate hours track U.S. hours much better than the simplest one-sector model without intangible investments. The model predicts three sizable booms over the 1985–2015 sample period and then a bust. Moreover, the standard deviation of the model's predicted-hours series is 65 percent of the actual series, as compared with 9 percent in the one-sector version without intangible investments. This improvement in the model's prediction is primarily due to fluctuations in intangible investments, which show up as a time-varying labor wedge for Chari et al. (2007).==== I also find significant comovement of sectoral hours because of the model's input-output linkages. Computing principal components for sectoral hours, I find that the variance that the first component accounts for is 56 percent in U.S. data and 69 percent in the model. For the extended model with financial frictions, I find that the implied labor wedges are smaller and less volatile than the wedge in Jermann and Quadrini's (2012) one-sector model, and as a result, financial shocks have only a small impact on real activity. A key difference here is the inclusion of intangible investments and the assumption that only tangible capital is externally financed.====After verifying that the baseline model effectively predicts U.S. hours, I put it to use to derive theoretically consistent summary statistics and time paths for latent TFP shocks and intangible investments.==== I first decompose the variances of U.S. data used in the maximum likelihood estimation (MLE) to determine the relative importance of idiosyncratic and common TFP shocks and to assess the role of input-output linkages. I do this decomposition in two ways: by computing the variance decomposition of the ergodic distribution, and by decomposing predicted growth rates in the technology boom of the 1990s and the Great Recession. I find that sector-specific shocks and industry linkages play an important role in accounting for fluctuations in the aggregate and industry-level gross outputs. Then I construct model time series for investments and TFP processes. I find that at business-cycle frequencies, the model's common component of TFP is not correlated with the standard measures of TFP used in the macroeconomic literature. In the case of investment, I find different time-series properties for intangibles and tangibles: intangible investments vary less over the business cycle than tangible ones and lag the cycle by several quarters.====Previous theoretical work related to this paper has either abstracted from intangible capital or been more limited in scope. Long and Plosser (1983) analyzed a relatively simple multi-sector model, arguing that firm- and industry-level shocks could generate realistic aggregate fluctuations. Horvath (1998, 2000) and Dupor (1999) extended Long and Plosser's (1983) model and studied the nature of industry linkages to determine if independent productivity shocks could in fact generate much variation in aggregate variables. Parameterizing the model to match the input-output and capital-use tables for the 1977 BEA benchmark, Horvath (2000) found that the multi-sector model that features only sectoral shocks is able to account for many patterns in U.S. data as well as a one-sector model driven by aggregate shocks. More recently, Foerster et al. (2011) did a full structural-factor analysis of the errors from the same multi-sector model and found that significant variation in quarterly data is explained by sectoral shocks. However, they used industrial-production data, which cover only about 20 percent of total production in the United States. Atalay (2017) extended the analysis to the entire economy and allowed for more general functional forms. None of these authors distinguished tangible and intangible investments. McGrattan and Prescott (2010) did distinguish the different investments but focused only on aggregate data for a specific episode—namely, the technology boom of the 1990s. Furthermore, they did their analysis well before the BEA completed the comprehensive revision introducing the category of intellectual-property products.====Previous empirical work has documented that intangible investments are large and vary with tangible investments over the business cycle. For example, Corrado et al. (2009) estimate that businesses' intangible investments are about as large as their tangible investments.==== McGrattan and Prescott (2014) use firm-level data and show that intangible investments are highly correlated with tangible investments such as plant and equipment.====This paper is also related to a burgeoning business-cycle literature in search of new sources of shocks and new sources of propagation mechanisms following the Great Recession of 2008–2009.==== During the downturn, GDP and hours fell significantly, but TFP fell only modestly and quickly recovered, rising in 2009 when real activity was still well below trend. These observations have led many to conclude that the Great Recession was inherently different from other downturns and certainly not consistent with the predictions of the real business cycle (RBC) theories developed in the early 1980s. In RBC theories, resources are efficiently allocated and fluctuations are driven by changes in TFP.==== My paper shows that a variant of those models—namely, one that takes into consideration the intangible investments of firms and allows for sectoral shocks to TFP—can go a long way in accounting for U.S. business cycles.====The model is described in Section 2. Estimation techniques and parameter estimates are described in Section 3. Section 4 summarizes the results. Section 5 concludes.",Intangible capital and measured productivity,https://www.sciencedirect.com/science/article/pii/S1094202520300466,20 June 2020,2020,Research Article,173.0
"Heathcote Jonathan,Perri Fabrizio,Violante Giovanni L.","Federal Reserve Bank of Minneapolis and CEPR, United States of America,Princeton University, CEPR, IFS, IZA and NBER, United States of America","Received 5 June 2020, Available online 19 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.002,Cited by (19)," persistent increases in inequality; that is, that the cycle drives the trend. The model features skill-biased technical change, which implies a trend decline in low-skill wages relative to the value of non-market activities. With this adverse trend in the background, recessions imply a potential ==== for low skilled men. This group is disproportionately likely to experience unemployment, which further reduces skills and potential earnings via a scarring effect. As unemployed low skilled men give up job search, recessions generate surges in non-participation. Because non-participation is highly persistent, earnings inequality remains elevated long after the recession ends.","Earnings inequality has dramatically widened over the past half century in the United States (====). This phenomenon has mostly been interpreted as reflecting slow-moving secular trends and, in particular, the effects of technical change that has generally favored relatively high skill workers (====; ====). The goal of this paper is to explore the hypothesis that cyclical fluctuations have had persistent effects on earnings inequality. We will argue that some of the observed increase in inequality over the past half century can be attributed to the recessions the US has experienced over this period.====The first part of the paper is purely empirical. We document changes in the cross-sectional distribution of earnings using Current Population Survey (CPS) data for prime-age men covering the period from 1968 to 2018. We establish two striking features of this data. First, while earnings dispersion has increased steadily at the top of the earnings distribution (as measured, for example, by the 90/50 percentile ratio), there is a strong cyclical component to the dynamics of earnings inequality at the bottom. In particular, earnings inequality in the left-tail of the distribution, as measured by the 50/20 percentile ratio, widens sharply during recessions and tends to decline gradually during subsequent recoveries. Thus, in a purely accounting sense, all the observed increase in left-tail earnings inequality since 1967 has occurred during recessions.====The second key finding from the CPS data arises when we decompose earnings into hourly wages and annual hours worked. Almost all men at the top of the earnings distribution work full time, and thus changes in relative wages entirely account for the dynamics of earnings inequality at the top. At the bottom of the earnings distribution, the pattern is quite different. Hours worked at the bottom tend to decline sharply in recessions and also have a clear downward trend over time. Thus, understanding the dynamics of hours worked at the bottom of the earnings distribution is crucial for understanding both the cyclical dynamics and the long run trends in earnings inequality at the bottom.====This empirical evidence suggests an intriguing possibility–namely, that some of the observed long run increase in inequality at the bottom of the earnings distribution reflects the cumulative impact of a series of recessions. Put differently, earnings inequality today might be lower had the US somehow avoided the recessions of the past half century. An alternative interpretation, of course, is that slow moving structural or technological trends drive the long run increase, and booms and recessions have delivered only temporary deviations around an immutable long run trend.====. As a result, some low skill workers who experience unemployment might decide to stop participating. In this way, a temporary surge in unemployment can potentially translate into a persistent increase in non-participation.====The second part of this paper develops a quantitative structural model that we can use to explore this potential interaction between cycle and trend. The model features over-lapping generations of men who differ in labor market skills. The only choice model individuals make is whether to participate in the labor market, a choice that balances the potential value of higher future earnings against the cost of lost leisure. Individual wages reflect individual skills, which tend to increase while working (====) but depreciate while unemployed (====). Conditional on participating, individuals face idiosyncratic unemployment risk, where the probability of being unemployed is lower for those currently employed and for high skilled workers relative to low skilled ones.====The model features two sources of aggregate dynamics. The background “trend” force is a steady increase over time in the relative importance in production of higher skilled workers. This simple model of skill-biased technical change implies a widening skill premium in wages and also a steady increase over time in the share of low wage men who have small or negative net present values from labor market participation. On top of this, we generate “cycles” in the economy by introducing time variation in job finding probabilities such that the economy generates realistic cyclical variation in the unemployment rate. We calibrate the model to replicate observed growth in top tail earnings inequality over time and over the life-cycle. We discipline the size of the scarring effect from non-participation by asking the model to replicate estimates of persistent earnings declines from mass layoffs from ====.====We first explore how well the calibrated model replicates the observed cyclical and long run dynamics for bottom tail earnings inequality – recall that the model is calibrated to replicate inequality at the top. The model generates realistic dynamics for the earnings share of the bottom 20%, for the 50/20 percentile ratio for earnings, and for the share of prime-age men with zero earnings in a given year. The fact that a combination of exogenous unemployment risk and endogenous participation choices can account for most of the cyclical dynamics in earnings inequality suggests a relatively minor role for cyclical variation in the process for hourly wages.====Next, we use the model to run some counterfactuals. In particular, we are interested in simulating two counterfactual histories for the United States, one in which we hold the unemployment rate constant over the past 52 years (no cycles) and a second in which we use the baseline cyclical variation in unemployment probabilities but shut down background skill-biased technical change.====The key finding from these simulations is that there is an important interaction between trend and cycle. In particular, the long run increase in the fraction of men with zero earnings is much larger in the baseline model – with both trend and cyclical drivers of inequality – than would be suggested by considering the two experiments in which one force or the other is switched off; the combined effect of the two drivers together is larger than the sum of the two parts taken separately.====Our interpretation for this finding is as follows. Absent cycles, skill-biased technical change disfavors low skilled men, making them susceptible to choosing non-participation. But as long as they remain mostly employed, declining skill prices are partly offset by the learning-by-doing effect, so that most men avoid negative earnings growth and choose to keep participating. However, if a recession hits, low skill unemployed men face a ====: they lose skills while unemployed, and skill prices continue to move against them because of the adverse trend. At some point, in the presence of (i) job search costs, (ii) low job finding probabilities, and (iii) low and continually declining skills, it becomes optimal to give up search and transition to non-participation. Once low skill men have been out of work for long enough, re-entering the labor force is not optimal, even once job finding probabilities recover. Key model ingredients for this model mechanism to generate sizable recession-induced declines in participation are, first, that unemployment is concentrated among low skill workers, and second, that unemployment spells in recessions are long enough to imply substantial skill loss.",The rise of US earnings inequality: Does the cycle drive the trend?,https://www.sciencedirect.com/science/article/pii/S1094202520300429,19 June 2020,2020,Research Article,174.0
"Wright Randall,Xiao Sylvia Xiaolin,Zhu Yu","Zhejiang University, China,University of Wisconsin-Madison, FRB Minneapolis & Atlanta, United States of America,Guanghua School of Management, Peking University, China,Bank of Canada, Canada","Received 11 April 2020, Available online 18 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.008,Cited by (7),"This project studies economies with markets for capital reallocation, where gains from trade are driven by firm-specific productivity shocks, but are hindered by search frictions and liquidity considerations. Results are provided on existence, uniqueness and efficiency. The model is tractable enough to analyze monetary and fiscal policy using simple graphs. Additionally, we calibrate it to investigate quantitatively the effects of changes in productivity and credit conditions. The framework can capture several facts deemed interesting in the literature – e.g., capital misallocation is countercyclical, while its price and reallocation are procyclical. We also discuss how well productivity dispersion measures inefficiencies or frictions.","This paper studies economies where capital is accumulated in primary markets and reallocated in frictional secondary markets driven by firm-specific productivity shocks. As motivation, note that efficient economic performance requires getting the right amount of investment over time, plus getting existing capital into the hands of those best able to use it at a point in time, and of course these are intimately related: the ease with which used capital can be retraded affects incentives for the accumulation of new capital, just like the attributes of secondary markets for houses, cars and other assets influence primary markets.====Also note that reallocation is sizable: purchases of used capital constitute 25% to 30% of total investment, even ignoring mergers, acquisitions and rentals, and only looking at big, publicly-traded firms (Eisfeldt and Rampini, 2006; Cao and Shi, 2016; Dong et al., 2016; Cui, 2017; Eisfeldt and Shi, 2018). Hence, it looks to be important at the macro level. Also, we only consider reallocation across firms, but one could also consider movement of capital within firms (Giroud and Mueller, 2015), across sectors (Ramey and Shapiro, 1998) or between countries (Caselli and Feyrer, 2007). Another reason to study capital reallocation is to see how outcomes depend on fiscal and monetary policy, and one version of our formulation is tractable enough to analyze these policies using simple graphs.====One reason to focus on ==== reallocation is that many people argue real-world capital markets are far from the perfectly competitive ideal.==== Imperfections include adverse selection, financial constraints, the difficulty of finding an appropriate counterparty, and holdup problems due to bargaining. We downplay adverse selection (on that, see Eisfeldt and Rampini, 2008 or Li and Whited, 2014) to concentrate on other issues: our secondary capital market features bilateral exchange and bargaining, as in search theory, and the use of assets in facilitating payments, as in monetary economics. In addition to the substantive issue of capital reallocation, our study contributes to the literatures on search and monetary theory.====As additional motivation, consider Ottonello (2015), who compares models of capital with and without search, and argues the former fit the facts better and generate more interesting propagation. Horner (2018) shows vacancy rates for commercial real estate resemble unemployment data, suggesting that search may be as important for capital as it is for labor, and argues that rents on these properties vary considerably, inconsistent with Walrasian theory. In a particular market that has been studied extensively, the one for aircraft, Pulvino (1998), Gilligan (2004) and Gavazza (2011a, 2011b) find that used sales are thrice new sales, that prices vary inversely with search time, and that market thickness affects trading frequency, average utilization, utilization dispersion, average price and price dispersion. This work emphasizes the importance of specificity, making it hard for firms to trade certain types of customized capital, consistent with a search-based approach.====Reallocation moves capital from lower- to higher-productivity firms (Andrade et al., 2001; Maksimovic and Phillips, 2001; Schoar, 2002). In the specification studied below, productivity differences come from idiosyncratic shocks.==== Conditional on investment, reallocation is efficient iff the nominal interest rate is ==== – the Friedman rule – but investment can be too high or low depending on the tax rate ==== and bargaining power ====. We prove that ==== implies optimal fiscal policy is ==== for ==== too low and ==== for ==== too high; and that ==== implies optimal monetary policy is ==== both for ==== too low and too high. This is interesting because monetary and fiscal policy are not symmetric, and because it is quite difficult to get ==== optimal in most monetary models.====We show that the model is able to match some observations deemed important in the literature: reallocation is procyclical but mismatch countercyclical (Eisfeldt and Rampini, 2006; Cao and Shi, 2016); the price of used capital is procyclical (Lanteri, 2016); and the ratio of spending on used capital to total investment is procyclical (Cui, 2017). However, to match these facts we need to have both increases in productivity and decreases in financial constraints during good times. Another quantitative finding is that the welfare cost of inflation is quite high due to the way it discourages capital reallocation. We also discuss quantitatively how well productivity dispersion measures misallocation, frictions, or welfare, related to some interesting empirical research.====Also related is much recent work on OTC (over-the-counter) asset markets, including in spirit work following Duffie et al. (2005), but there are major differences.==== More closely related is the extensive literature on real business cycle theory, where the textbook model is a special case of our model: by shutting down the idiosyncratic productivity shocks, we get exactly the model in Hansen (1985), right down to functional forms. A more recent business cycle model with productivity dispersion is Asker et al. (2014), but that setup has neither a secondary capital market nor liquidity considerations. So, one can say that we extend modern monetary theory to incorporate capital in more detail, or that we extend mainstream macro to include secondary markets with liquidity, search and bargaining frictions.====Finally, the paper contributes to research on money demand by firms. For older studies of firms' cash holdings, see Mulligan (1997), Bates et al. (2009) and references therein. More recently, and more closely related to this paper, Rocheteau et al. (2018a, 2018b) develop a model in the New Monetarist tradition, as surveyed by Lagos et al. (2017), except focusing on money demand by firms rather than the usual practice of focusing on households. He and Zhang (2019) have a related model where firms and households both use money; we consider a similar setup in Section 6.1. As in any good monetary model, a fundamental property of the theory presented below is that (some) economic activity decreases with the cost of liquidity, as measured by either inflation or nominal interest rates.====In particular, the model unambiguously predicts that reallocation decreases with inflation or interest rates. As shown in Fig. 1, this is consistent with the data.==== However, we do not make too much of that because it is mainly due to the trend – i.e., the effect is operative at the medium- to long-run frequency, not the standard business cycle frequency. This is not a problem per se, since we are happy to interpret the mechanism in the model as applying mainly in the medium- to long-run, as in Berentsen et al. (2011a, 2011b). Still, there are potentially many reasons why inflation has trended down and reallocation up over the sample, so we do not dwell on it here.====In what follows, Section 2 describes the environment. Sections 3 and 4 discuss equilibrium with perfect credit and with money. Sections 5 analyzes a tractable special case. Section 6 presents extensions where, among other things, we use price posting instead of bargaining. Section 7 presents the calibration and quantitative implications. Section 8 concludes.",Frictional capital reallocation with ex post heterogeneity,https://www.sciencedirect.com/science/article/pii/S109420252030051X,18 June 2020,2020,Research Article,175.0
"Huo Zhen,Ríos-Rull José-Víctor","Yale University, USA,University of Pennsylvania, USA,University College London, UK,CAERP, Spain,CEPR, UK,NBER, USA","Received 24 April 2020, Available online 18 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.011,Cited by (5),We build a variation of the ,"We build an equilibrium neoclassical real business cycle model where an increase in the desire to save and to work generates a recession. This is exactly the opposite of what happens in the standard neoclassical real business cycle model. We make three changes to the standard model, that we consider mild, and that we detail below.====We think that achieving this result within a neoclassical structure is important for three reasons. One is that our changes accommodate the notion that higher savings, i.e. lower consumption may be detrimental for the performance of the economy, which is the prevailing popular belief (among policy makers, the popular press and pundits of all persuasions). The changes in household behavior can be the result of (temporary but persistent) changes in preferences which is the form that we model them, but can also be the result of other exogenous changes that induce a negative wealth effect to which households respond by saving more and willing to work harder. A second reason is that our model delivers the same features that countries in Southern Europe experienced during the Great Recession (a sharp increase in unemployment, a large decrease in consumption, increases in the ratio of net exports to output and a mild reduction in total factor productivity (TFP)) as can be seen in Fig. 1. Finally, this result shows how demand induced fluctuations can be modeled with neoclassical notions of equilibrium without having to appeal to disequilibrium notions such as those present in New Keynesian models where prices do not adjust to clear markets and instead present arbitrary sluggishness.====To achieve a recession out of an increased willingness to save and to increase work effort is neither easy nor intuitive within the context of a neoclassical growth model. Consider the resource constraint, ====, where ==== denotes consumption, ==== investment, ==== output, ==== capital, ==== labor, ==== is an exogenous object that determines productivity, and ==== is a constant returns to scale neoclassical production. A reduction in consumption can be trivially achieved by an increase in investment without changing inputs of production. Moreover, an increased willingness to work (which is the case that we explore in this paper) together with having capital predetermined induces higher, not lower, output today and also in future periods as investment translates into higher future capital.====The changes that we make to revert this immediate implication of the standard model operate through three channels. There is a direct reduction of output given a certain amount of desired expenditure that works via a drop in TFP due to search frictions in the goods market. The second channel is the existence of adjustment costs in sectors that are not subject to the depressed demand. The third channel takes away from households the decision of how much to work and places it solely on the hands of firms while at the same time having a wage setting process that limits the reduction of wages. These channels reduce the incentives of firms to hire workers as a consequence of the reduction in consumption, which can in turn generate a persistent recession. To make these mechanisms transparent and to make it more directly applicable to the Southern European experience, we pose the environment as a small open economy avoiding all concerns about movements in interest rates.==== The saving/investment margin is now settled through trade deficits or surpluses and the implied changes in the net foreign asset position of the country.====We label the first channel as ====, implies that a reduction in consumption reduces profits of firms, something that operates via an ==== reduction of productivity. We follow here recent work in Huo and Ríos-Rull (2014) who develop these ideas in the context of an heterogeneous agents model where a shock with negative effects on wealth induces lower consumption and lower productivity. As a consequence, firms want to hire less workers to produce consumption goods. This mechanism relies on the existence of many varieties of goods and of household's consumption being a combination of the number of varieties of goods consumed and of the quantity consumed of each variety. For suitably chosen preferences, when households want to cut their consumption they do so by reducing the number of varieties that they consume and the quantity of each variety. Finding varieties requires search effort and the reduction of the number of varieties, and hence of search effort, yields idle capacity on the firms. That is, the occupancy rates for firms are tightly related to the number of consumers that use each variety. As a consequence, outputs and profits can fall even with the amount of inputs unchanged, which induces firms to cut employment.====The endogenous choice of varieties is related to, but quite different from, capacity utilization as used in the literature since at least Greenwood et al. (1988). In the endogenous choice of varieties the missing element in the determinant of productivity is household effort, while inputs of production are used at full capacity. In the capital utilization framework there is no notion of households engagement in the determinants of the quantity of output and it is extra effort of inputs of production what is responsible for the increased TFP, extra effort that has to be rewarded. This distinction becomes clear in the endowment economy presented in Section 2.====The predictions of our model for the relation of consumption and the number of varieties are consistent with the empirical findings of Jackson (1984) and especially Li (2020): consumers increase consumption by increasing both the number varieties and the quantity of each variety. In fact, Li (2020) shows that for the vast majorities of goods, both varieties and quantities are increasing in consumption expenditure, and that the Engel curve for varieties is upward sloping. The baseline model with a representative consumer displays no income dispersion, and the predictions of the model are only for time series. However, in an extended version of the model that accommodates financial frictions, employed households members consume more varieties than unemployed members. The time-series and cross-section predictions of the model are a clear positive correlation between the number of varieties and households' income.====The second channel in our model is that the inputs of production that are freed from usage in consumption goods cannot be easily used towards investment or net exports, making it difficult for the economy as a whole to save. This is achieved via the existence of differentiated sectors (in our case tradables and nontradables) and the existence of adjustment costs to increase production and of decreasing returns to scale in the tradables sector. This channel is also at work in Kehoe and Ruhl (2009), where they argue that without labor adjustment costs there is too much shifting of resources into the tradable sector. Alessandria et al. (2013) find that frictions in exports are necessary to match the gradual increase in exports that follows a devaluation. Extreme versions of the existence of adjustment costs can be found in the work of Mendoza (2001), Schmitt-Grohe and Uribe (2011), and Farhi and Werning (2017), where tradable goods are given exogenously.====The change to standard real business cycle models that operates on the labor market is to pose search frictions à la Mortensen–Pissarides and Nash bargaining for wage setting. This feature has a long tradition within the real business cycles literature since at least Merz (1995) and Andolfatto (1996).==== With these labor market frictions not all households can work, as it takes time to find a job. Perhaps more importantly, the wage is determined after the worker and the firm meet which makes wage reductions difficult even if workers have become more eager to get a job. There is such a wide body of literature on frictional labor markets in business cycle models that we do not attempt to give a summary here. We simply point out a particular kind of externality in our model economy when different sectors do not perfectly synchronize. Firms in the tradable sector do not face a lower demand and are willing to expand their labor force, but it makes hiring even more costly for firms in the larger and suffering nontradable sector due to a tighter market tightness, resulting in a bigger loss of aggregate output.====There is a large literature that engineers recessions induced by an increased incentive to saving by assuming nominal rigidities. Eggertsson (2011), Christiano et al. (2011), Eggertsson and Krugman (2012), Correia et al. (2013) do so by assuming a shock to the discount factor. Rendahl (2016) and Schmitt-Grohe and Uribe (2012) assume instead a decrease in confidence while Basu and Bundick (2017) use increased uncertainty. In all these papers insufficient demand triggers a recession because the economy is stacked at the zero lower bound on the nominal interest rate and there are either rigid prices or rigid wages. As explained in Basu and Bundick (2017), with price rigidity, output becomes demand determined.==== A large drop of the wage rate helps clear the labor market and the associated increase of the price markup is consistent with firm optimization. The marginal product of labor is no longer the same as the marginal rate of substitution, effectively breaking down the intratemporal Euler condition in the standard real business cycle models. When the nominal interest rate hits the zero lower bound, its stimulating role is no longer effective and the recession is more severe. Although our paper shares the same view with this literature that a recession is the result of insufficient demand, it does not hinge on the economy being stacked at the zero lower bound on the nominal interest rate nor on the existence of rigid prices or wages. Instead, we provide a neoclassical view of how increased savings generate a recession, which is based in physical frictions and not in frictions in the working of markets.====There is also a set of papers that explore how recessions can be induced by tightened financial frictions. This is a popular micro-founded rationale for the increased incentive to save by increased difficulties to borrow by some households in a heterogeneous-agent economy. Mian and Sufi (2010) and Mian and Sufi (2012) provide evidence of this link using county-level data to show that household demand is crucial in explaining aggregate economic performance and that it is also closely linked with households' financial conditions. Guerrieri and Lorenzoni (2017) consider a shock to households' borrowing capacity in an Aiyagari-type model. If combined with nominal rigidities, the financial shock can potentially push the economy into a liquidity trap and result in a decline in output (although typically this mechanism induces an increase in labor). Eggertsson and Krugman (2012) also study the effect of an exogenous reduction of the debt limit and highlight a Fisher deflation mechanism. Midrigan and Philippon (2011) focus on the home equity borrowing issue. With nominal rigidities, they show that a drop in the leverage ratio reduces the liquidity of households and, correspondingly, their demand. Bayer et al. (2019) build a heterogeneous-agent New Keynesian model and show that an increased uncertainty faced by individual households can depress demand and lead to a recession. Huo and Ríos-Rull (2014) is to our knowledge the only paper within the neoclassical context where a drop in demand generates a fall in output and employment. It includes some of the mechanisms described in this paper (except for wage rigidity instead of Nash bargaining) within a heterogeneous agents economy with assets in fixed supply (houses) so a tightening of financial markets induce a large negative wealth effect that reduces consumption, and with it the economy plunges into a recession.====In terms of goods market frictions, Alessandria (2009) uses endogenous variation in households' shopping time to explain the deviation from law of one price. Kaplan and Menzio (2016), Petrosky-Nadeau and Wasmer (2015), and Michaillat and Saez (2015) explore the interaction among goods market and labor market frictions in shaping business cycle fluctuations. A common feature in the aforementioned work is that firms' capacity or the probability of selling their products stay constant over the business cycle, which is a major difference compared with our work. The endogenous choice of varieties is close but different to the model of endogenous productivity in Bai et al. (2011). There, consumption is also subject to search frictions that require effort. However, search effort acts as a substitute of work effort implying that negative wealth effects increase productivity by inducing households to increase their search effort. Endogenous choice of varieties, on the other hand, poses the opposite implication in the presence of wealth effects for households, because less consumption requires less search effort productivity actually goes down when consumers feel poorer or want to delay gratification by cutting consumption.====The novelty of the endogenous choice of varieties induces us to dedicate a small section with a simple example that illustrates how it works in Section 2. We then move on to lay out the environment in a form suitable for quantitative analysis in Section 3. We map the model to data in Section 4. The main analysis of the baseline economy is in Section 5. We describe the quantitative importance of the mechanisms that we develop in this paper (which we deem to be large) in Section 5.1. We continue by exploring how the recession is either dimmed or it disappears when we remove each of the channels that we have changed relative to the standard model: goods market frictions in Section 5.2, adjustment costs in Section 5.3 and labor market frictions in Section 5.4. We also look at how the answers of the model change when we pose alternative calibration targets in Section 5.5. We also ask what is the response of the economy to other types of shocks: Section 6.1 describes what happens when the baseline economy becomes suddenly poorer (a wealth destruction shock if one could use such an expression). Section 6.2 extends the model to accommodate a notion of financial shocks as the trigger to households' increased desire to save without the need to abandon the representative agent abstraction by posing difficulties to smooth the consumption of the household members who are unemployed. Section 7 concludes.",Demand induced fluctuations,https://www.sciencedirect.com/science/article/pii/S1094202520300478,18 June 2020,2020,Research Article,176.0
"Kehoe Patrick J.,Lopez Pierlauro,Midrigan Virgiliu,Pastorino Elena","Stanford University, United States of America,Federal Reserve Bank of Minneapolis, United States of America,Federal Reserve Bank of Cleveland, United States of America,New York University, United States of America,Hoover Institution, Stanford University, United States of America,Stanford University, United States of America","Received 26 May 2020, Available online 17 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.006,Cited by (6),Although a credit tightening is commonly recognized as a key determinant of the ,"A large body of research on the Great Recession in the United States supports the view that a tightening of credit was a crucial factor behind the downturn. This research can be grouped into work that emphasizes the ====, which argues that a tightening of credit to households was most responsible for the observed decline in output and employment, and work that emphasizes instead the alternative ====, which argues that a tightening of credit to firms mostly accounted for the fall in output and employment.====In terms of evidence on the household-side channel, Mian and Sufi (2011, 2014) have documented that in the United States, regions that experienced the largest declines in household debt and housing prices also experienced the largest contractions in consumption and employment. These authors have also shown that when housing prices declined, households decreased their spending, in part because of their reduced ability to borrow against home equity. Specifically, Mian and Sufi (2014, 2015) estimated that the household-side channel accounts for approximately 65 percent of the job losses between 2007 and 2009.====As for evidence on the firm-side channel, various work has documented that a deterioration in firms' balance sheets, often emanating from a decline in lending from local banks, was an important component of the downturn. For example, Chodorow-Reich (2014), Giroud and Mueller (2017), and Huber (2018) have found that employment fell the most in firms that faced the worst credit conditions, which is consistent with the view that a tightening of credit to firms played a significant role in the transmission of credit shocks to the economy. Jermann and Quadrini (2012) have provided additional aggregate evidence that links the downturn in output during the Great Recession to firms' worsening ability to borrow in 2008 and 2009. For an early important paper on the role of firm financial frictions in amplifying the macroeconomic effects of aggregate shocks, see Cooley et al. (2004). See also Gertler and Gilchrist (2018, 2019) and Aikman et al. (2019) for surveys of the evidence on the firm-side channel and models that have addressed it.====To date, however, the role and relative importance of these channels are debated. In this paper, we propose a model in which the household-side and firm-side channels of credit frictions can be explicitly analyzed and clearly isolated. We also investigate the impact of transfers targeted to households and to firms on the recovery of aggregate employment after a recession induced by a credit contraction that affects households, firms, or both groups. In so doing, we help remedy the relative scarcity of explicit quantitative modeling of government transfers in business-cycle research on economies with heterogeneous agents, in which even lump-sum transfers can have a large impact on output, consumption, and employment.====Our first main finding is that for the same-sized credit shock to households and firms, the household-side credit channel is quantitatively much more important than the firm-side credit channel. Our second main finding concerns the effectiveness of some of the key policies that the U.S. government implemented to counteract the deterioration of credit conditions during the Great Recession. Based on the view that providing credit to firms would have been most helpful to boost investment, hiring, and production, several policies, such as many of those promoted as part of the Troubled Asset Relief Program, were designed to improve firms' financial conditions by providing them with credit. We find that, on the contrary, directing credit to easing households' debt constraints could have led to a faster recovery of employment. See also Kehoe et al. (2017) on this point.====Several recent empirical studies motivate us to consider targeted government transfers in downturns. Oh and Reis (2012), for example, have argued that although the vast majority of research on the fiscal response to the Great Recession has focused on government purchases, the actual response consisted mostly of targeted transfers. In particular, these authors document that from the end of 2007 to the end of 2009, only one-quarter of the increase in U.S. government expenditures was accounted for by government purchases, whereas the remaining three-quarters were accounted for by transfers. Similarly, Cogan and Taylor (2012) have provided a detailed analysis of the fiscal expenditure from the American Economic Recovery Act and showed that only a small percentage consisted of government purchases, as the vast majority took the form of transfers. Oh and Reis (2012) have shown that a similar pattern holds across OECD countries between 2007 and 2009. As for transfers specifically targeted to firms, often referred to as “bailouts,” see Bianchi (2016) for a discussion of the evidence on them, the relevant literature on their impact, and a quantitative evaluation of their effects in financial-type downturns like the Great Recession.====To examine the impact of targeted government transfers in recessions, we propose a model that builds on Kehoe et al. (2019), who have developed a version of the Diamond-Mortensen-Pissarides (====, henceforth) model of labor market search in the presence of risk-averse consumers and human capital accumulation. In their model, hiring workers is an investment activity that requires costs to be paid up front and entails benefits that accrue gradually. Thus, when credit tightens, this investment activity naturally falls and employment declines.====Although this force is present in any dynamic model, the employment drop caused by tighter credit is negligible in the textbook search model, which features no human capital accumulation. As Kehoe et al. (2019) show, when workers instead acquire human capital during employment, the duration of the flows of benefits from a match between a firm and a worker is much longer than in the absence of human capital acquisition. Intuitively, by acquiring skills that are at least partially transferable across matches, workers' benefits from matching with a firm are long lasting, as the new productive skills accumulated over the course of a match have a persistent effect on a worker's output beyond the course of the match. As a result, the value of a match is highly sensitive to credit conditions, and so the fall in employment associated with a tightening of credit is greatly amplified relative to that predicted by the textbook search model.====Formally, the magnitude of the employment decline in a recession implied by a typical search model is closely related to the sensitivity of the present value of the returns from a firm's investment in hiring to changes in a firm's discount rates induced by changes in a firm's debt constraints. In particular, the longer the duration of surplus flows is, the higher the sensitivity of the present value of firms' returns is to such discount rate changes, and so the larger the drop in employment is in response to any given increase in discount rates. Consequently, in a model with human capital acquisition, a tightening of credit leads to a much larger decline in employment than in the textbook search model.====A limitation of Kehoe et al. (2019), however, is that credit frictions are modeled as having symmetric effects on workers and firms. Hence, their framework is not amenable to exploring the separate channels through which credit market frictions affect an economy during a downturn, which we focus on here. We formalize these channels by considering an economy in which consumers belong to one of two types of families: families of workers who search for jobs, receive wages from supplying labor, and engage in home production, referred to as either ==== or ====; and families of entrepreneurs who hire workers and own firms, referred to as either ==== or ====. Since each type of family faces a separate borrowing constraint, this framework allows us to investigate the potentially different implications of a tightening of credit to workers and to entrepreneurs.====To examine the relative importance of household-side and firm-side credit frictions in accounting for the decline in consumption and employment after a credit tightening, we first consider a selective tightening of access to credit by workers and firms, and then a common tightening. Note that, because of general equilibrium effects, tighter credit for either group leads to lower consumption for both groups in equilibrium. Hence, both groups place greater value on current consumption relative to future consumption. Correspondingly, a tightening of either group's credit constraints induces an increase in the cost of diverting current resources from consumption to investment in hiring, relative to the future gains from increasing the number of matches between firms and workers. As a result, vacancies and employment fall.====The two types of tightening we consider, however, have different effects precisely because a tightening of household credit constraints mostly impacts ====' value of current goods, whereas a tightening of firm credit constraints mostly impacts ====' value of current goods. To see why this difference matters, note that for a firm, the value of a match with a worker accrues in the form of the profits earned over the course of the match. Therefore, once a match dissolves, a firm derives no more value from it. For a worker, on the contrary, the value of a match with a firm consists not only of the value of the wages received over its course but also of the incremental value of the wages received from all future matches, because of the increase in the worker's human capital occurring during the match.====Formally, we decompose the surplus from a match into two parts: the ==== that accrue over the course of the match and the ==== that arise as the human capital accumulated during the match increases a worker's output in all future periods. In terms of the current output gains, workers' and firms' discount rates symmetrically affect match surplus. In contrast, the future output gains are discounted solely by workers' discount rates. Since the future output gains are large and persistent and thus are sensitive to changes in discounting, match surplus is much more sensitive to fluctuations in workers' marginal valuations of future relative to present goods, and so workers' discount rates, than to firms' marginal valuations.====In a decentralized equilibrium, these distinct forces behind the impact of the two types of tightening manifest themselves in different responses of the present value of wages relative to the present value of output. After a worker-side tightening, the present value of wages falls significantly less than the present value of output, whereas after a firm-side tightening, the present value of wages falls nearly as much as the present value of output. In this sense, after a tightening of household credit constraints, the present value of wages is relatively stickier than after a tightening of firm credit constraints. This differential rigidity of wages helps account for how the fall in employment after a household-side credit tightening is larger than after a firm-side credit tightening leading to equal-sized drops in household and firm consumption.====We then turn to evaluate the relative benefits of easing credit frictions faced by households and firms. Our model implies that if the government intervenes by providing lump-sum transfers to households in response to a credit tightening affecting either households or firms, it stimulates employment substantially more than by providing the same-sized transfers to firms. The key intuition behind this result is similar to that for our first main finding. A transfer to workers has a large direct effect on workers' valuations of the associated benefits, which, as argued, determine the key discount rate of the surplus from a match between a firm and a worker. On the other hand, a transfer to firms has only a small and indirect general equilibrium effect on such valuations. Since workers' valuations of the benefits of a transfer have a larger impact on the surplus from a match than do firms' valuations, a transfer to firms has a much smaller effect on aggregate employment than a transfer to workers.====For clarity, we emphasize that throughout we consider intertemporal transfers from the future to the present financed by government debt and eventually repaid by higher future taxes, rather than by within-period transfers to one group financed by contemporaneous higher taxes on another group. We interpret such intertemporal transfers, which are considered part of current government expenditures but not government purchases, as capturing the essential component of government expenditures that the empirical literature discussed earlier has measured.====We remark also that our results are not due to different marginal propensities to spend by households and firms. Indeed, in our model, both sets of families have a marginal propensity to spend of one.==== Rather, our results depend on the relative importance of the two groups' discount factors in determining the value of the surplus from a match between a firm and a worker.====Next, we extend the model by considering an economy composed of many states, each modeled as a small open economy with tradable and nontradable goods. In this economy, meant to represent the United States, a state-specific credit tightening has two effects. The first is an ====, in that to a firm in a given state, the cost of posting a job vacancy to hire a worker increases in a downturn by more than the corresponding benefits, which leads to a reduction in the number of vacancies posted and hence to a drop in employment in that state. The second is a ==== and is due to the reduction in the demand for the nontradable goods produced in the state. Such a drop in turn gives rise to a decline in the demand for workers by the nontradable goods sector, which induces workers to reallocate to the tradable goods sector. When the cost of worker reallocation is small, a substantial reallocation occurs between the two sectors. As a result, states that experience larger credit tightenings also experience larger declines in nontradable employment but not necessarily in tradable employment, as is consistent with the data.====We then assess the relative desirability of transfers to workers and to firms in response to a credit tightening that impacts both workers' and firms' credit constraints. Specifically, we investigate our model's predictions for U.S. states during the Great Recession for two policies: one that implements transfers only to workers and one that implements equally sized transfers only to firms. As before, transfers are modeled as partially relaxing the tightened credit constraints that limit the ability of either group to borrow.====Our second main finding is that after a credit tightening, a transfer to workers has substantially larger positive effects on employment than does a transfer to firms. For example, for Nevada, a state that experienced an especially large decline in consumption and employment during the Great Recession, a lump-sum credit subsidy to firms that leads to a fall in employment between 2007 and 2009 that is 20 percent smaller than the observed one would lead instead to a 50 percent smaller fall if that same amount were transferred to workers. In sum, our model provides quantitative support for the view that the Great Recession would have been less severe if instead of focusing on easing credit to firms, the government had rather focused on easing credit to households.====We end with an important cautionary note. Throughout the paper, we studiously avoid formulating any statements about the welfare consequences of the transfers we consider. The reason is that we have endowed the government with policy instruments—namely, targeted lump-sum transfers—that can partially or totally counter the tightening of credit constraints, which for simplicity we have assumed as exogenous and invariant to such interventions. To be able to evaluate the welfare implications of these interventions, we would need to formalize from deeper first principles a government's ability to easily offset constraints on the private sector that arise presumably from underlying information frictions or contracting imperfections. Therefore, we interpret our exercise as simply highlighting the importance of accounting for alternative transmission channels of financial shocks when evaluating government interventions during recessions.",On the importance of household versus firm credit frictions in the Great Recession,https://www.sciencedirect.com/science/article/pii/S1094202520300442,17 June 2020,2020,Research Article,177.0
"Dong Feng,Miao Jianjun,Wang Pengfei","School of Economics and Management, Tsinghua University, Beijing, China,Department of Economics, Boston University, 270 Bay State Road, Boston, MA 02215, United States of America,Department of Economics of Hong Kong University of Science and Technology, China,Peking University HSBC Business School, China","Received 13 April 2020, Available online 11 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.003,Cited by (23)," reduces bubble volatility, but could raise ==== volatility. Whether monetary policy should respond to asset bubbles depends on the particular ==== rule and exogenous shocks.","The booms and busts of asset prices in stock markets and real estate markets around the world have typically been associated with business cycles in the macroeconomy (Borio et al. (1994) and Jordà et al. (2015)). The general public, policy makers, and academic researchers often attribute the large movements of asset prices to the emergence and collapse of bubbles because asset price volatility cannot be explained entirely by fundamentals (Shiller (1981)). How do asset bubbles affect the real economy? How does monetary policy affect asset bubbles? Should monetary policy respond to asset bubbles? The goal of our paper is to provide a theoretical framework to address these questions.====These questions have been the subject of a heated debate in the literature. Two views are dominant (Galí (2014)).==== The first view is that central banks should view price stability and financial stability as highly complementary and mutually consistent objectives. Even if asset prices can amplify and propagate shocks, including asset prices in monetary policy rules may result in only slight gains (Bernanke and Gertler (1999, 2001)). Moreover, asset bubbles driven by non-fundamental shocks are highly unpredictable. Thus monetary policy should not respond to asset prices. The second view is that central banks should act preemptively to prevent bubbles from forming, by raising interest rates or decreasing money supply to appropriate levels.==== Such a policy, often referred to as “leaning against the wind,” may call for a change in the inflation target.====One reason for the debate is that the literature has yet to agree on a theoretical framework for understanding the formation of asset bubbles and the mechanism of how asset bubbles interact with the macroeconomy and monetary policy. The debate stems from the model of Bernanke and Gertler (1999) who introduce an ==== bubble to the model of Bernanke et al. (1999) (henceforth BGG). This model cannot address the question of how and why a bubble can emerge and burst under rational expectations. We contribute to the literature by providing an infinite-horizon model of ==== asset bubbles in a dynamic new Keynesian (DNK) framework.====The key feature of our model is that entrepreneurs (or firms) are heterogenous in their investment efficiencies and face credit constraints. In a frictionless Arrow-Debreu economy, rational bubbles cannot emerge and movements of asset prices reflect changes in the underlying economic fundamentals. In this case central banks would not have to worry about asset prices. By contrast, due to credit constraints, an intrinsically useless bubble asset can provide liquidity and command a liquidity premium.==== If all agents believe the bubble asset is valuable, this belief can be self-fulfilling and the bubble asset can raise its owners' net worth. Efficient entrepreneurs sell the bubble asset to inefficient ones to finance investment, who want to buy the bubble asset for precautionary reasons because they may become efficient in the future. Thus the bubble asset can be traded at a positive price and has an intensive margin effect in that it raises an entrepreneur's net worth and hence his investment. It also has an extensive margin effect in that inefficient entrepreneurs must hold the bubble and will not make investment. The net effect on aggregate investment is typically positive.====To introduce money and monetary policy, we incorporate a banking system with legal restrictions in the sense that banks must meet reserve requirements. Reserve requirements generate a spread between the lending rate and the deposit rate. Households and entrepreneurs can save by making deposits in banks and banks can lend to investing entrepreneurs. The central bank changes the money supply by changing reserves (high-powered money or monetary base). Monetary policy is conducted by following an interest rate rule. In this case money supply is endogenous. To allow monetary policy to have a large impact on the real economy, we introduce monopolistic competition and sticky prices as in Calvo (1983).====Our main results can be summarized as follows. First, monetary policy can affect the conditions for the existence of a bubble. High inflation erodes entrepreneurs' real balance and hence their net worth, generating a large liquidity premium for holding a bubble asset. When the liquidity premium is sufficiently high, a bubble can emerge. Thus raising the inflation target by raising the money supply permanently can fuel a bubble. The higher the inflation target, the more likely a bubble will emerge. On the other hand, if the economy already has a bubble, cutting the inflation target by decreasing the money supply permanently can prick the bubble.====Second, monetary policy can affect the steady-state size and dynamics of the asset bubble including its initial size. In particular, a higher inflation target is associated with a higher steady-state size of the bubble. An expansionary monetary policy by cutting the interest rate or raising the money supply can raise the initial size of the asset bubble, which in turn generates a large amplification effect of monetary policy. Moreover, the coefficients in the interest rate rule affect the dynamics of the asset bubble in response to exogenous shocks. A higher interest rate response to asset bubbles reduces bubble volatility, but may raise inflation volatility.====Third, whether monetary policy should respond to asset bubbles depends on the particular interest rate rule adopted by the central bank and on the type of exogenous shocks hitting the economy. Following Bernanke and Gertler (1999, 2001) and Gilchrist and Leahy (2002), we consider two types of interest rate rules: (i) a Taylor rule, which responds to inflation and the gap between the actual output and steady-state output, and (ii) an inflation targeting rule, which responds to expected inflation, but not the output gap. We consider two types of shocks: (i) a fundamental TFP shock, and (ii) a non-fundamental sentiment shock to the bubble. We include a weight on the asset bubble in the interest rate rules and search for an optimal weight to maximize household utility. Our calibrated model shows that the central bank should cut interest rates in response to either a positive TFP shock or a positive sentiment shock. The welfare gains are relatively larger conditional on sentiment shocks, and are generally small for all cases.====In our model the benefit of an asset bubble is to improve investment efficiency and its cost is to increase business cycle volatility. In response to a positive TFP shock, an efficient real business cycles model implies positive comovements of consumption, investment, output, and labor. But labor drops under the Taylor rule in a DNK model (Galí (1999)). Cutting interest rates when the asset bubble is expanding can raise aggregate demand and hence labor. In response to a positive sentiment shock, the asset bubble expands, but consumption falls on impact under the standard Taylor rule because the nominal and real interest rates rise too much. Cutting interest rates when the asset bubble is expanding can also boost consumption and benefit households.====Under the inflation targeting rule, the optimal coefficients on the asset bubble are generally very small negative numbers. The welfare gains are also quite small as in Bernanke and Gertler (1999) and Gilchrist and Leahy (2002). As Bernanke and Gertler (1999) argue, to the extent that asset bubbles tend to be positively correlated with movements in output and inflation, policies based on these two variables subsume most of the gains from reacting to asset bubbles. In fact, in response to a TFP shock or a sentiment shock, the simple inflation targeting rule can generate the right comovements of macroeconomic quantities, giving very small welfare gains from reacting to asset bubbles.",Asset bubbles and monetary policy,https://www.sciencedirect.com/science/article/pii/S1094202520300508,11 June 2020,2020,Research Article,178.0
"Krusell Per,Mukoyama Toshihiko,Rogerson Richard,Şahin Ayşegül","IIES, Sweden,CEPR, United Kingdom,NBER, United States of America,Georgetown University, United States of America,Princeton University, United States of America,University of Texas at Austin, United States of America","Received 23 April 2020, Available online 11 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.010,Cited by (5), arise as a consequence of labor supply responses to price changes induced by TFP shocks.,"The ==== volume (Cooley, 1995) synthesized a body of frontier research that ushered in a new era in business cycle research. The research agenda that it laid out had several defining characteristics. One was the insistence on general equilibrium. A second was the insistence on micro-foundations, i.e., an explicit delineation of which parameters were primitives of preferences and technology and hence invariant to policy. The third was the insistence on studying business cycle fluctuations as deviations from trend growth. All three of these criteria remain strongly reflected in business cycle research today.====But the Cooley volume was not just about the methodology of studying business cycles; it also employed this methodology to address substantive issues. Given that labor market fluctuations have been a long-standing focus within the business cycle literature, it is not surprising that several of the chapters in the Cooley volume focused on aspects of labor market fluctuations. The goal was simple: heeding the three “axioms” described above, to deliver a version of the growth model that could account for the magnitude of employment fluctuations.====Our goal in this paper is to offer an updated perspective on modeling labor market fluctuations. The need for an update reflects two important developments since the writing of the Cooley volume, one methodological and one substantive.====A key methodological development in macroeconomic research more broadly over the last twenty-five years concerns the threshold for what it means for a model to have adequate micro foundations. Recent research in almost all areas of macroeconomic stresses the desire to build models that can not only address aggregate time series, but also the rich cross-sectional and panel variation found in large micro data sets.==== This trend is witnessed in studies that focus on aggregate consumption, aggregate investment, and price-setting.====The second and substantively important change since the publication of the Cooley volume has been the development of general equilibrium models featuring labor market frictions. These allow us to rigorously connect theory with data on unemployment.====At the time of the Cooley volume, the leading account of labor market fluctuations was Hansen (1985). Notably, Hansen's model only distinguished between the employed and the non-employed; it made no attempt to distinguish the two different categories of non-employment–unemployment and out of the labor force. And although his model featured individuals moving between employment and non-employment, it made no attempt to connect these flows to the flows in micro-data. Our goal in this paper is to understand aggregate movements in employment, unemployment, and non-participation, while at the same time accounting for the underlying movements of individuals between these three labor market states.====In the first part of this paper, we document the key features of the relevant data. In particular, we use the Current Population Survey (CPS) data to document both the average values for gross worker flows among labor market states as well the cyclical fluctuations in these gross flows.====The second part of the paper presents a version of the growth model with heterogeneous agents that in steady state matches both the aggregate distribution of workers across the three labor market states and the average flows of workers between these states. This model can be understood as bringing labor market frictions as modeled in the island economy of Lucas and Prescott (1974) into the heterogeneous agent-incomplete markets model of Chang and Kim (2006). The Lucas-Prescott formulation amounts to treating the frictions as exogenous; in the conclusion of our paper we emphasize that it would be valuable to study how the frictions are determined.====Having developed a model that can account for the behavior of gross worker flows in steady state, we then subject this economy to shocks to assess its implications for movements in both the levels of employment, unemployment, and non-participation and the movements in gross worker flows among these three states. Our first exercise is a traditional Real Business Cycle (RBC) exercise in which we consider aggregate shocks to Total Factor Productivity (TFP) as the sole aggregate shock. A striking result emerges: Even if TFP shocks are sufficiently large to generate employment fluctuations like those found in the data, they have strongly counterfactual predictions not only for the movements in gross worker flows but also for movements in the aggregate stocks of unemployment and participation.====Our second exercise considers shocks to both aggregate TFP and the model parameters that characterize frictions in the labor market, which consist of three job-finding rates and a job-separation rate. Our main finding is that empirically reasonable values for shocks to these frictional parameters in addition to shocks to aggregate TFP provide a good account not only for the cyclical movements in employment, unemployment, and non-participation but also for the cyclical movements in gross worker flows.====A decomposition exercise shows that shocks to the frictional parameters play a dominant role. In the traditional RBC literature, employment fluctuations were generated as labor supply responses to TFP-induced changes in prices. In our model, this mechanism is present but plays a secondary role. This has important implications for our understanding of labor market fluctuations. If TFP shocks are the dominant primitive source of labor market fluctuations, it is essential to include a channel through which they impact labor market frictions. More generally, our analysis shows that understanding the movements in frictions is central to understanding labor market fluctuations.====Our analysis in this paper draws heavily on the material in Krusell et al. (2017). But importantly, whereas the analysis in Krusell et al. (2017) was partial equilibrium, taking all prices as constant over time, our analysis here is general equilibrium in the sense that we have prices responding endogenously to the aggregate shocks.==== These general-equilibrium interactions are quantitatively significant and have an important effect on our conclusions. Second, Krusell et al. (2017) did not study TFP shocks, which are an important part of the focus here. Third, to simplify exposition, the model studied here is somewhat simpler than the one in Krusell et al. (2017): it abstracts from the presence of the unemployment insurance (UI). While the presence of a UI system does have a small impact on some quantitative aspects of the model, modeling UI in an empirically reasonable way increases the size of the model's state space and our results here suggest that the quantitative effects are not of first order.====We also want to highlight the method that we use to solve for the business cycle properties of our model. Whereas previous papers in the heterogeneous agent business cycle literature have largely relied on the method of Krusell and Smith (1998), we compute business cycle properties by adopting the method developed by Boppart et al. (2018). This method delivers equilibria for models with aggregate shocks based on perfect-foresight equilibria given unexpected shocks away from steady state: so-called MIT shocks. The latter are straightforward to compute even in models that are highly nonlinear on the microeconomic level, hence allowing heterogeneous agent general equilibrium models with aggregate fluctuations to be coded up and solved rather handily. The benefit is particularly large for a relatively complex economy like ours.==== An important contribution of the original Cooley volume was to provide researchers with a tool-kit, and we think that (the application of) this new computational method is a valuable innovation in this regard.====Our analysis builds on four strands of literature. The first is a large literature on gross worker flows.==== A second is the literature on individual labor supply in the presence of frictions. Ham (1982) rigorously considered unemployment in the context of optimal labor supply, and argued that unemployment spells should not be interpreted as a component of optimal labor supply responses. Our model is consistent with this finding; it features both an operative labor supply margin and unemployment, and unemployment in our model reflects a departure from desired labor supply. Low et al. (2010) is also concerned with labor supply in the presence of frictions. But whereas we focus on business cycle fluctuations, their analysis focused on life cycle variation.====A third strand is a recent literature that extends business cycle models of employment and unemployment to consider movements in participation.==== The key distinguishing feature of our analysis relative to these is that we study gross worker flows and not just labor market stocks.====The fourth strand of literature is heterogeneous agent models of aggregate labor supply, as in Krusell and Smith (1998, Appendix B) and Chang and Kim (2006). Our model contributes to this literature by introducing a labor market with realistic frictions.====An outline of the paper follows. In the next section we document the key business cycle facts for gross worker flows among the three labor market states for the US over the period 1978–2012. Section 3 describes our theoretical framework and Section 4 calibrates the model so that in steady state it matches both the distribution of workers across labor market states as well as the average flows of workers between states. Section 5 carries out our main business cycle exercises and shows that our model can account for the key facts laid out in Section 2. Section 6 presents the decomposition results to assess the relative importance of the three different shocks in our model. Section 7 documents the importance of general equilibrium effects. Section 8 concludes.",Gross worker flows and fluctuations in the aggregate labor market,https://www.sciencedirect.com/science/article/pii/S109420252030048X,11 June 2020,2020,Research Article,179.0
"Bianchi Javier,Mendoza Enrique G.","Federal Reserve Bank of Minneapolis, United States of America,University of Pennsylvania, NBER & PIER, United States of America","Received 29 May 2020, Available online 11 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.001,Cited by (19),"Sudden Stops are financial crises defined by a large, sudden current-account reversal. They occur in both advanced and emerging economies and result in deep recessions, collapsing asset prices, and real exchange-rate depreciations. They are preceded by economic expansions, current-account deficits, credit booms, and appreciated asset prices and real exchange rates. Fisherian models (i.e. models with ==== linked to market prices) explain these stylized facts as an outcome of Irving Fisher's debt-deflation mechanism. On the normative side, these models feature a pecuniary externality that provides a foundation for macroprudential policy (MPP). We review the stylized facts of Sudden Stops, the evidence on MPP use and effectiveness, and the findings of the literature on Fisherian models. Quantitatively, Fisherian amplification is strong and optimal MPP reduces sharply the size and frequency of crises, but it is also complex and potentially time-inconsistent, and simple MPP rules are less effective. We also provide a new MPP analysis incorporating investment. Using a constant debt-tax policy, we construct a crisis probability-output frontier showing that there is a tradeoff between financial stability and long-run output (i.e., reducing the probability of crises reduces long-run output).","The Mexican crisis of 1994 was a harbinger of a series of financial crises that have since affected both emerging and advanced economies. The defining feature of these crises is a large, sudden reversal in the current account, which is a country's broadest measure of net foreign financing. Because of the sudden loss of access to international capital markets, these events are known as ====.==== As we document in the next section, by the end of 2016 there had been 58 Sudden Stop events worldwide, 35 in emerging markets and 23 in advanced economies.====Sudden Stops have been the focus of a large theoretical and empirical literature since the mid-1990s. More recently, following the 2008 Global Financial Crisis (GFC), a growing literature has been studying macroprudential regulation as a way to avert financial crises. In this article, we start by reviewing the stylized facts of Sudden Stops, with a new event analysis spanning the 1979-2016 period and including advanced and emerging economies. Then, we provide a short survey of the findings of the empirical literature on the use and effectiveness of macroprudential policies to date. The paper then moves on to review the literature that examines the quantitative implications, both positive and normative, of a class of Sudden Stops models labeled ====. In these models, Sudden Stops are the result of financial amplification driven by the debt-deflation mechanism proposed in the seminal work on the Great Depression by Fisher (1933) and also emphasized in the classic studies by Minsky (1992) and Kiyotaki and Moore (1997).====The central element of a Fisherian model is an occasionally binding credit constraint that limits borrowing capacity to a fraction of the market value of the goods or assets pledged as collateral. This constraint is essential to the Fisherian debt-deflation mechanism: when the constraint binds, agents fire-sale the goods or assets that serve as collateral, and as they do so, they drive down the value of those goods or assets, which tightens the constraint further and forces further fire sales. From a normative perspective, the constraint entails a pecuniary externality that creates scope for macroprudential policy (MPP): individual borrowers do not internalize how their borrowing decisions made in “good times” affect the size of the deflation in collateral values and the reduction in the economy's borrowing capacity during a Sudden Stop (Bianchi, 2011; Bianchi and Mendoza, 2018). As a result, private agents undervalue the social marginal cost of borrowing, and hence they “overborrow.” Optimal MPP thus calls for tightening access to credit in a procyclical fashion, a feature consistent with newly introduced regulation such as the Countercyclical Capital Buffer (CCyB).====In addition to reviewing the findings of the existing literature, we conduct a new analysis examining MPP tradeoffs involving capital accumulation. To date, most studies quantifying MPP tradeoffs have focused only on consumption-smoothing tradeoffs. Filling this gap is important because policy discussions on MPP emphasize the potentially costly tradeoff between capital accumulation, long-run output, and financial stability.====We propose a new version of a widely used Fisherian two-sector model with tradable and nontradable goods, extended to introduce production of investment goods using both tradables and non-tradables as inputs. In the model, households consume tradables and non-tradables, make investment decisions, and face a Fisherian constraint that limits their debt to a fraction of the market value of the capital stock. Both the real exchange rate and the market price of capital (i.e., the value of collateral) are determined by the relative price of non-tradables to tradables and are central for the financial amplification mechanism.====We show that by raising the cost of borrowing above the risk-free rate, MPP also raises the effective opportunity cost of capital and reduces investment. Introducing a simple MPP rule in the form of a constant debt tax, allows regulators to reduce leverage and hence the magnitude and frequency of crises, but it also reduces output and investment in a manner akin to a capital income tax. In some states, reducing both consumption and investment is efficient, but in others it is not. Using simple MPP rules therefore induces costly investment and output tradeoffs of over- or under-regulating credit.====We calibrate the model to data and examine the model's quantitative implications. First, we study long-run cyclical properties and Sudden Stop dynamics in the absence of regulation. Then, we illustrate the costly tradeoffs of simple rules governing debt taxes. This includes a long-run crisis probability-output loss frontier that shows pairs of losses in long-run output and reduction in crisis probability attained with different constant debt tax rates. In addition, we examine the incentives of optimal policy to use debt taxes or adjust debt decisions in the short run.====We also use the model to analyze regulatory loan-to-value (LTV) ratios as an alternative MPP tool. We show that their incidence on incentives to borrow and invest is similar to that of debt taxes but they are not equivalent instruments. In particular, there is no LTV regulation policy that can replicate the equilibrium of a given debt tax policy. At the same levels of credit and consumption obtained with a given debt tax at some date ====, regulatory LTVs support higher collateral values, which also imply different prices and allocations in previous periods. This result is important because to date there is little research analyzing the macro implications of LTV regulation as an MPP tool, and yet, as we document in Section 2, empirical studies show that borrower-targeted instruments (i.e., LTVs) are an effective MPP tool, while evidence is at best mixed for lender-targeted instruments akin to debt taxes.====The rest of the paper is organized as follows. Section 2 conducts an empirical analysis of Sudden Stops using a cross-country dataset and reviews the evidence on MPP use and effectiveness. Section 3 reviews the main elements and findings of the quantitative literature on Fisherian models of Sudden Stops, both positive and normative. Section 4 proposes the new model we use to analyze MPP tradeoffs related to capital accumulation and to compare LTV regulation with debt taxes. Section 5 examines the quantitative implications of the model. Section 6 concludes.",A Fisherian approach to financial crises: Lessons from the Sudden Stops literature,https://www.sciencedirect.com/science/article/pii/S1094202520300430,11 June 2020,2020,Research Article,180.0
Zhang Jingyi,"Shanghai University of Finance and Economics, China","Received 26 April 2018, Revised 23 April 2020, Available online 21 May 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.05.004,Cited by (7),"This paper studies optimal time-varying capital requirements in a general equilibrium model with two types of financial intermediaries: regulated commercial banks (CBs) and unregulated shadow banks (SBs). Subject to capital regulation, each CB faces higher cost of funds and requires higher lending interest rate when it lends more to the borrower. The borrower internalizes this effect and chooses a lower leverage ratio when financing with a CB than when financing with an unregulated SB. Tightening the capital requirement reduces the credit supply within the CB sector but may stimulate total borrowings through an extensive-margin effect, as more borrowers choose high-leverage SB finance over low-leverage CB finance and the aggregate leverage ratio of all borrowers increases. If the extensive-margin effect outweighs the intensive-margin effect, tightening capital requirement will be expansionary and optimal capital requirements will become countercyclical.","The 2007-09 global financial crisis brought attention to the adoption of time-varying macroprudential policies for macro stabilization and to mitigate externalities that arise due to the presence of frictions in the overall financial market and lead to systematic risk (Bianchi, 2011; Mendoza and Bianchi, 2011). Optimal time-varying capital requirements have been one of the most studied area in this literature as capital requirement is widely used as a time-varying macroprudential tool in many countries, following the introduction of Basel III regime. Most of this literature has proposed that procyclical capital requirement, which rises in times of credit expansion and falls in times of credit depression, could generate significant gains in terms of macroeconomic stabilization and welfare improvement by curbing the credit cycle (e.g. Angelini et al., 2014; Benes and Kumhof, 2015; Gersbach and Rochet, 2017).====However, many of the borrowing and lending activities that generate negative externalities and justify the use of capital requirements occur outside of the formal banking sector and therefore are free from capital regulation (Jeanne and Korinek, 2014). Indeed, a new line of argument emerged, suggesting that the existence of the shadow banking==== sector could dampen the effectiveness of capital regulation since credit activities are capable of migrating from the regulated banking sector to the unregulated shadow banking sector (e.g. Huang, 2015; Plantin, 2015). A natural question is that, are optimal capital requirements still procyclical, taking into account the role that shadow banking plays?====This paper presents a general equilibrium model with both regulated commercial banks and unregulated shadow banks to address this question based on the quantitative predictions of the model. The model builds on the framework of Bernanke et al. (1999) with costly state verification as the main source of financial friction. Firms produce intermediate goods and face idiosyncratic productivity shocks. They need to finance working capital with both internal net worth and external debt. As in BGG, there is a threshold level of idiosyncratic productivity, above which firms repay the loan at the contractual rate, and earn nonnegative profits. Firms with productivity below the threshold level, however, will default, resulting in costly state verification and liquidation.====The model departs from the standard BGG framework in that firms can choose between two different types of banks: commercial banks (CBs) and shadow banks (SBs). These two types of banks are different in two dimensions.====First, CBs are subject to capital regulation while SBs are not. In particular, each CB needs to pay a quadratic cost if its capital-to-asset ratio deviates from the regulatory target. Therefore, each CB pays higher marginal regulatory cost and requires higher lending interest rate when it lends more to the borrower. The borrower internalizes this effect and chooses a lower leverage ratio when financing with a CB than when financing with an SB.====Second, CBs are able to provide their depositors with safe returns and liquidity services. By comparison, SBs cannot provide liquid and safe deposits as CBs do. As a result, CBs obtain deposits at a lower interest rate than SBs do. Lower CB deposit rate reduces CBs' funding cost, which is beneficial to the borrowers with CB finance.====In the general equilibrium, firms are indifferent between CB finance and SB finance. Firms with CB finance enjoy lower lending interest rate than firms with SB finance. But the tradeoff is that firms with CB finance take lower leverage than firms with SB finance. With lower leverage and lower lending rate, firms with CB finance are less likely to default than firms with SB finance.====In my model, financial frictions between banks and firms generate a “pecuniary externality” that leads to excessive credit fluctuations. Firms take their expected return to investment as given but do not internalize the equilibrium effects of their collective actions on their expected returns, which in turn affect their borrowing constraints. As a consequence, firms tend to overborrow during up-swings but underborrow in downturns, which inefficiently amplifies the fluctuations in the aggregate credit. This pecuniary externality provides a rational for time-varying capital requirements. In an economy without shadow banks, procyclical capital requirement, which discourages banks from credit expansion during upswings and encourages them to take leverage in downturns, helps reduce the excess amplification of the business cycle caused by the pecuniary externality and produces an increase in social welfare.====However, in the presence of unregulated SBs, procyclical capital requirement may not be able to help neutralize the pecuniary externality because the net effect of tightening capital requirement on total loans becomes ambiguous. At the intensive margin, tightening capital requirement raises the internal cost of funds in regulated CBs and therefore contracts the credit supply within the CB sector. However, there is an opposing effect working through the extensive margin. As regulated CBs raise their lending interest rate in response to tightened capital requirement, firms' expected payoff of borrowing from CBs becomes lower, leading to a shift from low-leverage CB finance to high-leverage SB finance. This shift leads to an increase in the aggregate firm leverage ratio and stimulates the aggregate credit. If the extensive-margin effect outweighs the intensive-margin effect, procyclical capital requirements will exaggerate the credit fluctuations and worsen the excess amplification of the business cycle arising from financial frictions. In this case, the optimal capital requirement may become countercyclical.====I calibrate the model to fit the steady-state properties of the real economy and the financial market in the United States. I posit two cases of firms' mobility between CBs and SBs. The benchmark case assumes perfect mobility where firms could freely choose between CBs and SBs for external finance. The alternative case assumes segmented credit market where firms with CB finance and firms with SB finance are separated from each other, and all firms could not choose between the two types of banks. The extensive-margin effect of capital requirement adjustments, which works through firms' shift between CB finance and SB finance, is present in the benchmark case but absent in the alternative case. Comparison of these two cases illustrates how the extensive margin effect affects the optimal capital requirement policy.====I first examine the impulse response of the economy to a positive capital requirement shock. The analysis shows that, in the benchmark case, raising capital requirement has positive net effects on both total loans and output. However, in the alternative case, raising capital requirement generates negative effects on both total loans and output, suggesting that the expansionary effect of raising capital requirement in the benchmark case is totally driven by the extensive-margin effect.====I then examine the optimal capital requirement policy when the economy is buffeted by either a technology shock or a borrower riskiness shock. In particular, I assume that capital requirement responds to loan-to-output ratio. The rule is procyclical if capital requirement responds positively to loan-to-output ratio. I find that the optimal capital requirement is procyclical in the alternative case with segmented credit market. However, in the benchmark case where firms can freely choose between CBs and SBs, the optimal capital requirement becomes countercyclical.====The major contribution of my paper is to investigate bank capital regulations in a quantitative general equilibrium framework with two types of banks that are heterogeneously regulated and compete with each other. Although several recent papers on bank capital regulation also feature general equilibrium settings, most of these papers abstract from competition by unregulated banks and typically assume that regulated banks either have monopolistic access to certain borrowers or compete only among each other.==== In these papers, tightening the capital requirement will reduce the credit supply from regulated banks and unambiguously help alleviate the excessive credit expansion during booms. As a result, these papers miss that tightening the capital requirement may exaggerate the credit boom through a shift of credit towards the shadow banking sector, which is highlighted in my paper.====My paper is also related to the growing literature that analyzes optimal bank regulation in the context of both regulated banks and unregulated banks, including those abstract from the general equilibrium effects. In this literature, unregulated shadow banks usually differ from regulated banks in two dimensions. On the asset side, unregulated shadow banking activities allow the origination of loans with high credit risks (Murfin, 2012; Dell'Ariccia et al., 2012) and lead to the excessive credit expansion (e.g. Adrian and Shin, 2010; Gorton and Metrick, 2010). On the liability side, shadow banks are more exposed to panics and runs because they cannot access public sources of liquidity or public sources of insurance and, as such, in times of crisis, can act as vehicles of shocks propagation (e.g. Goodhart et al., 2012; Gennaioli et al., 2013; Begenau and Landvoigt, 2017).====My paper contributes to this literature by exploring and highlighting the role of borrowers in the transmission mechanism of bank regulation. In particular, I characterize the financial frictions between banks and ultimate borrowers, and investigate how borrowers' leverages and funding costs differ when they borrow from different types of banks and how bank regulation affects the borrowers' endogenous choice between regulated banks and unregulated banks. My model shows that borrowers' decisions and their reactions to changes in bank regulation play an important role in the determination of optimal bank regulation. Although several theoretical papers have also focused on modeling the difference between regulated banks and unregulated banks on the asset side, the literature that explicitly model ultimate borrowers' decisions and investigate the role that ultimate borrowers play is scarce.====The paper is organized as follows. Section 2 describes the model and discusses the main features of the model. Section 3 describes the calibrated parameters. Section 4 presents the quantitative results. Section 5 presents a robustness analysis in an extended model with both deposit insurance and limited liabilities. Section 6 concludes.",Shadow banking and optimal capital requirements,https://www.sciencedirect.com/science/article/pii/S1094202520300351,21 May 2020,2020,Research Article,182.0
Tang Lixin,"Institute for Economic and Social Research, Jinan University, China","Received 14 January 2019, Revised 29 April 2020, Available online 8 May 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.05.001,Cited by (2)," increases the share of total profits received by exporters, and thus increases the aggregate supply of capital in the economy. The model reconciles a number of documented stylized facts. Quantitative analysis shows that the novel mechanism in the model amplifies the trade-induced increase in capital stock by 2.1 percentage points, and the trade-induced increase in wage by 0.7 percentage point relative to a comparable benchmark.","The effects of trade on inequality have received a great deal of attention (Goldberg and Pavcnik, 2007; Harrison et al., 2011, among others). While most research has focused on the inequality among workers, an increasing number of papers, including Haskel et al. (2012), Foellmi and Oechslin (2010), Dinopoulos and Unel (2014) and Ma and Ruzic (2019), have examined the effects of trade on the income distribution among “superstar” high-income households, such as entrepreneur households.==== This research tries to both document and explain the effects of trade on the top end of the income distribution. However, we know relatively little about the welfare implications of such effects. Specifically, even if trade increases income inequality among well-to-do entrepreneurs, do such effects matter at all for the wider population?====Studies on the relationship between trade and income inequality have relied mostly on static trade models, which have little to say about the consumption-saving decisions of agents. This limitation of static models is more pronounced when studying the effects of trade on entrepreneur income inequality because entrepreneurs have higher saving rates than typical workers and account for a large share of total wealth in an economy.==== Moreover, economists since Kaldor (1957), Kuznets (1955) and Stiglitz (1969) have hypothesized a link between income inequality and capital accumulation. Therefore, to better understand the effects of trade on entrepreneur income inequality and the associated welfare implications, it is important to incorporate the consumption-saving decisions of entrepreneurs in a dynamic model with capital accumulation.====In Section 2, I start by documenting three stylized facts. First, trade openness is positively associated with greater income inequality among high-income households across U.S. states. Second, among entrepreneur households in the U.S., those with higher income also have higher saving rates. Lastly, greater trade openness increases the aggregate saving rate both at the country level and across U.S. states. These stylized facts point to rich interactions between trade openness, income equality among entrepreneurs and aggregate saving.====To study the welfare implications of entrepreneur income inequality in international trade, I develop a dynamic model of trade with incomplete markets and capital accumulation. There are two types of households, workers and entrepreneurs. Entrepreneurs are ex-ante identical. They face uninsurable idiosyncratic income risk associated with their productivity and thus save. The ex-post heterogeneity in productivity among entrepreneurs translates into heterogeneity in exporting status, entrepreneur income, consumption and saving. In the model economy, a reduction in trade costs increases the dispersion of profits faced by ==== identical entrepreneurs, as market shares are reallocated toward more productive firms. This leads to an increase in aggregate saving due to the precautionary motive, as in Aiyagari (1994). It is also illustrative to examine the saving behaviors across ex-post heterogeneous entrepreneurs. Exporting entrepreneurs have both the highest profit and the highest saving rate in the economy. An increase in trade openness increases the share of total profits received by exporters, and thus increases total saving and the aggregate supply of capital in the economy via a mechanism similar to Kaldor (1957). I refer to this mechanism as the supply-side channel of capital accumulation. Meanwhile, a reduction in trade costs also increases the demand for capital, because it leads exporters to expand their production to serve foreign markets. I refer to this channel as the demand-side channel of capital accumulation. With respect to trade models, the supply-side channel is novel to this paper, while the demand-side channel is also found in previous work following Baldwin (1992). In equilibrium, greater trade openness leads to a large increase in the capital stock. Because the marginal product of labor increases with capital input, workers benefit from the large increase in capital accumulation and enjoy larger gains from trade than they would otherwise.====I calibrate the model using U.S. data. After using Survey of Consumer Finances (SCF) data to validate the model, I examine the effects of international trade on aggregate output, the consumption of workers, and the consumption of entrepreneurs with heterogeneous productivity. In the model, international trade increases the aggregate capital stock by 3.95%, the aggregate output by 2.55% and the real wage of workers by 3.48%. Meanwhile, the aggregate consumption of entrepreneurs is unchanged by international trade, but the increase in inequality of profits among them implies that the certainty-equivalent consumption of a typical entrepreneur actually decreases by 3.85%. In other words, while the ex-post most productive entrepreneurs gains from a decrease in trade costs, the ex-ante welfare of an average entrepreneur decreases due to greater dispersion of possible income realizations. Capital accumulation plays an important role in the model, accounting for 51.6% of the output gains from trade.====To isolate the effects of the proposed mechanism, I construct a benchmark model with complete markets, in which firms with heterogeneous productivity are owned by a single entrepreneur. In this complete-markets benchmark, the increase in aggregate capital stock and output due to international trade are both 1.85%, while the increase in the real wage of workers from trade is 2.78%. Compared to this benchmark, the novel mechanism in the full model amplifies the trade-induced increase in capital stock by 2.1 percentage points, and the trade-induced increase in output and wage, both by 0.7 percentage point. Therefore, accounting for the effects of trade on entrepreneur income inequality implies larger welfare gains from trade for workers.====I construct two additional benchmark models that abstract from capital accumulation, one with incomplete markets and the other with complete markets. I demonstrate that the interaction between capital accumulation and entrepreneur income inequality gives rise to higher aggregate output gains and higher real wage gains. In fact, the model in this paper collapses to the Chaney (2008) model when I shut down both entrepreneur income inequality and capital accumulation. The increase in aggregate output due to international trade in the complete markets benchmark without capital is 1.23%. This is the same as that calculated from the sufficient-statistics formula in Arkolakis et al. (2012) (henceforth “the ACR formula”) using the relevant import penetration ratio and the trade elasticity.====My paper is related to the literature that aims to quantify gains from trade (cf. Costinot and Rodriguez-Clare (2013)). Despite empirical evidence on the importance of capital accumulation (Levine and Renelt, 1992; Wacziarg, 2001; Wacziarg and Welch, 2008), most attempts to quantify gains from trade have abstracted from capital accumulation. Notable exceptions include Alessandria and Choi (2014), Anderson et al. (2015), Brooks and Pujolas (2017), and Ravikumar et al. (2017). Unlike the above papers, this paper emphasizes the capital response to a trade liberalization arising from the saving behavior of entrepreneurs due to rising inequality. Additionally, by comparing the model with relevant benchmarks, this paper shows that the new channel is quantitatively important for evaluating gains from trade.====As discussed earlier, my paper is related to the vast literature on the effects of international trade on inequality. The work most closely related to my paper is Ma and Ruzic (2019), who find that the executive-to-worker pay ratio within U.S. firms is strongly associated with exporting and FDI activities of these firms. They conclude that increased globalization accounts for about 44% of the observed increase in the top 0.1% income share in the U.S. between 1988 and 2008. While Ma and Ruzic (2019) are primarily concerned with explaining the observed patterns of inequality, my paper attempts to shed light on the welfare implications of this increased inequality. I find that accounting for the effects of trade on entrepreneur income inequality implies higher gains from trade, which appears to be a new finding in the literature.====Lastly, this paper is related to the research on top income shares and their aggregate implications (Piketty and Saez, 2003). Researchers have noted that top income shares may have different determinants and welfare implications than more traditional notions of income inequality such as the skill premium (Voitchovsky, 2005; Piketty, 2014; Acemoglu and Robinson, 2014; Aghion et al., 2015). Recent research emphasizes the role of the positive correlation between saving rates and income levels in macroeconomic outcomes (Kumhof et al., 2015; Auclert and Rognlie, 2018; Straub, 2018). Different from these papers, I study the welfare implications of entrepreneur income inequality in the context of international trade.====In Section 2, I document three stylized facts. In Section 3, I present the full model and the calibration strategy. I evaluate the performance of the model by comparing non-targeted moments from the model against SCF data. Section 4 presents the key results from the calibration exercise. Further robustness checks and model extensions are relegated to the appendices. Section 5 concludes.","Entrepreneur income inequality, aggregate saving and the gains from trade",https://www.sciencedirect.com/science/article/pii/S1094202520300326,8 May 2020,2020,Research Article,183.0
"Altug Sumru,Collard Fabrice,Çakmaklı Cem,Mukerji Sujoy,Özsöylev Han","American University of Beirut, Lebanon,CEPR, United Kingdom of Great Britain and Northern Ireland,University of Toulouse, France,Koç University, Turkey,Queen Mary University of London, United Kingdom of Great Britain and Northern Ireland,University of Oxford, United Kingdom of Great Britain and Northern Ireland","Received 17 June 2019, Revised 21 April 2020, Available online 8 May 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.005,Cited by (9),", ","In this paper we adopt the smooth ambiguity preferences of Klibanoff et al., 2005, Klibanoff et al., 2009 to study the cyclical dynamics of a Real Business Cycle model with investment irreversibility and labor augmenting technology shocks. We examine the evolution of beliefs under ambiguity, information and learning and their impact on the cyclical behavior of key aggregate variables (such as output, consumption, investment and hours worked). In this framework, the shock to aggregate TFP evolves as a function of a latent variable governing its persistence. Ambiguity of belief about the productivity process arises in our framework as agents do not know whether variation in aggregate TFP is driven by a process with high persistence and low volatility, or one with lower persistence but higher volatility, and they must, at the same time, infer the behavior of the unobserved temporary component using a Kalman filtering algorithm. In our framework, time-varying uncertainty shocks that emerge under learning and ambiguity aversion interact with the irreversible nature of investment, leading to a wait-and-see attitude in response to uncertainty shocks and in turn, creating business cycle-like effects.====In the smooth ambiguity model it is possible to hold the agents' information fixed while varying their ambiguity attitude; the variation may range from extreme aversion (when it corresponds to maxmin expected utility) through a continuum of intermediate cases to ambiguity neutrality (i.e., expected utility with Bayesian model averaging). This facilitates a natural way to understand the ==== effect of introducing ambiguity aversion into the environment. In essence, we are motivated by two questions. First, to what extent the results that were achieved in matching the dynamics of the equity premium, as studied by Collard et al. (2018), by moving from Bayesian model averaging to ambiguity aversion in an endowment economy is replicated in explaining the cyclical dynamics of real variables such as consumption, investment, output and hours. Second, in comparison to recent papers that study related questions, we seek to clarify the effect of changes in ambiguity and/or ambiguity aversion on dynamics of real variables in a neoclassical environment that is very minimally, if at all, perturbed by frictions, to better understand the effect of ambiguity ====.====Our major results may be summarized as follows. First, the standard business cycle facts hold in our model, and they are not significantly altered by changes in the degree of ambiguity aversion. Following Tallarini (2000) and Backus et al. (2015), this result arises from the complete markets assumption that underlies the allocations of the social planning problem and the precautionary saving motives that it induces. Second, changes in initial ambiguity as measured by the degree of informativeness of the unknown latent process driving temporary fluctuations in productivity growth have significant business cycle effects. This demonstrates a role for information and learning in the smooth ambiguity model.==== Holding the degree of ambiguity aversion constant, lower initial ambiguity or greater confidence lead to significantly lower cyclical variability in investment and hours worked. Lower ambiguity also leads to the lower responsiveness of output and consumption to shocks as well as amplifying co-movement.====Third, we compare our findings with those from the New Keynesian model with maxmin expected utility studied by Ilut and Schneider (2014).==== Based on a comparison of the model-generated moments from their model with those in the data, we find that removing the different types of frictions in their model has non-negligible effects on the results. In particular, a version of their model without nominal and real frictions, which corresponds more closely to our RBC framework, has limited success at matching the moments for the quantity variables. In the maxmin expected utility framework, which corresponds to the limiting case of the smooth ambiguity model as ambiguity aversion goes to infinity, agents make decisions based on the worst case distribution characterizing the TFP process. By contrast, in our framework, agents have endogenously distorted beliefs that depend on the properties of the unknown distributions characterizing TFP growth. In the absence of rigidities deriving from monopolistic competition in goods and labor supply and price and wage setting by firms of intermediate goods and by households, respectively, the worst case scenario instills too much caution on the part of agents, who end up excessively reducing their response to shocks from the estimated TFP process. These results indicate that the smooth ambiguity preferences together with the endogenous sources of pessimism that this framework generates are able to account for the cyclical dynamics of key quantity variables that the model with worst-case beliefs cannot capture.====The recent literature on models with ambiguity and ambiguity aversion has examined a variety of phenomena. A group of papers has examined asset pricing relations under smooth ambiguity preferences. These include Ju and Miao, 2007, Ju and Miao, 2012, who consider the implications of the generalized recursive smooth ambiguity model that distinguishes among risk aversion, intertemporal substitution and ambiguity aversion in an endowment economy or Jahan-Parvar and Liu (2014) and Liu and Zhang (2018), who extend this framework to a production economy. Collin-Dufresne et al. (2016) and Collard et al. (2018) exploit learning dynamics to generate significant effects of time-varying ambiguity on asset prices. Others have examined business cycle phenomena under alternative assumptions about the existence of real and nominal frictions; see, for example, Bidder and Smith (2012), Ilut and Schneider (2014) or Bhandari et al. (2016). Nimark (2014) presents a business cycle model with higher order beliefs and considers the impact of signals observed after unusual events that increase uncertainty and disagreement among agents. In all of these analyses, the presence of ambiguity and agents' aversion to ambiguity is taken as a key feature of the environment, as in our framework, and the focus of the analysis is to unravel the effects of these features on different outcomes.====The analysis of Jahan-Parvar and Liu (2014), Liu and Zhang (2018), and Bidder and Smith (2012) remains within a standard neoclassical environment describing investment and production. Bidder and Smith (2012) consider a Real Business Cycle Model with a version of the multiplier preferences that are robust to misspecification following Hansen and Sargent (2008) and find small but significant effects on business cycle moments of time-varying volatility shocks to TFP growth. In Ilut and Schneider (2014) or Bhandari et al. (2016), ambiguous preferences are paired with real and nominal frictions to account for the impact of changes in beliefs on cyclical outcomes.==== Our paper adds to this literature by examining the implications of a Real Business Cycle model with smooth ambiguity preferences and comparing them with the implications from a New Keynesian model with maxmin expected utility.====In the literature on ambiguity and/or robust decision-making, the issue of the extent of knowledge possessed by the econometrician versus agents plays a key role; see Hansen (2007). In the New Keynesian models of Ilut and Schneider (2014); Bianchi et al. (2018), survey expectations are used to empirically discipline time variation in ambiguity. Other papers that use survey data to identify agents' beliefs include Bhandari et al. (2016) and Rossi et al. (2016). In our model, the agent and the econometrician possess the same information and must infer the nature of the true TFP growth process based on past observations of TFP growth. Thus, in our analysis, the rational expectations assumption that agents are endowed with more precise information than the econometrician is relaxed. This unifies the treatment of uncertainty and ambiguity without recourse to exogenous sources of data to estimate beliefs or extraneous assumptions about the knowledge possessed by agents versus the econometrician.====The remainder of this paper is organized as follows. Section 2 describes the nature of uncertainty and ambiguity and presents Bayesian inference of the underlying TFP process. Section 3 describes a Real Business Cycle model with ambiguity and ambiguity aversion while Section 4 presents the quantitative results obtained from this model. Section 5 compares our results with those from a New Keynesian with maxmin expected utility while Section 6 describes the results from other alternative frameworks. Section 7 concludes.",Ambiguous business cycles: A quantitative assessment,https://www.sciencedirect.com/science/article/pii/S1094202520300296,8 May 2020,2020,Research Article,184.0
"Araujo Luis,Guimaraes Bernardo,Rodrigues Diego","Michigan State University, United States of America,Sao Paulo School of Economics – FGV, Brazil,University of Minnesota, United States of America","Received 15 May 2019, Revised 15 April 2020, Available online 7 May 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.006,Cited by (1),"Assessing the fundamental value of a wide range of asset-backed securities is costly. As a result, these assets can become information insensitive, which allows them to be used as collateral in credit transactions. In this paper, we show that while it is true that information-insensitive assets can play a liquidity role, the fact that they play this role reinforces their information insensitivity. This implies that the availability of alternative ways of financing can harm the liquidity role of assets, even if these alternatives are costly and not used in equilibrium. The reason is that such options raise the asset's sensitivity to information by increasing the relative importance of their fundamental value vis-a-vis their role as collateral.","Asset-backed securities were extensively traded in the period leading up to the 2008 financial crisis. The demand for these assets was driven not only by their fundamental value but also for their liquidity role, i.e., their use as collateral in credit operations. Gorton and Ordoñez (2014) links the liquidity of such assets to their complexity. They argue that if it is very costly to acquire information about the fundamental value of an asset, there exist equilibria in which this information is not acquired and the asset becomes information-insensitive, which allows it to perform money-like functions.====In this paper, we posit that while it is true that information-insensitive assets can play a liquidity role, the fact that they play this role reinforces their information insensitivity. Intuitively, one has fewer incentives to acquire information about the fundamental value of an asset if the main reason for holding the asset comes from its liquidity role as collateral. The downside of this result is that the emergence of alternative ways to address financing needs, by undermining the liquidity role of an asset and reinforcing its fundamental value, raises the incentives to acquire information about the asset, and reduces its information-insensitivity.====Our environment is based on Gorton and Ordoñez (2014). There is an overlapping generation structure, wherein each period the economy is populated by a unit continuum of young agents and a unit continuum of old agents. Each member of a new generation is born with an endowment of capital but she is only able to use the capital in a productive way when she becomes old. There is also an initial generation of old agents with no endowment of capital, but with an endowment of one unit of land. The land captures the role of asset-backed securities. It has no productive use, but it has an unobservable intrinsic value. Old agents borrow from young agents to finance their projects, and land is used as collateral.====Absent any information asymmetries, the environment in Gorton and Ordoñez (2014) is a standard OLG economy in which land is performing the role of money. In fact, like money, land is transferred across generations, allowing the borrower to credibly pledge his production to the lender. The key feature of their environment is the assumption that the intrinsic quality of the land held by the borrower can only be verified by the lender at a cost. They also assume that capital fully depreciates. We depart from their environment by assuming that capital is storable, and subject to a positive depreciation rate. With this modification, we aim at capturing the idea that a young agent may choose to self-finance her production activity. Our objective is to have the use of land as collateral competing with alternative sources of resources, and self-finance is a simple way to introduce such an alternative.====Our main result is that the incentives of a lender to privately verify the underlying quality of land depends on the relative abundance of her endowment. The reasoning runs as follows. Consider a match between a borrower and a lender with a relatively abundant endowment. In this case, whether the lender chooses to verify the quality of the land or not has no bearing on her decision about implementing a project in the following period. She will have enough resources to do so either way. As a result, she has stronger incentives to deviate and enjoy the extra-benefit coming from keeping land of good quality instead of selling this land at a price below its fundamental value. Consider now a scenario where the endowment of the lender is relatively scarce. In this case, if the lender chooses to privately incur the cost of verifying the land quality and keep it in case it is good, she will be unable to fund her future production activity. This reduces her incentives to deviate. This result implies that information-insensitive contracts are more prevalent in the economy when the lender has a relatively small endowment and, therefore, no access to alternative ways of financing. This has a positive effect on welfare, as lending always takes place under information-insensitive contracts.====A low endowment captures, in a stylized way, a situation where alternative forms of financing projects are scarce (or expensive). This could happen to lenders for a variety of reasons: a shortage of deposits, little cash (perhaps owing to past investment decisions), or a high demand for credit from borrowers.==== In these cases, lenders face a high opportunity cost for their resources. A high endowment corresponds to a situation when financial institutions have a low opportunity cost for their deposits, resources are somewhat idle, either because there is little demand for loans or because funding is cheap.====While a formal test of the model is beyond the scope of this paper, we conjecture that the driving forces of our model might have played a role in the collapse of the market for asset-backed securities in 2007. The demand for loans from banks seems to have plummeted after 2005. In the model, this would be captured by a surge in the endowment of lenders. By raising incentives for assessing the value of assets used as collateral, this fall in demand for credit could jeopardize the market of collateralized credit. Hence the adverse shocks hitting the economy would be amplified by a change in the behavior of financial institutions.====A growing literature argues that a shortage of safe assets has led to the use of complex assets as collateral, i.e., assets whose information about their intrinsic value is costly to acquire. Key references are Dang et al. (2012), Xie (2012), Gorton and Ordonez (2013) and Gorton and Ordoñez (2014).==== The innovation of our model in relation to those papers lies in exploring how the difficulty in finding viable alternatives to the use of collateral may reinforce the latter's information insensitivity.====The remaining of the paper is organized as follows. In the next section, we present the model. Section 3 presents the equilibrium. Section 4 discusses how the model could be connected to the collapse of the market for asset-backed securities. Section 5 concludes.",Financial constraints and collateral crises,https://www.sciencedirect.com/science/article/pii/S1094202520300302,7 May 2020,2020,Research Article,185.0
Douenne Thomas,"Paris School of Economics, Paris 1 Panthéon-Sorbonne, 48 Boulevard Jourdan, 75014, Paris, France","Received 7 August 2018, Revised 14 April 2020, Available online 4 May 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.007,Cited by (9),"This paper studies the role of preferences on the link between disasters, growth, and welfare. An ","Risk is more than ever an essential concern for economic policies. The renewal of interest for the study of risk in macroeconomic models is not only the result of the 2008 economic and financial crisis but also reflects the growing concerns around environmental risks such as climate change and environmental disasters. The risk of rare catastrophic events bears critical welfare implications not only as disasters hurt when they strike but also as their anticipation may affect agents decisions. This view has been first introduced by Rietz (1988) in an attempt to explain the equity premium puzzle (Mehra and Prescott, 1985). Since, his idea that a low subjective probability of a catastrophic event may drive agents investment decisions has gained momentum with a development of new theoretical frameworks (e.g. Barro, 2006, Barro, 2009; Gabaix, 2012) supported by empirical evidences on the history of catastrophic events (e.g. Barro and Ursua, 2008). More recently, some authors have adopted similar frameworks to analyze the macroeconomic impacts of environmental disasters in endogenous growth frameworks (Ikefuji and Horii, 2012; Barro, 2015; Müller-Fürstenberger and Schumacher, 2015; Bakkensen and Barrage, 2016; Bretschger and Vinogradova, 2017; Akao and Sakamoto, 2018). As pointed out by Bakkensen and Barrage (2016), if these disasters reduce output or production means when they strike, they also affect consumption and savings decisions in an ambiguous way, resulting in potentially important long-term impacts.====In line with this literature, the objective of the present paper is to better understand the link between environmental disasters, economic growth, and welfare. To investigate the underlying mechanisms, I propose an endogenous growth model with endogenous disasters that can be fully solved analytically. The model builds on the frameworks proposed by Müller-Fürstenberger and Schumacher (2015) and Bretschger and Vinogradova (2017), and extends these earlier works by allowing for a more general representation of individuals' preferences. In particular, the model is solved for the class of utility functions proposed by Epstein and Zin (1989) and Weil (1990), building on Kreps and Porteus (1978) non-expected utility theory. As shown by a large literature in finance (see Bansal and Yaron, 2004), by distinguishing risk aversion from the inter-temporal elasticity of substitution, these utility functions enable to better explain individuals decision in front of risk. As the objective of the paper is to understand how disasters affect growth and welfare, allowing for this more general and flexible representation of preferences will prove critical. In particular, the paper shows analytically that the restrictions imposed by more standard utility functions — e.g. logarithmic or time-additive power utility — bias our understanding of the mechanisms that link disasters to growth, and welfare. In a calibration of the model that matches empirical evidence on environmental disasters, the paper also shows that these biases matter quantitatively.====In order to start from a simple benchmark, the model is first solved in the case of exogenous disasters. Several preliminary intuitions are derived in this situation. I then turn to the case of disasters whose probability can be mitigated through a policy. In the appendix the model is also solved for multiple types of disasters including catastrophes of endogenous intensity. It follows from the model that the optimal shares of output consumed, saved, and spent in risk-mitigation are all constant on the optimal path. The effects of the model's parameters are studied and in particular the role of the preference parameters are emphasized. While risk and risk aversion (RRA) drive the decision to mitigate risk, the inter-temporal elasticity of substitution (IES) plays no role in this decision. However, it appears to be critical in the risk sensitivity of the consumption/savings decision. When the risk of disasters increases, current consumption is partly transferred to the future through savings when the IES is below unity. Interestingly, if the ==== of this effect solely depends on the IES, its ==== depends on the RRA. While a low IES — i.e. high aversion to fluctuations — unambiguously leads to more precautionary savings, a high aversion to risk may increase either precautionary savings or precautionary consumption. This result shows that it is essential to depart from the standard time-additive utility function as aversion to risk and to fluctuations end up having very different effects on the optimal solution. A second result of importance is that, when introducing an instrument to mitigate disasters, an increase in risk also generates a transfer from savings to risk-mitigation spending. As a result, and contrary to what has been emphasized so far in the literature, an IES below unity is a necessary but insufficient condition to guarantee a net positive response of savings to risk.====From the law of capital accumulation, one can compute analytically the stochastic growth rate as well as the average long-run growth rate of the economy. Most interestingly, one can look at the effect of disasters on the latter. Following the terminology used by Bakkensen and Barrage (2016) I distinguish the impact of disaster ==== from the one of disaster ====. If damages from catastrophes (i.e. from strikes) reduce expected growth, their anticipation (i.e. risk) has an ambiguous effect through the sensitivity of capital accumulation to risk. For realistic parameter values — i.e. unless the crowding out of risk-mitigation spending over savings is too high — disasters foster average long-run growth if aversion to risk and to fluctuations are both large enough. Since the existence of disasters necessarily reduces welfare, there are therefore situations in which growth and welfare are inversely linked. To further examine the impact of disasters on welfare, I compute analytically the marginal rate of substitution between disaster parameters (i.e. frequency and intensity) and output, as well as a measure proposed by Lucas, 1987, Lucas, 2003 to assess the welfare benefits of the policy instrument relative to a business-as-usual scenario.====In order to illustrate quantitatively the analytic findings of the paper, the model is then calibrated so as to represent the U.S. — a country among the most impacted by environmental disasters (see Shi et al., 2015) — disaggregated at the county level. Disaster parameters are proxied from the most recent study on the impact of disasters in U.S. counties over the last 80 years by Boustan et al. (2017). From this exercise, we reach three important conclusions. First, if a positive impact of disasters on long-run growth is theoretically possible in this framework, such a positive relationship can occur only for extremely large disasters and (rather implausibly) high values of aversion towards risk and fluctuations. Second, the effects of disasters on welfare appear significant, even ignoring their impacts on human lives. For instance, reducing by only 10% the likelihood of disasters would be equivalent to an increase by 0.65% of GDP in our main scenario, even though yearly expected damages on GDP are as low as 0.13%. Interestingly, holding expected damages constant but increasing disaster intensity, the welfare effects become much larger. This result stresses the role of insurance as an adaptation strategy, as the welfare gains from trading-off disaster intensity against likelihood appear important. Third and last, the two previous results are sensitive to the calibration of preferences parameters. Thus, the constraints imposed by logarithmic or power utility functions do not only affect our qualitative understanding of the effects of disasters, but they also matter quantitatively. In particular, when using high values for the elasticity of the utility to capture risk aversion, one overestimates the importance of precautionary savings and may wrongly conclude that disasters positively affect growth. When using lower values to better match the IES, he instead underestimates the impact of disasters on welfare, and the level of optimal mitigation policies.====This paper contributes to two strands of the literature. First, it provides a novel framework to study the effect of environmental disasters on economic growth. Improving our understanding of the mechanisms underlying this link is critical not only from a theoretical point of view but also as to guide future empirical research on this issue. Indeed, the empirical literature on the link between disasters and growth points towards contrasted evidence. Skidmore and Toya (2002) conclude that higher frequencies of climatic disasters may foster growth, possibly through an effect on human capital accumulation and technology. While Cavallo et al. (2013) find no significant impact of disasters on short and long-run growth, Sawada et al. (2011) find significant negative effects in the short run, but positive effects in the longer term. Strobl (2011) studies hurricanes in the U.S. coastal counties and finds evidence of negative effects with very partial recovery, but the macroeconomic impact of these local catastrophes appears to be negligible. Noy (2009) also finds negative but heterogeneous impacts, with more developed countries being less exposed. More recently, Hsiang and Jina (2014) found a strong negative long-run effect of hurricanes on output and long-run growth with no evidence of a rebound effect in the twenty years following a catastrophe. Some previous theoretical works have recently attempted to understand these diverging empirical evidence. Ikefuji and Horii (2012) stress the role of human capital as a substitute for physical capital to sustain growth when physical capital pollutes. Bakkensen and Barrage (2016) try to reconcile the heterogeneous empirical findings by disentangling hurricanes ==== and hurricanes ====. They show that while the former may persistently reduce output, the second may foster growth through more accumulation due to precautionary savings. They argue that the contradictory results found in empirical studies might partly be explained by different methodologies that either capture the effect of disaster strikes or disaster risks. Akao and Sakamoto (2018) study exogenous disasters and discuss the role of human capital and technology. As Bakkensen and Barrage (2016), they emphasize the key role of the elasticity of the utility function for disaster risks to foster growth through precautionary savings. Although they do not focus directly on growth, Müller-Fürstenberger and Schumacher (2015) and Bretschger and Vinogradova (2017) both analyze the effect of risk on capital accumulation in a Ramsey type of model where risk can be mitigated through abatement activities. Their results also support the idea that disasters may accelerate capital accumulation depending on the elasticity of the utility function. By contrast, using a more satisfactory representation of preferences towards risk, calibrated so as to match empirical evidence of disaster impacts, this paper shows that precautionary savings are unlikely to be sufficient to generate a positive link between disasters and growth as sometimes found in the empirical literature. It remains an open question whether this empirical observation is robust, but if that is, future research will have to determine which other mechanisms could explain it.====Second, this paper adds to the theoretical literature on the optimal mitigation of environmental risks. In particular, it contributes to recent literature that incorporates recursive preferences into environmental models where risk matters. Previous studies have analyzed the effect of pollution (Soretz, 2007) or biodiversity losses (Augeraud-Véron et al., 2018) on fluctuations, and shown how optimal policies depended on preferences parameters. Considering larger shocks, Barro (2015) extends the previous disaster model of Barro (2009) to disentangle environmental disasters from other types of catastrophes. In a different set-up, Bansal and Ochoa (2011), Bansal et al. (2016), and Karydas and Xepapadeas (2019) examine the effect of temperature-driven disasters on market returns with non-expected utility. van der Ploeg and de Zeeuw (2017) study precautionary savings as a reaction to an endogenous climate tipping point. They characterize savings responses to the tipping depending on its impact delay and on the distance of the economy from its steady-state. However, the model does not provide closed-form solutions and does not enable to study repeated catastrophes. Other papers using numerical methods have introduced Epstein-Zin-Weil preferences in climate economy models, such as DSGE models (e.g. van den Bremer and van der Ploeg, 2018) and Integrated Assessment Models (see Crost and Traeger, 2014; Jensen and Traeger, 2014; Cai and Lontzek, 2018; Olijslagers and van Wijnbergen, 2019). To my knowledge, this paper is the first to present a framework to study analytically the relationship between endogenous growth and endogenous disasters in which agents display recursive preferences. Both through analytical results and a calibration consistent with observed impacts of disasters, the paper shows the importance of separating aversion towards risk and fluctuations, in order to better understand the effects of disasters on growth, welfare, and the implications for optimal policies.====The rest of the paper is organized as follows. Section 2 presents the general framework. Section 3 considers the case of exogenous disasters as a benchmark to highlight the first intuitions of the model. Section 4 turns to endogenous disasters whose probability can be reduced through a risk-mitigation policy. Section 5 provides a calibration of the model and a quantitative assessment of the link between disasters, growth and welfare, and the importance of using non-expected utility over more restrictive representations of preferences. Section 6 concludes. Computations are reported to the appendix, where the model is also extended to multiple types of disasters including of endogenous intensity.","Disaster risks, disaster strikes, and economic growth: The role of preferences",https://www.sciencedirect.com/science/article/pii/S1094202520300314,4 May 2020,2020,Research Article,186.0
"Cruz Edgar,Raurich Xavier","University of Guanajuato, Mexico,Universitat de Barcelona, Spain","Received 8 July 2019, Revised 7 April 2020, Available online 20 April 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.004,Cited by (2)," in leisure, and employment differences across countries caused by differences in income taxes.","We observe two important patterns of structural change during the last fifty years. The first one is the large shift in employment and production from the goods to the service sector. Fig. 1 illustrates this pattern for the US economy, during the period 1965-2015. In 1965, only 55% of workers were employed in the service sector, whereas 77% were employed in this sector in 2015. Fig. 1 also shows a similar pattern for the shares of value added. The recent multisector growth literature has explained these patterns of structural change as the result of income effects (Kongsamut et al., 2001) or price effects (Acemoglu and Guerrieri, 2008; and Ngai and Pissarides, 2007). More recently, this literature has argued that the significant increase of the service sector can only be explained by combining both types of effects (Boppart, 2014; Dennis and İşcan, 2009; Foellmi and Zweimüller, 2008; and Herrendorf et al., 2013). Herrendorf et al. (2014) offers an exhaustive review of this literature and shows that this process of structural change is not specific to the US, but it is quite a general feature.====The second pattern is the increase in leisure time. Using survey data, Aguiar and Hurst (2007), and Ramey and Francis (2009) document that leisure time increases in the US economy during the second half of the last century.==== This increase is also illustrated in Fig. 1, where it is shown that leisure, as a fraction of total time devoted to leisure and work in the market, increases from 46% in 1965 to 54% in 2015.==== Duernecker and Herrendorf (2018) show the same pattern in other countries.====The increase in leisure time is mainly explained by an income effect due to non-homothetic preferences (Duernecker and Herrendorf, 2018; and Restuccia and Vandenbroucke, 2013 and 2014). More recently, Aguiar et al. (2017) argue that the introduction of new recreational activities, such as video gaming and other recreational computer activities, has reduced the labor supply of young men. Note that these explanations are entirely independent of the multisectoral structure of the economy. In fact, there are few papers relating the rise of the service sector with changes in the uses of time. Examples are the papers by Buera and Kaboski (2012); Gollin et al. (2004); Moro et al. (2017); Ngai and Pissarides (2008), and Rogerson (2008). In these papers, the relationship between the service sector and the uses of time is based on home production and its different substitutability with the market production of the different sectors. More precisely, the reduction in home production causes the increase in the employment share of the service sector because home production is a better substitute for services than for the goods produced in the other sectors. The relationship between uses of time and the service sector is also obtained by Greenwood and Vandenbroucke (2005) and Ngai and Pissarides (2008), who introduce recreational activities that combine leisure time with durable goods produced in the manufacturing sector.==== Again, the different substitutability of these activities with the market production of the different sectors contributes to explain the increase in the employment share of the service sector.====In this paper, we also provide a joint explanation of the increase in both the service sector and leisure time. We contribute to the aforementioned papers by assuming that individuals consume recreational services during leisure time. Therefore, the consumption of these services increases with leisure time, which introduces a mechanism that relates leisure time with the service sector.====An advantage of our approach is that we can identify the industries that provide recreational services and, therefore, we can obtain a direct measure of the effect of recreational activities on structural change.==== To this end, we measure the fraction of the value added of the service sector explained by the consumption of recreational services. The details of the procedure followed to obtain this fraction are in Appendix A, and the results are shown in Fig. 1. This figure shows that this fraction has increased from 5.2% in 1965 to 8.6% in 2015. This increase has a sizable effect on sectoral composition, as it accounts for 19% of the observed increase in the service sector share of total value added.====Our purpose is to analyze the effect that recreational activities have on both the sectoral composition and the labor supply. To this end, we measure both effects using a multisector exogenous growth model. In the supply side of this model, we distinguish between two sector-specific technologies that produce goods and services. These technologies are differentiated only by the exogenous growth rate of total factor productivity (TFP). In the demand side, we assume that households obtain utility from consuming goods, non-recreational services, and recreational activities. Following Ngai and Pissarides (2007), the utility function is a constant elasticity of substitution (CES) function. Therefore, the only new feature of this model is the introduction of recreational activities. These activities are defined as another CES function relating the amount of time devoted to leisure and the consumption of recreational services. Hence, the utility function considered in this paper is a non-homothetic version of the nested CES function introduced by Sato (1967).====In this model, technological progress drives structural change through three different mechanisms: substitution, income, and recreational mechanisms. First, the substitution mechanism is due to the assumption of different growth rates of sectoral TFP. Consistent with empirical evidence, we will assume that the goods sector experiences the largest TFP growth rate, which causes the increase in the relative price of services in units of goods. As shown by Ngai and Pissarides (2007), this relative price increase contributes to explain the rise of the service sector when the elasticity of substitution of consumption goods is smaller than one.====Second, the income mechanism is due to the introduction of a minimum consumption requirement on the consumption of goods. Preferences are then non-homothetic and the income elasticity of the demand of services is larger than one. As a consequence, the employment share of the service sector increases as income grows with technological progress. Thus, the income mechanism also contributes to explain the rise of the service sector.====Third, the recreational mechanism is the new mechanism introduced in this paper. Both leisure time and the fraction of the value added of the service sector explained by the consumption of recreational services increase when the elasticity of substitution between leisure time and recreational services is smaller than one but larger than the elasticity of substitution of consumption goods.==== Thus, the recreational mechanism also contributes to explain the rise of the service sector when leisure time and recreational services are not strong complements, which implies that individuals can substitute between leisure time and recreational expenditures.====We simulate three different models, that are calibrated to match the patterns of structural change of the US economy in the period 1965-2015, to show that the recreational mechanism is a relevant factor explaining these patterns. The first one is our benchmark model in which individuals obtain utility from recreational activities. In the second model, we do not consider these activities, and we instead assume that individuals obtain utility directly from leisure. Finally, the third model is a standard multisector growth model without leisure. Therefore, the three mechanisms of structural change are operative only in the benchmark model. We show that the interaction among the three mechanisms accounts for almost all the observed increase of the share of employment allocated in the service sector, of leisure time and of the fraction of the value added of the service sector explained by the consumption of recreational services. In the other two models, the recreational mechanism is not operative and, hence, the increase of the service sector is explained only by the other two mechanisms. We compare the performance of these three models to conclude that the introduction of recreational activities improves substantially the performance of the model in explaining the increase of the service sector.====The substitution between leisure time and expenditure in recreational services, introduced by recreational activities, has interesting implications on leisure inequality and employment differences among countries. First, in line with Boppart and Ngai (2018) and Bridgman (2017), we analyze an extension of the basic model that introduces inequality in labor income. We show that high labor income individuals perform expenditure intensive recreational activities, whereas low labor income individuals perform time intensive recreational activities. This result is consistent with the evidence in Fig. 4, that shows that high labor income individuals consume a larger fraction of services in recreational activities, although they devote a smaller fraction of time to leisure.====Second, the introduction of recreational activities worsens the reduction in employment due to a labor income tax increase, because these activities increase the substitutability between leisure time and consumption expenditures. As a consequence, labor income tax differences across-countries result into larger employment differences when the recreational mechanism is considered. This result is related to Rogerson (2008), who shows that the larger labor income taxes in Europe in comparison to the US make home production larger. In his analysis, this explains that European economies exhibit both a lower level of employment and a smaller fraction of working time employed in the service sector. In contrast, in our analysis, the larger taxes make recreational activities be more time intensive in European economies. This result is consistent with evidence obtained from the comparison between US and France, a country with larger labor income taxes than in the US. In Fig. 6, we show that the fraction of recreational services over total services is smaller in France than in the US, whereas the fraction of time devoted to leisure is larger. This also explains that both the level of employment and the employment share in services are smaller when taxes are larger. Hence, our paper offers a complementary explanation of the differences between Europe and US regarding both sectoral composition and uses of time.====The rest of the paper is organized as follows. Section 2 introduces the model and Section 3 characterizes the equilibrium. Section 4 solves the model numerically and obtains the main results. Section 5 introduces inequality. Section 6 studies the effect of labor income taxes on employment. Finally, Section 7 includes some concluding remarks and discusses other possible extensions of the basic model.",Leisure time and the sectoral composition of employment,https://www.sciencedirect.com/science/article/pii/S1094202520300284,20 April 2020,2020,Research Article,187.0
"Bachmann Rüdiger,Bai Jinhui H.,Lee Minjoon,Zhang Fudong","University of Notre Dame, CEPR, CESifo and ifo, United States of America,Washington State University, United States of America,Peking University HSBC Business School, China,Carleton University, Canada,Tsinghua University, China","Received 18 May 2018, Revised 20 March 2020, Available online 16 April 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.001,Cited by (3),This study explores the welfare and distributional effects of fiscal volatility using a neoclassical ,"One consequence of the financial crisis followed by political turmoil has been the perception of high volatility in government policies in both the U.S. and in Europe.==== In this paper, we study, from the viewpoint of the household, the welfare costs of the volatility of government purchases, both in the aggregate and across different wealth holdings. We do so in a neoclassical model with incomplete markets and a richly specified government sector, where we eliminate the volatility of government purchases once and for all.====Most of the existing research on the consequences of fiscal volatility has focused on the aggregate effects of short-run volatility fluctuations on various macroeconomic variables. In one study, Baker et al. (2016) analyze Internet news and find a (causal) relationship between high policy uncertainty and subdued aggregate economic activity. In another study, based on a New Keynesian DSGE model, Fernández-Villaverde et al. (2015) find large contractionary effects of fiscal volatility on economic activity accompanied by inflationary pressure, especially when the nominal interest rate is at the zero lower bound. By contrast, we study the effects of permanently eliminating fiscal volatility on household welfare with a particular emphasis on distributional aspects.==== In studying the welfare effect of permanent changes in fiscal policy, we take a similar approach to McKay and Reis (2016). They focus on permanent changes in the automatic stabilizer role of fiscal policy. Our study complements theirs through its focus on government purchases rather than transfers (see below for a more detailed discussion of the literature).====To quantify the welfare costs of fluctuations in government purchases for households, we follow the approach of Krusell and Smith (1998) and use an incomplete market model where heterogeneous households face uninsurable idiosyncratic risks in their labor income and discount factor processes. We then calibrate this model with U.S. data, in particular data on U.S. wealth inequality. Our model has aggregate uncertainty arising from both productivity and government purchases shocks. We thus specify government purchases shocks as the only fundamental source of fiscal volatility. In line with the data, we further assume that government purchases shocks are independent of aggregate productivity and employment conditions.==== Government purchases enter the utility function of the households as separable goods.==== We also employ an empirical aggregate tax revenue response rule, which includes government debt and is estimated from U.S. data.====Because the government partially funds its expenditures through taxation, purchases fluctuations generate volatile household-specific tax rates. To capture the distributional effects of fiscal shocks through taxation, we model key features of the progressive U.S. income tax system. Importantly, even though all the households face the same progressive tax schedule, depending on where they belong in the income distribution, their household-specific tax rate risks are differentially impacted by the aggregate fiscal risk. In U.S. data, we indeed find that higher tax revenues are associated with more progressive income taxes, rather than uniform shifts up in the tax schedule. This fact calls for capturing realistic household heterogeneity in our model.====To eliminate fiscal volatility, following Krusell and Smith (1999) and Krusell et al. (2009), we start from a stochastic steady state of the economy with both productivity and government purchases shocks, and remove the fiscal shocks at a given point in time by replacing them with their conditional expectations, while retaining the aggregate productivity process. We then compute the transition path towards the new stochastic steady state in full general equilibrium. Based on the quantitative solution for this transition path, we then compare the welfare of various household groups in the transition-path equilibrium to their welfare level with both aggregate shocks in place.====Our results show that the aggregate welfare costs from fiscal shocks are fairly small. The effect of removing fiscal volatility is equivalent to a 0.03% increase in the lifetime consumption on average. This is comparable to the welfare costs of business cycle fluctuations reported in Lucas, 1987, Lucas, 2003, as well as to those from a representative-agent version of our model, even though in our model aggregate (spending) fluctuations lead to differential impacts on household-specific tax rate risks in addition to the before-tax factor prices volatility, so that they, a priori, may lead to larger welfare costs than in Lucas, 1987, Lucas, 2003 (see Krusell et al., 2009).====By contrast, our results reveal interesting variations in the welfare costs of fiscal volatility along the wealth distribution. The welfare gains of eliminating fiscal volatility are increasing in household wealth according to the baseline specification, where the implementation of the progressive U.S. federal income tax system and the aggregate tax revenue response rule is modeled to best match the cyclicality of important moments of the U.S. tax system.====Since volatile tax rates pre-multiply labor income levels, they generate – loosely speaking – multiplicative after-tax labor income risk.==== Just as with the additive labor endowment risk in early incomplete market models, this after-tax labor income risk leads households to self-insure through precautionary saving. Wealth-rich households can thus achieve a higher degree of self-insurance relative to wealth-poor households. Consequently, from a precautionary saving perspective, wealth-poor households should gain more when fiscal volatility is eliminated.====However, due to the multiplicative nature of the after-tax capital income risk, the tax-rate uncertainty induced by government purchases fluctuations also creates a rate-of-return risk to savings, which in turn, impacts the quality of capital and bonds as saving vehicles.==== In a realistic incomplete asset market model where the after-tax return of all the financial assets is subject to tax rate uncertainty, wealth-rich households have much larger exposure to such a rate-of-return risk. As a result, from the rate-of-return risk perspective, wealth-rich households should gain more when fiscal volatility is eliminated.====Finally, the distributional effects of eliminating fiscal volatility can depend on its effect on the average factor prices.==== The precautionary saving and rate-of-return risk effects lead to endogenous responses of the aggregate capital stock, changing both the pre-tax capital rate-of-return and real wages. In our baseline specification, the aggregate capital stock first declines and then increases after the elimination of fiscal volatility, causing a higher interest rate and lower wage rate in the early transition periods followed by a reversal later on.====Whether the combination of these three effects favors the wealth-rich or the wealth-poor households depends ==== – as we will show – on the details of the implementation of the progressive tax system and the aggregate tax revenue rule. Under the baseline specification, which is calibrated to best mimic the cyclical behavior of key moments of the U.S. tax system, the wealth-rich households are significantly exposed to the rate-of-return risk caused by tax-rate uncertainty, and they also benefit from changes in average factor prices. As a result, we find that the welfare gains are increasing in household wealth. The first contribution of the paper is thus to provide a calibration strategy that allows us to quantify the net effect of the precautionary saving, the rate-of-return risk, and the average factor price effects.====In addition to our baseline, we consider alternative implementations of how the progressive tax system and the aggregate tax revenue rule interplay. The distributional effects of fiscal volatility vary in these exercises, and thus, despite their counterfactual implications, help us uncover the mechanisms through which fiscal volatility influences economic welfare. A second contribution of the paper is thus to map out the relationship between tax instruments in a progressive tax system used to obtain the cyclical adjustment of the government budget and the distributional effects of fiscal volatility.====We also consider several alternative fiscal regimes: for example, a balanced budget regime with a progressive tax system, a linear tax system, and a lump-sum tax system, with the latter two again allowing for government debt. The welfare results under those three regimes are all in line with our baseline. In another variation, we show that when private and public consumption are complements, the overall welfare gains from eliminating government purchases fluctuations are higher, because a higher government purchases level leads to a higher marginal utility of private consumption when taxes are high (because government purchases are large). In addition, we extend our baseline model to allow for a positive fiscal impact multiplier consistent with the data and find similar distributional effects. Finally, motivated by recent policy discussions of the possible permanence of heightened fiscal volatility, we examine the welfare consequences of doubling the historical government purchases volatility level. Our results suggest that the welfare effects of fiscal volatility are symmetric between zero and twice the pre-crisis volatility of government purchases.====In addition to its substantive contributions, our study also makes a technical contribution to the literature. Specifically, we merge the algorithm for computing the ==== transition path in heterogeneous-agent economies from Huggett (1997) and the algorithm for computing a ==== recursive equilibrium in Krusell and Smith (1998) to show that an approximation of the wealth distribution and its law of motion by a finite number of moments can also be applied to a stochastic transition path analysis. Recall that after fiscal volatility is eliminated, our economy is still subject to aggregate productivity shocks. This solution method should prove useful for other quantitative studies of stochastic transition-path equilibria.====Besides the general link to the literature on incomplete markets and wealth inequality (see Heathcote et al., 2009 for an overview), our study is most closely related to three strands of literature.====First, our paper contributes to research on the welfare costs of aggregate fluctuations (see Lucas, 2003 for a comprehensive discussion). As in Krusell and Smith (1999), Mukoyama and Sahin (2006) and Krusell et al. (2009), we quantify the welfare and distributional consequences of eliminating macroeconomic fluctuations. However, while these studies focus on TFP fluctuations, we examine the welfare consequences of eliminating fluctuations in government purchases. Our study complements theirs by examining fluctuations due to fiscal policy, arguably a more plausible candidate fluctuation to be (fully) eliminated by a policy maker – they are, after all, the result of a policy decision.====Second, our paper relates to the recent literature about the effects of economic uncertainty on aggregate economic activity. Most of the research in this stream of literature has focused on the amplification and propagation mechanisms for persistent, but temporary volatility shocks, which are typically modeled and measured as changes to the conditional variance of traditional economic shocks. These uncertainty shocks include second-moment shocks to aggregate productivity, and policy and financial variables, which are often propagated through physical production factor adjustment costs, sticky prices, or financial frictions (see e.g., Arellano et al., 2019, Bachmann and Bayer, 2013, Bachmann and Bayer, 2014, Baker et al., 2016, Basu and Bundick, 2017, Bloom, 2009, Bloom et al., 2018, Born and Pfeifer, 2014, Croce et al., 2012, Fernández-Villaverde et al., 2015, Gilchrist et al., 2014, Kelly et al., 2016, Mumtaz and Surico, 2018, Mumtaz and Zanetti, 2013, Nodari, 2014, Pastor and Veronesi, 2012, Pastor and Veronesi, 2013, and Stokey, 2016). Other studies investigate the effects of uncertainty in the (time-varying) parameters of monetary or fiscal feedback rules (Bi et al., 2013, Davig and Leeper, 2011, and Richter and Throckmorton, 2015), or in the bargaining power parameter of search and matching models (Drautzburg et al., 2017). Our study complements this literature by focusing on the welfare and distributional effects of a permanent change in fiscal volatility.====Finally, our work contributes to the growing body of literature on macroeconomic policy in heterogeneous-agent environments (Auclert, 2019, Bachmann and Bai, 2013a, Bhandari et al., 2017b, Bhandari et al., 2017a, Bhandari et al., 2018, Böhm, 2015, Brinca et al., 2016, Dyrda and Pedroni, 2017, Ferriere and Navarro, 2017, Gornemann et al., 2016, Gomes et al., 2013, Hagedorn et al., 2019, Heathcote, 2005, Hedlund et al., 2016, Kaplan and Violante, 2014; Kaplan et al., 2018, Li, 2013, McKay and Reis, 2016, and Röhrs and Winter, 2017). In particular, Heathcote (2005) provided the first quantitative investigation into aggregate and distributional effects of exogenously varying fiscal policy in a heterogeneous-agent incomplete market model à la Krusell and Smith (1998). In Heathcote (2005), the source of aggregate fiscal risks consists in temporary changes in the level of proportional income tax rates. We provide a complementary analysis by focusing on the effects of a permanent change in the volatility of government purchases in a progressive income tax system. There is also a budding empirical literature on the distributional consequences of policy actions: see Coibion et al. (2017) for the case of monetary policy and Giorgi and Gambetti (2012) for the case of fiscal policy.====The remainder of the paper is structured as follows. Section 2 presents the model. Section 3 discusses its calibration. Section 4 describes our solution method. Section 5 presents the baseline findings on the welfare and distributional effects of eliminating government purchases fluctuations, while Section 6 investigates these welfare and distributional effects in alternative model specifications. We close in Section 7 with final comments and relegate the details of the quantitative procedure to various appendices.",The welfare and distributional effects of fiscal volatility: A quantitative evaluation,https://www.sciencedirect.com/science/article/pii/S1094202520300259,16 April 2020,2020,Research Article,188.0
Pasten Ernesto,"Central Bank of Chile and Toulouse School of Economics, Chile","Received 30 August 2018, Revised 1 April 2020, Available online 9 April 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.003,Cited by (1),"This paper calls attention to the non-trivial (and sometimes pervasive) effects of ex-ante policies, such as prudential policies, on banks' risk taking through their effects on the ex-post incentives to ","After the 2007-2009 financial crisis there has been a surge of “prudential” regulatory and policy proposals. A common premise behind these proposals is that financial intermediaries (in short, banks) misbehave by taking too much risk which leads to financial crises. Prudential policies are then called to correct such a misbehavior and limit its effects on households' welfare. From a different angle, one reason for banks' excessive risk taking is lack of commitment of bailout policy which fuels expectations of large bailouts if a crisis hits.==== This paper studies the interplay between prudential policies and this lack of commitment problem to make a cautionary point on the design and implementation of prudential policies.====In brief, prudential policies work as predetermined variables at the time of crises affecting the benefits and costs involved in a bailout decision sometimes in various, opposite ways. Thus, prudential policies may backfire on inducing banks' risk taking or reinforce their effectiveness depending on their net effect on the authority's incentives to bailout. In particular, a crisis resolution fund, a prudential tax and liquidity requirements are examples of well-intended prudential policies that may backfire. In turn, public debt may play a prudential role when used as a “burning the bridges” policy to raise the welfare cost of bailouts. Beyond these examples, similar mechanism applies for any other prudential policy or, for what matters, any policy before crises affecting the authority's incentives during crises to bailout.====This paper makes this point in an infinitely repeated policy game. The stage game is taken from Farhi and Tirole (2012). Short-lived bankers borrow from short-lived households to invest in riskless assets and partially pledgeable risky assets. When a crisis hits, confronted with bankers' need of refinancing risky assets, an infinitely-lived benevolent authority must decide by how much to reduce the riskfree interest rate by balancing momentary and intertemporal trade-offs. The momentary trade-off arises because the interest rate reduction is a bailout that benefit bankers at the cost of distorting households' consumption. The intertemporal trade-off steams from fueling expectations of large bailouts in the future.====A no-bailouts policy is optimal under commitment as it gives incentives to bankers to hold enough riskless assets to ensure refinancing with no need of costly bailouts. Without commitment, for expositional purposes I first focus on the stage game – or equivalently, when the authority fully discounts future implications of its bailout decisions. In this stage game, any bailout below a certain cap is an equilibrium policy. This is because the authority implements the needed bailout to avoid the loss of bankers' risky assets while households' welfare loss is not too high. As households' utility is concave, their welfare loss is convex on the size of bailouts. This imposes a cap on the maximum bailout admissible in equilibrium.====This equilibrium multiplicity gives raise to a ==== instead of the standard time-inconsistency. In other contexts, such as taxation or monetary policy, lack of commitment yields time-inconsistency as the optimal policy under commitment is not an equilibrium policy. In contrast, for bailouts, the optimal no-bailouts policy under commitment is indeed an equilibrium policy. The problem, then, is that the no-bailouts policy is not the ==== equilibrium policy, so it is fragile to the realization of inferior equilibria when banks' risky investment is exposed to inefficient liquidation during crises.====Focusing on the stage game, I show that prudential policies affect the severity of this fragility problem measured as the size of the largest time-consistent bailout. This is the worst the fragility problem can get. I start with a crisis resolution fund policy motivated by those implemented in the U.S. and Europe after the 2007-2009 financial crisis and the Euro crisis. Consider a tax on bankers' risky investment before crises to finance transfers during crises to avoid the need of costly bailouts. In addition to encouraging bankers to increase their exposure to liquidity risk, this policy has two effects: Before crises, it reduces bankers' financing capacity of risky investment and, during crises, it reduces the burden on households of avoiding the loss of risky assets. The former reduces the benefit of a bailout while the latter reduces its cost. The net effect on the fragility problem is ambiguous and it may be detrimental depending on the probability of a crisis and how pledgeable risky investment is.====The second example is a prudential tax as proposed by Kocherlakota (2010) and Bianchi (2011) among many others to discourage bankers' risk taking. This prudential policy prescribes a tax on bankers' risky investment before crises and a rebate after risky assets return is realized. In the model, the only effect of this policy is to tilt bankers' portfolios more to risk as they offset the effect of the tax on the scale of their risky investment. This is because, from a contracting perspective, the riskless asset return and the rebate are equally pledgeable as both involve certain future income. Thus, the rebate acts as perfect substitute of riskless assets to raise funds for reinvestment in a crisis. As a result, the trade-offs implicit in a bailout decision remain unaffected by this prudential policy.====The third example is the use of public debt with a prudential motivation. Although there are alternative ways to get this effect, the key is that more public debt means more taxes. If taxes have wealth and distortive effects on households, the welfare cost of bailouts increases. This in turn implies that the fragility problem becomes less severe as the largest time-consistent bailout becomes smaller.====Once established the basic mechanism, the paper turns to add the intertemporal trade-off in the bailout decision by allowing the authority to partially discount the future. This forces to introduce a technical discussion. The standard approach for infinite-horizon policy games is Sustainable Plans (Chari and Kehoe, 1990) which, roughly speaking, are defined as time-consistent policies when private agents use a trigger strategy: If they observe the authority deviating from the policy plan in the past, they behave in a way consistent with the worst equilibrium in the stage game. If the authority does not deviate, then such a plan is “sustainable” by some equilibrium in the infinite horizon game. Then, for policy analysis, they propose to focus on the plan yielding highest welfare, which varies from the best time-consistent policy in the stage game when the authority fully discounts the future to the optimal policy under commitment when the authority is patient enough. In the context of bailouts, the optimal no-bailouts policy under commitment is time-consistent in the stage game, so this is the best sustainable plan regardless the authority's discounting. This means that this approach is not suitable for the study of the effect of prudential policies on the authority's intertemporal trade-offs involved in a bailout simply because the best sustainable plan is invariant to the weight given or the size of these trade-offs.====Thus, I propose to focus on a different sustainable plan, the “best resistant bailout plan,” as a refinement. This is the smallest bailout policy (the one with highest welfare) which, in addition to be sustainable, is ==== to the threat of all bankers taking more risk than what is optimal given such a bailout policy (i.e., some risky investment is lost in a crisis). I argue this refinement recovers the logic of best sustainable plans when lack of commitment creates a fragility problem instead of the standard time-inconsistency. As it is sustainable, the same trigger strategy followed by private agents create an intertemporal cost if the authority deviates from the plan. However, in contrast to the best sustainable plan, the best resistant bailout plan does vary with the authority's discounting, spanning from the largest time-consistent bailout in the stage game when the authority fully discounts the future to the no-bailouts policy when the authority is patient enough. Therefore, it embeds the prudential policy analysis in the stage-game as a special case. In addition, it is rather natural criterion to evaluate the fragility problem: It represents the smallest sustainable bailout policy the authority would not deviate from even if some risky investment gets lost in a crisis.====With this criterion at hand, I turn to study liquidity requirements to highlight the working of prudential policies on the intertemporal cost in a bailout decision. Liquidity requirements is appealing because it is an example of many regulations that impose cap restrictions on banks' choices. I show that, although this policy may attain the first best, it may backfire when its enforceability is limited. In particular, when the authority fully discounts the future, this policy monotonically alleviates the fragility problem as it approaches to the first best. However, this monotonicity may be broken when the authority's partial discounting is introduced. This happens when the liquidity requirement policy is more binding when banks take more risk, so it is more welfare improving in the worst equilibrium than in equilibria with smaller bailouts. Thus, the trigger strategy followed by banks implies smaller intertemporal cost of a bailout, so the fragility problem worsens. In general, this result holds for any policy that is more welfare improving (less detrimental) in the worst equilibrium than on the path of the smallest resistant bailout.====Finally, I revisit results in the stage game for a crisis resolution fund, prudential taxes and public debt. For the first, by decreasing bankers' risky investment, the crisis resolution fund is more detrimental on welfare as bailouts are larger. Thus, the crisis fund increases the intertemporal cost of a bailout. If the authority puts enough weight on this cost, this effect dominates its pervasive effect on the momentary cost of bailouts, so the crisis fund may both discourage bankers' excessive risk and reduce the burden on households of bailouts. For the second, as prudential taxes are innocuous on the momentary trade-off involved in a bailout, the same applies for the intertemporal cost. Hence, the result that this prudential policy only tilts bankers' portfolios more to risk also holds here. At last, public debt increases the cost of bailouts by more as bailouts are larger; this reinforces its prudential role as a “burning the bridges” policy that increases the welfare cost of bailouts.====The rest of the paper is organized as follows. Section 2 discusses the contribution of this paper relative to different strands of literature. Section 3 studies prudential policy in the stage game. Section 4 introduces the infinitely repeated game and Section 5 does the prudential policy analysis. Section 6 concludes. An Appendix collects proofs for main results.",Prudential policies and bailouts – a delicate interaction,https://www.sciencedirect.com/science/article/pii/S1094202520300260,9 April 2020,2020,Research Article,189.0
"Damjanovic Tatiana,Damjanovic Vladislav,Nolan Charles","Durham University, United Kingdom of Great Britain and Northern Ireland,University of Glasgow, United Kingdom of Great Britain and Northern Ireland","Received 22 March 2018, Revised 20 March 2020, Available online 9 April 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.04.002,Cited by (4),Should we break up banks and limit ,"The recent financial crisis dramatically reaffirmed that financial instability can induce macroeconomic instability. Similar experience in the past led some to recommend partitioning financial intermediaries into safer and riskier entities and adjusting regulatory practice appropriately. Some proposals were quite radical, but policymakers over time appeared largely to step back from wide-ranging structural reforms. Following the recent crisis, restructuring policies are again being introduced or considered.==== This paper asks: Should we break up banks?====Specifically, we consider vertical integration between risky investment and retail banks. We call ‘investment banking’ the downstream part of financial intermediation which directly finances risky entrepreneurs by purchase of their equities. The investment banks fund their equity stake by borrowing from retail banks. Retail banks, the safer part of financial intermediation, are funded by private agents' deposits. Initially the problems facing the retail banks and the investment banks are set out separately; these institutions are then ‘merged’ to model universal banking. The model also incorporates protection for retail depositors similar to aspects of the Glass-Steagall Act and the wider response (e.g., deposit insurance) to the Great Depression.==== There are few macroeconomic models in the literature appropriate for assessing such structural reforms. This paper is an attempt to begin filling that gap.====The model has three distinctive features. First, investment banks, having leveraged equity stakes in intermediate goods producers, have projects with uncertain returns. They choose the expected profit maximizing level of borrowing before demand conditions are known, so determining the likelihood of their defaulting. Second, retail and investment banks have monopolistic pricing power and enjoy a form of limited liability. Although harvesting assets of a failed bank is costly, that bank continues trading next period without carrying over the loss. An alternative description is that the bank goes bust and is replaced next period so that market structure is identical period-to-period. In any case, limited liability makes banks ==== risky by narrowing credit spreads, whilst monopoly power widens spreads.====Finally, there is a rich menu of shocks. Investment and universal banks face idiosyncratic and common shocks. Retail banks can diversify idiosyncratic risk. Merging retail banks with investment banks puts depositors' funds at greater risk. Hence, depending on the size of common and idiosyncratic shocks and whether or not banks are universal, the economy is relatively well-insured against, or vulnerable to, financial shocks.==== The main disadvantage of separated banking compared with universal banking is higher borrowing costs for firms. That reflects the retail bank spread (mark-up), in turn the net effect of monopolistic power, limited liability and a risk premium.==== The main benefit of separated banking is fewer calls for government bailout of retail banks: Retail banks are diversified and profitable so the probability and size of default are lower than universal banks'. Bailouts mean government incurs deposit insurance and resolution costs,==== plus an excess cost of those actions. Increased bailouts are the main disadvantage of universal banking==== but these are set against its main advantages; higher lending, as the retail spread is eradicated, and higher average output and consumption. So the attractiveness of universal banking depends importantly on the efficiency of government in bank resolution: the more efficient is government, the more attractive is universal banking.====The calibrated model shows limited liability is usually a minor determinant of bank spreads compared to monopoly power. So, along with banking structure, we consider regulatory policies covering bank profits/mark-ups (the only source of own funds in the model). Compared with competitive equilibria, optimal regulation further limits bailouts via higher own funds for universal and investment banks, along with strict anti-trust of retail banks.","Default, bailouts and the vertical structure of financial intermediaries",https://www.sciencedirect.com/science/article/pii/S1094202520300272,9 April 2020,2020,Research Article,190.0
"Kurtzman Robert,Zeke David","Federal Reserve Board of Governors, United States of America,University of Southern California, United States of America","Received 14 February 2018, Revised 24 March 2020, Available online 3 April 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.03.006,Cited by (2),"Central bank large-scale asset purchases, such as the purchase of securities of nonfinancial firms, can induce a misallocation of resources through their heterogeneous effect on firm cost of capital. First, we analytically demonstrate the mechanism in a two-period model. We then evaluate the misallocation of resources induced by corporate security purchases and the associated output losses in a calibrated heterogeneous firm New Keynesian ====. The calibrated model suggests misallocation effects from corporate security purchases can be large enough to make them less effective in increasing output than government bond buys.","Central banks, such as the Bank of Japan (BOJ), the Bank of England (BOE), the European Central Bank (ECB), and the Federal Reserve (the Fed), have used large-scale asset purchases (LSAPs) as a policy tool once they have reduced the short rate under their control to its effective lower bound.==== While the BOJ, BOE, ECB, and the Fed have all purchased government bonds as part of their LSAP programs, as they have chosen to buy other assets, their choices have differed: between them, the different central banks have purchased corporate securities, exchange-traded funds, mortgage-backed securities (MBS), and other assets. Yet, there is little theory to guide central banks on the tradeoffs associated with different types of private asset purchases.====This paper addresses this gap in the literature, focusing specifically on the costs associated with large-scale purchases of nonfinancial corporate securities. Purchases of corporate securities may reduce financing costs more for firms whose securities are purchased than for those whose securities are not purchased and potentially create distortions in the cost of capital among firms.==== In standard models of firm financing and capital choice, differences in the cost of capital induce differences in firm investment decisions and thus the allocation of capital, which has consequences for the efficiency of the allocation and aggregate output. Our work builds a simple theory of LSAPs with this mechanism present and delivers analytical results regarding how large-scale purchases of corporate securities affect the allocation of capital among firms. We then introduce the key elements of our simple model into a New Keynesian DSGE model. With our calibrated DSGE model, we quantify the potential misallocative effects of large-scale purchases of nonfinancial firm corporate securities.====To demonstrate the economic mechanism at work within our DSGE model, the first part of the paper takes a two-period model of firm dynamics with a financial intermediary sector and demonstrates the conditions under which a large-scale purchase of nonfinancial firm corporate securities by the central bank affects the allocation of resources. In our model, following Gertler and Karadi (2011) and Gertler and Karadi (2013) (hereafter, GK11 and GK13), a financially constrained intermediary helps facilitate the financing of capital by firms. Intervention by the central bank in asset markets affects the quantity and distribution of assets that are intermediated and the constraint faced by the financial intermediary. Our simple model introduces the two key additional model elements we use to study the effects of misallocation.==== The first is heterogeneity in production among multiple “groups” of firms. Specifically, we allow there to be two (or more) types of intermediate good firms that have similar production technologies. Their outputs are used in the production of the final good and are imperfect substitutes.====Second, we introduce a nonlinear collateral constraint that allows for richer heterogeneity in spreads than in the model of GK13. We show that a nonlinear constraint can be derived from a limit on the variance of returns to bank equity holders. If all firms have proportional exposures to aggregate shocks, then such a limit leads to a standard linear collateral constraint.==== However, more generally, when assets have heterogeneous risk exposures to aggregate shocks, the limit on the variance of returns to bank equity holders yields a nonlinear constraint, which effectively penalizes holding assets with similar risk exposures.====Using our two-period model, we show that when the central bank buys government bonds, if spreads between securities of various types of firms (large and small firms, for instance) are small in steady state, the central bank reduces the cost of capital for all firms approximately evenly. This occurs because government bond purchases affect the concentration of government bonds in intermediaries' portfolios but do little to affect the relative concentration of large and small firm security holdings, all else being equal. However, when the central bank buys the securities of only one set of firms, it reduces the concentration of those firms' securities in the portfolios of financial intermediaries, which reduces the exposure of intermediary portfolio returns to the risks inherent in those securities. This lowers the spread on the purchased securities relative to others, which affects the allocation of resources.====To assess the quantitative importance of this mechanism for large-scale purchases of different types of securities, we nest our two-period model within a DSGE model which incorporates additional model elements: sticky prices, endogenous net worth of banks, and a representative household with habits that can hold securities facing a holding cost. We calibrate the new parameters using U.S. data. In the DSGE model, the response of credit spreads to an asset purchase is not only heterogeneous, as in our two-period model, but also time-varying. The calibrated model suggests a QE policy in which the central bank purchases government debt produces a large, positive effect on investment and output after a bad shock. However, a QE policy in which the central bank purchases the securities of large firms — although potentially inducing a similar effect on output — reduces the response of investment for small firms whose securities are not purchased by the central bank and induces a non-negligible misallocation of resources. In fact, our calibration implies that the misallocation effect is large enough to make a government bond purchase ==== than a private security purchase in increasing output — even though a government bond purchase is ==== in increasing output than a large-scale corporate security purchase when one ignores misallocation effects.====The misallocation effect is sizable relative to movements in output away from the zero lower bound (ZLB); however, at the ZLB, the average effects of LSAPs on spreads and thus output are larger, while the misallocation effect is about the same size. Therefore, the misallocation effect as a percentage of the potential output gain from large-scale corporate security purchases is smaller at the ZLB. Moreover, the relative size of the misallocation effect as compared to the output response to LSAPs can shrink even away from the ZLB. If there is a significant degree of interest rate smoothing, corporate security purchases can stimulate output by more than government security purchases.====The rest of the paper, after the literature review below, follows as such. Section 2 presents the simple model and the associated analytical results. Section 3 describes the New Keynesian DSGE model. Section 4 discusses the calibration and performs quantitative exercises with the DSGE model. Section 5 concludes.",Misallocation costs of digging deeper into the central bank toolkit,https://www.sciencedirect.com/science/article/pii/S1094202520300247,3 April 2020,2020,Research Article,191.0
"Coffey Bentley,McLaughlin Patrick A.,Peretto Pietro","Department of Economics, University of South Carolina, United States of America,Mercatus Center at George Mason University, United States of America,Department of Economics, Duke University, United States of America","Received 21 May 2018, Revised 3 March 2020, Available online 2 April 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.03.004,Cited by (33),We estimate the effects of federal regulation on the value added to GDP for a panel of 22 ,"Regulations have long been the primary tool used by policymakers to advance their goals when free markets fail to deliver the outcomes that they desire. Even if we could thwart all attempts of rent-seeking forces to capture the regulatory process as described by Stigler (1971), skeptics have long suspected that the “law of unintended consequences” often undermines even otherwise virtuously conceived regulations in a complex and dynamic economy (Peltzman, 1975). Economists' efforts to gain an understanding of regulations' consequences, which can potentially be opposing in their effects on the economy's output, has laid the groundwork for a growing literature.====We add to this literature primarily by examining the cumulative effect of federal regulation on the United States economy, based upon the observation that the accumulation of an increasingly complex set of regulatory constraints is arguably the most striking characteristic of the history of regulations in the US. We model this cumulative effect in the context of endogenous growth. First, we build a formal model of economic growth from microeconomic foundations: growth is based on firms' decisions to invest in improving their productivity. By allowing our measure of regulations to flexibly enter into our econometric model, we rely on the data to tell us how much regulation distorts the investment decisions of firms and thus hampers long-run economic growth. Hence, our model features enough complexity in detail and flexibility in specification to capture the aggregate effect of regulatory accumulation as a drag on the long-run macroeconomy. Simultaneously, our model accommodates a rich mix of short-run outcomes in output, (net) entry, and investment in particular industries, which can be spurred by some types of regulations.====We have constructed our theoretical model from a well-established model of endogenous growth (Peretto and Connolly, 2007).==== This style of model has consistently received strong support in empirical investigations of competing models of economic growth (Ang and Madsen, 2011).==== To conduct our analysis, we have extended the model from a single-sector economy to a truly multisector economy, where the growth process for each heterogeneous industry is governed by a set of linear equations that can be influenced by regulatory shocks. Ours is one of the first multisectoral endogenous growth model with closed-form solutions.====We then estimate our model using industry data from the Bureau of Economic Analysis (BEA) and the US Census Bureau in combination with RegData 2.2, a publicly available text-based-quantification of regulation by industry over time from the RegData project (Al-Ubaydli and McLaughlin, 2015; McLaughlin and Sherouse, 2019). The available data permit us to estimate our model for a panel dataset of 22 industries observed annually from 1977 to 2012. Specifically, we examine not only the direct effect of regulation on the output per employer establishment and (net) entry of additional establishments but also the indirect effect of regulation on three different types of investment that in turn affect the output per establishment and (net) entry of additional establishments.====This combination of modeling and data enables us to estimate the effects of regulation on investment in an endogenous growth context. In endogenous growth theory, innovation is not an exogenous gift from the gods but rather the result of costly effort expended by firms to realize gains. The growth generated by that entrepreneurship can be stunted by misguided public policy. By deflecting firm resources away from the investments that maximize the stream of profits and toward regulatory compliance, regulations can theoretically slow the real growth of an industry. Indeed, each additional (binding) regulation should be seen as a (binding) constraint on the firm's profit maximization problem. Yet some regulations may yet succeed in enhancing growth. For instance, if a regulation does manage to efficiently correct a market failure, some deadweight loss in output will be recovered by that regulation. Alternatively, a regulation requiring the use of certain inputs like safety equipment can address a prisoner's dilemma in accumulating that capital which can enhance labor productivity. Naturally, these regulations can cause a select set of industries to grow faster, at least in the short run.====These countervailing effects of regulation can now be handled in an empirical study because RegData provides separate observations for multiple sectors over a relatively long time frame. We show that the average industry does indeed experience a slower growth rate as a result of the accumulation of regulations that target the industry. However, a handful of industries do appear to grow faster because of regulation. Nonetheless, our results are dominated by the slower growth caused by regulations' distortion of firms' investment decisions. On net, cumulated regulations slow the growth of the entire economy by an average of 0.8 percent per annum. Because economic growth is an exponential process, this seemingly small figure grows powerfully over time into a truly dramatic difference in the level of GDP per capita. Had regulations been held constant at levels observed in 1980, our model predicts that the economy would be nearly 25 percent larger. In other words, the growth of regulation since 1980 cost the United States roughly $4 trillion in GDP (nearly $13,000 per person) in 2012 alone.====Of course, our estimate says little about the benefits of regulation, aside from those that are captured within GDP. Some regulations may lead to benefits, such as improvements in environmental quality, which are well known to be mainly missing from GDP measurements. Other regulations, such as those designed to prevent monopolistic practices, or even those designed to improve human health, may only be captured in GDP to a limited degree. For example, if regulations decrease employee absenteeism because they reduce asthma-inducing air pollution, we would expect that positive health effect to register as a marginal increase in GDP. Nonetheless, we caution that this study is not a weighing of the costs and benefits of regulation. It is an examination of how regulatory accumulation in specific sectors of the economy affects the growth path of those sectors, and a deduction from our findings of how the effects of regulation change the US economy's growth path.====The remainder of this study proceeds as follows. Section 2 expands on the background briefly described in this introduction. Section 3 describes how we measure regulation, as well as the other data used in our estimation. Our theoretical model is outlined in section 4. Section 5 details our estimation methodology. We explain our results in section 6. Section 7 concludes our discussion.",The cumulative cost of regulations,https://www.sciencedirect.com/science/article/pii/S1094202520300223,2 April 2020,2020,Research Article,192.0
"López José Joaquín,Torres Jesica","University of Memphis, United States of America,The World Bank, United States of America","Received 16 June 2018, Revised 16 March 2020, Available online 23 March 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.03.005,Cited by (3),We study the allocation of talent in knowledge-based hierarchies subject to a ,"Workers in large establishments are, on average, more skilled than workers in smaller plants. Further, larger establishments tend to be run by more educated managers, relative to those of smaller size. If more talented individuals form larger establishments, which, in turn, tend to have a larger share of highly skilled workers, then policies that affect plants of different sizes differently will distort the allocation of talent, as well as the return to skill. In this paper we argue, in particular, that regulations which favor small businesses misallocate talent throughout the entire economy, and lower the return to skill.====The relationship between skill composition and establishment size holds across countries, and for different measures of skill and size. Headd (2000) reports that in the U.S. small firms are more likely to employ workers with a high school diploma or less, whereas workers with at least some colleges are more likely to work in larger firms. Also for the U.S., Cardiff-Hicks et al. (2014) find that higher quality workers are sorted into large firms and large establishments in retailing. Fox (2009) documents evidence for Sweden consistent with hierarchical matching, while Busso et al. (2012) document a positive relation between cognitive skills and firm size for both employers and employees in OECD and Latin American countries.====In this paper, we study the allocation of talent in a knowledge economy where plants are subject to size-dependent regulations, which we model as a tax on labor that increases with the number of employees in the establishment. To this end, we develop a model where production is organized in knowledge hierarchies, building on the work of Garicano and Rossi-Hansberg, 2004, Garicano and Rossi-Hansberg, 2006. We make two extensions to their original framework that allow us to analyze how individuals with heterogeneous abilities are sorted into occupations and into plants of varying sizes, while delivering a more realistic distribution of sizes relative to the original model. First, we allow communication costs between production workers and managers to decrease with the skill of wage workers. Second, we embed this environment into a production economy subject to decreasing returns to scale, as in Lucas (1978). The latter modification provides a way to organize production under certain distortions that break down positive sorting, as well as comparability with the rest of the misallocation literature. Under a specific parameterization detailed below, our environment generates closed-form solutions for all equilibrium objects, including a Pareto distribution of plant sizes.====In the model, the skill of an agent completely determines his occupation, the quality of his match, as well as the size of his plant, and the return to skill derives from matching with more talented workers in larger plants. The size-dependent tax encourages managers to constrain the size of their productive unit, which they achieve by matching with workers of lower skill. The tax distorts not only the allocation of the marginal individual in each occupation, but also the sorting of the infra-marginal workers, thus attenuating the strength of the positive sorting throughout the entire economy. Both talent misallocation and a lower return to skill originate from the same source: distortions on occupational choices, which reorganizes production by re-sorting everyone within occupations, generating a talent mismatch.====To understand the magnitude of these effects, we first estimate a tax policy to match the establishment-level evidence on the size-dependent compliance of social security contributions in Mexico, as documented in Busso et al. (2018). Our tax policy features a finite asymptotic rate characterized by a single parameter that can be set to match any statutory rate, whereas the degree of progressivity (enforcement) is also summarized by one parameter. Thus, our proposed tax policy improves on some of the issues, while maintaining the advantages, of the popular tax policy featured in Benabou (2002), which is used by Guner et al. (2018) to study the effects of hypothetical size-dependent policies.====We then calibrate two benchmark economies: an undistorted economy with parameters set to match some features of the distribution of establishment sizes and the allocation of workers across occupations in the U.S., and a distorted version with our fitted tax policy, where the parameters are set to match the observed allocation of talent, as well as several moments of the distribution of establishment sizes in Mexico. In a recent paper, Poschke (2018) argues that differences in entrepreneurial technology and—to a lesser degree—the severity of (general, hypothetical) size-dependent distortions, account for most of the variation in average firm sizes across countries. Our findings are largely consistent with this view. Our calibrated model economy for the U.S. features a superior communication technology, higher returns to scale, more abundant skill, and more complicated production problems than our calibrated distorted benchmark for Mexico, which suggests that technology and skills are of first-order importance to understand the observed differences in establishment size distributions and occupational shares across countries.====We then conduct two sets of counterfactual experiments. We start by introducing two different policies in our undistorted U.S. benchmark. First, we introduce the size-dependent tax calibrated from the micro-evidence on labor regulations in Mexico. Under this tax, self-employment increases more than two-fold, to a level close to that observed in Mexico, and the returns to skill for wage workers (managers) decrease by 75 (3) percent. The resulting misallocation of talent has the best wage workers turn into self-employment, thus reallocating the best managers to smaller teams comprised of employees of lower skill. When we isolate the margins of misallocation in our model, we find that slightly over half of the output losses are due to talent mismatch, whereas the rest is accounted for by the best wage workers turning into self-employment.====Even though the average establishment size decreases by only 10 percent as a result of the size-dependent tax, output and average plant productivity decrease by 10 and nine percent, respectively.==== Thus, the positive sorting mechanism generates a nearly one-to-one positive relationship between percentage changes in plant size and output, which is considerably stronger than what has been previously documented (e.g., see Bento and Restuccia (2017)). Size-dependent policies in our model generate a reduction in average plant size that is only a fifth of that obtained using a standard span-of-control model calibrated to match the same moments of the establishment size distribution, yet the implied output losses are three times as large.====In the second experiment using our U.S. benchmark, we introduce a perfectly-enforced proportional tax of the same level as the average tax obtained under size-dependent enforcement. The effects of a flat tax are large: they account for more than half the output and productivity losses from the size-dependent tax. However, the effects on the returns to skill are much different. Workers at the bottom of the skill distribution are the main losers from the flat tax, which implies that the returns to skill for wage workers increase modestly.====Next, we investigate how our model can address a set of documented facts about the Mexican economy: persistently-high levels of business ownership (especially self-employment), a proliferation of small businesses, a growing mismatch between the supply and demand for skill, and high statutory labor costs that are more stringently enforced on larger establishments. To this end, we take our distorted benchmark calibrated to Mexico, and then consider two possible counterfactual policy scenarios: complete elimination of the payroll tax, and perfect enforcement of the average effective tax rate under size-dependent enforcement. A complete elimination of the payroll tax has large positive effects: self-employment drops by more than 40 percent, and wage employment increases by 13 percent. Output and average plant productivity increase by nine and eight percent, respectively. Average returns to skill for workers increase by 14 percent, and, while all workers see an increase in their wages, high-skill workers experience the largest gains. Perfect enforcement of the average tax accounts for slightly over one fifth of the output and productivity gains from eliminating the tax altogether. Wage workers reap the highest benefits from this policy, as their average returns to skill increase by 35 percent, which represents almost two thirds of the gains from dispensing with the payroll tax. However, these gains are not evenly distributed across skill levels: high skill wage workers benefit the most, while low skill workers see a reduction in wages, brought about by the higher taxes paid by smaller plants.====We find that the skill composition of productive units, which arises endogenously from positive sorting, is crucial to fully understand the effects of plant-specific distortions. In doing so, we fill an important gap in the vast literature on misallocation spurred by Restuccia and Rogerson (2008) and Hsieh and Klenow (2009), where the skill composition of plants has remained absent. Further, by fully considering the role of the skill distribution, our work complements the literature on size-dependent policies. For instance, the works of Braguinsky et al. (2011), Garicano et al. (2016), and Guner et al. (2008), consider heterogeneity in managerial skill, but this talent becomes useless if agents choose to work for a wage. The evidence on positive sorting in quality and quantity discussed above suggests potentially larger aggregate effects through talent misallocation. Indeed, output losses from size-dependent policies in our model are at least three times larger than those in a standard span-of-control model calibrated to the same moments of the establishment distribution in the U.S. In a closely related paper, Alder (2016), shows that deviations from positive sorting between CEO and projects (firms) can have sizable aggregate effects, depending on the degree of complementarity between projects and managers, as well as the correlation between mismatch and project quality. Our paper differs from Alder's in that we focus on the effects of specific, observable size-dependent distortions on talent misallocation across the entire skill distribution—including wage workers, the self-employed, and managers—as opposed to the allocation between heterogeneous managers and projects of varying quality.====Another important contribution of our paper is showing that size-dependent regulations could significantly lower the average return to skill in the economy. Our model features superstar effects, in the sense that the earnings schedule is convex in ability, as in Rosen (1981) and, more recently, Scheuer and Werning (2017). Therefore, because the best managers are matched with the best wage workers, distortions that increase with establishment size act as an increasing marginal tax schedule that attenuates the strength of the positive sorting, lowering the return to skill for both wage workers and managers. The effects of size-dependent regulations on the returns to skill has remained absent from the literature in part because most misallocation models treat labor as a homogeneous input. In our model, just as in Garicano et al. (2016), the main losers from size-dependent regulations are wage workers. However, we are able to provide more nuance to that effect: it is the most talented wage workers that suffer from size-dependent enforcement.====Our model predicts misallocation of talent and a lower return to skill as consequences of size-dependent policies for a given distribution of skills. That means that even if individuals became more skilled for other reasons, they would still be misallocated as long as the size-dependent policies persisted. This prediction is in line with the evidence for Mexico presented in Levy and López-Calva (2016), who document a growing mismatch between the supply and demand for skilled labor: while the amount of available skilled workers has grown, the earnings of more educated workers have decreased. While Levy and López-Calva (2016) suggest a link between size-dependent policies and the return to skill, our paper is the first to formalize such link.====Our work also contributes to the theory of Pareto distributions of plant sizes by providing a specific example of how a Pareto team-size distribution can arise from positive sorting between heterogeneous workers and managers. Our result is closely related—yet independently and contemporaneously developed—to the more general theory of Pareto distributions by Geerolf (2017). The Pareto shape is the result of positive sorting in a purely static environment, and stands in contrast with previous work where Pareto size distributions arise from either assuming that some primitive distribution is itself Pareto—e.g. managerial talent, or firm-level productivity—as in Lucas (1978), or from a long sequence of large and persistent positive shocks, as in Luttmer (2007), investment in managerial skills as in Guner et al. (2018) and Bhattacharya et al. (2013), or skill-biased change in entrepreneurial technology, as in Poschke (2018).====Finally, we argue that the positive-sorting framework offers new insights on the sources and consequences of misallocation. In economies that exhibit positive sorting, the best managers match with the best wage workers to form the largest plants. Thus, distortions that affect the production decisions of the largest productive units will generate a trickle-down effect, re-sorting workers within and across occupations, which is ultimately reflected in reductions in average size, and output. We find that the level of the tax is of first-order importance for the magnitude of the output losses, while the degree of enforcement affects the returns to skill. In other words, flat taxes can have large effects on output, but do not impact the returns to skill in a negative way, whereas size-dependent enforcement disproportionately lowers the wages of the best employees.====The presence of two-sided heterogeneity—where heterogeneous labor is allocated to heterogeneous producers—implies that larger productive units exhibit a higher marginal product of labor relative to smaller ones. This positive relationship between size and marginal products is strongest when complementarities in production are not hindered by the presence of size-dependent distortions. That is, an undistorted economy with knowledge hierarchies exhibits a larger dispersion of marginal products, and a steeper relationship between size and marginal product, than an economy under size-dependent regulations. These results imply that some of the dispersion in marginal products observed in the data may be “good” dispersion—in the sense that plants differ in the skill composition of their workforce—and, in fact, in economies affected by size-dependent policies, there may be too little of it.====The rest of the paper is organized as follows. In the next section, we present an overview of our modified version of the hierarchical model of Garicano and Rossi-Hansberg, 2004, Garicano and Rossi-Hansberg, 2006, and completely characterize a parametric example that serves as the basis of our analysis. We also describe the qualitative effects of different tax policies. Section 3 contains a description of the data used in our analysis—including a set of motivating facts about the Mexican economy that our model can address—as well as a description of our calibration strategy. The results from our counterfactual experiments for the U.S. and Mexico are reported in Section 4. Section 5 details the main takeaways from our analysis and Section 6 concludes.","Size-dependent policies, talent misallocation, and the return to skill",https://www.sciencedirect.com/science/article/pii/S1094202520300235,23 March 2020,2020,Research Article,193.0
"Candian Giacomo,Dmitriev Mikhail","HEC Montréal, Canada,Florida State University, United States of America","Received 6 March 2018, Revised 9 March 2020, Available online 19 March 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.03.003,Cited by (8),"We develop a tractable model to study jointly the role of non-diversifiable risk and financial frictions for business cycles. Non-diversifiable risk induces strong precautionary motives, which reduce the exposure of entrepreneurs to aggregate disturbances ex-ante, and make it easier to increase leverage ex-post. In ","According to Knight (1921), bearing risk is one of the defining features of entrepreneurship. Entrepreneurs are exposed to significant undiversified risk (Gentry and Hubbard, 2004) and face substantial dispersion in equity returns (Moskowitz and Vissing-Jorgensen, 2002). When entrepreneurial activity depends on external finance, the presence of this large undiversifiable risk may have important implications for macroeconomic dynamics. Yet, the theoretical literature on financial frictions has paid little attention to how entrepreneurs' ==== to take on this risk affects the transmission of shocks over the business cycle. Indeed, general equilibrium models with credit market frictions assume that idiosyncratic risk is either absent (Kiyotaki and Moore, 1997), fully diversified (Forlati and Lambertini, 2011; Liu and Wang, 2014; Dmitriev and Hoddenbagh, 2017), resolved at the beginning of the period so that there is no within period uncertainty (Bassetto et al., 2015) or present but coupled with the assumption of risk-neutral entrepreneurs (Bernanke et al., 1999, BGG).====In this paper, we fill this gap by introducing uninsurable idiosyncratic investment risk for risk-averse borrowers into a standard business cycle model. Our framework is flexible enough to study the consequences of uninsurable risk in an otherwise standard New Keynesian model, as well as to analyze the interaction of the former with agency frictions, yet maintaining an analytically tractable, log-linear setup.====We show that non-diversified risk mitigates agency frictions and stabilizes business cycles. Indeed, in the presence of asymmetric information the response of output to financial shocks, such as risk and wealth redistribution shocks, is 35 to 80 percent smaller when entrepreneurs are risk averse than when they are risk neutral. Additionally, the responses of key macro variables to technology and monetary shocks are about 20 percent smaller when idiosyncratic risk is non-diversified.====This paper uncovers a novel interaction between idiosyncratic and aggregate risk, showing that the former dampens the effect of aggregate disturbances in the presence of financial frictions. The key idea is that idiosyncratic risk induces strong precautionary motives, which reduce the exposure of entrepreneurs to asset price movements ex-ante, and make it easier to increase leverage ex-post. In general equilibrium, these precautionary motives stabilize asset prices and risk premia, thus making the economy more resilient to aggregate shocks.====We consider an environment where entrepreneurs use their net worth and funds borrowed from households to invest in physical capital, whose returns are subject to both aggregate and idiosyncratic risk. If there are no agency frictions and idiosyncratic risk is fully diversified, our model collapses to a standard New Keynesian framework, where capital returns are equal to the safe rate of interest up to a first-order approximation. In this model, the dynamics of net worth are irrelevant for economic outcomes, as entrepreneurs freely substitute equity with debt consistently with the Modigliani-Miller theorem.====If lenders cannot verify the borrower's return ex-post because of agency problems, they charge a premium on external finance as a compensation for the cost of defaults. This external finance premium drives a wedge between the returns to physical capital and safe rate of return. In bad times, when asset prices and net worth are low and expected returns to capital are high, agency costs prevent entrepreneurs from borrowing enough to support purchases of physical capital, as higher borrowing increases the risk of default. This leads to even lower asset prices and net worth, and even higher cost of defaults. As a result, the premium on external finance becomes highly volatile and countercyclical, thereby amplifying business cycle fluctuations.====Introducing risk aversion results in lower leverage and creates an additional risk premium relative to risk neutrality, as entrepreneurs require compensation for the volatility of their returns associated with uninsurable idiosyncratic risk. This precautionary behavior helps in bad times for three reasons. First, lower leverage reduces losses from falling asset prices. Second, the external finance premium becomes less important for risk-averse borrowers. Third, risk-averse entrepreneurs have more flexibility to raise borrowing and invest in bad times if the risk premium increases. These precautionary motives support asset prices in recessions, preventing a further decline in net worth. The wedge between capital returns and the safe rate, now reflecting mostly the risk premium, becomes less countercyclical and less volatile, which stabilizes the business cycle. We found these results to be robust to empirically plausible values of risk aversion and sizes of idiosyncratic risk.====We turn to microeconomic data to study empirically the firm-level relationship between investment and capital returns in the presence of non-diversified idiosyncratic risk and risk aversion. We use data on insider ownership, as in Panousi and Papanikolaou (2012), based on the intuition that when ownership is more diversified, entrepreneurs behave in a more risk-neutral way with respect to idiosyncratic risk. The findings indicate that firms with higher insider ownership exhibit a stronger precautionary behavior and, importantly, a higher responsiveness of investment to future returns to capital. One important concern with this exercise is that insider ownership could be endogenous and could be correlated with firm characteristics that might be affecting the investment-returns relation. We find that our results are robust when we make insider ownership fixed over time and when we control for the firm size, age, and investment opportunities, as proxied by Tobin's ====. This suggestive evidence is consistent with the key channel of our model that delivers the stabilizing effects of non-diversified idiosyncratic risk and risk aversion.====Methodologically, this is the first work that incorporates risk-averse borrowers in a model of idiosyncratic, uninsurable investment risk and agency frictions, while keeping the analytical tractability and a solution by standard perturbation techniques. A common challenge with incomplete-market models is that the wealth distribution—an infinite-dimensional object—is a relevant state variable for aggregate dynamics. We address this difficulty using overlapping generations of entrepreneurs, where only new-borns work, so that future labor income does not affect financial decisions. Under these assumptions, the entrepreneurial problem is homothetic in net worth, which implies that aggregate quantities and prices in equilibrium are invariant to the wealth distribution.====We solve the model using a two-step procedure that is similar in spirit to Reiter (2009) and Winberry (2018). First, we compute for the steady state of the model, where there are no aggregate shocks but still idiosyncratic shocks. Noticeably, this steady state does not display certainty equivalence because of the presence of risk-averse agents and idiosyncratic shocks. We then solve for the aggregate dynamics using first-order perturbation techniques around this steady state. We also consider a second-order perturbation of the model to compare how aggregate shocks affect households' utility in two economies characterized by risk-neutral and risk-averse entrepreneurs.==== ====We build on the literature that studies the role of non-diversified risk and incomplete markets in business cycles, starting from Kimball (1993), Krusell and Smith (1998) and more recently Angeletos and Calvet (2005), and Angeletos (2007). Our work differs from theirs, in that we study the interaction of non-diversified risk and agency frictions. Covas (2006) examines the link between precautionary motives and steady-state aggregate capital in an environment in which entrepreneurs face idiosyncratic investment risk and borrowing constraints. Meh and Quadrini (2006) study the long-run implications of risky investment for capital accumulation in an environment with asymmetric information. In contrast to these authors, our focus is on the implications of non-diversified entrepreneurial investment risk for the transmission of macroeconomic shocks over the business cycle.====More recently, Bassetto et al. (2015) study business cycles with entrepreneurs that are subject to financial constraints and idiosyncratic risk. They find that credit shocks have a very persistent effect on economic activity by eroding entrepreneurial net worth. Relatedly, Zetlin-Jones and Shourideh (2017) consider an environment where firm ownership is not diversified and obtain a large effect of financial shocks in a one-sector version of the model. In contrast to our paper, the precautionary motives in both of these works are limited since idiosyncratic uncertainty is resolved before capital and labor choices are made. Our results suggest that such amplification effects are muted when entrepreneurs make their capital choice before idiosyncratic shocks are realized, as precautionary behavior limits their exposure to adverse shocks ex-ante and allows them to increase their leverage ex-post.====Arellano et al. (2019) build a model with idiosyncratic risk and financial frictions, and show that a realistic increase in firm-level volatility can generate most of the decline in output and labor observed in the Great Recession. Both our paper and theirs feature precautionary incentives, but they differ in the extent to which these incentives can translate into actions. In their setup, uncontingent debt contracts and managerial moral hazard greatly limit the ability to self-insure. By contrast, in our model the optimal financial contract between lenders and entrepreneurs results in a stronger precautionary behavior, which dampens the effect of firm-level volatility shocks.====Our findings are also related to Dmitriev and Hoddenbagh (2017) and Carlstrom et al. (2016), who show that in models with agency frictions à la BGG, indexation of lenders' repayments to aggregate variables stabilizes business cycle fluctuations. In their setup, entrepreneurs effectively buy insurance for aggregate risk from households in order to limit balance sheet movements. By contrast, here we study a setting in which borrowers are unable to insure their consumption either from aggregate or from idiosyncratic risk. Thus, while Dmitriev and Hoddenbagh (2017) and Carlstrom et al. (2016) suggest that insuring aggregate risk stabilizes the economy, we show that hedging idiosyncratic risk increases the economy's vulnerability to aggregate disturbances and raises the household's cost of business cycle fluctuations.====The paper proceeds as follows. In Section 2, we conduct a partial-equilibrium analysis of non-diversifiable risk with and without agency frictions. Section 3 incorporates the partial-equilibrium results into the general equilibrium framework. Section 4 contains our quantitative analysis. Section 5 conducts the empirical analysis using firms-level data, and Section 6 concludes.","Risk aversion, uninsurable idiosyncratic risk, and the financial accelerator",https://www.sciencedirect.com/science/article/pii/S1094202520300156,19 March 2020,2020,Research Article,194.0
Willems Tim,"International Monetary Fund, 700 19th Street, 20431, Washington DC, United States","Received 15 April 2019, Revised 4 March 2020, Available online 13 March 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.red.2020.03.002,Cited by (3),"As the “Volcker shock” is believed to have generated useful information on the effects of ==== than emerging/developing countries, as the former combine a more muted price response with a larger effect on output.","If one is seeking to uncover the causal effect of changes in monetary policy, one would like to have access to shifts in monetary policy that are free of endogenous and anticipated movements (cf. Romer and Romer (2004: 1055); Cochrane (2018: 94-95)). This paper aims to find such shifts by searching for significant rate hikes following a protracted period of loose monetary policy (giving the Central Bank a dovish reputation, adding plausibility to the assumption that the exact timing of the hike was unanticipated), where the Central Bank's objective seems to have been fighting inflation (yielding exogeneity with respect to output). Herewith, this paper tries to identify non-US equivalents to the “Volcker shock”, which is believed to have improved our understanding of monetary policy (see e.g. Romer and Romer (1989), Summers (1991), and Goodfriend and King (2005)).====Causal analysis of the effects of monetary policy is complicated by the fact that monetary policy often ==== economic developments. For example, the observation that high interest rates tend to coexist with high growth rates could easily lead to the mistaken conclusion that high interest rates cause high rates of growth. Further complications arise because many changes in the policy stance are anticipated: if a Central Bank has a strong record of responding to inflation, an anticipated acceleration of inflation will immediately lead to expectations of future rate increases. Subsequent materializations of such rate hikes might prove uneventful (as they were already “priced in” by financial markets and forward-looking price-setters) – possibly leading to the erroneous conclusion that policy changes don't establish much. Similarly, seeing inflation rise following a rate hike does not necessarily imply a causal link: the Central Bank might have contracted because it saw future inflation coming, which would have been even higher had the Central Bank refrained from increasing its policy rate (Sims, 1992).====To overcome these problems, researchers have tried to identify “monetary policy shocks” (unanticipated changes in monetary policy, not driven by economic conditions). Some have done this by relying on recursive identification (e.g. Christiano et al., 1999), others have used sign restrictions (cf. Uhlig, 2005), while yet others have based inference on countries without monetary autonomy (for whom monetary policy is set by a foreign Central Bank; cf. Willems (2011) and Jorda, Schularick, and Taylor (2019)). Results between the various methods differ,==== as a result of which there is not even a broad consensus on ==== of the effects of monetary policy (cf. Cochrane (2018)). Prominent models like Christiano et al. (2005) also feature a working capital channel to match the price puzzle.====This paper takes a different “algorithmic” approach to identifying shifts in monetary policy, which – I will argue – has some advantages over existing approaches.====I start by constructing a dataset covering data on short-term interest rates, prices, and output in 162 countries. Next, I say that a monetary contraction takes place in a country in year ==== if two conditions are met:====While it might have seemed tempting to phrase condition 2 (which is partly aimed at selecting inflationary periods) directly in terms of inflation, that produces difficulties as countries differ in their inflation tolerance (which is unobserved). By requiring the ==== interest rate to be negative for a protracted period, condition 2 ensures a focus on periods where inflation was high relative to nominal interest rates in the country. This is a way to take country-specific factors into account, whilst simultaneously yielding a sense of the monetary policy stance (tight or loose).====In addition, incorporation of the nominal rate in criterion 2 enables the algorithm to select cases where the Central Bank initially did ==== respond to inflation accelerations (causing the real rate to go negative).==== This is to strengthen the case that the subsequent rate hike came with a surprise element. After all, the prolonged period of negative real rates is likely to have given the Central Bank a dovish, inflation-tolerant reputation – potentially operating under a fiscal regime (being “passive” in the sense of Leeper (1991)) and unlikely to raise rates by several hundred basis points in a year. Agents may very well realize that such a situation is unsustainable and will ==== be followed by a regime change,==== but at least they face uncertainty about the exact timing of the contraction (where agents might be inclined to follow the so-called “Lindy principle” according to which events that have been going on for a while, will continue for a while).====Calculating average inflation over years ==== through ====, and comparing it with inflation in other years, indeed suggests that the algorithm is able to select interest rate hikes that followed periods of high inflation (see Table 1 in Section 2). As a result, it seems reasonable to assume that the identified rate hikes were (a) motivated by inflation (as opposed to output====) while (b) taking agents by surprise, at least with respect to the exact timing of the shock (limiting the presence of anticipated policy shifts which might obscure inference).====Such unexpected policy changes can for example be caused by a new incoming Central Bank Governor (Paul Volcker taking over from William Miller as Chairman of the Federal Reserve in 1979), by a sitting Governor suddenly changing his or her mind about the right course of action, or through pressure exerted by international lenders or rating agencies.====The recent case of Turkey forms a good example: coming from a protracted period of accelerating inflation and negative real rates, during which President Erdogan expressed the view that rate hikes would only fuel inflation, the Central Bank of Turkey finally gave in to (predominantly international) pressures in May 2018 – increasing their policy rate by 850 basis points. Bloomberg survey data from that time however suggest that market participants were expecting the Central Bank to ==== at 8 percent – as it had done in the years before – thus suggesting that the rate change was a surprise, at least on that date.====The route taken by this paper is most closely related to the “narrative” approach to monetary policy shock identification (Romer and Romer (1989); also see Sargent (1982) for a case-study approach to the end of four big inflations in the 1920s). While the large cross-country scale of the exercise precludes a truly narrative approach, conditions 1 and 2 specified above can be seen as transparent, algorithmic short-cuts to “narratively-identifying” episodes during which monetary policy appears to have been looking to bring inflation down – without agents expecting it, and without the policy shift being driven by output developments. Exactly because the identified interest rate increases are unlikely to be motivated by (future) output considerations, they are particularly suitable to assess the causal impact of monetary contractions on output (see Proposition 1 in Cochrane (2004), while this is also the strategy underlying Romer and Romer (1989) and the subsequent “narrative” literature====).====At the same time, inference on the price level is complicated by the fact that the policy changes were taken in response to inflation. The usual worry is that the Central Bank contracts in response to signs about ==== inflation. This poses a challenge and might contribute to the emergence of the price puzzle, as future inflation typically isn't part of the econometrician's information set (Sims, 1992). But since conditions 1 and 2 select monetary contractions ==== high inflation (which has materialized and hence ==== part of the econometrician's information set), Sims' concern about the Central Bank responding to not-yet-existing future inflation is less relevant. This holds particularly true in less developed countries, where monetary policy tends to be more backward looking (in the absence of forward-looking Central Bank models and/or data on inflation expectations). As an additional safeguard, I proceed by treating the contraction dummy as an endogenous variable in the VAR and impose identifying restrictions, which re-opens the door to causal inference on the price level (whilst bearing in mind that the price response might still suffer from an upward bias, especially in more advanced economies).====As with all approaches, the one entertained by this paper comes with both strengths and weaknesses. The most obvious weakness relates to the fact that the necessary cross-country data are only available at an annual frequency, preventing this paper from analyzing the short-term effects of monetary contractions, while also complicating shock identification.====The present paper's focus on larger rate hikes comes with particular advantages. First of all, it increases the signal-to-noise ratio – overcoming the “small power problem” which for example hampers high frequency studies based on US data (Nakamura and Steinsson, 2018a, Nakamura and Steinsson, 2018b). Secondly, and relatedly, the more systematic conduct of monetary policy in advanced economies makes true monetary policy shocks rare and hard to identify (Ramey, 2016); instead, unbiased causal estimates are more easily obtained in environments where monetary policy is conducted in a more erratic fashion. In this context, going cross-country whilst focusing on larger shifts – thereby including instances where monetary policy was executed in a less systematic manner – makes practical sense. Third, episodes featuring sharp interest rate increases are unlikely to be accompanied by confounding “information effects”. As for example set out in Romer and Romer (2000), monetary policy comes with signaling effects through which a tightening can be interpreted as the Central Bank being relatively bullish on growth prospects. Potentially as a result of this channel, Nakamura and Steinsson (2018a) find that professional forecasters tend to ==== their growth projections following a more hawkish Fed signal, with various other studies showing output puzzles as well (see e.g. Uhlig (2005) and Ramey (2016)). While the signaling channel might play a role for monetary shocks that are of a relatively small magnitude (as found in US data), it is hard to believe that the sharp rate hikes central to this paper will ever be interpreted as the Central Bank being optimistic about the state of an economy; the Central Bank being worried about inflation seems a more natural interpretation.====Applying the algorithm identifies 147 “monetary contractions” (Appendix Table I provides a full overview, along with a possible underlying reason). 71 of the identified contractions were implemented in the year of, or in the year following, the arrival of a new Central Bank Governor (very much like the Volcker shock). The identified contractions increased the bank deposit rate by just over 650 basis points on average.====Results indicate that monetary contractions reduce both output and prices: following a rate hike of 100 basis points, real GDP declines by about 0.5 percent, while the price level falls by 1.5 percent. The significantly negative response of the price level (not showing any signs of the price puzzle) is a strong finding given that this paper's identification strategy implies that the price response might suffer from an upward bias. Emerging/developing countries seem closer to a situation of monetary neutrality, combining a greater impact on prices with smaller output effects (although still non-negligible: –0.3 percent per 100 basis points). Results point to larger output effects of monetary contractions in advanced economies (–1.1 percent per 100 basis points). In all cases, the contractionary output effects of monetary tightenings seem to be rather persistent – pointing to some degree of hysteresis in the economy.",What do monetary contractions do? Evidence from large tightenings,https://www.sciencedirect.com/science/article/pii/S1094202520300144,13 March 2020,2020,Research Article,195.0
"Niemann Stefan,Pichler Paul","Department of Economics, University of Essex, United Kingdom,Department of Economics, University of Vienna, Austria","Received 1 August 2018, Revised 27 February 2020, Available online 4 March 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.02.003,Cited by (7), occur only in exceptionally good times.,"A well-established stylized fact in international macroeconomics is the procyclical pattern of fiscal policy in emerging economies. Different from advanced economies, fiscal policy in these countries does not contribute to smoothing business cycle fluctuations over time but rather exacerbates underlying boom-bust cycles. A common explanation for this phenomenon is that governments in emerging countries have limited access to international borrowing, and particularly so in bad times. Hence, in order to confront concerns about the sustainability of their public finances and to retain financial market access, they often impose contractionary fiscal measures even during severe recessions (cf. Vegh and Vuletin, 2015; Born et al., 2015). This fiscal procyclicality can be rationalized insofar as austerity in the form of spending cuts or tax hikes helps to create better borrowing conditions as reflected by reduced sovereign spreads. Importantly, however, these spreads are not only determined by fundamentals but also subject to market sentiment (Calvo, 1988; Cole and Kehoe, 2000).====In this paper we study how fundamental sovereign risk and market sentiment shape the pattern of optimal debt issuance and the cyclical behavior of optimal fiscal policy. We develop a model with distortionary taxation, government spending and endogenous default costs driven by disruptions to the import of intermediate inputs (cf. Mendoza and Yue, 2012). Market sentiment can give rise to belief-driven debt rollover crises when international lenders lose confidence in the government's willingness to honor its liabilities and hence refuse rolling over their debts (cf. Cole and Kehoe, 2000). When debt is sufficiently high and repayment of the maturing liabilities is hence sufficiently costly, the inability to issue new debt gives the government an incentive to default, thus making the investors' initial loss of confidence a self-fulfilling prophecy. Debt levels for which these self-fulfilling dynamics are possible fall into the so-called ====.====At the heart of the fiscal policy problem is a trade-off between the government's motives to frontload and to smooth consumption: Increasing current consumption comes at the cost of accumulating more debt and thus higher exposure to default risk, which makes future consumption more volatile. To what extent this happens actually depends on the sources of sovereign risk, and this is reflected in the pattern of optimal fiscal policy. We show that, while optimal taxes and public spending are generally procyclical,==== the incidence of rollover risk gives rise to occasional episodes of severely countercyclical fiscal activity. These episodes are manifestations of ==== when the economy enters or leaves the crisis zone. Below the crisis zone, the government's optimal fiscal policy during recessions is then expansionary. It exploits its fiscal space in order to avoid an excessive depression in output and consumption, even though this policy ultimately entails a transition into the crisis zone and hence a discontinuous increase in sovereign risk and future borrowing costs. By contrast, within the crisis zone, a sustained boom can induce the government to adopt a severely contractionary fiscal stance. It reduces public spending and raises taxes in an effort to bring down debt to the point where rollover risk is completely eliminated. This policy lowers the default premia charged by international creditors and hence significantly reduces borrowing costs, which provides space for future governments to reduce distortionary taxes. Fiscal austerity, if initiated during booms, can thus be expansionary and facilitate consumption smoothing. The traditional rationale for countercyclical fiscal policy familiar from advanced economies is therefore at work also in our framework where fiscal policy is constrained by both solvency considerations and market sentiment.====In addition to prescribing large and fast-paced debt dynamics during regime switches, our results also uncover a fundamental asymmetry underlying the optimal fiscal policy: Large fiscal expansions are triggered already by relatively mild (but sustained) recessions, whereas large fiscal contractions are triggered only by exceptional boom episodes, that is, when output is significantly above trend. Our normative results thus also facilitate a new perspective at empirically observed debt dynamics. Absent rollover risk, Eaton-Gersovitz-type models prescribe a procyclical debt policy, so that debt rises (falls) in good (bad) times. The empirical evidence, however, indicates that ==== in public debt display a different, countercyclical pattern (Abbas et al., 2011). The endogenous regime switches in our model are consistent with this and also with the finding that debt-consolidating fiscal austerity programs pay off particularly if initial economic conditions are benign (Born et al., 2015).====The empirical relevance of the policy trade-off at play can also be illustrated with reference to the Mexican debt crisis of 1994-95. Our theoretical model provides a structural account of this crisis based on the idea that a countercyclical policy stance in the face of adverse economic and political conditions led to the accumulation of debt, which entailed the economy's entry into the crisis zone. The induced regime switch was associated with a pronounced jump in sovereign spreads and ended in a confidence crisis in the form of a series of failed debt auctions. Despite the eventual crisis, our model rationalizes these dynamics as potentially optimal in view of the government's competing motives for frontloading versus smoothing consumption.====Our paper connects to an expanding literature investigating the role and properties of fiscal policy in environments subject to sovereign risk. This literature has evolved into three main directions. A first branch documents the empirical regularities of fiscal policy in emerging economies and contrasts them to those observed in developed economies. A common theme is that both public spending and taxes are procyclical in the former but much less so in the latter group of countries (see e.g. Gavin and Perotti, 1997; Talvi and Vegh, 2005; Kaminsky et al., 2005; Frankel et al., 2013; Vegh and Vuletin, 2015). The explanations advanced to rationalize this pattern include weak political institutions and tax enforcement, incomplete markets and borrowing constraints (see e.g. Tornell and Lane, 1999; Cuadra et al., 2010; Ilzetzki, 2011; Bauducco and Caprioli, 2014). Relatedly, a second strand employs models of sovereign default to examine the relationship between fiscal rules or restrictions, bailouts and conditionality imposed by international financial institutions (see e.g. Juessen and Schabert, 2013; Goncalves and Guimaraes, 2015; Hatchondo et al., 2015; Fink and Scholl, 2016; Arellano and Bai, 2017). Finally, a third branch integrates fiscal policy into dynamic models of endogenous sovereign default from the perspective of optimal taxation with or without commitment (see e.g. Adam and Grill, 2017; Pouzo and Presno, 2015; Niemann and Pichler, 2017).====Most closely related to our work are the papers by Cuadra et al. (2010) and Cole and Kehoe (2000). Like us, Cuadra et al. (2010) consider a small open economy model where not only the repayment of debt but also taxes and public spending are endogenously chosen by the government in a time-consistent fashion. They consider only fundamental default risk and obtain fiscal procyclicality in a framework with exogenous default costs as in Arellano (2008). By contrast, our approach allows us to examine the interaction between fiscal policy and endogenous default costs and accommodates self-fulfilling debt crises, which we show to have important consequences for optimal fiscal policies. Our detailed account of fiscal policy and default costs also differentiates our work from Cole and Kehoe (2000) from whom we borrow the foundation for rollover risk. While distortionary, taxation in their model is limited to a time-invariant income tax; likewise, productivity is not stochastic and, following default, subject to a permanent exogenous reduction. Instead, our paper integrates flexible and optimally determined fiscal policy into a fully-fledged stochastic environment, which facilitates the quantitative assessment of its properties in normal times and during crisis and transition episodes. This has important implications. Whereas Cole and Kehoe (2000) establish that, generically, the optimal policy is to escape the crisis zone by decumulating debt, our model of optimal fiscal policy rationalizes persistent debt positions within the crisis zone as the generic outcome, which is broken only through rare regime changes following exceptional productivity dynamics. Finally, in a variation of the setup with time-invariant taxation in Cole and Kehoe (2000), Conesa and Kehoe (2017) obtain debt dynamics driven by the government's motive in recessions to ‘gamble for redemption’, which may result in a transition into the crisis zone. This is similar to our model, which, however, features empirically plausible productivity dynamics and thus also the potential for transitions out of the crisis zone, and in addition details the dynamic pattern of (variable) taxation and spending during these regime switches.====The rest of this paper is organized as follows. Section 2 presents our formal model environment, and Section 3 presents conditions characterizing optimal fiscal policy. Section 4 calibrates the model to data for the Mexican economy. Section 5 presents the results of our quantitative exercise, and Section 6 compares them to the empirically observed dynamics during the Mexican debt crisis of 1994-95. Section 7 concludes.",Optimal fiscal policy and sovereign debt crises,https://www.sciencedirect.com/science/article/pii/S1094202520300119,4 March 2020,2020,Research Article,196.0
"Hirose Yasuo,Kurozumi Takushi,Van Zandweghe Willem","Faculty of Economics, Keio University, Japan,International Department, Bank of Japan, Japan,Research Department, Federal Reserve Bank of Cleveland, United States of America","Received 17 June 2019, Revised 3 March 2020, Available online 4 March 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.03.001,Cited by (10),A large literature has established the view that the Fed's change from a passive to an active policy response to ,"What led to macroeconomic stability in the United States after the Great Inflation of the 1970s? A large literature has regarded the Great Inflation as a consequence of self-fulfilling expectations in indeterminate equilibrium, which lasted until determinacy was restored by changes in the Fed's policy under the chairmanship of Paul Volcker and his successors.==== In particular, the literature has established the view that the U.S. economy's shift from indeterminacy to determinacy was achieved by the Fed's change from a passive to an active policy response to inflation. A monetary policy response to inflation is called active if it satisfies the Taylor principle that the nominal interest rate should be raised by more than the increase in inflation. Otherwise, it is called passive. Clarida et al. (2000) demonstrate the literature's view by estimating a Taylor (1993)-type monetary policy rule during two periods, before and after Volcker's appointment as Fed Chairman, and combining the estimated rule with a calibrated New Keynesian (henceforth NK) model to analyze determinacy. Lubik and Schorfheide (2004) confirm the view by estimating a Taylor-type rule and an NK model jointly during similar periods using a full-information Bayesian approach that allows for indeterminacy and sunspot fluctuations.====This paper revisits the literature's view by estimating a generalized NK (henceforth GNK) model jointly with a Taylor-type rule.==== This model differs from canonical NK (henceforth CNK) models used in the literature mainly in that some prices remain unchanged in each period in line with micro evidence.==== Consequently, instead of a canonical one, a generalized NK Phillips curve appears in the GNK model, with the distinct features that its coefficients depend on the level of trend inflation and that it includes additional forward-looking terms through which inflation responds to expected changes in future demand and discount rates on future profits under nonzero trend inflation. These features cause the GNK model to be more susceptible to indeterminacy than CNK models, as indicated by Hornstein and Wolman (2005), Kiley (2007), Ascari and Ropele (2009), and Coibion and Gorodnichenko (2011).==== Indeed, even an active policy response to inflation that generates determinacy in CNK models can induce indeterminacy in the GNK model.====Our estimation is performed using a full-information Bayesian approach based on Lubik and Schorfheide (2004).==== In their approach, however, when a model is estimated over both determinacy and indeterminacy regions of the model's parameter space, its likelihood function is possibly discontinuous at the boundary of each region. As a consequence, the Random-Walk Metropolis-Hastings (henceforth RWMH) algorithm—which has been the most widely used in Bayesian estimation—can get stuck near a local mode and fail to find the entire posterior distribution for the model's parameters. To deal with this difficulty, our paper adopts the sequential Monte Carlo (henceforth SMC) algorithm developed by Herbst and Schorfheide (2014; 2015). As they illustrate, the SMC algorithm can produce more reliable estimates of model parameters than the RWMH algorithm when the parameters' posterior distribution is multimodal. This is particularly the case when the likelihood function of a model to be estimated exhibits discontinuity as in our paper.====Our empirical analysis makes three main contributions to the literature. First of all, the GNK model empirically outperforms CNK models during both periods before and after the Volcker disinflation of 1979–1982. We consider two types of CNK models. One type is a CNK counterpart to the GNK model and assumes that prices that remain unchanged in the GNK model are updated by indexing to trend inflation as in Yun (1996).==== The GNK model and its CNK counterpart are both augmented with backward-looking rule-of-thumb price-setters as in Galí and Gertler (1999) to take into account the possibility of intrinsic inertia in inflation.==== The other type of CNK model instead incorporates, as in Smets and Wouters (2007), price indexation to past and trend inflation, which has been extensively used in empirical studies. The superior empirical performance of the GNK model relative to the two CNK models indicates that the GNK model's features that are more consistent with the micro evidence on price setting also contribute to a better fit of the model to U.S. macroeconomic time series, and thus the GNK model is more suitable for the analysis of what led to U.S. macroeconomic stability after the Great Inflation.====Second, the U.S. economy was likely in the indeterminacy region of the GNK model's parameter space before 1979, while it likely entered the determinacy region after 1982, in line with the result obtained in the literature. However, the estimated GNK model points to substantial uncertainty about whether the policy response to inflation was active or passive during the pre-1979 period in the Taylor-type rule, which adjusts the interest rate for contemporaneous values of inflation, the output gap, and output growth in the presence of interest-rate smoothing.==== In the GNK model even an active policy response to inflation possibly fails to ensure determinacy, as noted above. The ambiguous result contrasts with the literature's finding that the policy response to inflation was surely passive during the Great Inflation and that the subsequent change to an active response led to the U.S. economy's shift from indeterminacy to determinacy (in CNK models).====Last but not least, the increase in the policy response to inflation from the pre-1979 to the post-1982 estimate alone does not suffice for explaining the U.S. economy's shift to determinacy, unless it is accompanied by either the estimated decline in trend inflation or the estimated change in policy responses to the output gap and output growth. This finding reveals that a lower rate of trend inflation (or equivalently a lower inflation target), a more dampened response to the output gap, and a more aggressive response to output growth play a key role in accounting for the U.S. economy's shift, along with a more active response to inflation. Therefore, our finding extends the literature by emphasizing the importance of the changes in other aspects of monetary policy in addition to its response to inflation.====This paper is an extension of Lubik and Schorfheide (2004) and a complementary study to Coibion and Gorodnichenko (2011). It strengthens the analysis of Lubik and Schorfheide by adopting the SMC algorithm in their full-information Bayesian approach and estimating the GNK model (jointly with the Taylor-type rule) as well as the CNK models, which are similar to their model. While Lubik and Schorfheide estimate their model separately for the determinacy and indeterminacy regions of the model's parameter space, the SMC algorithm enables us to conduct our estimation for both of the regions in one step. Coibion and Gorodnichenko revisit the literature's view by using a calibrated GNK model in an approach analogous to Clarida et al. (2000).==== They offer the alternative view that the U.S. economy's shift to determinacy after the Great Inflation is due to their estimated change in a Taylor-type rule and their calibrated fall in trend inflation.==== An advantage of our analysis is that we estimate both trend inflation and the Taylor-type rule's coefficients as well as other structural model parameters under cross-equation restrictions and show that our GNK model empirically outperforms the CNK models, giving strong support to our view on the shift from indeterminacy to determinacy.====The remainder of the paper proceeds as follows. Section 2 presents a GNK model with a Taylor-type rule. Section 3 explains the estimation strategy and data. Section 4 shows the results of the empirical analysis. Section 5 concludes.",Monetary policy and macroeconomic stability revisited,https://www.sciencedirect.com/science/article/pii/S1094202520300132,4 March 2020,2020,Research Article,197.0
Bento Pedro,"Texas A&M University, , United States","Received 6 June 2019, Revised 26 February 2020, Available online 3 March 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.02.004,Cited by (6),"Theories of endogenous markups, firm entry, and innovation consistently predict a positive relationship between barriers to competition and firm-level innovation, in stark contrast to a large empirical literature that documents a negative relationship. I argue that this puzzle can be resolved by considering two distinct types of barriers – barriers to starting a firm, and barriers to competing in a product market. Barriers to starting firms discourage startups, resulting in higher markups, higher market shares, and more innovation. Barriers to product-market entry also increase market shares within each market, and thereby encourage innovation. But firms also enter fewer markets, which discourages innovation. On net, market-entry barriers ==== firm-level innovation and the number of competitors per market, while ","There is now an abundance of empirical evidence that more competition (i.e., lower markups) and lower barriers to competition are associated with more firm-level innovation and higher aggregate productivity.==== Current models with free entry and endogenous markups connect markups to the number of firms through Bertrand competition (Peters, 2013), Cournot competition (Edmond et al., 2015), Lancaster preferences (Desmet and Parente, 2010), and linear demand (Melitz and Ottaviano, 2008). All of these models predict a positive relationship between barriers to firm entry and average markups. Those that consider innovation, however, predict that barriers to entry should ==== firm-level innovation. Developing a model with free entry that predicts higher firm-level innovation when barriers to competition are reduced has proven to be difficult, as the lower market shares that result from higher firm entry should result in lower incentives to innovate – a point made by Schumpeter (1942).====In this paper I propose a potential solution to this disconnect between the theory and the data. I consider a model of endogenous firm entry and markups, with multi-product firms and firm-level innovation. Crucially, I distinguish between barriers to starting a firm and barriers to entering a product market.==== I keep the model as simple as possible to highlight the central mechanism. Potential firms choose whether or not to become producers, how many product markets to enter, and how much to invest in firm-level productivity (I call this innovation). Importantly, innovation affects firm-level productivity across all products. In each product market firms then produce a differentiated good in competition with other firms. Barriers to starting a firm generate the usual result − fewer firms, higher markups, and more innovation per firm. But barriers to market entry reduce the number of products per firm. For a given number of firms this must reduce the number of competitors within each product market. If markups are constant in the number of competitors, then both the total number of firms and innovation per firm are both unaffected (while fewer competitors mean higher market shares per product, the drop in the number of products per firm exactly offsets this, leaving the incentives to enter and invest unchanged). But if markups increase when the number of competitors drop (say, due to Cournot competition), then these higher markups increase the value of starting a firm. As a result, firm startups and the number of firms in equilibrium increase. And since the price elasticity of demand is decreasing as the number of competitors drops, the incentive to innovate decreases, resulting in less innovation per firm. Under the assumption that innovation is firm-specific and affects productivity across all products, productivity must therefore decrease. To the extent that barriers to competition are product-market specific, rather than firm specific, the model can therefore rationalize the widely-documented positive empirical relationship between competition and firm-level innovation that has been so difficult to generate in models with free entry.====To test the quantitative importance of the theory, I offer evidence to address the following three questions. First, do lower barriers to market entry in fact lead to fewer firms per industry? Second, are market-entry barriers quantitatively important relative to firm startup barriers? And third, are higher market-entry barriers in fact associated with fewer products per firm? The evidence I document in this paper suggests that the answers to each of these questions is ‘yes’.====I use panel data from a number of European Union (EU) members and non-EU countries to test how the implementation of the Single Market Programme (SMP), which lowered market-entry barriers in certain industries in the early 1990s, affected the number of firms in affected industries. Griffith et al. (2010) use the data from the same episode and show that lower market-entry barriers resulted in lower markups. Interpreted through my model, their results suggest that the number of firms competing in each product market (within affected industries) increased as a result of the lower barriers. I show that the total number of firms within each affected industry decreased, relative to industries and countries that were unaffected by the SMP. This is exactly what the model predicts when market entry barriers are reduced. Across countries, I consider the impact of the OECD's Indicators of Product Market Regulation (PMR), a measure of barriers to competition that aggregates across different types of barriers. I show that higher barriers are associated with more manufacturing firms, suggesting that differences in market-entry barriers are quantitatively more important than differences in startup costs across countries. To test the central mechanism in the theory, that barriers to market entry encourage higher markups while decreasing the number of products per firm, I use firm-level data from a number of low- to medium-income economies. The data show that higher barriers are indeed associated with higher average markups and fewer products per firm.====The model developed in this paper is quite parsimonious, and can easily be taken to the data. By calibrating the model to match some relevant elasticities estimated in the empirical literature, as well as the estimated impacts on markups and the number of firms, I first show how the model can be used to predict the quantitative impact of the Single Market Programme on outcomes. The impact is substantial. I estimate that in industries affected by the SMP, markups dropped by 9%, the number of firms dropped by 31%, productivity investment increased by 9% (as a share of output), and industry-level total factor productivity increased by 8.4%. Data on the number of firms is generally not used in the empirical competition and innovation literature, in large part because of a lack of firm count data. As a result, researchers have been prone to interpreting variation in markups as coming from variation in startup costs. How much does this matter? I show that if I were to assume the drop in markups due to the SMP was driven by lower firm startup costs, then I would mistakenly predict a ==== in productivity investment. As a result, the estimated increase in total factor productivity would be 30% lower.====Turning to more recent cross-country data, I use the model to calculate the implied impact on aggregate output from the barriers to competition measured by the OECD's Indicators of Product Market Regulation. These measures of barriers do not distinguish between startup and market-entry barriers, so I use the model to make inferences using the empirical relationships between measured barriers and both the number of firms and markups, as well as parameter values obtained from my analysis of the SMP. The model implies that increasing measured barriers from the U.S. level to that of Malta (median level in the sample) causes a drop in output of 16%. I then use the model to decompose the effects of these measured barriers into those coming from inferred barriers to market entry and those coming from inferred barriers to starting a firm. Interpreted through the lens of the model, the data suggest that cross-country variation in barriers to market entry are much more important in rationalizing the data than variation in startup costs. In fact, the startup costs implied by the data seem to be ==== in poor countries with more restrictive measured barriers to competition, even while implied market entry costs are substantially higher than in developed countries.====A large literature has documented a positive empirical relationship between competition (generally measured as the inverse of average firm-level profit rates or markups within an industry) and firm-level innovation. Widely-cited studies include Nickell (1996); Blundell et al. (1999); Nicoletta and Scarpetta (2003); Aghion et al. (2005, 2008); and Griffith et al. (2010). Nickell (1996), Blundell et al. (1999), and Aghion et al. (2008) examine how changes in average profit rates and/or market shares are associated with average firm-level productivity and innovation. They all find a negative relationship on average, but Aghion et al. (2008) test for and find an inverted-U relationship – higher average profit rates are associated with higher average productivity when starting from very low profit rates, but lower productivity after some threshold. Aghion et al. (2005) and Griffith et al. (2010) also examine the relationship between industry-level profit rates and firm-level innovation (or R&D), but additionally isolate changes in profit rates driven by implementation of the SMP and other market reforms. Both find a negative linear relationship between average profit rates and firm-level innovation, as well as a positive relationship between the magnitude of reforms and firm-level innovation. But Aghion et al. (2005) also test for and find an inverted-U relationship between reforms and firm-level innovation.====Why has it proven so difficult to generate a negative relationship between barriers to competition and firm-level innovation? Several papers have highlighted how an increase in average markups due to higher barriers should reduce the elasticity of demand with respect to price, for example Desmet and Parente (2010). This reduces the incentive to innovate (which would lower the price of a product), thus providing a mechanism by which innovation might decrease when barriers are imposed. But in a model with only one-product firms, this mechanism is always dominated by a stronger force – with higher barriers, fewer firms are left in equilibrium and these firms are left with higher market shares. These higher market shares encourage innovation enough that, on net, firm-level innovation always increases (this has been referred to as the Schumpeterian effect).==== A number of papers have developed mechanisms through which competition can somehow encourage innovation when the number of competitors is exogenous. For example Aghion et al. (2005) consider markets made up of two firms, and compare equilibria under exogenously different levels of collusion. Many other variations on the same theme have been proposed.==== But all of these papers rely on a mechanism that depends on demand elasticities changing while market shares are on average constant. Consistent with the dominance of the Schumpeterian effect I point to above, Etro (2007) shows the negative relationship between markups and innovation generated by these mechanisms disappears when entry is made endogenous. As a result, almost no theory has been able to rationalize the relationship between lower markups (generated by lower entry barriers) and higher firm-level innovation. The sole exception (to my knowledge) is Bento (2014a). But the model in Bento (2014a) is somewhat stylized and difficult to take to the data. Further, it considers only partial equilibrium, while the present model suggests general equilibrium effects can potentially undo some partial equilibrium results.====All current models with free entry and endogenous markups can generate the empirical link between markups and barriers to entry. But only Bento (2014a), described above, has been able to generate a negative relationship between entry barriers and firm-level innovation. Desmet and Parente (2010) highlight barriers to trade as another type of barrier that can simultaneously increase markups, increase the number of firms, and decrease innovation, just as market entry barriers do in this paper. But the mechanism that Desmet and Parente (2010) rely on for their results generates counter-factual implications. An increase in population in their model (which is equivalent to a country moving from autarky to free trade) increases the size of the market. This increases the number of firms but not proportionately as in, say, Melitz (2003). As the number of firms increases, markups decrease, thus lowering the value of entry (relative to that in a smaller market size) and ==== the number of firms per capita. Indeed, the same mechanism is present in this paper's model. Bento and Restuccia, 2017, Bento and Restuccia, 2019 and Fernandes et al. (2019), however, show that the number of firms per capita (or per worker in an industry) is unrelated to population and the size of an industry. The present paper offers a different mechanism, linking barriers to market entry with competition and innovation.====The model I develop in this paper is a very standard one of free entry and endogenous markups, extended to allow for both multi-product firms (Bernard et al., 2010) and firm-level investment in productivity. I hasten to emphasize that none of the separate features of the model are new. The contribution of the model is to show how incorporating all of these features allows me to rationalize several empirical phenomena related to entry barriers and competition (in particular, the relationship between competition and innovation), and to quantify the impact of observed differences in entry barriers. In the baseline model, I assume firm investment (innovation) affects productivity across all products. I also extend this baseline model to allow for both firm-level and product-level innovation. In the extended model, firm-specific productivity (applicable to all products) still increases when barriers to market entry are reduced, but product-specific productivity decreases. As a result, the extended model generates an inverted-U relationship between barriers and total firm-level productivity. Nevertheless, when I take the extended model to the data, it still generates a negative relationship between barriers and productivity over the range of barriers implied by the data.====Although in this paper product-specific barriers are interpreted as barriers to product market entry, they could be interpreted alternatively as barriers to entry into geographical markets.==== As such, the results of the model developed in this paper are consistent with the U.S. experience with barriers (and the reduction of barriers) to branch banking. Carlson and James (2006) document evidence of how the easing of restrictions on branch banking resulted in more bank branches and more competition between branches, but fewer banks, all consistent with the results of this paper.====I emphasize that in this paper I do not use the number of firms present in an industry as a measure of competition. Indeed, I show that barriers to market entry increase the number of firms while ==== competition and innovation, and increasing markups. There is a long tradition in the antitrust literature (both in and outside of economics) of drawing a distinction between the number of firms in an industry and the number of firms competing in a market.==== The analysis in this paper complements this literature by showing how barriers to competition can lead to opposite effects on the number of competitors in a market, and the number of firms in an industry. This is especially important for empirical work due to the difficulty of identifying the relevant market for particular firms and products. The model I develop suggests that markups are a useful measure of competition, as they reflect the number of competitors in the relevant market, while the number of firms provides information about the underlying causes of markups. If both markups and the number of firms in an industry are high, this is evidence of high market-entry barriers. If markups are high and the number of firms is low, this is evidence of high startup costs.====By analyzing the effects of entry barriers on the number of firms, both empirically and in theory, this paper contributes to the growing literature attempting to explain differences in average firm size (the inverse of the number of firms per worker) across countries. See, for example, Bhattacharya et al. (2013), Hsieh and Klenow (2014), and Bento and Restuccia, 2017, Bento and Restuccia, 2019, who point to policies and institutions causing both a misallocation of inputs across firms and a reduction in average firm size. The present paper complements this literature by showing how a specific type of policy (increasing barriers to competition) can lead to differences in average firm size across countries.====In the next section I describe the model, characterize equilibrium, and discuss its key implications. In Section 3 I test one implication of the model – that market-entry barriers increase the number of firms – using both panel and cross-country data. In Section 4 I document evidence of the relationships between barriers, average markups, and products per firm. The final section concludes.","Competition, innovation, and the number of firms",https://www.sciencedirect.com/science/article/pii/S1094202520300120,3 March 2020,2020,Research Article,198.0
"Andrés Javier,Arce Óscar,Thaler Dominik,Thomas Carlos","University of Valencia, Spain,Banco de España, Spain","Received 24 September 2018, Revised 14 February 2020, Available online 20 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.02.002,Cited by (10)," in a monetary union. The coexistence of long-term private debt and collateral constraints on new loans implies that, following an adverse financial shock, the economy enters a slow private deleveraging process, the duration of which is endogenous to collateral and debt dynamics. In this context, large and/or front-loaded consolidations increase the length and depth of private deleveraging, causing higher relative output losses over the medium run. As a result, such aggressive consolidation strategies entail larger present-value multipliers. Our results thus speak in favor of ‘deleveraging-friendly’ (smaller/more gradual) consolidations.","Since 2008, public debt has increased by more than 20 percentage points of GDP on average in the OECD countries, up to levels hardly ever seen before in peacetime. In the euro area, ratios of general government debt over GDP at the end of 2017 were at least 1.5 times higher than in 2007 in most countries, and more than twice as high in some of them. Against a context of subdued inflation, and with the prospect of moderate growth in the coming years, considerable budgetary adjustments are called for to reduce public debt. This raises a number of questions about the macroeconomic impact of fiscal consolidations.====Issues like the appropriate pace and composition (spending cuts vs. tax hikes) of fiscal consolidations have been thoroughly discussed in the literature. In the aftermath of the financial crisis, these questions acquire a new dimension since the effectiveness and costs of fiscal retrenchments are likely to be affected by the dynamics of a preexisting large stock of private debts. Yet, the literature has paid scarce attention to the bidirectional links between public and private debt-reduction processes, despite the central role that such interaction has occupied in policy discussions (see e.g. Draghi, 2014; and IMF, 2016). In this paper we develop a framework to analyze this interaction and how the latter shapes the macroeconomic costs of alternative fiscal deleveraging strategies in a context of private deleveraging.====Inspired by the experience of some euro area countries embarked in both private deleveraging and fiscal consolidation processes, we build a general equilibrium model of a small open economy that belongs to a monetary union. In the model, private debt is nominal and long-term, and borrowers face collateral constraints. The combination of long-term debt and collateral constraints is essential to reproduce realistic paths of protracted private debt deleveraging in the aftermath of a sufficiently adverse shock. In the model, when the value of collateral is above that of outstanding debt, i.e. when ‘excess collateral’ is positive, borrowers can obtain new loans up to the value of such excess collateral. On the contrary, when the value of collateral falls below the outstanding debt, due e.g. to of a negative financial shock, credit flows freeze and debt is reduced at the contractual amortization rate, thus pushing the economy into a gradual deleveraging path. As legacy debts are progressively paid back, the value of collateral recovers sufficiently relative to outstanding debt so as to sustain new credit flows, thus bringing the deleveraging process to its end. In this way, the length and depth of private deleveraging are endogenously determined. Against the backdrop of this deleveraging scenario, we analyze how the size and speed of fiscal consolidations affect the economy.==== Our main results can be summarized as follows.====First, the size of the fiscal consolidation has different effects on fiscal sacrifice ratios (i.e. output losses relative to consolidation size) at different horizons. Compared to small consolidations, large ones imply similar sacrifice ratios in the short run but higher ones in the medium run. Consider first the short-run implications. While deleveraging, borrowers' debt repayments are capped and hence their net debt flows are isolated from any impact of the fiscal contraction on collateral values.==== At the same time, their disposable income is reduced roughly in proportion to the size of the fiscal adjustment, so the latter size tends to have a fairly linear effect on output losses during the deleveraging phase. Over the medium run, by contrast, larger consolidations imply higher relative output losses because they ==== the end of private deleveraging, both by further depressing collateral values and through debt deflation. By prolonging the duration of deleveraging, larger consolidations postpone borrowers' access to new credit flows and hence the recovery in their spending capacity, causing more-than-proportional GDP losses over the medium run. We call this key mechanism the ‘duration effect’. When computing present-value output multipliers to summarize these intertemporal effects in a single metric, we find that for a sensible calibration of the model the duration effect makes larger consolidations proportionally costlier than smaller ones.====Second, as regards the speed of a fiscal consolidation (for a given size), we find that faster consolidations entail higher present-value multipliers than more gradual adjustments. The speed of a fiscal consolidation implies an intertemporal trade-off: front-loaded consolidations are more costly in the short-to-medium-run, but more benign in the medium-to-long-run. However, the former effect ends up dominating in present-discounted terms. Key to this result is the fact that faster consolidations produce larger falls of collateral values and debt-deflation effects in the short run, which prolongs the duration and intensity of borrowers' deleveraging and postpone the economic recovery – i.e. the ‘duration effect’ again at play.====Our baseline results are derived for consolidations based on government spending cuts, but we assess their robustness when the adjustment rests on hikes in different kinds of taxes. We find that our ‘size’ result carries over whenever the fiscal adjustment prolongs the private deleveraging – thus confirming the importance of the duration effect. Likewise, slow consolidations are generally less costly than fast ones, and the duration channel is found to play a prominent role in boosting the benefits from gradualism. However, the degree to which the deleveraging process is prolonged differs across instruments. Labor taxes are an interesting exception: Being inflationary and stabilizing the real estate price, they actually have the potential to shorten the deleveraging process. Thus the previously mentioned results flip sign – large consolidations are relatively less costly and gradualism less beneficial.====All in all, our results uncover a novel channel that may be relevant in the design of fiscal consolidation packages in the aftermath of a financial crisis. In short, our analysis provides support for the adoption of ‘deleveraging-friendly’ fiscal consolidation strategies, insofar as they favor an earlier reactivation of private credit.====Finally, we note that we do not model explicitly the reasons that would lead the government to engage in fiscal consolidation in the first place, such as endogenous sovereign risk (see e.g. Bianchi et al., 2017). Short of minimizing the importance of such reasons both in motivating fiscal adjustments and in shaping their effectiveness, our analysis focuses on the costs from fiscal consolidations in a context in which private borrowers undergo a deleveraging process. Our paper addresses the question of how to design fiscal consolidations in order to minimize those costs.====. Our paper is related to several branches of the literature on the macroeconomic effects of fiscal consolidations. Some authors have advocated large and quick fiscal adjustments as a means of bringing public debt back into a sustainable path. According to this view, even if they accentuate the depth of the recession, sharp fiscal adjustments may make recessions shorter and eventually less painful.==== Others support milder and/or more gradual consolidations whenever possible, arguing that fiscal multipliers are larger in recessions. Along this line of reasoning, postponing the bulk of the fiscal retrenchment until the economy starts recovering seems preferable.==== Our analysis sheds further light on this debate by inspecting the consequences of fiscal consolidations in a context of private debt deleveraging.====Our work is also connected to a strand of literature that, inspired by the recent global financial crisis, focuses on the specificities of the fiscal policy transmission mechanism in the aftermath of a large financial shock. Against that background, the capacity of monetary policy to mitigate the short run costs of fiscal retrenchments may be limited by the ZLB, which tends to raise the magnitude of fiscal multipliers.==== In addition, the recent financial crisis has left behind a landscape of heavily indebted households and firms.==== In their empirical analysis of financial crisis, Jordá et al. (2016) show that, while private debt accumulation – not the level of public debt – is a main precursor of financial crises, a high volume of public debt exacerbates the costs of private sector deleveraging following an adverse financial shock.====In spite of the rising interest in the connection between private and public debt, there have been few attempts to analyze such connection within a unified theoretical framework. Eggertsson and Krugman (2012) offer a model where fiscal multipliers increase in the presence of private debt overhang. Batini et al. (2016) also find that public debt is not a major triggering factor of financial crises but it may aggravate them, and explore in a DSGE model how fiscal transfers in the aftermath of a financial shock may alleviate the costs of households and firms deleveraging.====In our model fiscal contractions affect the length and depth of private deleveraging through two channels. The first is the Fisherian debt deflation channel, which arises due to nominal debt contracts. In our model, a deflationary fiscal contraction worsens the deleveraging-driven debt deflation effect and in so doing it prolongs the duration of such deleveraging; in this sense, our analysis is related to Eggertsson and Krugman (2012), who show how an increase in public spending attenuates debt deflation in a deleveraging context. The second is that a fiscal adjustment depresses collateral values and this too lengthens the deleveraging phase. The link from fiscal consolidation to collateral values is also analyzed in Bianchi et al. (2017), who show that a fiscal contraction can reduce collateral values in a model with one-period debt and collateral constraints. The novelty here is in the buffering and duration effects stemming from the combination of long-term debt and collateral constraints, and how this shapes the trade-offs regarding size and pace of fiscal consolidations. Echoing the empirical findings of Jordá et al. (2016), our analysis suggests that in the aftermath of a financial crisis a fiscal retrenchment motivated by a high volume of public debt will tend to exacerbate endogenously the duration and macroeconomic costs of private deleveraging.====Private deleveraging in the context of a model with borrowers and lenders is also analyzed in Guerrieri and Lorenzoni (2017) and Benigno et al. (forthcoming). These papers argue that the deleveraging process depresses not only activity but also the natural interest rate, and study what this implies for monetary (and in the latter case also fiscal) policy in the context of the ZLB. Abstracting from the ZLB, in our paper we study the interaction between fiscal consolidations and private deleveraging under long term debt contracts, which allow borrowers debt to temporarily exceed their debt limit during deleveraging and hence introduce a nonlinearity.====Romei (2017) explores the implications of different fiscal consolidation paths for households' welfare in the context of an economy with heterogeneous agents. She finds that households' wealth is the best predictor of their preferences over different consolidation strategies in terms of their speed and composition.==== Philippon and Roldán (2018) discuss another important reason why savers and borrowers may have different preferences with respect to the speed of deleveraging to the extent that a reduction in public debt reduces the risk of sovereign default, a risk that is borne by the former. While not our primary focus, we also analyze how different consolidation strategies affect different households' welfare, in an economy with long term private debt and collateral constraints. We find that borrowing constrained agents benefit the most from slower fiscal adjustments, but gradualism is preferred too by unconstrained households.====The rest of the paper is organized as follows. Section 2 illustrates the two novel mechanisms generating the intertemporal trade-offs in the effects of the size and gradualism of fiscal adjustments – the ‘buffering’ and ‘duration’ effects – using a simple model of private debt deleveraging that admits an analytical solution. The full model and the baseline calibration are presented in Section 3. In Section 4 we construct our baseline private deleveraging scenario and, against the backdrop of the latter, we analyze the impact of alternative consolidation strategies in terms of size, gradualism and instruments. Section 5 concludes.",When fiscal consolidation meets private deleveraging,https://www.sciencedirect.com/science/article/pii/S1094202520300107,20 February 2020,2020,Research Article,199.0
"Sanchez Manuel,Wellschmied Felix","University of Bristol, Priory Road Complex, BS8 1TU Bristol, UK,Universidad Carlos III de Madrid, Madrid, Spain,IZA, Bonn, Germany","Received 21 May 2018, Revised 21 November 2019, Available online 5 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2019.11.003,Cited by (4),"We estimate explicit age-varying distributions of idiosyncratic persistent and transitory earnings shocks over workers' life-cycles using a German administrative data set. Large positive shocks, both transitory and persistent, are characteristic for the first eight years of the working life. After the age of 50, large negative shocks become a major source of earnings risk. Between the ages of 30 and 50, most shocks are small and transitory. Large persistent positive shocks that occur early in the working life help to rationalize large ==== and consumption shares of the top one percent in an incomplete markets model.","Individual earnings risk changes over the life-cycle. During the early stage of the working life, finding all-year-round employment and moving up the job-ladder are associated with large individual earnings fluctuations.==== During prime-age (ages 30-50), workers settle into more stable employment and large earnings changes become less frequent. Once closer to retirement, periods of non-employment and losing a high-tenured job become major risks.==== Karahan and Ozkan (2013), Blundell et al. (2015), and Lopez-Daneri (2016) study this age variation in terms of changing variances of idiosyncratic transitory and persistent earnings shocks. This paper also decomposes male earnings changes into transitory and persistent earnings shocks, but it goes beyond age-variations in the second moment of these shocks. In particular, it estimates explicitly age-varying distributions of positive and negative earnings shocks. We find that the probabilities to experience large positive and large negative earnings shocks vary substantially with age, and this age-variation furthers our understanding of households' consumption and savings decisions.====Using German administrative individual earnings data, we first document that moments of positive and negative residual earnings growth behave very differently from each other over the life-cycle. Positive residual earnings growth is relatively rare before the age of 30, but growth rates are large on average and highly dispersed.==== The relatively frequent negative residual earning growth is small on average leading to a positively skewed distribution of residual earnings growth. The average size and the dispersion of positive residual earning growth fall throughout the life-cycle, and the average size and the dispersion of negative residual earnings growth grow throughout the life-cycle. Brought by less frequent large positive residual earnings growth and more frequent large negative residual earnings growth, the distribution of residual earnings growth becomes negatively skewed from age 40 onwards. After the age of 50, negative residual earnings growth is 10% larger on average and its variance is 60% larger than at age 25. In contrast, positive residual earnings growth is 70% smaller on average and its variance is 70% smaller than at age 25. Finally, the first autocovariance of positive growth is small relative to the first autocovariance of negative growth throughout the life-cycle.====Using simulated methods of moments, we estimate a parametric model that maps the distribution of residual earnings growth into age-varying distributions of transitory and persistent earnings shocks. We obtain these distributions explicitly by modeling shocks as a mixture of specified parametric distributions, similar to Geweke and Keane (2000), Bonhomme and Robin (2010), and Guvenen et al. (2016). To be specific, we parametrize residual log earnings as a mixture of three components that, given our decomposition of the data, have a natural interpretation: a positive, a negative, and a mean-zero component. The latter is a transitory normally distributed shock. In addition to this shock, with age-varying probabilities, workers draw either an innovation to their positive component, an innovation to their negative component, or no further shock. An innovation to the positive (negative) component is a combination of a transitory and a persistent log-normally distributed shock. Hence, persistent and transitory shocks are partially correlated in our model which deviates from the more standard zero-correlation assumption in the literature. A positive correlation allows the model to be consistent with the earnings dynamics occurring around two prominent (observable) persistent labor market shocks: unemployment and job-to-job transitions. That is, earnings are lowest on average in the year of an unemployment spell but return partially to their former level afterward (see also Jacobson et al. (1993)). Similarly, average earnings are highest in the year of a job-to-job transition but reverse somewhat thereafter. To capture the age-varying frequency and severity of these and other earnings shocks, we allow the means and variances of the parametric shock distributions to vary with age. These age variations in the shocks underlying the three components, together with the age-varying sampling probabilities of the three components, allow the model to generate rich age-variations in the overall distributions of transitory and persistent earnings shocks.====We find that at prime-age, most workers experience only small transitory shocks. At age 40, only 33% of workers experience any persistent earnings shocks during a given year. These probabilities are much higher, around 58 percent, at ages 25 and 55. Turning to the properties of these persistent shocks, we find that the autocorrelations of persistent positive and persistent negative shocks are above 0.97, i.e., these shocks are close to permanent. The probability to draw a positive persistent shock increases from 11% at age 25 to 44% at age 55. Nevertheless, experiencing a positive persistent increase in log earnings of more than 0.2 is 7 times more likely at age 25 than at age 55. The reason is that the mean and the variance of persistent positive shocks are about 5 times larger at age 25 than at age 55. Persistent negative shocks show the exact opposite life-cycle behavior of positive shocks. They are small, have little dispersion, and occur with relatively high frequency early in life, and become rare, large on average, and more dispersed late in life. To put these findings in perspective to the U-shaped variance of persistent shocks over the life-cycle found by Karahan and Ozkan (2013), our results imply that the initial decrease is entirely driven by positive persistent shocks becoming less dispersed and the latter increase is entirely driven by negative persistent shocks becoming more dispersed.====Transitory shocks to the positive and negative components are large and highly dispersed. On a life-time average basis, the variance of transitory negative shocks is 2.6 times larger than the variance of transitory positive shocks. Moreover, it is 11 times larger than the variance of persistent negative shocks. As a consequence, most large negative shocks are transitory. A negative change in log earnings of more than 0.2 is in 71% of the cases due to a transitory shock. The corresponding number for positive shocks is only 55%. The difference is even more pronounced early in life. At age 25, 94% of all negative changes in log earnings of more than 0.2 are the result of a transitory shock. In contrast, 50% of all positive changes in log earnings of more than 0.2 result from persistent shocks at age 25.====Next, we introduce this estimated earnings risk into an Aiyagari (1994) type model to study the implications of age-varying, non-normally distributed risk for consumption and savings decisions. We contrast the results to the widely used age-invariant risk model (====) with mean-zero normally distributed transitory and persistent shocks. Compared to this latter model, the large but rare persistent positive shocks early in life imply, as in the data, a relatively high dispersion in the right tail of the cross-sectional earnings distribution. A few lucky workers, therefore, accumulate large wealth holdings for life-cycle purposes, particularly to finance consumption during retirement, and hold a relatively large share of the overall wealth. This channel has a strong amplification mechanism for cross-sectional wealth inequality because these shocks occur early in life; thus, they imply large cross-sectional differences in lifetime earnings. Compared to the ====, the share of wealth holdings by the top 1% more than doubles, bringing the model closer to the data.====Similar to wealth inequality, consumption inequality is more pronounced in the right tail of the cross-sectional distribution in our model than in the ====. That is, the ratio of consumption of the top 1% relative to the median worker is relatively high and it grows relatively rapidly over the life-cycle. This shift of resources away from the median and towards the highest lifetime consumption workers reduces welfare in our model relative to the ====. Counteracting this effect, consumption inequality at the bottom of the distribution is somewhat lower in our age-varying risk model. Measuring welfare in terms of the consumption an unborn household is willing to pay to insure against idiosyncratic earnings heterogeneity, we find that the former effect dominates, that is, the welfare costs of incomplete insurance markets are higher in the age-varying risk model.====Age-varying non-normally distributed risk also helps to explain the dynamics of cross-sectional consumption inequality over the life-cycle. In specific, large negative tail shocks late in life increase the desired stock of workers' precautionary savings. We show that more precautionary savings and a shift towards more persistent and positive shocks increase the speed at which consumption dispersion increases late in life. As a result, the cross-sectional variance of log consumption grows close to linear in age, which is consistent with the German data analyzed by Fuchs-Schündeln et al. (2010).====Our findings contribute to the recent macroeconomic literature that studies the implications of non-normally distributed shocks for individuals' savings and consumption. Civale et al. (2017) show that wealth inequality decreases when earnings shocks become more negatively skewed. Castañeda et al. (2003) calibrate the earnings process such that it matches the observed right tail of the wealth distribution which implies a “superstar” earnings state. The large and persistent positive shocks we find early in the life-cycle have qualitatively the same effect. De Nardi et al. (2019) use a two-step approach to study higher-order earnings risk. First, they estimate the model proposed by Arellano et al. (2017) and, thereafter, estimate Markov processes on simulated data resulting from step one. Importantly, this approach allows for non-linear log earnings dynamics that imply shocks being less persistent; therefore, less costly in terms of welfare. Our finding that a shift of resources towards the right tail of the earnings distribution increases the welfare costs relative to an age-invariant risk model is complementary to theirs.====The rest of the paper is organized as follows. Section 2 describes the German data set. Section 3 presents the moments of residual earnings growth over the life-cycle. Section 4 describes the econometric model. Finally, Section 5 introduces our earnings process into a life-cycle savings model.",Modeling life-cycle earnings risk with positive and negative shocks,https://www.sciencedirect.com/science/article/pii/S1094202520300016,5 February 2020,2020,Research Article,200.0
"Conesa Juan Carlos,Domínguez Begoña","Department of Economics, Stony Brook University, Stony Brook, NY 11794, USA,School of Economics, The University of Queensland, Colin Clark Building (39), St Lucia, Brisbane, Qld 4072, Australia","Received 18 January 2019, Revised 26 November 2019, Available online 5 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2019.11.006,Cited by (3),Should capital income be taxed for redistributional purposes? ,"Should capital income be taxed for redistributional purposes? The seminal work of Judd (1985) suggests that it should not. He finds that the optimal redistributive tax on capital income is asymptotically zero even from the perspective of wealth-poor workers.==== Given this result, most of the literature on optimal taxation has focused on the implications of capital taxation for economic efficiency in representative agent models rather than the redistributive side of capital taxation. This paper challenges this view by introducing management time and tax deductible investments, as in Conesa and Domínguez (2013), in a worker-capitalist model a la Judd (1985).====While the standard optimal fiscal policy literature finds little support for substantial taxation of capital for redistributive purposes, in practice capital taxes are far from zero. This is illustrated in Table 1 and Fig. 1, where capital tax rates are shown to vary substantially across countries or over time because of political turnover. Our results suggest that some forms of capital taxation are good vehicles for redistribution while others are not, and provide quantitative magnitudes for the range of tax values that are optimal depending on social welfare weights on capital owners relative to workers.====Recently, in an important contribution to the literature of optimal taxation, Albanesi and Armenter (2012) identify a general optimality principle. Optimal policy should be such that permanent intertemporal distortions are eliminated in the long run. In terms of capital taxation, this general principle can be put into practice in two alternative ways: (i) capital taxes are set to zero, or (ii) investment is fully and immediately tax deductible so that constant capital taxes do not generate permanent intertemporal distortions. A recent example is Conesa and Domínguez (2013). In a representative agent model with intangible assets and investment deductibility, we consider two forms of capital taxation: one for which investment is not tax deductible, and a second one for which investment is fully and immediately tax deductible. We refer to them as corporate taxes and dividend taxes, respectively. Our analysis of a representative agent economy showed that optimal corporate taxes are zero, while the optimal tax rate on dividend income and labor income are both positive and equal to each other.====In contrast, this paper re-examines that question in a model environment that allows for redistributive considerations: an infinitely-lived worker-capitalist model, in which workers provide raw labor to firms while capitalists invest resources and provide management time to build capital. Management time, sometimes called sweat equity, is a form of intangible investment in which managers devote time and effort to their businesses in order to build equity. In comparison to Judd (1985), the different features of our environment are government debt, elastic labor supplies, management time, and the differential tax treatment of tangible and intangible investments.====Our main results are as follows. First, as in Judd (1985), the optimal corporate tax is zero at steady state regardless of the weight of agents in the social welfare function.==== Since tangible investment is not deductible from corporate income taxes, those create permanent intertemporal distortions that are not optimal for efficiency or redistributive purposes. In contrast, as in Conesa and Domínguez (2013), a constant dividend tax does not imply any intertemporal distortions, and can be set at levels different than zero for both efficiency and redistributive considerations. For efficiency purposes, Conesa and Domínguez (2013) find that dividend taxes should be set at the same level as labor taxes. For redistributive purposes, however, this paper finds that the optimal mix of dividend and labor income taxes depends on preference parameters, the life-time wealth of all agents and the point of view (Pareto weights) of the benevolent policymaker.====After these analytical results we resort to a computational model and discuss numerical results. To obtain the quantitative targets for our benchmark economy we mostly rely on aggregate data from the National Income and Product Accounts (NIPA). However, as NIPA does not include all intangible investments, we make use of our theory to revise the national accounts. For our benchmark calibration, and for a policymaker with a very low Pareto weight on workers, the optimal long-run policy is to set labor tax rates as high as 46 per cent and dividend tax rates as low as −9 per cent. As the policymaker becomes more pro-workers, optimal steady-state labor taxes decrease toward 2 per cent and dividend tax rates follow an inverted U-shape, peaking at 49 per cent, and reaching 40 per cent when the welfare function puts most weight on workers' welfare. For weights consistent with Pareto-improving reforms (relative to the benchmark), we find that the optimal long-run labor tax ranges from 4 to 9 per cent, while optimal dividend taxes vary from 32 to 49 per cent. A utilitarian government (equal weight on workers and capitalists) would set the long-run labor tax rate to 4 per cent and dividend tax rates to 42 per cent. For all Pareto-improving reforms optimal dividend and labor taxes are higher during the transition and result in negative levels of public debt in the long run.====We consider two extensions. First, we explore the role of managerial effort. Without management time, we find that optimal taxes are rather constant during the transition and higher in the long run, with a positive level of government debt in the long run. Second, we allow for heterogeneity in discount factors and endogenous participation in financial markets by workers, and we find that this feature has little impact on our results.====Our paper is related to the public finance results on taxation with investment tax deductibility (such as Hall and Jorgenson (1971) and Auerbach (2002)). More recently, in a general equilibrium framework, Abel (2007) shows that full and immediate investment tax deductibility changes the distortionary properties of capital taxation.====Our work is also related to those of Rogers (1986) and Armenter (2007). Both study the relationship between credibility (of the optimal policy) and redistribution motives in the setup of Judd (1985). Armenter (2007) shows that the specific Pareto weights are important for the credibility of an optimal redistributive tax policy. More recently, Bassetto (2014) studies optimal redistributive taxes in an economy with real shocks and no capital. He finds that for extreme values of the Pareto weights, a government may impose large distortions in the economy in order to redistribute wealth. Related to our paper but without intangible investments, Greulich et al. (2016) study Pareto-improving reforms and also find that labor taxes should be cut.====The rest of the paper is organized as follows. Section 2 presents the model economy. Section 3 sets the Ramsey problem and derives the main results. Section 4 discusses the numerical exercises. Section 5 explores the extensions we consider. Section 6 concludes.",Capital taxes and redistribution: The role of management time and tax deductible investment,https://www.sciencedirect.com/science/article/pii/S1094202520300077,5 February 2020,2020,Research Article,201.0
"Curtis Chadwick C.,Garín Julio,Saif Mehkari M.","Department of Economics, Robins School of Business, University of Richmond, Richmond, VA 23173, USA,The Robert Day School of Economics and Finance, Claremont McKenna College, Claremont, CA 91711, USA","Received 21 May 2018, Revised 2 November 2019, Available online 5 February 2020, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.11.004,Cited by (2),We present a model of a ,"Prior to 2018, the U.S. government collected taxes on the worldwide profits of U.S. based corporations. In addition to paying foreign taxes on profits earned abroad, corporations were often also subject to U.S. taxes once these profits were repatriated to the U.S. This is known as the repatriation tax. Many firms argued that these additional repatriation taxes deterred them from repatriating foreign sourced income. Leading up to the enactment of U.S. tax reforms in 2018, these foreign profits, not yet taxed by the U.S. government, stood at over $2 trillion. This large accumulation of assets abroad pushed changing repatriation tax policies high on the legislative agenda. Motivated by long and ongoing policy discussions, as well as past and recent repatriation tax policy reforms, we build a dynamic model to quantify the effects of repatriation tax policy changes on firm-level variables and to understand the mechanisms driving those responses. Can tax reforms lead to an increase in repatriated assets? Do these reforms stimulate employment and investment? What are the associated tax revenue costs? How are the costs and benefits of a reform influenced by protracted legislative deliberations and policy uncertainty? The goal of this paper is to shed light on these questions.====While the economic and tax revenue consequences of reforming repatriation tax policy are potentially large and involve a non-trivial dynamic aspect, the literature, for the most part, has abstracted from studying the dynamic behavior of the firm that accounts for expectations of changes in repatriation tax policy. To the best of our knowledge, this paper presents the first quantitative framework capturing the dynamic impacts of repatriation tax changes that includes the anticipation effects of such reforms.====We find that accounting for the anticipatory behavior of a firm, along with the responses after the policy change, is essential to fully understand the effects of repatriation tax policy changes. In our model, firms respond to news of a repatriation tax policy change in advance of the actual policy change. Conceptually, we consider news as any information that alters the likelihood of future repatriation tax changes such as a policy proposal, the deliberation of a policy, or the legislative lag. Receiving news of a potential future reduction in repatriation tax rates leads to a reduction of repatriated income from abroad, an accumulation of foreign assets untaxed by the U.S. government, and a fall in U.S. government tax revenue. At the enactment of a repatriation tax policy change, firms repatriate back the assets withheld during this ==== – the period between the arrival of news and the policy change. As in our baseline experiment, modeled after the temporary repatriation tax reduction in 2004-2005, firms additionally bring forward the repatriation of assets that were planned to be remitted at a future date to the time of the policy change, causing repatriations to once again fall after the implementation of the policy. As a result, policy evaluations that do not account for a firm's anticipation of lower future repatriation tax rates overstate the amount of income repatriated from abroad and the effects on labor and capital, while they understate the losses in tax revenue.====One of the primary motivations for reforming repatriation tax policy is to incentivize firms to repatriate assets to the U.S., thereby stimulating domestic employment and investment. We find that the effects of a reduction in the repatriation tax rate on U.S. employment and investment crucially depend on the firm's ability to access external credit markets. When the cost of accessing credit is high, the firm is more dependent on internal funds to support production activities. For such a firm, a contraction in repatriations during the news period corresponds with a large contraction in its U.S. production, and the influx of foreign income at the time of the policy change leads to a sizable expansion of domestic activity. Since most multinational firms are large and relatively unconstrained in their access to credit, our analysis indicates that a repatriation tax rate reduction has a relatively minor impact on domestic employment and investment. Firms that are not credit constrained are able to operate close to their optimal scale independent of whether or not they access their foreign assets. Thus, while a policy change may result in a large inflow of foreign assets, this change in asset flows does not affect production but primarily affects the firm's debt position and shareholder payouts.====Our model consists of a firm that is incorporated in the U.S., but operates and holds assets both domestically and abroad, with the objective of maximizing dividend streams paid to U.S. shareholders. Within each country, the firm decides on the levels of capital and labor required for production, its holdings of liquid financial assets, and the amount of debt to carry in the U.S. Across geographies, profits originating from abroad and repatriated back to its U.S. parent are subject to a repatriation tax levied by the U.S. government. Thus, repatriation taxes play a key role in the across-geography allocation decision. We use this framework to quantify the impact of repatriation tax changes on the firm's decisions within and across geographies.====Our baseline experiment studies the effects of a temporary repatriation tax rate reduction that is anticipated a year in advance of its implementation. While we consider a range of repatriation tax policies, this experiment is motivated and disciplined by the American Jobs Creation Act of 2004 (AJCA), which granted a one-time “tax holiday” on repatriated assets in 2005. In our model, during the news period the firm reduces the rate of repatriations from abroad and accumulates foreign assets to maximize its tax savings from the tax holiday. This reduced flow of assets into the U.S. leads to a small contraction in domestic production and to losses in U.S. tax revenue. At the enactment of the policy, the accumulated foreign assets flow into the U.S.; the firm then uses the additional inflow of assets to primarily pay U.S. shareholders, and reduce its debt.====We show that during the period between when a proposal is presented and its (potential) approval, there is a change in the relative cost of repatriating funds. The period of deliberation can be thought of as a wedge that distorts the firm's decision relative to the status quo without these announcements. We capture and quantify this wedge, generated by the news itself, which we refer to as a “shadow tax.” By altering the intertemporal cost of repatriating foreign assets, news of a future tax reduction has both an income effect – higher expected future disposable income induces the firm to repatriate income for dividend payments today – and a substitution effect – repatriating funds today is relatively more expensive than in the future.====Prior to 2018, repatriation rates were set as the larger of zero and the difference between U.S. and foreign tax rates, and by 2017 the top marginal U.S. corporate income tax was the highest among OECD countries. U.S. repatriation taxes were recently eliminated from legislation under the ====. However, calls for repatriation tax reform preceded these reforms by many years. Following the enactment of the AJCA, bills were introduced to congress nearly every year requesting temporary and/or permanent reductions in repatriation tax rates. To study the effects of changes in expectations about repatriation tax reforms that such protracted discussions may introduce, we additionally model news with uncertainty surrounding ==== and ==== a repatriation tax change will occur. We show that uncertainty in the timing of the policy change generates a ‘wait-and-see’ effect. If the firm deems that future repatriation tax reform is likely but they are unsure ==== it will occur, they steadily accumulate foreign asset holdings, which can persist over a long time horizon. Even though the intent of the many proposals was to attract offshore assets held by U.S. firms, the discussions of such proposals arguably had the opposite effect of inducing firms to further accumulate assets abroad while they await a resolution of policy.====An innovation of our dynamic analysis is the inclusion of the anticipation, or new effects, of repatriation tax reform. In this regard, we complement models analyzing the impacts of repatriation taxes such as the static analysis of repatriation/investment decisions from an uncertain arrival of a tax holiday of De Waegenaere and Sansing (2008), Altshuler and Grubert (2003)'s theory of tradeoffs between investment and repatriation decisions of multinational corporations, and the structural model of the relationship between firm-level cash holding and repatriation tax rates in Gu (2017). In an influential paper, Hartman (1985) argues that if tax rates on multinational firms were constant across time, the level of repatriation tax rates would have no impact on the repatriation decisions of mature firms because these taxes would be unavoidable. In our model, in the absence of an actual policy change, a reduction in repatriations and an increase in the stock of foreign asset holdings only occurs if firms expect a future repatriation tax reduction.====The partial equilibrium nature of our model allows for tractability and enables us to focus on the portfolio allocation – cross and within country – and study the mechanisms and adjustments occurring at the firm level. However, the partial equilibrium model abstract from both indirect price adjustments that could dampen the firm-level responses and from long-run efficiency gains from the reduction of a distortionary tax. In this sense, our paper is most suitable for understanding news and the periods around a repatriation tax change rather than its long-term effects. Incorporating general equilibrium effects, Spencer (2017) investigates the aggregate implications of a permanent repatriation tax reform. We believe both papers to be complements. His equilibrium environment can draw conclusions on the welfare consequences of permanent repatriation tax changes, whereas our paper focuses on the dynamics surrounding reforms – and discussions changing the probability of future tax rates, – including firm level responses to news of a reform. Further, we abstract from corporate inversions and model multinational firms with headquarters in the U.S. which are subject to U.S. repatriation tax policy. We focus on the dynamic effects of repatriation tax changes rather than long-term decisions of corporate headquarter relocation (or even the choice to become a multinational) that, arguably, may arise from repatriation tax policy or uncertainty in future repatriation tax policy changes.====Our contribution can also be viewed as a counterpart to the empirical literature looking at the effects of repatriation tax policy change from the AJCA such as Dharmapala et al. (2011), Blouin and Krull (2009) and Faulkender and Petersen (2012). As external validation of our model, we find that our policy experiments of a one-time repatriation tax reduction capture the salient features found in these empirical studies.====Our paper follows the large literature on fiscal policy news shocks such as House and Shapiro (2006), Yang (2005), Leeper et al. (2012), and Beaudry and Portier (2007). We investigate a specific fiscal policy shock – repatriation tax changes – and evaluate the tax revenue consequences and firm-level responses to the shock across a set of variables. In this regard, our analysis is closest to the news and uncertainty of future tax policy studied in Mertens and Ravn (2011) and Stokey (2016). Stokey presents a model with tax uncertainty that can generate an investment boom after the resolution of the policy. In that environment, firms reduce investment in new projects and accumulate liquid assets as a ‘wait-and-see’ policy until the uncertainty is settled. We differ from Stokey in two ways. First, ours is a quantitative study. This allows us to map some objects in our framework to the data. Second, we allow for firms to access financial markets. Our framework generates similar dynamics to Stokey but that behavior crucially depends on the firm's ability to access credit markets. In our model, allowing the firm to access external and internal financing dampens the investment effects of news of a policy change. Specifically, in the news period, the firm finances domestic operations with external financing while simultaneously accumulating liquid assets abroad.====In the DSGE model of Mertens and Ravn (2011), the economy experiences a contraction of output and investment in anticipation of a tax cut and then an expansion of these variables once the tax cut is implemented, regardless of whether it was anticipated or not. Whereas firms in Mertens and Ravn (2011) adjust domestic inputs in response to a tax cut, our focus on international firms provides an additional margin of adjustment. In our benchmark model, responses to output and investment to either an anticipated or unanticipated reduction in the repatriation tax rate are small due to the firm's ability to borrow and alter asset flows between domestic and foreign operations. This is consistent with the investment dynamics found in the empirical literature of the most recent U.S. repatriation tax change under the AJCA (Dharmapala et al., 2011; Faulkender and Petersen, 2012).",Repatriation taxes,https://www.sciencedirect.com/science/article/pii/S1094202520300041,5 February 2020,2020,Research Article,202.0
"Mendoza Enrique G.,Villalvazo Sergio","Department of Economics, University of Pennsylvania, Philadelphia, PA 19104, USA,NBER, USA,PIER, USA","Received 16 September 2019, Revised 28 January 2020, Available online 4 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.01.001,Cited by (17),"We propose a simple and fast fixed-point iteration algorithm (====) to obtain the global, non-linear solution of macro models with two endogenous state variables and occasionally binding constraints. This method uses fixed-point iteration on Euler equations to avoid solving two simultaneous non-linear equations (as with the time iteration method) or creating modified state variables requiring irregular interpolation (as with the endogenous grids method). In the small-open-economy RBC and Sudden Stops models provided as examples, ==== is much faster than time iteration and various hybrid methods.","Important branches of the recent macroeconomics literature study quantitative solutions of models in which constraints are triggered endogenously (i.e. they are “occasionally binding”), as in studies of the zero-lower-bound on interest rates or financial crises triggered by credit constraints. Because these models typically feature non-linear decision rules that lack analytic solutions and capture precautionary savings, global solution methods (e.g. time iteration or endogenous grids methods) are the preferable tool for solving them. Global methods are, however, less practical than perturbation methods, because of limitations that make them slow and difficult to implement with widely used software (e.g. Matlab). On the other hand, perturbation methods for solving models with occasionally binding constraints, such as OccBin developed by Guerrieri and Iacoviello (2015) and DynareOBC proposed by Holden (2016), have caveats that limit the scope of the findings that can be derived from using them (see Aruoba et al. (2019), de Groot et al. (2019))====This paper proposes a simple and fast algorithm to obtain the global solution of models with two endogenous states and occasionally binding constraints. This algorithm is denoted as ==== because it is based on the well-known fixed-point iteration approach to solve systems of transcendental equations. It is easy to implement in a Matlab platform and is significantly faster than the standard time iteration algorithm and several hybrid alternatives. ===='s solution strategy builds on the class of time iteration methods that originated in the work of Coleman (1990), who first proposed a global solution method based on policy function iterations of the Euler equation. Since then, various enrichments and modifications of this approach have been developed, in particular the endogenous grids method proposed by Carroll (2006) (see Rendahl (2015) for a general discussion of these methods and an analysis of their convergence properties). ==== differs from these methods in that it applies the fixed-point iteration method to solve a model's Euler equations. For instance, in the Sudden Stops model solutions provided as example in this paper, the bonds (capital) Euler equation is used to solve directly for a “new” bonds decision rule (capital pricing function) without the need of a non-linear solver. The capital decision rule is solved for in “exact” form using the models' optimality conditions.====The endogenous grids method also avoids using a non-linear solver, but it does so by defining alternative state variables so that obtaining analytic solutions of Euler equations for control variables (e.g. consumption, investment) requires irregular interpolation of functions defined over endogenous grids of the original state space. This is innocuous in one-dimensional problems, but in two- and higher-dimensional problems it requires elaborate interpolation methods to tackle the non-rectangular nature of the endogenous grids. In particular, Ludwig and Schön (2018) developed a method using Delaunay interpolation, and showed that it is significantly faster than standard time iteration.==== Alternatively, Brumm and Grill (2014) proposed a variant of the time iteration method that still uses a non-linear solver but gains speed and accuracy by updating grid nodes to track decision rule kinks using also Delaunay interpolation. In contrast, ==== retains the original state variables so that standard multi-linear interpolation on regular grids can be used.====We apply the algorithm to solve the model proposed by Mendoza (2010), which is a model of Sudden Stops (financial crises) in a small open economy. This model includes an occasionally binding credit constraint limiting intertemporal debt and working capital not to exceed a fraction of the market value of physical capital (i.e. pledgeable collateral). The results show that, relative to the time iteration method, ==== reduces execution time by a factor of 2.5 (or 18.1 when solving an RBC variant of the model).==== We also found that ==== continues to perform well for several parameter variations, despite the well-known drawback of fixed-point iteration methods indicating that their convergence is not guaranteed. Execution times for seven parameter variations of the model were smaller than using time iteration by factors of 2.0 to 18.1. Ludwig and Schön (2018) report reductions by factors of 2.7 to 4.1 using endogenous grids with Delaunay interpolation v. standard time iteration, or 1.8 to 2.5 using their hybrid method v. standard time iteration, when solving a perfect-foresight model of human capital accumulation in a small open economy.====In addition to the Delaunay interpolation, a second drawback of the endogenous grids method relative to the ==== method is that it still requires a root-finder in order to determine equilibrium solutions in points of the state space in which occasionally binding constraints bind (see Ludwig and Schön (2018)). ==== requires a non-linear solver only if the solution of the allocations when the constraint binds cannot be separated from the solution of the multiplier of the constraint. The two are separable in models that feature several widely-used occasionally binding constraints, including standard no-borrowing constraints, maximum debt limits, and constraints on debt-to-income and loan-to-value ratios that depend on endogenous variables. Solving variations of the SS model using these constraints, ==== reduced execution time relative to the time iteration method by a factor of 13.0 for a loan-to-value-ratio constraint and 17.9 for a maximum debt limit.====There are applications in the literature that solve models using fixed-point iteration algorithms with some features similar to the one we proposed here. Carroll (2011) described and implemented a fixed-point iteration algorithm for solving the workhorse complete-markets RBC model of a closed economy. Boz and Mendoza (2014), Bianchi and Mendoza (2018) and Bianchi et al. (2016) solved open-economy models with occasionally binding collateral constraints iterating on bond decision rules and/or pricing functions. All these applications considered only one endogenous state variable. Perri and Quadrini (2018) solved a two-country model with two endogenous state variables and a credit constraint resulting from an enforcement friction using Fortran and a state space with 121 points (11 nodes for each state variable). This paper differs from these studies in that we develop an algorithm that solves models with two endogenous states easily and fast in a standard Matlab platform and with a sizable state space including 2,160 points. ==== can be used in a variety of models with two endogenous states. The choice of functions that are iterated on using the Euler equations can vary across models, and there can be more than one arrangement for the same model.====The rest of the paper proceeds as follows. The next Section describes the principles of the algorithm in the simple case of a model of savings with endowment income, and uses this example also to explain how ==== differs from the time iteration and endogenous grids methods. Section 3 describes the Sudden Stops model and provides a step-by-step description of the complete algorithm. Section 4 provides quantitative results, evaluates the robustness of the algorithm, and conducts performance comparisons with alternative algorithms, including the standard time iteration method. Section 5 presents conclusions. In addition, the Matlab codes and an Appendix that provides a user's guide to the codes are available online.",": A simple, fast global method for solving models with two endogenous states & occasionally binding constraints",https://www.sciencedirect.com/science/article/pii/S1094202520300028,4 February 2020,2020,Research Article,203.0
"Samaniego Roberto M.,Sun Juliana Yu","Department of Economics, The George Washington University, 2115 G St NW Suite 340, Washington, DC 20052, United States of America,School of Economics, Singapore Management University, 90 Stamford Road, Singapore 178903, Singapore","Received 16 January 2019, Revised 9 January 2020, Available online 4 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.01.004,Cited by (5),"Are trends in the price of capital technological in nature? First, we find that trends in the relative price of capital vary significantly across countries. We then show that a multi-industry growth model, calibrated to match differences in economic structure around the world and productivity growth rates across industries, accounts for this variation – mainly due to variation in the composition of capital. The finding indicates that the rate of change in the relative price of capital can be interpreted as investment-specific technical change – the extent to which productivity growth is relatively more rapid in the capital-producing sector. The model also accounts for the empirical dispersion of investment rates, but not of rates of economic growth.","Declines in the relative price of capital are viewed as an important factor of economic growth in the United States (US). See for example work by Hulten (1992), Greenwood et al. (1997), Cummins and Violante (2002) and Oulton (2007).==== These studies typically identify the decline in the price of capital as being technological in nature, reflecting faster productivity growth in the production of new capital than in the production of consumption and services – a phenomenon known as ==== (ISTC). However, the extent to which the relative price of capital declines ==== is not known. In addition, it is not known whether trends in the price of capital around the world can be given a ==== interpretation, such as ISTC. An alternative hypothesis is that these differences are due to the presence of barriers to capital accumulation, as proposed by Restuccia and Urrutia (2001) to account for differences in ==== of the price of capital.====We begin by documenting that the rate at which the relative price of capital changes over time varies significantly across countries. We find that the median growth rate of the price of capital is zero. In addition, the price of capital ==== in as many places as it decreases. This indicates that, if there is a technological explanation for this phenomenon, technical progress in capital relative to other sectors must vary widely around the world.====If the explanation is indeed technological, however, one would expect such glaring differences in productivity to be evidence of draconian barriers to international technology transfer (or trade). The alternative possibility is that capital and consumption are themselves highly disaggregated, and that there are substantial differences in the ==== of capital and consumption around the world that account for the aggregate differences in the trends in the relative price of capital.====We ask whether this variation can be accounted for by differences in ====. The reason we do this is as follows. It is well known that rates of technical progress in the US differ significantly not just between capital and non-capital, but also across ==== of consumption, services and capital. Thus, even if productivity growth rates are constant across countries for each industry, the rate of change in the relative price of capital may be different if the ==== of capital – or the composition of consumption and services – is different. Indeed, we find that the composition of capital is skewed towards high-TFP growth capital types in countries where the price of capital declines rapidly. We therefore ask: to what extent can differences around the world in industry composition account for variation in the rate at which the relative price of capital changes?====To this end, we employ a canonical multi-industry growth model. In the model, the composition of the economy evolves as a result of changes in prices of different goods or services that agents consume, as well as changes in the prices of different capital goods.==== In turn, these are determined by differences in productivity growth rates across industries. We calibrate the model using detailed productivity growth data from the US, as well as data on the initial composition of economies around the world in the year 1991. We use constant productivity growth rates for a given industry in all countries partly because of data limitations; however, as mentioned, significant barriers to technological transfer would have to exist to significantly deviate from this assumption. Composition is a key part of the “no barriers” hypothesis.====Strikingly, we find that the model delivers a close match to the rate of change in the relative price of capital, as measured using the Penn World Tables (PWT) version 7.1.==== In a statistical sense, the model can account for the entirety of the magnitude of variation of the growth rate in the relative price of capital over the period from 1983 to 2011, simply based on industry TFP growth rate differences and on differences in industry composition across countries. Not only does the model match the extent of variation, but also the correlations between model-generated capital price growth rates and those in the data are highly significant. We conclude that differences in the relative price of capital around the world can be interpreted as a technological phenomenon – ISTC – and that a key factor behind these differences is industry composition.====The link between composition and the decline in the relative price of capital could be for two reasons: differences in the composition of capital, or in the composition of non-capital. We refer to these possibilities as the ==== and the ====, respectively. We study the importance of each hypothesis by removing productivity growth differences in the capital producing industries, and then separately removing them in the non-capital producing industries. We find that the ==== is mainly responsible for cross-country variation in ISTC: removing productivity growth in non-capital makes very little difference to the results, whereas removing productivity growth in capital-producing industries results in model-generated statistics that bear little relationship with the data.====Finally, we ask to what extent a growth model driven solely by these factors can account for differences in aggregate behavior across countries over the sample period. Specifically, we look at investment rates and rates of economic growth. This is a non-trivial task, as it requires solving for investment patterns in a model where conditions for a balanced growth path do not hold in general. We find that the model generates investment rates that are strongly correlated with investment rates in the PWT 7.1 data and the PWT 9.1 data, although they underpredict the extent of empirical variation in investment rates. Thus, the model is able to capture cross-country variation in both ISTC and (to a lesser extent) investment rates, solely based on differences in industry composition. However, the model does not generate a good match to variation in rates of ==== in the PWT 7.1 data, nor in the PWT 9.1 data. We conclude that there is widespread divergence in the rate of ISTC around the world, and that this accounts for variation in investment, but that economic growth rates are due to other factors. Interestingly, when we give each country an aggregate productivity trend that exactly matches its economic growth rate in the data, investment rates are no longer correlated with those in the data, suggesting that whatever factors do underlie rates of economic growth are not simply captured by a trend in productivity.====The results contribute to a long-standing debate regarding whether or not changes in the efficiency of investment are an important factor of growth. This debate goes back to Solow (1962), Abramovitz (1962) and Denison (1964). Greenwood et al. (1997) find that, in the US, more than half of economic growth can be accounted for by ISTC in a general equilibrium growth accounting framework. We provide a clear answer to the question about whether differences in the relative price of capital can be attributed to barriers or to technological factors, indicating that changes in the efficiency of investment are an important factor affecting growth rates. This is not to say that there is no scope for barriers to be important for the relative price of capital; however, their impact might not be direct, but rather ====, through their influence on economic composition. More broadly, this suggests that future work on the manner in which factors of economic growth might be affected by policy through the channel of economic composition could be fruitful.====Section 2 presents data on trends in the relative price of capital around the world, as well as evidence that variation in these trends is linked to variation in economic composition. Section 3 introduces the model economy. Section 4 details the calibration procedure. Section 5 describes the results, and Section 6 discusses whether there is any link between trends in the relative price of capital and country characteristics. Section 7 concludes.",The relative price of capital and economic structure,https://www.sciencedirect.com/science/article/pii/S1094202520300089,4 February 2020,2020,Research Article,204.0
Park Choonsung,"Korea Institute of Finance, Republic of Korea","Received 21 May 2018, Revised 27 January 2020, Available online 4 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.01.002,Cited by (4),"This paper exploits micro data on the joint distribution of consumption and wages to measure the Frisch labor supply elasticity at the extensive margin. The approach is based on a derived reservation property of the working decision that holds in a class of models where the labor choice is intratemporal: given consumption, there exists a unique wage level above which individuals work and below which they do not. It is shown that this property is robust to heterogeneity in borrowing constraints, discount factors, and wage processes within the class of models. Applying it directly to the observed joint distribution of consumption and wages, I find that the aggregate Frisch elasticity at the extensive margin is 0.4 and that across various demographic groups, the elasticity ranges from 0.2 to 0.6. However, the elasticity variation is more pronounced across consumption groups and the elasticity is decreasing in consumption within demographic groups. The latter findings imply that the measured elasticity critically depends on the composition of consumption groups in the sample.","Understanding individual employment decisions is critical in the macroeconomic and public finance literature. A large portion of the fluctuations in aggregate hours is attributed to the variation of employment over the business cycle, and the impacts of fiscal policy on welfare and inequality depend crucially on how responsive employment is to policy changes.==== Since the behavioral responses of employment are best summarized by the labor supply elasticity at the extensive margin (or the wage elasticity of the employment rate), the measurement of the elasticity has generated significant research attention.====In principle, the magnitude of the extensive margin elasticity depends on the distribution of reservation wages in the economy (Chang and Kim, 2006): The number of workers whose wages are close to their reservation wages is key to the size of the elasticity. But the difficulty is that we do not observe reservation wages in the data. A typical solution is to build a structural model and study the model-implied elasticity. While this approach allows us to perform rich analyses within the model structure, it is well known that the results are sensitive to the model specification. For example, Gourio and Noual (2009) report that the extensive margin elasticity ranges from 0.4 to 7 depending on distributional assumptions.====This paper attempts to measure the aggregate labor supply elasticity at the extensive margin in a manner robust to model specifications in a class of heterogeneous-agent macroeconomic models.==== The argument proceeds in two steps. First, I derive a reservation property of the working decision which is robust to model specifications. Second, this property is directly applied to micro data on the joint distribution of consumption and wages to measure the elasticity without restrictive specifications.====The derived reservation property states that given a consumption level, there exists a ==== wage level above which individuals work and below which they do not, independent of the state variables. I define this unique wage level as the reservation wage associated with the given consumption, and I call it the reservation wage conditional on consumption.====To understand this property, consider the following thought experiment. In the framework based on heterogeneous agents under incomplete markets (Huggett, 1993 and Aiyagari, 1994), individuals want to smooth consumption and they build a buffer stock of assets for future consumption. Consider two individuals ==== and ====, and assume that ==== has more assets with other things being equal. Since working is costly in terms of utility, workers choose not to work if the marginal value of additional consumption is low. Then, ==== is less likely to accept low wages because his assets imply higher consumption, and therefore a lower marginal value of consumption. This illustrates that the reservation wage is increasing in assets. But if I allow another dimension of heterogeneity such that ==== has more incentive to save (possibly because he wants to bequest more), then ===='s larger assets do not necessarily imply a higher reservation wage: it could in fact be lower than ===='s because he might accept very low wages to increase his savings. However, if that is the case, ===='s high propensity to save would lead him to consume less, which suggests that ===='s low reservation wage is likely to be reflected in his low consumption. In this environment, the reservation wage depends on both assets and saving incentives, but it would be revealed by a single variable, consumption.====I formalize the above argument within a class of macroeconomic models. In particular, under the assumption that preferences are time separable, the period utility function is identical across agents, and the choice of hours is intratemporal, I show that the reservation wage conditional on consumption is unique and independent of the state variables, irrespective of the following specification choices: (i) time horizon (infinite time versus life cycles), (ii) flexibility of hours choices (continuous hours versus indivisible labor), (iii) separability of consumption and leisure in the period utility function, and (iv) arbitrary heterogeneity in borrowing constraints, saving incentives (represented by the discount factor), and wage processes. It is further shown that if labor is indivisible (Rogerson, 1988 and Hansen, 1985), the property remains valid in the setting of the two earners' problem.====In the second step, I use the reservation property conditional on consumption to measure the extensive margin elasticity directly from the data. Intuitively, if I observe all workers' wages and consumption without measurement error, the reservation property tells me that given consumption, the observed minimal wage is the reservation wage conditional on consumption. Then in response to a negative wage shock, the share of workers whose wages fall below the minimal wage would stop working, producing the extensive margin elasticity. That is, those workers with low wages relative to consumption are marginal workers. Notice that in this experiment, consumption is held constant so that I measure the Frisch elasticity in the sense that the wealth effect is held constant. This procedure implies that the cross-sectional joint distribution of consumption and wages of workers is sufficient to infer the aggregate labor supply elasticity at the extensive margin. This is the point that allows me to measure the elasticity without assumptions on asset markets or wage processes.====Formally, I use quantile regression (Koenker and Bassett, 1978) to describe the observed distribution of consumption and wages. This technique is useful because (i) it allows me to look at the distribution in a parsimonious and systematic way, and (ii) I can simply control for potential preference heterogeneity up to the observable variables. That is, through quantile regressions, I obtain the distribution of workers' wages conditional on consumption and other covariates (e.g., age and education), and measure the elasticity within the group using the quantiles near the bottom. The aggregation of the group elasticities produces the overall wage elasticity of employment rate with consumption held constant.====A challenging issue, however, is that consumption and wages are measured with error. Measurement error increases the observed dispersion of the distribution relative to the true one. Since the increased dispersion tends to imply less density at the margin, suggesting a relatively small number of marginal workers, ignoring measurement error would bias the measured elasticity downward. While this issue of inferring the underlying true density in the presence of measurement error is not clearly resolved in the literature, I develop a tractable procedure to take into account measurement error. The idea is to shrink the observed distribution to some extent using the wage quantiles in measuring the elasticity. After some validation experiments based on external studies about measurement error, the proposed estimator is applied to the data.====The findings are as follows. First, in my preferred setting, the aggregate Frisch labor supply elasticity at the extensive margin is 0.4, and across demographic groups, the elasticity ranges from 0.2 to 0.6. Second, the variation of the elasticity estimates are more pronounced across consumption groups. For instance, the elasticity ranges from 0.2 (95th consumption quantile group) to 0.9 (5th consumption quantile group) for single (not married) individuals. This pattern that workers with low consumption have larger elasticities remains valid within demographic groups. Intuitively, permanent income hypothesis shows that people with lower consumption have lower permanent income, which in turn, implies that the wage variation among these individuals is not large. Then, given a low level of consumption, wages are relatively more bunched near the bottom. The theory implies that those with these low wages are marginal workers and the measured elasticity becomes larger for this group. The results imply that the measured elasticity critically depends on the composition of consumption groups in the sample. Third, both the theory and the measurement procedure in this paper provide important implications for calibrating structural models. Since the number of marginal workers is revealed by the joint distribution of consumption and wages irrespectively of assumptions on asset markets or wage processes, it is important for models to match moments from the joint distribution.====This paper is organized as follows. Section 2 defines a class of models and shows the reservation property of the working decision in a general framework. Section 3 defines the Frisch labor supply elasticity at the extensive margin using the joint distribution of consumption and wages. Section 4 describes the procedure to measure the elasticity using the quantile regression. Section 5 describes the data. Section 6 reports the main results and discusses them along with related literature. Section 7 concludes.","Consumption, reservation wages, and aggregate labor supply",https://www.sciencedirect.com/science/article/pii/S109420252030003X,4 February 2020,2020,Research Article,205.0
Varvarigos Dimitrios,"School of Business, Division of Economics, University of Leicester, 266 London Road (Mallard House, Brookfield), Leicester LE2 1RQ, United Kingdom","Received 19 November 2018, Revised 11 November 2019, Available online 4 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2019.11.005,Cited by (6),"This study emphasises the interdependence between physical capital formation and the cultural transmission of human capital-promoting attitudes and values. It demonstrates how this interdependence establishes a powerful propagation mechanism that generates multiple, divergent paths ====. It also highlights the role of physical capital formation in expanding the conditions that are propitious to path-dependency in models of cultural transmission: Even in the absence of a cultural complementarity, the long-run equilibrium is sensitive to the initial distribution of cultural attitudes among the population, as long as the combined effects of physical capital formation and social segregation permeate the process of cultural transmission.","The idea that cultural change and economic performance are mutually dependent is by no means a new one. Karl Marx's view was that cultural traits are mere by-products of the prevailing economic/material conditions, whereas Max Weber surmised that the prevailing culture is a significant determinant of economic change. Can the idea of an interrelation between culture and economic activity be pertinent to the role of human capital for economic growth and development?====According to Lucas (1993), features such as significant improvements in educational attainment and human capital, which distinguish countries that have achieved and sustained high levels of economic development, should not be merely viewed as the explanatory factors behind their economic success. Instead, these ==== (Lucas, 1993, p. 252). Motivated by this assertion, van Hoorn (2016) employed an empirical study to argue that cultural traits such as attitudes, values and social norms, have a significant impact on people's views on the importance of education and, therefore, their propensity towards human capital-promoting activities.==== Given these arguments, the aim of this study is to shed more light on the interplay between economic growth and the cultural transmission of education-promoting attitudes and values; to identify some new mechanisms that underlie this interplay; and to offer new insights not only into the process of economic development but also into the process of cultural transmission itself.====Although human capital has been a cornerstone of our understanding of economic growth, the current literature lacks a body of systematic work that delves deeper into the behavioural and cultural traits that influence people's propensity towards human capital-promoting activities, as well as the implications of these factors for economic development.==== One notable exception is the work of Doepke and Zilibotti (2008) who argued that the two-way causal effects between economic progress and the interegenerational transmission of occupational choice-related and work ethic-related preferences can account for the socioeconomic and structural transformations that coincided with the Industrial Revolution in Britain.==== While their intention was to provide a theoretical underpinning for phenomena and developments that are particular to the Industrial Revolution, one can hardly dispute with the idea that the underlying principle, which views economic progress as being inherently linked to the cultural traits that encourage people to undertake human capital-promoting activities, is also pertinent to more recent circumstances and events.====Consider the case of East and West Germany for example. In spite of their reunification more than two decades ago, and the ensuing adjustment of their formal institutions, significant economic disparities between them seem to be persistent (Uhlig, 2008, Smolny, 2009). Seeking to identify factors that could account for the differing economic fortunes between East and West Germany, van Hoorn and Maseland (2010) argued that Germany's division generated differences in ==== (van Hoorn and Maseland, 2010, p. 791). Their empirical results showed that West Germans' greater esteem for higher academic education is among those persistent and significant cultural differences between the two regions.====In a similar vein, the empirical study of Desdoigts (1999) showed that human capital accounts for differences in economic development between East Asia and Africa, more than any other institution-related variable. He attributed this outcome to the persistent cultural heritage of Confucianism, whose ingredients of ==== (…) ==== (Desdoigts, 1999, p. 317).====Since the interplay between economic progress and the cultural evolution of attitudes on the importance of education possesses a broader explanatory power, which applies to more recent observations of divergent economic performance among different regions, the objective of this study is to embed these characteristics in a theory of ‘modern’ economic growth, i.e., a theory whose point of departure is the endogenous growth paradigm. I construct a model that integrates the accumulation of both human and physical capital with an explicit process for the cultural transmission of a behavioural trait that embodies people's attitudes towards human capital-promoting activities. The cultural transmission process draws on Bisin and Verdier (2001), i.e., young generations are inculcated with education-promoting attitudes, either by their parents (direct transmission) or by imitating role models (oblique transmission).==== Based on the notion of ‘neighbourhood effects’, for which Borjas (1995) and Patacchini and Zenou (2011) offer empirical support, I adapt this process by considering a scenario where, due to self-selection and social segregation, the parents' social environment, as well as their interests and activities, affect the likelihood that their children will find suitable role models to motivate them into the adoption of human capital-promoting attitudes. Physical capital, together with the time/effort devoted by young individuals, is an input to the human capital technology, thus it increases the return to education and motivates altruistic parents to intensify their efforts to instil the education-inducing cultural trait in their children. At the same time, an increase in the population share of those individuals who have been inculcated with the education-inducing trait, promotes physical capital formation through its positive effect on productivity, saving and investment.====In terms of economic growth and development, the model's analysis and results highlight the powerful propagation mechanism that is prompted by the interdependence between physical capital accumulation and the cultural transmission of human capital-promoting attitudes. As a result, the long-run equilibrium is characterised by multiple, divergent paths of economic development, which establish persistent differences in income per capita, due to the (either virtuous or vicious) circles of mutually-reinforcing processes of economic change and culturally-induced evolution in attitudes towards education. While this description echoes the underlying ideas of Doepke and Zilibotti (2008), my focus, set-up and mechanisms differ in comparison. Their study is focused at circumstances that surrounded the Industrial Revolution, for which the key elements are the cultural factors behind occupational choice and the work ethic, structural transformation and the lack of financial deepening. My study draws on the paradigm of modern economic growth, thus its key elements are the impact of physical capital on the return to education, the endogenous distribution of different education-related cultural traits among the population, as well as the implications of this distribution for aggregate productivity, aggregate investment and the accumulation of (both human and physical) capital.====In order to check the robustness of the proposed mechanism, I also examine the outcomes that transpire when either the distribution of different attitudes towards education is fixed among the population, or when this distribution does change endogenously, but cultural transmission is not affected by the formation of physical capital. Under such circumstances, path-dependent outcomes do not materialise, i.e., the long-run equilibrium is invariant to initial conditions. This establishes that, indeed, the interdependence between economic and cultural factors fosters the emergence of multiple paths of economic development.====In terms of the conditions that underpin the process of cultural change, this study reveals that, as long as physical capital formation influences the process of cultural transmission – in this context, through its effect on the return to education – the circumstances under which the initial distribution of cultural traits (e.g., attitudes, values, norms etc.) among the population matters for the long-term establishment of these traits, are broader than has hitherto been assumed. While path-dependent outcomes in existing models of cultural transmission typically require some sort of cultural complementarity (Bisin and Verdier, 2001, Bisin and Verdier, 2008) – i.e., when parental efforts towards the cultural instruction of children are intensified, following an increase in the share of the population who carry the behavioural trait that parents seek to diffuse – in my model, such outcomes can emerge even in the absence of this complementarity. Instead, the initial distribution of cultural attitudes among the population may still determine the long-run equilibrium, as long as the combined effects of physical capital formation and social segregation permeate the process through which parents try to instil these attitudes in their children.====In order to facilitate its exposition, the remainder of this study is structured as follows: Section 2 outlines the results from the relevant literature. In Section 3, I discuss the characteristics of the model, while in Section 4 I derive its equilibrium. Section 5 provides the characterisation of the model's dynamics, while Section 6 presents some further analysis which aims at establishing the robustness of the proposed mechanisms. In Section 7, I investigate how similar outcomes emerge under alternative set-ups, while Section 8 concludes.","Cultural transmission, education-promoting attitudes, and economic development",https://www.sciencedirect.com/science/article/pii/S1094202520300065,4 February 2020,2020,Research Article,206.0
"Baydur Ismail,Mukoyama Toshihiko","School of Economics, Singapore Management University, 90 Stamford Road, 178903, Singapore,Department of Economics, Georgetown University, 3700 O St. NW, Washington, DC 20057, USA","Received 27 August 2018, Revised 21 January 2020, Available online 4 February 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2020.01.003,Cited by (1)," with competing risks, distinguishing between different types of separations. A higher unemployment rate at the start of an employment relationship increases the probability of job-to-job transitions, whereas its effect on employment-to-unemployment transitions is negative. We then build a simple job-ladder model to interpret our empirical results. A model with two-dimensional heterogeneity in match (job) characteristics has the same qualitative features as the data. Once the model is calibrated to include cyclicality in the offered match characteristics, it can also fit the quantitative features of the data. The model reveals matches formed in booms provide more nonwage utility to the workers but are subject to a higher future probability of separation shocks.","This paper analyzes how the characteristics of the match between jobs and workers vary over the business cycle. In a frictional labor market with heterogeneous workers and jobs, the match between workers and firms can differ from the first best. The degree of frictions in the labor market may vary over different phases of the business cycle. A natural question, therefore, is whether better matches are formed during booms than in recessions. We approach this question by analyzing workers' job durations.====The main idea behind our study is that a better worker-firm match would last longer. If a worker has a better match with a particular firm than with another firm, he is more likely to feel better off working in the former firm, and thus he is less likely to leave that firm. To break such a match would require a large negative shock. Our approach is in the spirit of the revealed-preferences theory in that we infer the characteristics of a match from observed behavior. The advantage of our approach over the alternative approach of directly measuring productivity and wages from matched employer-employee data is that we can consider a broader concept of match quality that does not show up in output or wages, such as the attachment of a worker to a particular firm or the worker's geographical preferences.====In the following, we first empirically study the effects of aggregate labor-market conditions on job duration using data from the National Longitudinal Survey of Youth (NLSY) 1979 cohort. We use the Cox (1972) proportional hazard model for job separations. A novel feature of our estimation is that we estimate the effects of the unemployment rate using a competing-risk structure in which an individual can experience different types of separations. We find the likelihood of a separation that leads the worker into a job-to-job transition (==== transition) and the likelihood of a separation that moves the worker into unemployment (==== transition) have strikingly different relationships to aggregate labor-market conditions at the formation of the match.==== A high unemployment rate at the start of a match increases the probability that it ends with an ==== transition, but it has a negative effect on the probability that it ends with an ==== transition. Our results are robust to several alternative specifications.====Our motivation for distinguishing between the types of job separation stems from the fact that different types of job separations are known to exhibit different cyclical properties. For example, the ==== transition rate is known to be procyclical, whereas the ==== transition rate is countercyclical, and thus the current unemployment rate is expected to have the opposite effect on job duration. These opposing forces can cancel each other out if the estimation does not distinguish between ==== and ==== transitions. In our setup, we can estimate both effects separately. As expected, an increase in the current unemployment rate (for a given unemployment rate at the start of the job spell) reduces the probability that the job spell ends with an ==== transition, but it increases the probability that it ends with an ==== transition.====We establish these results regarding the responses of ==== and ==== transitions to the unemployment rate after controlling for the starting wages. The pioneering work by Bowlus (1995) also studies the job duration over the business cycle but without distinguishing between different separation types. She shows the unemployment rate at the start of the job has a negative effect on job durations, but this effect disappears once the starting wages are controlled for. We obtain similar estimates but smaller in magnitude when we mix together all the separation types in our sample, as in Bowlus (1995).==== Bowlus's (1995) interpretation of her result is that the cyclical movements in the match quality are internalized by wages. Our findings with different separation types draw a different conclusion. The opposite movements in ==== and ==== transitions over the business cycle can make overall separation rate close to acyclical. Analogously, our results suggest Bowlus's (1995) result reflects the opposite responses of ==== and ==== transitions to the unemployment rate at the start of a spell.====Using data from NLSY 1979, Mustre-del-Rio (2019) also studies job duration over the business cycle. Our paper methodologically differs from his paper in that he does not employ a competing risk structure. Instead, he estimates the hazard rates for certain transition types based on pre- and post-employment status and studies job duration for each transition type using their respective hazard-rate estimates. For example, he finds that a match formed during a boom lasts longer for jobs that are created from non-employment and ends with a transition to non-employment. Under a competing-risk structure, we are able to interact the hazard rates for different transition types and study overall job duration over the business cycle. Finally, using a matched employer-employee dataset, Kahn (2008) finds job spells are shorter when a match is formed during a recession. However, this relationship reverses after controlling for firm fixed effects.====The contrasting effect of aggregate conditions on different types of separations calls for a theoretical interpretation. To this end, we build a quantitative job-ladder model that features both endogenous separation and on-the-job search. We focus on the worker side of the problem, because our dataset contains information only on workers. (Thus, we use the terms “matches” and “jobs” interchangeably in the context of the model.) The model contains endogenous and exogenous separations: a match can break up either because the worker chooses to separate from a job (to move to another job or to unemployment) or because an exogenous shock moves a worker into unemployment. An employed worker can receive a job offer from another employer with some probability and decide whether to accept it. She may instead receive an exogenous-separations shock, or may choose to move into unemployment. An unemployed worker can receive a job offer with some probability and decide whether to accept it. Matches (or, equivalently, jobs) are heterogeneous and the characteristics of matches are randomly drawn when job offers are generated.====We consider two-dimensional heterogeneity in match characteristics. The first is the (nonwage) utility from working at the particular firm; this type of heterogeneity is emphasized, for example, by Sorkin (2018). We call this component ====. The second is the frequency of the exogenous job-destruction shocks. We call this component ==== (or ====), and we consider two types of jobs for this dimension. This type of heterogeneity in jobs received attention in the recent literature; see Jarosch (2015) and Mukoyama (2019), for example.====We estimate the model-generated data in the same manner that we treat the NLSY data. In our baseline calibration, which assumes the job-offer distribution (in terms of the match quality and job stability) is acyclical, the Cox-estimation coefficients are of the same sign and of similar quantitative magnitude as the empirical counterparts except for one. We provide intuitions for the Cox coefficients by providing counterfactual experiments using the model.====To match the quantitative magnitudes of all coefficients in the Cox estimation, we allow for the cyclicality of job-offer distribution. We are able to match the empirical coefficients by assuming (i) the average of the offered match quality is better during booms and (ii) the matches (jobs) that are offered in recessions are more stable. With this specification, the matches that are formed during the booms are of higher quality but are less stable. In that sense, the answer to the question “Are matches formed in booms better?” is quite subtle; they are better in the sense that they provide more nonwage utility to the workers, but they are worse in the sense that their future probability of exogenous separation is higher.====In the next section, we describe the dataset. Section 3 describes the empirical methodology, in particular, cause-specific and subhazard regressions. We present the estimation results in Section 4. Section 5 describes the model and the baseline calibration. The model results are shown in Section 6. Section 7 extends the model calibration to allow for cyclicality in the characteristics of offered jobs. Section 8 concludes.",Job duration and match characteristics over the business cycle,https://www.sciencedirect.com/science/article/pii/S1094202520300053,4 February 2020,2020,Research Article,207.0
"Antunes António,Ercolani Valerio","Banco de Portugal, Portugal,NOVA SBE, Portugal,Banca d'Italia, Italy","Received 21 May 2018, Revised 7 November 2019, Available online 30 December 2019, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2019.11.002,Cited by (10),". Moreover, the tightening affects the political support to the policies mainly through price effects.","Policies have the potential to influence credit markets and, in particular, the households' ability to either borrow or lend. In this paper, we explicitly consider the endogeneity of the household borrowing constraint and show that this channel accounts for a sizeable part of the effects in output, credit and welfare of typical fiscal policies entailing public debt expansions.====It is well known that government debt expansions significantly influence the households' financial conditions (Woodford, 1990; Aiyagari and McGrattan, 1998; Angeletos et al., 2016; Röhrs and Winter, 2015, among others).==== For example, Aiyagari and McGrattan (1998) put forward the view that, within an economy where households face borrowing constraints and the precautionary saving motive is active, public debt can act as if it loosened the household borrowing constraint. That is, higher levels of public debt result in higher interest rates, making assets more attractive to hold and, hence, enhancing households' self-insurance possibilities.====An increase in the interest rate contributes to a “loosening” of the borrowing limit, but it also makes borrowing more costly, generating ==== an actual tightening in the borrowing constraint. Virtually any endogenous borrowing constraint has the property of being proportional to the inverse of the borrowing cost, as are the cases of the natural borrowing limit in Aiyagari (1994) and of the constraint with collateral in Kiyotaki and Moore (1997). This property is supported by empirical evidence. For example, we document a negative correlation between the evolution of the uncollateralized household borrowing limit estimated by Fulford (2015) and the interest rate on credit card plans over the 2000s in the U.S., which implies that increases in the interest rate are associated with a tightening of this limit (see Appendix A for more details). Maddaloni and Peydró (2011) also find a significant correlation between proxies for borrowing constraints, such as credit standards, and interest rates.====We perform our analysis within a general equilibrium, incomplete-markets, flexible prices model with physical capital and a neoclassical labor market which relies on the early contribution of Bewley (1977). Households are heterogeneous in terms of wealth and productivity, and the wealth distribution evolves endogenously. In order to self-insure against the occurrence of bad productivity shocks, they borrow or lend without using collateral. We endogenize the borrowing constraint by considering limited commitment for the repayment obligations of the households. In particular, in case of default they are permanently excluded from intertemporal trade, thus entering an autarky regime. We assume that honoring its own debt is at least as good as defaulting. Within our incomplete-markets environment and consistently with the empirical evidence, the temptation of declaring bankruptcy—measured by the relative value of autarky vis-à-vis the value of honoring debt commitments—decreases as the household's labor income increases.====We calibrate the stationary distribution of our model at quarterly frequency for the U.S. economy. We then study the transition of the economy due to transitory public debt expansions that finance stylized but realistic spending policies. On one hand, we analyze a policy of transfers amounting to 1% of steady-state output, evenly distributed across households. The aggregate profile for transfers follows the one estimated by Leeper et al. (2010). On the other hand, the debt expansion finances an increase in purchases of goods and services, the so-called government spending stimulus, similar to that set in the American Recovery and Reinvestment Act (ARRA). The two policies have a certain degree of persistence over time and entail future increases in taxation to repay the debt. To the best of our knowledge, our work is the first that studies the transition associated to public debt expansions within a framework in which the households' wealth distribution evolves endogenously and the borrowing constraints emerge as an equilibrium outcome.====An increase in public debt impacts positively the interest rate. All else equal, the option to stay in the market becomes relatively worse than going to autarky for the borrowers, giving them a higher incentive to default. Lenders will therefore be less willing to provide funds in the credit market. This will endogenously tighten the household borrowing constraint, which means that the maximum quantity that households can borrow becomes smaller. Because of the tightening, constrained agents are forced to deleverage, while the unconstrained save more for precautionary purposes. The appetite for assets generates a downward pressure on the interest rate that dampens the abovementioned tightening. Eventually, the borrowing limit tightens by roughly 0.5–1% of steady-state output across the two simulated policies and its dynamics is very persistent over time.====The tightening induces households to cut consumption and work harder, though we show that the magnitude of the households' reactions heavily depends on which region of the wealth distribution the household pertains to. For example, constrained agents react on average five times more than the unconstrained.====On top of the tightening, the two fiscal policies may generate opposite reactions. For example, in what concerns output dynamics the purchases policy fosters labor supply which generates a positive output multiplier. The dynamics of the borrowing constraint, identified by its pure movement and price effects (lower interest and higher wage rates), amplifies the output reaction: the impact multiplier of our baseline model with the endogenous borrowing constraint is roughly 0.9, while the multiplier generated by the model in which the constraint is exogenously kept at its steady-state level is close to 0.65. In contrast, the transfers policy allows agents to work less, thus generating a contraction in output. In this case the tightening dampens the output fall.====In other dimensions the two policies produce similar outcomes; for instance, they both crowd out credit and physical capital. Over a five-year horizon an average of 20% of the fall in credit is explained by the tightening.====The endogenous borrowing limit plays an important role also in determining the welfare effects generated by the fiscal policies. In the case of the transfers policy the dynamics of the borrowing limit significantly influences political support. In the baseline model, the majority of agents (roughly four fifths) supports the policy, whereas in the fixed constraint version of the model such support is far from majoritarian. This is mainly due to the price effects induced by the tightening along the transition path.====Finally, we perform a crisis experiment by studying how the debt expansions influence the dynamics of an economy experiencing a credit crisis characterized by falling credit and output. The fiscal policies contribute to a further tightening in the borrowing limit and reinforce the fall in credit, whereas their effect on output is not substantial.====An important lesson can be drawn from these results. Constrained agents react the most to the tightening but it is the reaction of the unconstrained that allows it to produce sizeable aggregate effects. Indeed, the unconstrained—which constitute roughly 90% of the population—react non-trivially to both the shift of the borrowing limit and the price effects that it generates. Between half and two thirds of the effects generated by the tightening are due to its price effects.====There is a recent stream of the literature that studies the effects of (i) taxes and monetary transfers and of (ii) government consumption within incomplete-markets frameworks. For example, Heathcote (2005), Ábrahám and Cárceles-Poveda (2010), Oh and Reis (2012), Kaplan and Violante (2014), Huntley and Michelangeli (2014) and McKay and Reis (2016) belong to the first class of papers, while Challe and Ragot (2011), Brinca et al. (2016) and Ercolani and Pavoni (2019) belong to the second. In particular, our paper is tightly linked to Heathcote (2005) and Challe and Ragot (2011). The first paper primarily shows how the consideration of a binding exogenous borrowing limit breaks the Ricardian equivalence, as a debt-financed transfers policy allows wealth-poor agents to consume more, thus affecting the interest rate. The second paper, using a model where households face collateralized borrowing constraints but the wealth distribution is not a state variable, shows that a government spending stimulus may crowd in private consumption, depending on the extent to which the fiscal policy enhances self-insurance possibilities. Unlike these papers, we focus on the endogeneity of the borrowing constraint and by how much this endogeneity influences the effects of the policies.==== To some extent our work is also related to Ábrahám and Cárceles-Poveda (2010), who address the effects of revenue-neutral tax reforms in an economy encompassing endogenous borrowing limits with limited commitment. They show that these borrowing constraints significantly influence the effects of the reforms. Their paper, unlike ours, does not contribute to the debate about the effects of public debt expansions on the tightness of the household borrowing constraint. Moreover, they use an analytical framework similar to ours but without endogenous labor supply. Our results show that labor significantly interacts with the dynamics of the borrowing constraint. For details, see Section 4.4 and Section E.4 in the appendix.====There is also the well-established stream of the literature that studies government spending stimuli within general equilibrium models, with complete markets and representative agent(s). The seminal contribution is that of Baxter and King (1993). Galí et al. (2007), Hall (2009), Fernández-Villaverde (2010), Christiano et al. (2011), Eggertsson and Krugman (2012), Corsetti et al. (2013), Bilbiie et al. (2013) and Rendahl (2016) study the effects of fiscal policies in the presence of various financial frictions. We see two contributions to this literature. First, unlike these papers our framework of analysis allows us to study how the combination of borrowing constraints, wealth heterogeneity and market incompleteness influences the households' reaction to public debt expansions. Second, part of this literature stresses how specific channels, such as price stickiness and nominal rates constrained at zero, are able to generate larger government spending multipliers. We isolate an additional mechanism—the tightening in the borrowing limit—that is able to amplify the output reaction to a government spending stimulus even within a fully flexible prices environment.====Finally, our work is also related to those papers studying the effects of a credit crunch, within frameworks of heterogeneous agents and incomplete markets, as, among others, Guerrieri and Lorenzoni (2017), Buera and Moll (2015), Huo and Ríos-Rull (2015) and Kehoe et al. (2016). We contribute to this literature by studying the interactions between the dynamics of the borrowing constraint and debt-financed fiscal policies.====The paper is structured as follows. Section 2 presents the model. Section 3 presents the results for the stationary distribution. Section 4 reports the transitional dynamics of the economy generated by the government debt expansions. Section 5 concludes.",Public debt expansions and the dynamics of the household borrowing constraint,https://www.sciencedirect.com/science/article/pii/S1094202519305617,30 December 2019,2019,Research Article,208.0
Takahashi Shuhei,"Institute of Economic Research, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan","Received 6 January 2019, Revised 5 November 2019, Available online 19 December 2019, Version of Record 12 June 2020.",https://doi.org/10.1016/j.red.2019.11.001,Cited by (4),"Idiosyncratic wage risk exhibits cyclical variation. This study analyzes how such risk fluctuations affect business cycles. I use a heterogeneous agent model with uninsured idiosyncratic wage risk, indivisible labor, and a borrowing constraint. I introduce risk fluctuations as uncertainty shocks and calibrate those shocks using micro-level wage data in the United States. I find that uncertainty shocks affect labor market dynamics through ex-ante uncertainty and ex-post distribution effects. In particular, uncertainty shocks mainly influence the employment of low-productivity individuals, generating negative comovement between total hours worked and average labor productivity. Including uncertainty shocks in addition to aggregate total factor productivity shocks helps the model account for the weakly negative hours–productivity correlation and large fluctuations in the labor wedge seen in the United States.","Idiosyncratic earnings risk exhibits cyclical fluctuations (Storesletten et al., 2004; Heathcote et al., 2010, and Guvenen et al., 2014). Previous studies find that such risk fluctuations have important implications for various macroeconomic issues including asset pricing and the welfare cost of business cycles. However, little is known about how fluctuations in idiosyncratic earnings risk affect aggregate fluctuations. How do cyclical fluctuations in idiosyncratic risk affect business cycles, particularly labor market fluctuations? The present study examines this question quantitatively using a general equilibrium model.====The model analyzed here is built on a heterogeneous agent model with incomplete markets and indivisible labor (Chang and Kim, 2006, Chang and Kim, 2007; Alonso-Ortiz and Rogerson, 2010, and Krusell et al., 2010, Krusell et al., 2011).==== Individuals face idiosyncratic wage risk because idiosyncratic labor productivity changes stochastically. Individuals cannot fully insure against this risk because there is only one asset, physical capital. They partially self-insure by adjusting savings and employment in the presence of a borrowing limit. Importantly, when the process for idiosyncratic productivity is calibrated to micro-level wage data in the United States, the model generates inequality in labor income and wealth close to that seen in the U.S. economy.====This study introduces fluctuations in idiosyncratic productivity risk using uncertainty shocks, that is, exogenous changes in the standard deviation of idiosyncratic productivity shocks (e.g., Bloom, 2009). I measure the variation in idiosyncratic productivity risk in the United States using individual wage data from the Panel Study of Income Dynamics (PSID) and then calibrate uncertainty shocks to the micro-level evidence. Further, following the literature on uncertainty shocks (e.g., Bloom, 2009; Bachmann and Bayer, 2013, and Bloom et al., 2018), I assume that individuals learn of the standard deviation of idiosyncratic productivity shocks one period in advance.====I find that uncertainty shocks generate labor market dynamics through two effects: ex-ante uncertainty and ex-post distribution effects.==== First, the ==== occurs when individuals learn that the standard deviation of shocks to their next-period idiosyncratic productivity will increase. Since they become more uncertain about their future earnings, as a means of self-insurance, individuals, especially those with little wealth, increase labor supply in the current period. This increases aggregate employment and total hours worked. Crucially, employment increases mainly among individuals with low idiosyncratic productivity. This is because individuals with little wealth and high idiosyncratic productivity choose employment even without the rise in wage uncertainty. Thus, as a result of the increased uncertainty, employment mainly increases among low-productivity individuals and average idiosyncratic productivity decreases among employed individuals, which lowers average labor productivity (i.e., output per labor hour). The labor wedge, which is computed as the gap between average labor productivity and the marginal rate of substitution of leisure for consumption in a representative agent setting, decreases because average labor productivity decreases and the marginal rate of substitution of leisure for consumption increases.====By contrast, the ==== occurs when larger idiosyncratic risk is realized. As the standard deviation of idiosyncratic productivity shocks actually increases, the distribution of idiosyncratic productivity becomes more dispersed than before. Consequently, a larger proportion of individuals see their wage fall below their reservation wage and choose not to work. This decreases aggregate employment and thus total hours worked. Importantly, employment decreases among individuals with low idiosyncratic productivity because those whose wage (productivity) is lower than their reservation wage (productivity) choose nonemployment. By contrast, individuals with high idiosyncratic productivity remain employed. Hence, average idiosyncratic productivity increases among employed individuals, which raises average labor productivity. The labor wedge increases because average labor productivity increases and the marginal rate of substitution of leisure for consumption decreases. While both the uncertainty and the distribution effects generate labor market dynamics, a further analysis reveals that the distribution effect plays a dominant role.====I then evaluate the effect of uncertainty shocks on overall business cycles. I find that with both aggregate total factor productivity (TFP) and uncertainty shocks, the calibrated model generates a weakly negative correlation between total hours worked and average labor productivity and large fluctuations in the labor wedge similar to those seen in the U.S. data. By contrast, without uncertainty shocks, the model generates a strongly positive hours–productivity correlation and small fluctuations in the labor wedge, similar to those seen in a typical equilibrium business cycle model (e.g., Kydland and Prescott, 1982 and Hansen, 1985). Importantly, business cycle moments concerning output, consumption, and investment are largely unaffected by the introduction of uncertainty shocks.====This study contributes to the literature on varying idiosyncratic earnings risk by analyzing its impact on labor market dynamics. While studies have analyzed how time-varying labor income risk affects aggregate fluctuations (Krusell and Smith, 1998 and McKay, 2017), the welfare cost of business cycles (Krusell and Smith, 1999; Storesletten et al., 2001; Mukoyama and Şahin, 2006, and Krusell et al., 2009), and asset pricing (Krusell and Smith, 1997; Pijoan-Mas, 2007, and Storesletten et al., 2007), they assume exogenous earnings or inelastic labor supply and thus they do not analyze the implications for labor market fluctuations.==== The present paper shows that cyclical fluctuations in idiosyncratic risk have important effects on labor market dynamics.====The present study is also related to recent work on uncertainty shocks to firm-specific risk pioneered by Bloom (2009). Bachmann and Bayer (2013) and Bloom et al. (2018) investigate how uncertainty shocks interact with input adjustment costs in generating aggregate fluctuations. Arellano et al. (2019) consider financial frictions, while Schaal (2017) analyzes labor search frictions. I introduce uncertainty shocks into a heterogeneous ==== model and analyze the uncertainty and distribution effects in a way similar to these studies on firm-side uncertainty shocks. I find that for household-side uncertainty shocks, the distribution effect is dominant.====Lastly, the present study is related to the literature that analyzes shocks other than aggregate TFP shocks in an equilibrium business cycle model. This stream of the literature is important because when driven by aggregate TFP shocks only, a typical model fails to account for labor market dynamics in the United States. Benhabib et al. (1991) and Christiano and Eichenbaum (1992) respectively show that including home production technology shocks and government spending shocks mitigates the problem of the counterfactually strong comovement between total hours worked and average labor productivity. More recently, Schaal (2017) and Arellano et al. (2019) use a heterogeneous firm model to analyze uncertainty shocks, whereas Jermann and Quadrini (2012) and Perri and Quadrini (2018) examine financial shocks in the presence of financial frictions. These recent models successfully account for labor market dynamics in the United States, especially fluctuations in the labor wedge. The present paper uses a heterogeneous household, incomplete market model and proposes time-varying idiosyncratic productivity risk as another plausible explanation of U.S. labor market fluctuations. In particular, I find that fluctuations in idiosyncratic wage risk are quantitatively important when accounting for the weakly negative hours–productivity correlation and large fluctuations in the labor wedge seen in the United States.====The rest of the present paper proceeds as follows. Section 2 lays out the model and Section 3 calibrates the model to the U.S. economy using micro- and macro-level data. Section 4 examines the impact of uncertainty shocks on business cycles. Section 5 concludes. Appendix explains the data, the solution methods, and the results of additional exercises.","Time-varying wage risk, incomplete markets, and business cycles",https://www.sciencedirect.com/science/article/pii/S1094202519305605,19 December 2019,2019,Research Article,209.0
Park Jaevin,"Department of Economics, University of Mississippi, United States of America","Received 23 October 2017, Revised 21 June 2019, Available online 15 November 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.09.005,Cited by (2),"A search theoretical model is constructed to study bank capital requirements in a perspective of inside money. In the model bank liabilities, backed by bank assets, are useful for exchange, while bank capital is not. When the supply of bank liabilities is not sufficiently large for the trading demand, banks do not issue bank capital in competitive equilibrium. This equilibrium allocation can be sub-optimal when the bank assets are exposed to the aggregate risk. Specifically, a pecuniary externality is generated because banks do not internalize the impact of issuing inside money on the asset prices in ====. Imposing a capital requirement can improve welfare by raising the prices of bank assets in both states.","Why do we need to impose capital requirements on banks? A conventional rationale for bank capital requirements is based on deposit insurance: Banks tend to take too much risk under this safety net, so bank capital requirements are needed to correct the moral hazard problem created by deposit insurance. Alternatively, systemic risk also justifies capital requirements on banks because a failure of one bank may lead to a chain reaction in which many other banks can go bankrupt. In this case capital can function as a buffer to prevent the defaults of the banks. Specifically, pro-cyclical capital requirements are proposed in practice since it forces banks to accumulate bank capital in credit booms to mitigate credit crunches in the following recessions.==== These two rationales for bank capital requirements focus mainly on the financial intermediation function of banks, but banks also have a variety of other roles in an economy. Thus, it would be worthwhile to evaluate bank capital requirements in the perspectives of the other bank functions.====One primary function of banks is to provide methods of payment to facilitate transactions. For example, bank liabilities such as deposit claims and bank notes have been used either as a medium of exchange in retail markets or as collateral for secured credit in the interbank markets.==== If bank liabilities are useful for transactions while bank capital is less or not useful, then bank capital requirements can be used to manage the total supply of liquidity in an economy.==== In this paper I study a new role for capital requirements by focusing on this liquidity provision function of banks, where banks issue inside money backed by their asset portfolios.====In order to explore this issue I develop a search-theoretical monetary model a la Lagos and Wright (2005) with a banking arrangement shown in Williamson (2012). This micro-founded model has an advantage of incorporating the frictions such as limited commitment and imperfect memory in a simple way and is highly tractable with an array of assets and banking contracts. This framework is also suitable for welfare analysis in that the cost for holding assets is determined endogenously in the model. The main features of the model are as follows. Agents can produce consumption goods with an elastic labor supply, but cannot consume their own outputs. Agents need a medium of exchange to trade with each other under limited commitment and lack of memory, but only bank liabilities are accepted for transactions while bank capital is not. Given limited commitment, the issued bank liabilities and capital must be secured by asset holdings of the banks. So if the supply of the underlying assets is insufficient to support the demand for transactions, the consumption level of agents can be restricted. The assets in this economy are exposed to a non-diversifiable risk in the form of a random dividend, which can be either high (====) or low (====). Everyone knows the realization of the dividend before the consumption period, so there is no asymmetric information.====Raising bank capital naturally reduces the proportion of bank liabilities, so that the supply of liquidity in the economy decreases.==== Specifically, a state-contingent bank capital contract can reduce the proportion of underlying assets that is used for exchange by states. For example, consider a bank capital, which promises a proportion of bank assets in a high dividend state ==== while nothing in a low dividend state ====. If banks sell this type of bank capital, in state ==== the rest of bank assets, the shaded rectangle in Fig. 1, can be used for transactions, but the slashed rectangle cannot. On the other hand, in state ==== all of the bank's assets can be used for exchange.====If the supply of assets is not sufficiently large to support transactions in both states, it is not profitable for banks to issue a strictly positive amount of bank capital because it is useless for trade. Given the scarcity of assets, banks will provide only liabilities without bank capital in competitive equilibrium, so the consumption level of the agents fluctuates with the realization of the state.====This competitive equilibrium allocation can be constrained suboptimal. Given a constant transactions demand across the states, requiring banks to issue capital, which provides a proportion of assets in state ==== while nothing in state ====, can improve welfare by raising the asset prices in both states. Requiring a proportion of assets for capital holders in state ==== lowers the supply of liquidity for transactions and consumption in state ====. However, restricting the liquidity supply in state ==== can raise the liquidity premium in state ====, so the prices of bank assets in both states rise in general equilibrium because the asset prices reflect the liquidity premia in both states. The consumption level in state ==== will increase as the asset prices rise, because the whole bank asset portfolio is used for transactions in state ====. Therefore, there is a trade-off between a decrease of the consumption in state ==== by restricting the liquidity supply, and each increase of the consumption in both states by relaxing collateral constraints through higher asset prices.==== Thus, provided that agents are risk-averse, bank capital requirements can improve welfare by smoothing the marginal utilities across the states.====This constrained inefficiency of competitive equilibrium is associated with a pecuniary externality in the banking sector. Asset prices are determined in the competitive asset market where banks purchase assets and issue liabilities and capital given the asset prices. Thus, individual banks do not internalize the impact of issuing a positive bank capital on the asset prices in general equilibrium.==== In other words, although issuing a strictly positive bank capital is welfare-enhancing for all of the banks, given the prices individual banks will not issue bank capital in equilibrium since it is not profitable for them. This pecuniary externality can provide another rationale for bank capital requirements in a perspective of providing liquidity.====This finding provides a new insight for business cycle stabilization. Requiring higher payoff for capital in state ==== transfers a purchasing power from state ==== to state ==== by raising the prices of the assets in both states. Therefore, when storing or transferring consumption goods from one state to the other is limited, bank capital requirements can be useful to transfer a purchasing power by adjusting the asset prices. This result also suggests a role for capital requirements to stabilize business cycles as well as the literature on capital buffers, but the main mechanism is different from it because the capital buffer requires a real transfer from expansionary periods to recessionary periods.====Finally, I extend the model by introducing a nominal debt issued by the government, called as money, in order to compare the effectiveness of capital requirements with the effectiveness of monetary policy. The government can adjust the price of money by providing lump-sum transfers or collecting taxes, so that a state-contingent monetary policy can transfer a purchasing power across states directly without incurring the cost of requiring bank capital. However, the effect of monetary policy is limited by the level of outstanding money balances, while bank capital requirements can be applied to the whole asset portfolio of banks including money and other private assets.","Inside money, business cycle, and bank capital requirements",https://www.sciencedirect.com/science/article/pii/S1094202519305599,15 November 2019,2019,Research Article,210.0
"Atolia Manoj,Chahrour Ryan","Florida State University, USA,Boston College, USA","Received 18 July 2017, Revised 17 October 2019, Available online 31 October 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.10.003,Cited by (2),"What are the aggregate consequences of information frictions? We address this question in a multi-sector ==== in which firms learn from market-based information. Theoretically, we present two distinct cases in which the aggregate effects of incomplete information completely disappear: either (i) market-based information reveals sector-level optimal actions, or (ii) market-based information reveals aggregate conditions in the economy. When the model is calibrated to United States' sectoral data, both conditions hold almost exactly and incomplete information has a negligible effect on aggregate dynamics.","The literature on information frictions in macroeconomics describes two main channels through which incomplete information may drive aggregate economic outcomes. First, peoples' correlated errors about local conditions may lead to aggregate fluctuations that exceed those warranted by economic fundamentals alone. Second, peoples' disagreement about the state of the economy, combined with an incentive to coordinate actions, may lead to delayed aggregate responses to shocks.====For either of these information channels to operate, economic agents must have different information from one another. But, why should peoples' information differ? A natural hypothesis is that information differs because people participate in different markets, and therefore observe different pieces of the economy. Differences in market participation, and the corresponding differences in information, are often founded in the literature by assuming agents inhabit informationally-isolated islands, which serve as a short-hand for deeper sources of informational heterogeneity.====In this paper, we examine one underlying source of such differences—the sparse nature of firms' input-output connections—and ask whether it can give rise to the sort of disagreement that is essential to information-based theories of the business cycle. We analyze this question in the context of a neoclassical model in which informational asymmetries are ====: Firms observe their own productivity, the price of their output, and the prices of those goods that are inputs in their production.==== In this setup, the information held by agents depends directly on the pattern of input-output linkages in the economy. A long standing research agenda explores how intermediate production structures influence the propagation of sectoral shocks to the aggregate economy.==== We study the role of such linkages in propagating information.====This environment is a natural place to look for the main information channels. First, the dependence of agents' information on endogenous variables—prices—leads to the potential for informational feedback effects. Other authors have demonstrated that such feedbacks can magnify initial errors made by agents.==== Second, realistically sparse input-output structures imply that firms' information is significantly non-overlapping or dispersed, while sectoral linkages lead to complementarity in firms' investment.==== These two channels—dispersed information and strategic complementarity—are also the focus of the new-Keynesian literature that uses information frictions to explain delayed responses to nominal shocks.==== In our environment, the same forces could deliver gradual investment responses to fundamental shocks, offering an alternative to the investment adjustment costs and other frictions common in the DSGE literature.====Our central finding is that, once firms are allowed to learn from the prices emerging from the markets in which they participate, it is extremely difficult to generate a substantial impact of incomplete information. To provide intuition for this result, we use a simplified sectoral model to establish three themes that, together, suggest a limited role for the informational channels emphasized by the literature. These themes are (1) sectoral prices, regardless of input-output structure, are very informative about both local ==== aggregate conditions; (2) when incomplete information leads sectors to make suboptimal choices, knowledge of aggregate conditions leads sectoral mistakes to cancel out; and (3) even when information about aggregate conditions is incomplete, beliefs about those conditions tend to be the same across sectors. In short, regardless of sector level disagreement, the aggregate effects of dispersed information are virtually nil.====We analyze a standard (linearized) sectoral model in which sectoral productivity shocks are the only shocks hitting the economy and investment choices are made with incomplete information. We begin by providing a sufficient condition for the existence of an ==== of the sectoral model ====, a situation in which the aggregate dynamics of the economy are determined by a set of equations that are isomorphic to a single-sector real business cycle model. Our result subsumes Dupor (1999) and demonstrates that, for a broad class of economies, aggregate dynamics can be determined without reference to any sector-specific quantities or shocks. Our subsequent results regarding the economy under incomplete information then draw a tight link between the existence of an aggregate representation and the irrelevance of incomplete information.====In our first proposition, we show that when the elasticity of final goods aggregation is unity, incomplete information has no consequence for either aggregate or sectoral quantities in the economy. In this case, irrelevance at the sectoral level can be recovered regardless of the sectoral structure of the economy. The proof of irrelevance proceeds by showing that ==== prices are in fact sufficient statistics for the aggregate state of the economy, and that information on this state is all that firms need to forecast the marginal value of additional investment. In short, local sectoral prices have a remarkable ability to transmit the information relevant for investment choices, even when those choices depend on all shocks hitting the economy.====This proposition is related to the findings of Hellwig and Venkateswaran (2014), who also describe circumstances in which market-based information leads to irrelevance of incomplete information. In their environment, irrelevance arises from the static optimality conditions of a price-setting firm and deviations occur when firms face dynamic considerations or strategic complementarities in their price-setting decisions. In contrast, our result emerges from the full general equilibrium relations of the economy and holds even though the firm's investment choice is dynamic and strategically related to that of other sectors.====In our second proposition, we show that an appropriately symmetric version of the economy can deliver aggregate dynamics that are identical to the full-information economy even when sector-level dynamics are not. The key requirement for aggregate irrelevance, beyond symmetry, is that agents observe a variable that reveals the aggregate state of the economy. With this information, firms can use the structure of the economy, including market-clearing conditions, to back out the average actions of other sectors. In general, this aggregate information is not sufficient to determine the optimal investment choice of that particular sector. Nevertheless, if any one sector under-invests relative to the full-information benchmark, the symmetry of the information structure implies that other sectors will over-invest by an offsetting amount. Thus, ====.====In our final proposition, we consider a similarly symmetric economy in which firms must distinguish between local and aggregate disturbances to productivity. In general, firms can no longer perfectly infer the realization of shocks and therefore cannot reproduce the full information equilibrium. Nevertheless, we show that beliefs about the aggregate economy remain common knowledge and, therefore, aggregate quantities remain isomorphic to those of a single-sector economy in which the representative firm forecasts future aggregate productivity based on its observations of current average productivity. Thus, even when the economy is complex enough that agents cannot infer the true shocks hitting the economy, the role for dispersion of information, and the associated hump-shaped dynamics, disappears with market-based information.====After establishing these analytical results, we extend the analysis to a quantitatively realistic version of the multi-sector model. Our numerical simulations show that, if firms make investment choices based on an exogenous information structure consisting of their own productivity and a noisy local signal about average productivity, the economy indeed delivers realistic gradual hump-shaped responses of aggregate investment to productivity shocks. We then show that, once firms are free to condition their investment choices on the information embedded in their local markets, the irrelevance results of the earlier sections reemerge very robustly. Yet, sector-level responses are generally different and individual sectors are not able to determine the sectoral distribution of shocks.====Finally, we calibrate our model to match the empirical input-output structure of the United States economy, and solve the model using processes for aggregate and sectoral TFP estimated to match Jorgenson et al. (2013)'s sectoral data. This version of the model violates the analytical conditions shown to imply aggregate irrelevance and it delivers substantial heterogeneity of expectations about which shocks are hitting the economy. Nevertheless, aggregate dynamics remain remarkably similar to those of the corresponding full-information model. We conclude our quantitative analysis by demonstrating that, despite the complexity of intersectoral linkages, market-based information almost perfectly reveals firms' optimal sectoral investment choices and the aggregate state of the economy.====The paper proceeds as follows. In Section 2, we motivate our results using a simple reduced-form model of intersectoral linkages. In Section 3, we describe the full model and, in Section 4, we establish a set of analytical results characterizing the cases in which information is irrelevant for either sectoral or aggregate outcomes. Section 5 performs a series of numerical experiments to demonstrate the importance of deviations from the assumptions underlying the analytical results. Section 6 calibrates the input-output structure and the exogenous processes of the economy to match US data, and examines the consequences of incomplete information in an empirically realistic setting. Section 7 concludes.","Intersectoral linkages, diverse information, and aggregate dynamics",https://www.sciencedirect.com/science/article/pii/S1094202518302771,31 October 2019,2019,Research Article,211.0
Luo Shaowen,"Virginia Tech, United States","Received 21 June 2017, Revised 18 October 2019, Available online 31 October 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.10.004,Cited by (22),"Firms are connected through the production network, in which the production linkages coincide with financial linkages owing to delays in input payments that amount to a form of trade credit. In this paper, I investigate the roles of these interconnected production and financial linkages in the propagation of financial shocks. Empirically, I find, based on U.S. input-output matrix and loan data, that the upstream propagation of financial shocks is stronger than the downstream propagation. Theoretically, I elaborate a model that can capture this pattern of shocks, of which trade credit is an important component. Moreover, the model reflects the fact that trade credit attenuates the propagation of financial shocks when shocks are relatively small through the sharing of liquidity and amplifies their propagation when shocks are relatively large through illiquidity contagion.","An economy is an entangled network of specialized productions that are interconnected through inter-firm trade within and across sectors. In the course of this trade, the products of one firm may be purchased and consumed by another firm as inputs. In addition, there is often a waiting period between the moment when a cost is incurred and the later point when the corresponding cash flow materializes. In this respect, the production network also creates and sustains a trade credit network.==== A financial shock may, therefore, spread through firms' production and financial linkages. This paper studies how the interaction between the production network and the trade credit network affects the propagation of financial shocks.====The automotive industry crisis of 2008-2010 offers an example of how a financial shock to one firm impacts its suppliers and customers. First, a firm that endures a financial shock may reduce its demand for the goods and services of its suppliers. Thus, for example, General Motors Co. (GM) significantly reduced its demand during the crisis owing to a severe liquidity problem.==== As a consequence, American Axle & Manufacturing Holdings Inc., one of GM's major suppliers, experienced a net loss of $ 112.1 million in the fourth quarter of 2008.==== Fitch Ratings likewise downgraded the long-term ratings for Johnson Controls, Inc., another of GM's major suppliers.==== Second, when a firm experiences a financial shock, it may postpone repaying trade credit to its suppliers and reduce its provision of trade credit to its customers. GM and Chrysler Group LLC., for example, had in excess of $21 billion in domestic trade payables as of September 30, 2008.==== The failure to meet these trade payments quickly crippled GM's suppliers during the crisis.====Notably, trade credit is an important source of short-term external finance for firms of all sizes. It is to be found on the balance sheets of almost every firm (in the forms of accounts-receivable and accounts-payable), where it accounts for more than 20% of the median firm's total assets (see Peterson and Rajan, 1997; Boissay and Gropp, 2007; Klapper et al., 2012). Moreover, the choice of trade credit terms has been used as a screening device in order to make inferences regarding firms' credit worthiness by financial institutions (see Smith, 1987). Despite its importance, however, trade credit has not been examined thoroughly in macroeconomics, especially in the context of the transmission of shocks.====This paper presents an input-output model that takes into account trade credit in order to facilitate the study of the propagation of financial shocks. Specifically, the model reflects the fact that, when shocks are sufficiently small, trade credit attenuates their propagation through the sharing of liquidity within the network. During severe financial crises, by contrast, trade credit amplifies the propagation of shocks through illiquidity contagion within the network.====To start with, I present a partial equilibrium model in an effort to explain the mechanism behind the propagation of financial shocks. Specifically, the model involves a production network structure and financial linkages among firms. To capture financial frictions, I assume that firms have a working capital requirement for inputs and therefore need to pay wages and intermediate input costs in advance of production. To satisfy this requirement, firms obtain bank credit from financial institutions and trade credit from suppliers. Accordingly, firms are financially interlocked through trade, for suppliers can use payments from customers as working capital, and, at the same time, the balance sheets of trading parties become interlocked through accounts-payable and accounts-receivable. Trade credit in this way creates a ==== that reallocates liquidity among firms.====The production and financial linkages among firms serve to propagate financial shocks both upstream and downstream. For example, a positive interest rate shock to bank credit increases the input cost, reduces intermediate input demand, and affects intermediate input suppliers, which results in upstream propagation. The same shock increases the production price, creates a supply impact, and affects consumers, which results in downstream propagation. Two countervailing effects, the output effect and the input substitution effect of financing cost variation, determine the extent of upstream propagation. Two other types of effects, the cost effect and the discount effect of financing cost variation, determine the extent of downstream propagation. An important aspect of trade credit is that it affects the relative strength of upstream compared with downstream propagation.====While my model indicates that the propagation of financial shocks depends on the nature of the credit frictions in conjunction with the input-output structure in the economy, the U.S. data that I have investigated suggest that ====. Using Dealscan syndicated loan data (to measure liquidity shocks to 102 subsectors of the U.S. economy) and the industrial production (of 49 subsectors in the manufacturing and utility industries), I find both a significant negative impact of financial shocks on the focal subsector and significant upstream propagation of these shocks. In particular, my results show that subsectors are more sensitive to financial shocks that impact their customers than to shocks that impact their suppliers. Specifically, a 1% exogenous decline in the bank loan supply of a subsector's customers generates a 2.9% decrease in the output of the focal subsector during the 2008-2009 financial crisis.====Afterwards, I present a calibrated general equilibrium model for studying the U.S. economy along with two types of trade credit adjustments, namely adjustments of credit size and of its payment schedule. In the model, when a firm experiences a negative financial shock, size adjustment serves to increase its accounts-payable and to decrease its accounts-receivable so that the liquidity pressure can be absorbed through liquidity sharing. In this way, trade credit increases firms' output correlation and attenuates both the propagation and the aggregate impact of financial shocks. Furthermore, during temporary illiquidity, firms tend to delay repayments of trade credit, which means that suppliers are exposed to counter-party risk and may be penalized by their creditors owing to debt rollover. Because liquidity pressure thus readily spills over to surrounding firms, trade credit amplifies the propagation and the aggregate impact of financial shocks.====One key finding from the general equilibrium model is that trade credit leads to a “robust-yet-fragile” production network.==== When the magnitude of negative shocks falls below a certain threshold, the liquidity-sharing channel serves as a kind of shock absorber, stabilizing the production network. In the event of large negative shocks, by contrast, deferred payments on trade credit lead to illiquidity contagion and fragility of the production network. This finding is related to the discussion in Acemoglu et al. (2015b) that a more densely interconnected financial network is associated with a more stable financial system when shocks are sufficiently small but with greater fragility when shocks are sufficiently large. The architecture of a network, then, has significant implications for systemic risk.====The rest of the paper is organized as follows. After a literature review, Section 2 presents a partial equilibrium model to illustrate the propagation of financial shocks. Section 3 discusses my empirical findings on the propagation of shocks. Section 4 presents a general equilibrium model with size adjustment and deferred payments on trade credit, and Section 5 calibrates the model and forms quantitative predictions. Finally, Section 6 summarizes the findings and discuss their implications.==== This project fits into three strands of literature, namely (1) production networks, (2) trade credit, and (3) financial frictions. Long and Plosser (1983) inaugurate the study of sectoral co-movements using a network model, sowing the seeds of a rich literature that has focused on the aggregate volatility generated by idiosyncratic shocks.==== As di Giovanni et al. (2014) present, two effects are at work. First, owing to the linkages effect, idiosyncratic shocks have sizable aggregate effects when the input-output linkages among firms are strong (Bak et al., 1993; Horvath, 1998, Horvath, 2000; Dupor, 1999; Shea, 2002; Conley and Dupor, 2003; Foerster et al., 2011; Jones, 2011, Jones, 2013; Levine, 2012; Acemoglu et al., 2012, Acemoglu et al., 2015c, etc.). Second, owing to the direct effect, idiosyncratic shocks can contribute directly to aggregate fluctuations (Jovanovic, 1987; Gabaix, 2011; Carvalho and Gabaix, 2013). Of particular note in this regard is the study by Acemoglu et al. (2015a) of the propagation of supply and demand shocks through input-output and geographic networks. My emphasis here, however, is on the transmission of financial shocks in the input-output structure.====In research on the production network, financial frictions were not considered until the study by Bigio and La'O (2016). The present paper, like those of Su (2014) and Altinoglu (2018), build on Bigio and La'O (2016) by taking network-based approaches to the study of the propagation of financial shocks. My paper and these other three address these issues in distinct ways, particularly with regard to the nature of the financial friction investigated. Unlike Bigio and La'O (2016), my approach considers firms' financial linkages; and while Su (2014) presents a network model that accommodates financial frictions in the capital input, that model does not account for financial frictions in inter-firm trade. The independent work by Altinoglu (2018) has demonstrated the importance of trade credit in business fluctuations, but there are a number of substantive differences with my paper. In Altinoglu (2018), trade credit always amplifies financial shocks, whereas in my paper it may either amplify or attenuate financial shocks depending on the nature of the shocks. Moreover, in my paper, trade credit is allowed to adjust through size variation and deferred payments. Firms commonly increase trade borrowing during the contraction period of bank loans (see Nilsen, 2002) and delay repayments of trade debt during times of financial distress (see Cuñat, 2007). Incorporation of these features into the model is important when assessing the role of trade credit in the propagation of financial shocks.====Several theories have been put forward to explain why suppliers provide trade credit to their customers (e.g., Peterson and Rajan, 1997; Burkart and Ellingsen, 2004; Cuñat, 2007). My analysis follows that of Kiyotaki and Moore (1997) in emphasizing the role of trade credit in the propagation of shocks.====Regarding the third strand of inquiry, financial frictions have been extensively studied in the literature on the 2008-2009 financial crisis (e.g., Gertler and Kiyotaki, 2010; Cúrdia and Woodford, 2011; Gertler and Karadi, 2011; Jermann and Quadrini, 2012; Perri and Quadrini, 2018). In particular, Perri and Quadrini (2018) present the international transmission of credit shocks through the cross-country ownership of shares of firms. This paper builds on these previous approaches in order to elaborate an input-output model that incorporates trade credit for the study of the impact of financial shocks.",Propagation of financial shocks in an input-output economy with trade and financial linkages of firms,https://www.sciencedirect.com/science/article/pii/S1094202518302734,31 October 2019,2019,Research Article,212.0
"Daruich Diego,Kozlowski Julian","University of Southern California, United States of America,Federal Reserve Bank of St. Louis, United States of America","Received 29 October 2018, Revised 17 October 2019, Available online 23 October 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.10.002,Cited by (5),Poor families have more children and transfer less resources to them. This suggests that family decisions about fertility and transfers dampen ,None,Explaining intergenerational mobility: The role of fertility and family transfers,https://www.sciencedirect.com/science/article/pii/S1094202518305702,23 October 2019,2019,Research Article,213.0
"Desdoigts Alain,Jaramillo Fernando","Université Paris 1 Panthéon Sorbonne, IEDES and UMR D&S, France,Universidad del Rosario, Calle 12c # 4-69, Bogotá, Colombia","Received 18 July 2018, Revised 29 September 2019, Available online 23 October 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.10.001,Cited by (4)," in the range of goods produced. Learning is sector specific, bounded from above, and constrained by a minimum scale restriction. It starts with a switch from traditional to modern technologies, understood as structural change. A higher share in the purchasing power of the middle class expands the market size for modern goods, and generates more learning in non-mature modern technologies, contributing to productivity gains, and under certain conditions (e.g. a significant learning potential), to sustained growth. Eventually, we make the case for a strong middle class; that is, redistributive policies towards the middle class and the poor.","Wondering about the relative strength of the middle class and the prominent role it has to play in economic development has become increasingly common in the press, as well as in policy circles. The issue has also been widely discussed by researchers in the social sciences, including in economics.==== In this paper, we develop a multi-sector model of economic growth in which technological progress is driven by learning by doing in expanding emerging sectors that benefit in particular from middle-class-led consumption.====Learning interacts with scale economies and nonhomothetic, hierarchical preferences, thus giving rise to endogenous growth. Specifically, low-income households consume only products that have a low income elasticity of demand. Thus, sectors that produce low income elasticity goods must cope with a large demand. As a consumer market grows, dependent on the distribution of income, the expanding sector may experience technology switching from small- to large-scale production or, in other words, from traditional (CRS) to modern (IRS) technology as formulated by Murphy et al. (1989). It is only after the technology has changed from CRS to IRS that learning can take place. Learning and productivity gains are therefore constrained by a minimum scale restriction. Learning is also assumed to be bounded from above so that the accumulated experience in a specific technology matures. Finally, the experience that the economy has accumulated across all sectors contributes to aggregate productivity growth.====Hence, the core mechanisms of our middle-class-led growth model are threefold. Firstly, learning takes place in an intermediate range of sectors, where demand is high enough to allow a firm to take advantage of scale economies, and where the learning possibilities have not yet been exhausted. Secondly, how much a sector contributes to cost reduction is determined by its accumulated experience, which results from cumulative output driven by its market size evolution, and in turn depends on the shape of the income distribution. Thirdly, sustained growth requires that technology providing increasing returns be implemented in sectors one after the other. Knowledge exchange, resulting in productivity gains, must create sufficient additional demand for higher-ranking goods in the consumer's hierarchy of needs for learning to never cease. Only then will the economy move forward.====Two original features of our model stand out. Firstly, both the size of the middle class and its total income share are simultaneously an input to our model and an outcome of past economic growth. Neither do we define the middle class in an ad hoc manner as is often the case in the theoretical and empirical literature addressing the issue,==== nor do we consider it as a potentially influential actor in relation to political instability or changes in institutions (see Alesina and Perotti, 1996, Birdsall, 2010, and Halter et al., 2014, among others). Hence, the “middle class” label is an endogenous, purely economic outcome of the model.==== Secondly, our model shows how the distribution of income/skills across middle-class households determines the intensity of sector-specific learning by doing, and whether learning takes place across a broad range of sectors or not. In particular, we bring to light a mechanism through which an inverted-U relationship between inequality and sustained growth emerges, which fits well with the empirical findings of Banerjee and Duflo (2003), Chen (2003), and Grigoli and Robles (2017). On the one hand, in a highly unequal economy, where an elite owns nearly all of the wealth, modern technologies are confined to a small range of sectors with limited opportunities for learning. On the other hand, an economy with an almost perfectly egalitarian income distribution needs to have a large population and/or a low minimum scale restriction to be able to ignite the learning process and achieve positive growth in the long run. The inverted U-shaped curve reflects the trade-off between labor allocation to a broad range of sectors in which learning may take place, and the speed of the learning process in each sector. Such a trade-off is, for the most part, influenced by the distribution of income/skills, which governs the duration and the extent of the learning process.====Our model also has implications for growth-enhancing strategies by means of redistributions of income.==== At the core of our results is that a higher share in the purchasing power of the middle class (i) expands the market size for modern products, and (ii) generates more learning in non-mature IRS technologies. This contributes to increased productivity and potentially to sustained growth. Nevertheless, if the middle class is too small, growth may only be short term and the economy will stagnate in the long run. More generally, we establish under what conditions income distribution and domestic market size help to promote sustained growth through a continuous process of structural changes involving all sectors of economic activity.====Our study is thus related to the theoretical work carried out by Zeira and Zoabi (2015) who interpret structural changes as the transition between traditional and modern technologies in a growing economy where growth is driven by rising productivity in the modern sectors of economic activity only. However, they treat countries as small, open economies that benefit from new technologies invented elsewhere, and as such assume that productivity in modern sectors rises exogenously over time. Demand-led endogenous growth models with hierarchic preferences, which are closely related to our theoretical framework, have been developed by Matsuyama (2002), Foellmi and Zweimüller (2006), and Foellmi et al. (2014), among others. We tread in their footsteps here. On the supply side, all three models also exhibit increasing returns and feature both price and market-size effects.====Matsuyama (2002) offers a model where there is only one possible production technology, namely CRS, on one hand. On the other hand, bounded sector-specific learning by doing in perfectly competitive markets leads to external increasing returns to scale at the sector-wide level. This rules out market concentration dominated by large firms, as well as the role they play in modern economic development.==== Firstly, a critical mass of rich households is needed for an industry to take off. This results in lower prices and triggers a sound trickle-down process, so that lower-income households may purchase additional goods, which further lowers prices, potentially giving rise thereafter to a trickle-up mechanism, and so on. Secondly, the extent of the trickle-down process depends on the income distribution. It starts at the top end of the distribution, but may be interrupted if the income gaps are too large. The author draws a useful analogy with a domino effect. The distance between two dominos, for example, the rich and the upper middle class, must be neither too large nor too small in order for the chain reaction not to be interrupted too early, even if this is at the expense of the poor, who may be used to narrow the income gap between the rich and the upper middle class.====Foellmi and Zweimüller (2006) study a two-class (rich and poor) society, and develop an innovation-based model, where goods are produced in monopolistic firms under IRS, and only rich households are initially willing to pay high prices for newly invented luxury goods. Thus, some degree of inequality defined by a high income ratio of rich to poor and/or a high concentration of income among the rich is needed to promote incentives for innovation. However, with too much inequality, the market for new goods is small, and these goods are unlikely to move into mass markets, which inhibits growth in the long run. As long as the price effect prevails over the market-size effect, a regressive transfer towards the rich (i.e. more inequality) fosters growth. If they were to introduce a middle class, either a transfer from middle-class to rich households or from poor to middle-class households would promote economic growth. Foellmi et al. (2014) distinguish between the invention of new high-quality products only accessible to the rich, and process innovations that transform luxury goods into low-quality mass products, thus becoming affordable to the poor. A household never consumes two qualities of the same product. Hence, either middle-class households consume only high-quality goods and are therefore almost as affluent as the rich, or they consume only low-quality goods and are almost as deprived as the poor. As long as the main driver of growth is product innovation, redistribution from the rich to the middle class reduces growth. It is only when process innovations become the driving force due to the income distribution that the positive effect of such a redistribution on growth becomes more likely.====The results in Matsuyama (2002) and Foellmi and Zweimüller (2006) stand in contrast with our core finding, outlined above. In these papers, the product life cycles and transmission channels through which inequality affects economic growth are different from ours. Their results emphasize both the importance of inequality at the top end of the income distribution and the distance between the different income groups in order to help an economy to form mass markets. Unlike these papers, we underline the importance of the economic situation of the middle class and the poor, and in particular the middle-class size and total income share, to account for the spread of modern sectors either to developing countries, where R&D and the scope for realizing scale economies remain limited,==== or to any country where the trickle-down process may be ineffective due to too large an income gap between the wealthy elites and the middle class. We thus view our theory as complementary to theirs. It has a unique, special feature: the dynamic interplay between structural change (i.e. switch to modern technologies developed and implemented by large firms) and the share of the middle class's purchasing power, both of which are interrelated endogenous outcomes of the model.====The rest of the paper is organized as follows. In Section 2, we present the model. Section 3 discusses our set up regarding middle-class-led learning by doing and knowledge-based productivity growth. Section 4 characterizes the emergence of a middle class as an outcome of past economic growth and the steady-state growth rate. In Section 5, we examine the nexus between inequality and, in particular, the strength of the middle class and growth. Section 6 concludes.","Bounded learning by doing, inequality, and multi-sector growth: A middle-class perspective",https://www.sciencedirect.com/science/article/pii/S1094202518303946,23 October 2019,2019,Research Article,214.0
"Jiang Janet Hua,Shao Enchuan","Bank of Canada, 234 Wellington Street, Ottawa, ON, K1A 0G9, Canada,University of Saskatchewan, Arts 811, 9 Campus Dr, Saskatoon, SK, S7N 5A5, Canada","Received 13 December 2017, Revised 10 September 2019, Available online 8 October 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.09.003,Cited by (8),"In many industrialized countries, cash usage at points of sale has been decreasing owing ","The retail payment landscape has undergone significant changes in the past few decades with the emergence of various new payment instruments. As a result, cash has been losing ground to other means of payment at the point of sale. These developments can potentially impact the demand for government currency. Given that currency in circulation (CIC) constitutes a significant share of the balance sheet of a central bank, the shrinking demand for currency will have critical implications on the central bank's seigniorage revenue, its independence, and its ability to conduct monetary policy (see Friedman, 1999, King, 1999, Freedman, 2000; and Fung et al., 2014).==== It is therefore important to monitor and understand the trend in the demand for cash.====Given that standard monetary models predict that cash usage and demand tend to move in the same direction, one would naturally expect the demand for cash to decrease as it is squeezed out by other payment methods at points of sale. However, surprisingly, the demand for cash, measured as the value of CIC over GDP, after a steady decrease in earlier years, has remained flat or even increased in the past two to three decades despite continuous declines in cash usage; this phenomenon has been observed in various industrialized economies (see Section 2 for more detailed documentation). Rogoff (2002) wonders at the surprising popularity of paper currency in many industrialized economies. Bailey (2009) calls it the “paradox of banknotes.” Williams (2012) terms this phenomenon of decreasing cash usage and robust cash demand the “cash paradox.”====In this paper, we document the cash paradox and develop a parsimonious model that is consistent with the phenomenon. To account for the cash paradox, we introduce two innovations to standard cash-credit monetary models (where credit is a stand-in for alternative payment methods to cash). The first innovation is that the substitutability between cash and credit is uneven across different economic activities. Credit directly competes with cash in many point-of-sale transactions. However, credit is a less-ideal substitute for cash as a store of value and transactions in the underground economy, in bars and casinos, or in activities where record-keeping or monitoring technology is not available, where agents desire anonymity, or where unbanked or underbanked agents are involved. To capture the variation of substitutability between cash and credit across different activities, we model a economy with two sectors: a cash-credit sector and a cash-only sector. The cash-credit sector captures activities where cash and credit are perfect substitutes for each other as a means of payment. The cash-only sector captures activities where it is difficult or impossible for credit to replace cash.====The second innovation is the modeling of cash flows across different sectors. In our model, some agents actively plan their cash inflows and outflows, receiving cash revenues (inflows) in the cash-credit sector to finance spending (outflows) in the cash-only sector. Some examples of these activities are taxi drivers acquiring cash from passengers to dine in cash-only restaurants, bakeries receiving cash from customers to purchase ingredients at local farmers' markets, farmers selling produce in cash to pay unbanked temporary workers, firms using cash revenues to pay suppliers, and all of them retaining some cash revenues as a store of value.====With these two innovations, our model predicts that higher credit usage initially reduces both the share of cash transactions and cash demand. However, once credit usage expands beyond a certain level, the tight connection between cash usage and demand breaks down. More specifically, agents adjust their cash management practices in response to further credit expansions, so that the velocity of cash slows down and the demand for cash remains flat despite further diminishing cash transactions. The intuition is as follows. With more credit usage in the cash-credit sector, agents who are buyers in that sector lower their cash demand. This change implies that agents who are sellers in the cash-credit sector will receive fewer cash revenues or inflows. To finance their purchases or outflows in the cash-only sector, these agents acquire more cash in advance to make up for the shortfall in cash receipts. The decrease in cash demand by the first group of agents is offset by the increasing demand by the second group, so that the total demand for cash remains flat despite shrinking cash transactions. As for velocity, note that compared with cash acquired by the first group of agents, which is used in both sectors, cash acquired by the second group of agents has a lower velocity because it is used only in the cash-only sector. As credit usage expands, there is a redistribution of cash demand from the first group of agents to the second group, implying that cash with a lower velocity constitutes a higher fraction of the total demand for cash. As a result, the overall velocity of cash decreases.====The decoupling of cash usage and demand in response to credit expansions enables our model to capture the cash paradox successfully. In particular, the simultaneous decrease in cash usage and robust cash demand in many industrialized countries can be explained as a result of credit expansions together with falling nominal interest rates.====To quantify the importance of our new two modeling features in capturing the cash paradox, we carry out a series of calibration exercises. More specifically, we calibrate our model and alternative models where these features are absent, to cash usage and demand in four advanced economies: Australia, Canada, the United Kingdom and the United States. We find that both modeling features are important in capturing the cash paradox. The standard monetary model, where both features are absent, cannot explain the robust cash demand. The model with two segregated sectors featuring different substitutability between cash and credit can capture only the time trend in cash demand under implausible assumptions about the size of the cash-credit sector relative to the cash-only sector. In contrast, the model with the cash-flow channel captures the trend in cash usage and demand well in all four countries with much more reasonable parameterization. We also find some empirical support for our model's prediction on the cash velocity in the retail sector.====Our paper contributes to the literature on the cash paradox by developing the first formal model to capture the phenomenon with plausible quantitative results. Our quantitative exercises also formalize and deepen the popular indicative discussion that attributes the paradoxical robust demand for cash in the face of credit expansion to increased demand in the underground economy and/or as a store of value by either domestic or foreign agents, as these demand categories are less sensitive to competition from other means of payment.==== We believe that this is an important first step to explain the cash paradox. The cash-only sector in our model captures these demand categories as economic activities where it is difficult for credit to replace cash (and our model predicts that cash is increasingly used for activities in that sector). The next and more challenging step is to identify the forces that can plausibly counteract the fall in cash demand driven by competition from other means of payment.====The increasing demand in less credit-sensitive activities can result from (1) structural changes, which cause the money demand curve to shift, and/or (2) decreasing nominal interest rates, represented by movement along the money demand curve. We think it is unlikely that structural changes could (fully) account for the cash paradox. In general, there is no evidence that the underground economy has experienced significant growth. According to Schneider and Buehn (2012), from 1999 to 2010, the shadow economy as a percentage of GDP shrunk slightly in 39 OECD countries. Another potential source of structural changes is increased perceived uncertainty and distrust in the financial system, which induce agents to hold more cash for precautionary motives or as a store of value (see, for example, Williams, 2012, Berentsen and Schar, 2018). However, in many countries, the cash paradox phenomenon was observed long before the financial crisis, with the CIC/GDP starting to level off or increase since the 1980s or 1990s. The financial crisis raised perceived uncertainty and distrust in the financial system and spurred the particularly strong cash demand afterward. Because of that, the cash paradox phenomenon has drawn more attention following the crisis. However, we think increased uncertainty and distrust in the financial system are unlikely to be the main force behind the robust cash demand before the financial crisis. Foreign demand could be an important factor for major international reserve currencies, such as the US dollar or euro. It is, however, less applicable to other currencies. Given the ubiquity of the cash paradox, country-specific factors, such as strong foreign demand, are unlikely to fully account for the phenomenon.====The critical question is then whether the fall in the interest rate can plausibly generate a robust demand for cash as a store of value or in the underground economy to sufficiently counteract the shrinking (transnational) demand for cash due to expanding credit usage. Our calibration exercises suggest that the answer is “very unlikely” without additional channels such as the cash-flow channel proposed in our paper, unless one is willing to make extreme assumptions about the relative size of the cash-credit sector to the cash-only sector (the former is less than 1/10,000 of the latter). The cash-flow channel that we emphasize strengthens the role of the falling interest rate relative to credit expansion, and helps to capture the paradoxical behavior of cash demand with much more plausible model parameterization.====Our model also closely relates to works that study credit in monetary economies within the framework developed by Lagos and Wright (2005) and Rocheteau and Wright (2005). Gu et al. (2016) and Araugo and Hu (2017) discuss the essentiality of credit in a monetary economy. Berentsen et al., 2007, Berentsen et al., 2014, Rojas Breu (2013), and Chiu et al. (2018) study the welfare effects of credit arrangements. Telyukova and Wright (2008) apply a model with money and credit to explain the credit card debt puzzle. Sanches and Williamson (2010) and Gomis-Porqueras and Sanches (2013) investigate the optimal monetary policy in a model with money and credit. Bethune et al. (2014) study the relationship between unsecured consumer credit and unemployment. Lotz and Zhang (2015) and Dong and Huangfu (2017) model costly credit and examine how monetary policies affect credit adoption. Liu et al. (2016) analyze the effects of inflation on price dispersion, markups, and welfare.====Our modeling innovation is to introduce cash flows between activities where cash and credit have different extents of substitutability, and we apply this new model to explain the paradoxical behavior of cash demand in response to increasing competition from credit usage. We take a short cut in modeling credit expansion as the increasing fraction of agents having access to credit.==== One could endogenize credit expansion by introducing heterogeneous preferences and a fixed cost of adopting credit, as in Dong and Huangfu (2017). In this setup, agents with high marginal utilities tend to use credit. Financial innovations that reduce the cost of adopting credit will induce more agents to use it. One could also endogenize credit limits as in Kehoe and Levine (1993), and derive credit expansion along the intensive margin as higher credit limits resulting from improvements in the monitoring technology to detect defaults. Endogenizing credit usage would add an interesting interaction between inflation (or nominal interest rate) and credit adoption or usage. However, given that the data on the cost of using credit and the monitoring technology are not readily available, in the end, one still needs to calibrate the cost or monitoring parameter to match observable data on credit adoption or usage, such as the credit access rate used in our calibration.====Finally, by modeling the cash flows across different sectors where cash and credit have different substitutability, our model generates a new channel through which credit and the nominal interest rate affects cash velocity, as apposed to the traditional way of modeling precautionary demand (as in, for example, Hodrick et al., 1991, Jafarey and Masters, 2003, Peterson and Shi, 2004, Lagos and Rocheteau, 2005, Faig and Jerez, 2006; Ennis, 2008, Ennis, 2009; Liu et al., 2011, Nosal, 2011, Dong and Jiang, 2014; and Hu et al., 2019).====The rest of the paper is organized as follows. Section 2 documents in more detail the long-term trend in cash demand and the recent puzzling phenomenon of continued decreasing cash usage and robust cash demand. Section 3 describes the model and characterizes the equilibrium. Section 4 derives the comparative statics and discusses the effect of credit expansions and the nominal interest rate on cash usage and demand. In Section 5, we calibrate three models–the standard single-sector cash-credit model, the segregated two-sector model where cash and credit have different substitutability across the two sectors, and the full model where cash flows between the two sectors–to quantify the importance of the modeling features. In that section, we also evaluate in detail existing alternative explanations about the cash paradox and pinpoint the value-added of the cash-flow channel that we emphasize. Section 6 provides empirical support to our model's predictions on cash velocity. Section 7 concludes. In the appendices, we provide additional figures, the proofs, document the data used in our quantitative analysis, discuss different measures of nominal interest rates, lay out the standard single sector cash-credit model and the segregated two-sector model for comparison, investigate the possibility of unregistered economic activities to capture the cash paradox, and discuss model extensions.",The cash paradox,https://www.sciencedirect.com/science/article/pii/S1094202518302977,8 October 2019,2019,Research Article,215.0
"Schünemann Johannes,Strulik Holger,Trimborn Timo","University of Fribourg, Department of Economics, Bd. de Perolles 90, 1700 Fribourg, Switzerland,University of Goettingen, Department of Economics, Platz der Goettinger Sieben 3, 37073 Goettingen, Germany,Aarhus University, Department of Economics and Business Economics, Fuglesangs Allé 4, 8210 Aarhus V, Denmark","Received 7 May 2018, Revised 19 September 2019, Available online 1 October 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.09.004,Cited by (11),"In this paper we explore how marriage affects longevity of men and women through income pooling and public-goods sharing as well as joint utility maximization of partners with different preferences and biology. We integrate joint decision making of couples into a biologically founded life-cycle model of health deficit accumulation and endogenous longevity, calibrate the model with U.S. data, and perform the counterfactual experiment of preventing the partnership. We elaborate four economic channels and find that, as singles, men live 8.5 months shorter and women 6 months longer. We conclude that about 25% of the marriage gain in longevity of men can be motivated by economic calculus while the marriage gain for women observed in the data is attributed to selection or other (non-standard economic) motives.","On average, married individuals, in particular men, live longer than singles (Waite and Gallagher, 2002). This stylized fact, which we will refer to as the marriage gap in longevity, is so well established that it is regarded as one of the most robust relationships in social sciences (Liu, 2012). Whether and to what extent being married ==== better health and higher longevity is less well known. One potential threat to econometric identification is selection into marriage, since wealthier and healthier individuals are more likely to get (and stay) married. The marriage gap declines substantially but does not vanish when socioeconomic status and initial health (as measures of selection into marriage) are taken into account (Dupre et al., 2009, Rendall et al., 2011). Many, but not all studies, also find a significantly higher gain from marriage for men and that in particular married men behave less unhealthy than single men (Dupre et al., 2009, Rendall et al., 2011). Moreover, the marriage gain seems to be increasing over time (Murphy et al., 2007).====Most studies assess the marriage gap by estimating hazard ratios for mortality of married vs. unmarried individuals (see Manzoli et al., 2007, for a survey and a meta study). While technically convenient, the hazard rate approach is not very intuitive. More interesting would it be to asses how marriage affects the longevity of men and women. Such estimates were recently provided by Pijoan-Mas and Rios-Rull (2014) for 50 years old white U.S. Americans. Their study takes behavioral changes into account and measures the life expectancy of hypothetical cohorts with given characteristics. It estimates for the year 2008 a marriage gain of 2.7 years for men and 1.5 years for women. In 1992, the gain was 1.5 years for men and 0.6 years for women.====Several mechanisms through which marriage could have a protective effect have been suggested but their specific role and quantitative importance is less well researched. For example, marriage could provide psychological and social benefits, which lead to improved health outcomes (Waite, 1995), or monitoring and encouragement through the spouse may induce health-promoting behavior (Umberson, 1992, Ross, 1995). Aside from the emotional channels emphasized in the medical and social sciences, marriage provides also significant economic gains from sharing of household public goods (e.g. Manser and Brown, 1980, Lam, 1988, Salcedo et al., 2012).====Acknowledging the potential socio-emotional benefits of marriage, we use counterfactual computational experiments in order to assess how marriage affects longevity of men and women through “cold-hearted” economic calculus. For that purpose, we integrate joint decision making of couples into the health deficit model (Dalgaard and Strulik, 2014). We then calibrate the model such that it fits health behavior, health outcomes, and life expectancy at the age of entry in marriage for the average U.S. American man and woman in the year 2010. The counterfactual experiment is that we prevent marriage and let men and women solve their life-cycle problem as singles. The model predicts that marriage provides a gain in life expectancy of 8.5 months for men, i.e. about 26% of the observed marriage gap, and a loss of 6 months for women. This is an interesting and non-obvious result because women benefit from income pooling and public-goods sharing in marriage. It suggests that the marriage gain in longevity of women observed in the data cannot be motivated by standard economic channels. The marriage gain is thus either motivated by non-standard economic channels like informal health care or the socio-emotional channels mentioned above. Another plausible explanation is that the data show selection rather than causality, i.e. healthier and wealthier women are just more likely to get and stay married.====The counterfactual method allows us to causally examine four economic channels through which living as a couple (in marriage or cohabitation) may affect health and longevity. The first and perhaps most obvious channel is income pooling. Given gender income differences in favor of men, income pooling in marriage benefits the wife at the expense of the husband. This should lead ceteris paribus to greater longevity for the wife and a shorter life for the husband compared to longevity when single. The second channel is given by public-goods sharing. Sharing public goods leads to a (quasi-) increase of income since more resources are available for private-goods consumption. Since individuals face decreasing marginal returns from instantaneous consumption, increasing income induces a higher propensity to smooth consumption over a longer lifetime and thus the incentive to spend more on health and to reduce unhealthy goods consumption.====The other two channels are based on joint utility maximization in partnerships and they tend to benefit only men. The biological channel takes into account that at any age, men accumulate health deficits at a higher speed (e.g. Mitnitski et al., 2002a, Mitnitski et al., 2002b). This means that in partnerships, at a given age of the couple, investments in the health of men provide a higher marginal return in terms of health deficits avoided and thus in terms of life extension and the value of life. Compared to individual utility maximization, it is thus optimal for couples to allocate health investments away from the woman and towards the man.====The quantitatively dominating mechanism with regard to the preference channel works through the feature that men prefer to smooth consumption less than women (Mazzocco, 2008). In a CES-utility framework (such as in our study) this is tantamount to the observation that men are less risk-averse (Cohen and Einav, 2007, Croson and Gneezy, 2009, Sunden and Surette, 1998). Both features are reflected in less curvature of the utility function, which means that men, as individuals, tend to like instantaneous consumption more than women and invest less in their health and longevity. As singles, at a given age, men thus experience more utility and less marginal utility from consumption than women. In partnerships, however, allocative efficiency requires that marginal utility from consumption is equalized between men and women. This means that the couple puts less emphasis on the instantaneous gratification of men and more on their health, now and later in life. Obviously, the opposite holds true for women.====In order to keep the analysis tractable we do not endogenize the marriage market in which the selection process takes place. Such a procedure would be necessary if one seeks to completely explain the marriage gap observed in the data. Here, we assume that the marriage decision has been already made, and aim to analyze the causal contribution of four different channels to the longevity gain (or loss) from marriage. The construction of our counterfactual experiment shuts down reverse causality with respect to these four channels. The reason is that we compare longevity of a married and a single individual, holding constant health, preference, and environmental parameters. As a consequence, the resulting longevity differential can only originate from the economic channels considered in this paper. In the comparative dynamic analysis we consider exogenous sorting into marriage by computing the gain from marriage when one partner is substantially richer or older.====Summarizing, the model predicts that partnerships are conducive to a longer life of women through income pooling (at the expense of men) and beneficial for both men and women through the public-goods channel, while only men gain (at the expense of women) through the biological channel and the preference channel. The model thus explains why the marriage gain is larger for men than for women. Surprisingly, our quantitative analysis finds that the biological and preference channel outweigh the positive effect of income pooling and public-goods sharing on longevity for women. It thus predicts that differences in economic behavior between single and married women lead to a marriage loss in longevity for wives. We use the term “marriage gain” for linguistic ease. The same results hold true under joint utility maximization of cohabiting couples. In this respect it is interesting that a recent study by Kohn and Averett (2014) finds that, controlling for selection, cohabitation is better than marriage for the health of men and women. While their results could be viewed as challenging for the earlier literature emphasizing the socio-economic benefits from marriage as an institution (Waite and Gallagher, 2002) it is broadly supportive of the public-goods channel emphasized by our theory.====Previous theory-based research on the marriage gap in longevity is relatively scarce. One potential explanation for this observation may be that health economic theory in the past was dominated by the Grossman (1972) model, which builds on health capital rather than health deficit accumulation. The major drawback of the Grossman approach is that health capital represents a latent variable, which is unknown to the medical and biological sciences. Our conceptualization of aging, in contrast, is based on the accumulation of health deficits which can be easily reported by virtually everyone in the population. The frailty index equips us with an intuitive metric for health deficits while its association with age and mortality can be precisely estimated. As we will show below, this feature allows us to replicate actual health behavior observed in the data.====Our study is related to the work of Jacobson and coauthors who introduce into the health capital framework the idea that, in a family, one could also invest into the health of the partner (Jacobson, 2000, Bolin et al., 2001, Bolin et al., 2002). In these studies the solution is only obtained up to the first-order conditions. Since there are infinitely many trajectories in the phase space fulfilling the first-order conditions, this means that the earlier literature did not fully identify how marriage affects health outcomes and longevity. For that purpose one needs to inspect the boundary conditions and the transversality condition (that hold at the time of death of each partner) since these conditions identify the unique optimal lifetime trajectory and the lifespan of men and women. The Jacobsen approach has been refined by Felder (2006). By simplifying the model, Felder manages to show, by relying only on the first-order conditions, that a longevity differential between sexes exists under certain assumptions and that it is smaller in marriage than for singles. In terms of our model, Felder focuses (in reduced form) on the income channel as well as the biological channel and ignores the public-goods channel and the preference channel. Our paper builds on our earlier study of the gender differential in longevity among singles (Schünemann et al., 2017b). Conceptually, the task of this paper is more challenging, both theoretically and in its numerical implementation, because it requires to solve simultaneously two free terminal time problems of optimal control, one at the interior when the first partner dies and one at the boundary when the widow passes away.====The paper is organized as follows. In the next section we propose a life-cycle model of health behavior, health outcomes, and longevity of couples. In Section 3 we calibrate the model with U.S. data and in Section 4 we discuss the implied life-cycle trajectories for married men and women. In Section 5 we perform the counterfactual exercise of preventing marriage and predict the life course of the calibrated couple as singles. In Section 6 we provide sensitivity analyses with respect to non-calibrated assumptions and income effects on the marriage gain. We also discuss the impact of marital sorting. Section 7 concludes.",The marriage gap: Optimal aging and death in partnerships,https://www.sciencedirect.com/science/article/pii/S1094202518302102,1 October 2019,2019,Research Article,216.0
Gabrovski Miroslav,"University of Hawaii at Manoa, USA","Received 11 October 2018, Revised 6 September 2019, Available online 25 September 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.09.002,Cited by (5),"There is ample evidence that R&D investment is mildly pro-cyclical. Whereas the existing literature can explain the positive correlation between investment in R&D and output, the moderate strength of the relationship remains under-explored. This paper develops a stochastic expanding-variety ==== that accounts for the observed mild pro-cyclicality of R&D. In the model, several firms may simultaneously make the same innovation. Innovations made by many firms simultaneously are of higher quality, on average, and contribute relatively more to the expansion of the knowledge stock in the economy. This delivers an endogenous mechanism that breaks the otherwise perfect correlation between R&D and output. A calibration of our model closely matches the cyclical properties of R&D.","It is a well-established empirical regularity that R&D investment is mildly pro-cyclical.==== Depending on the data source, the correlation between the cyclical components of R&D and output is between 0.3 and 0.5.==== Whereas the existing literature has proposed several mechanisms that can explain the sign of this relationship, its magnitude remains under-explored.==== In particular, existing theoretical models predict near-perfect correlation between output and R&D, which is at odds with the data.====This paper complements the existing literature by developing an expanding-variety endogenous growth model that can resolve the discrepancy. In our model, there is the possibility that several firms make the same innovation simultaneously. Innovations simultaneously made by many firms contribute more, on average, to the expansion of the knowledge stock in the economy, i.e. they are of higher quality. This mechanism delivers endogenous oscillations in the economy and, as a consequence, mildly pro-cyclical R&D. A calibrated version of our model closely matches the pro-cyclicality and volatility of R&D observed in the data.====The source of technological progress in our model is the invention and adoption of new intermediate varieties. As in Gabrovski (2018) and Kultti et al. (2007), the innovation process makes the distinction between potential innovations (that we refer to as ideas and research avenues interchangeably) and actual innovations (new varieties). Upon entry into the R&D sector each firm is randomly matched with a particular idea from a pool of feasible research avenues. In particular, the number of firms matched with a given idea is a Poisson random variable. This matching technology generates the possibility that some ideas are simultaneously innovated by many firms while others are not innovated at all.==== This feature allows our model to account for the commonly observed in practice phenomenon of simultaneous innovation.==== When a firm is matched with an idea and successfully innovates that idea, it applies for a patent over the corresponding variety. If more than one firm apply for the same patent, they each have an equal chance of receiving it. We follow Romer (1990) and Kortum (1997), among others, and assume that knowledge is cumulative — inventing a new variety allows firms to “stand on the shoulders of giants” and gain technological access to a number of new ideas.====Each variety is equally productive, but innovations differ in the number of new research avenues they generate, i.e. their “quality”. In particular, innovations made simultaneously by many firms lead to more research avenues, on average. This captures the intuition that i) when firms invest more in a given idea, it is more likely to be of higher quality; ii) when more firms work simultaneously on the same idea, it is more likely that at least one of them will develop a high quality invention.====The main contribution of our paper is the model's ability to reproduce the mild pro-cyclicality of R&D observed in the data. In our model the correlation between output and R&D is 0.44 and R&D is 1.89 times as volatile as output. In the data, the correlation is ==== and the relative volatility of R&D is ==== times that of output.==== In our model, innovations made simultaneously by many firm are, on average, of higher quality. This generates a mechanism which leads to endogenous oscillations. As a result, R&D investment is mildly pro-cyclical. In particular, following a positive technology shock both R&D and output converge to their new, higher balanced growth paths (BGP henceforth). Thus, their cyclical components are positively correlated. However, during this transition both series oscillate around their convergent paths in such a way that whenever output overshoots its convergent path, R&D investment undershoots it and vice versa. This reduces the strength of the relationship and leads to mildly pro-cyclical R&D.====To see the intuition behind the endogenous oscillations, suppose that at a given period, ====, the economy features relatively more varieties and relatively less available research avenues. This scarcity of ideas implies that there will be few innovations made and, as a result, relatively less varieties next period. This relatively low number of varieties at ==== implies lower competition between firms and increases the expected profitability of innovation. Hence, firms have more incentives to enter the R&D sector. This in turn leads to higher congestion in the “market” for ideas (i.e. relatively higher ratio of firms to ideas) and to a higher average number of firms that simultaneously innovate the same idea. Because of this, the average quality of innovations is higher, which leads to more feasible research avenues at period ====. Thus, at period ==== the economy features relatively more ideas and less varieties. As a result, at that period, there is relatively more innovation which leads to more varieties at ====. This then leads to lower expected profits and lower incentives for firms to enter the R&D sector, which ultimately leads to lower mass of ideas at ====. Furthermore, in periods when there are more varieties output is relatively high, whereas R&D investment is relatively low because research avenues are scarce. Conversely, when ideas are relatively abundant R&D investment is high and output is low because such periods feature a relatively lower mass of varieties.====We proceed by introducing the environment and characterizing the equilibrium. Next, we simulate the model and examine its impulse response functions and the oscillations therein. Lastly, we show our model can match key moments in the data and this ability is driven by the presence of endogenous oscillations.",Simultaneous innovation and the cyclicality of R&D,https://www.sciencedirect.com/science/article/pii/S1094202518305350,25 September 2019,2019,Research Article,217.0
Brendler Pavel,"Institute for Macroeconomics and Econometrics at the University of Bonn, Adenauerallee 24-42, 53113 Bonn, Germany","Received 28 January 2019, Revised 16 September 2019, Available online 25 September 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.09.001,Cited by (4),"Despite increasing earnings ==== and aging population, there has been no major reform of Social Security since 1977. To account for this puzzling observation, I construct a heterogeneous agent model with incomplete financial markets. There is a government who chooses the replacement rate schedule by maximizing the weighted welfare of all generations alive. I specify the Pareto weights as a function of household's age and identify them both in 1977 and 2018. I find that there was a slight bias towards younger workers in 1977. However, the bias has shifted towards older households in 2018. This shift is a result of counteracting forces induced by: 1) population aging and 2) increased labor productivity risk and increased college premium. The effect of population aging turns out to be stronger quantitatively.","In 1977 the U.S. Congress introduced the Social Security Amendments, whose main purpose was to stabilize future replacement rates, defined as the ratio of an individual's pension benefit to this individual's average lifetime earnings. Upon signing the Amendments, President Carter announced that “the provisions of this Law are tremendous achievements and represent the most important Social Security legislation since the program was established.”====Since the adoption of the Amendments, the U.S. economy has changed significantly. In particular, earnings inequality has increased sharply and the U.S. population has continued to age. Nevertheless, there has been no major reform of Social Security (henceforth – SS) since 1977. In this paper, I explore one potential reason for why the SS system did not adjust to these important developments in the U.S. economy.====I use a general equilibrium overlapping generations model with incomplete markets and labor-augmenting technological progress in the style of Huggett (1996). Pension benefits are determined by a replacement rate schedule with one key policy variable that controls the ==== of the replacement rate schedule. The government sets this policy variable in order to maximize the ==== welfare of all generations alive.==== Furthermore, the government chooses the replacement rate schedule ====. Given the replacement rate schedule, the SS tax rate adjusts in each period to satisfy the government budget. Apart from running the pension system, the government distributes lump-sum transfers across all agents and pays for other expenses (wasted in the context of my model), both of which are financed by exogenous linear income taxes.====I assume that Pareto weights in the government maximization problem depend on agent's age according to a power function with one unknown parameter, which I refer to as the ====. The age bias reflects the preferences of the government over inter-generational income redistribution, the provision of public insurance against labor productivity risk, etc. In the absence of the age bias, the government's problem boils down to a utilitarian social welfare function. As I show, this specification of the Pareto weight function allows me to clearly identify the bias parameter using the ==== of the replacement rate schedule in the data.====I calibrate the benchmark model to the U.S. data in 1977, assuming that the U.S. economy is in a steady state. Then I adjust some key model parameters which capture the major changes in the U.S. economy between 1977 and 2018 and resolve the model assuming that the economy is in a steady state in 2018. The first change in the model parameters refers to population aging. I account for this by reducing the birth rate and increasing age-dependent survival rates, which control agent's longevity. The second change refers to a drastic rise in cross-sectional earnings inequality. Consistent with the data, I capture this through a deterministic and a stochastic component. In particular, I adjust the deterministic skill premium and the share of college graduates and increase the dispersion of the idiosyncratic labor productivity risk.====Assuming that it was optimal for the government to sustain the SS Amendments of 1977 as an optimal policy, I compute the age bias parameter both in 1977 and 2018. I find that there is a slight bias towards young workers in 1977. On the contrary, the age bias has shifted towards older households in 2018. As I show, these results are largely in line with the observed changes in voter turnout rates in the data. The shift in Pareto weights in the model turns out to be a result of two major counteracting forces. On the one hand, population aging is solely responsible for the rise in SS tax rate between 1977 and 2018. Everything else equal, the government should have reduced the replacement rates to reduce the upward pressure on the SS tax. Given that it optimally decided not to so, the Pareto weights must have shifted towards older agents. On the other hand, increased labor productivity risk raised the demand for publicly provided insurance as well as ex-post income redistribution. Similarly, the increase in the college premium further pushed the demand for ex-post income redistribution. Everything else equal, the government should have raised the replacement rates to protect those with low realizations of earnings. Given that this didn't happen on the equilibrium path, the Pareto weights must have shifted towards younger households. Quantitatively, the effect of population aging turns out to be stronger leading to an overall rise in Pareto weights between 1977 and 2018.====The paper builds upon three strands of the literature. The first strand takes SS as given and studies macroeconomic implications of transiting from the publicly provided to a fully funded pension system. Conesa and Krueger (1999), Fuster et al. (2007), and Nishiyama and Smetters (2014) find that quantitatively SS plays an important role as a partial insurance device against idiosyncratic risk. In my paper, I confirm the importance of SS as a partial insurance device. However, in my model SS arises endogenously.====Second, the paper relates to the macroeconomic literature which endogenizes government policies. Corbae et al. (2009) assume a utilitarian social planner in order to account for the observed level of the income tax rate in the U.S. Not surprisingly, the equilibrium tax rate in their model exceeds the one in the data. As opposed to this study, I use a weighted social welfare function. Song et al. (2012) uses a social planner's approach with Pareto weights on retired households to account for the level of public good provision and public debt in the U.S. Relative to this literature, my paper focuses on SS and calibrates the Pareto weights inside the model.====Bachmann and Bai (2013) and Chang et al. (2018) are two noteworthy exceptions. Bachmann and Bai (2013) recover Pareto weights which make the equilibrium contemporaneous correlation between output and government purchases in their model consistent with the U.S. data. Note that their model misses any form of income redistribution, which is the key aspect of my paper. Chang et al. (2018) develop a quantitative heterogeneous agents model with progressive income taxes. They uncover Pareto weights in the social welfare function for a large set of countries that justify the observed redistribution policy. As opposed to this paper, I focus on SS and analyse changes in Pareto weights over time.====Third, this paper also builds upon the growing literature on the ==== (or ====) approach. Bourguignon and Spadaro (2012), Lockwood and Weinzierl (2016), and Saez and Stantcheva (2016) combine analytical results from optimal tax theory in the Mirrleesian framework with assumptions on economic parameters to infer the marginal social welfare weights prevailing in the data. Bai and Lagunoff (2013) assume a weighted majority voting process in which an individual’s vote share depends on this individual's wealth holdings according to a power function and show how to recover the parameter of this function from the observed policy. Similar to my work, Lockwood and Weinzierl (2016) not only recover but also use the positive, empirical estimates of the weights in order to provide normative assessments of the income tax policies in the U.S. All of these studies, however, are based on stylized static model economies, which is not the case in my paper.",Why hasn't Social Security changed since 1977?,https://www.sciencedirect.com/science/article/pii/S1094202519300559,25 September 2019,2019,Research Article,218.0
"Ashman Hero,Neumuller Seth","UC Berkeley, United States,Department of Economics, Pendleton East, Wellesley College, 106 Central Street, Wellesley, MA 02481, United States","Received 27 November 2018, Revised 8 April 2019, Available online 16 September 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.06.004,Cited by (12),"We use a quantitative, overlapping-generations, incomplete markets, life-cycle model to explore the extent to which income differences can explain the racial ","Differences in the wealth held by black and white households in the U.S. are staggering. According to data from the 1989–2016 waves of the Survey of Consumer Finances (SCF), the median net worth of white households is nearly seven times greater than that of black households.==== Wealth determines a household's consumption level, ability to withstand labor market fluctuations, and access to quality housing and education services. Wealth is also a source of both political power and social influence. The racial wealth gap thus serves as a stark reminder of the persistent political and economic racial inequality in the U.S.====Racial differences in income, though considerable, are far less pronounced; the median income of white households is less than two times greater than that of black households.==== Whether income differences can explain the wealth gap between black and white households has been the subject of numerous empirical studies.==== Scholz and Levine (2002), in their survey of the literature, conclude that “differences in labor income have a major impact on black-white wealth differentials,” as observed income differences have been found to account for as much as three quarters of the racial wealth gap. What remains unexplained by these empirical findings, however, are the mechanisms by which observed income differences between black and white households result in vast disparities in wealth.====We explore the extent to which income differences can explain the racial wealth gap and evaluate potential amplification mechanisms through the lens of a quantitative, overlapping-generations, incomplete markets, life-cycle model.==== Our model builds upon the framework developed by De Nardi (2004), itself an extension of Huggett (1996). The key features of the model are race, education, and family structure-specific stochastic household income processes estimated using data from the Panel Study of Income Dynamics (PSID), race and education-specific lifespan risk, an income floor, and intergenerational transfers of wealth and labor market ability.==== A calibrated version of the model fully accounts for the racial wealth gap observed in the data. In addition, it endogenously generates an empirically plausible degree of wealth inequality, both conditional and unconditional on race, and is broadly consistent with observed patterns of wealth accumulation over the life cycle.====We use the model to isolate the effect of income on wealth by considering the case in which the income floor and bequest motives are shut-down, accidental bequests are equally distributed across recipient households, and life expectancy is independent of race. In this counter-factual scenario, the model slightly over-predicts the wealth held by the median white household ($166,050 in the model vs. $146,790 in the data), but severely over-predicts the wealth held by the median black household ($109,089 in the model vs. $21,866 in the data). The model-implied racial wealth gap in levels (white–black) is thus $56,961, which suggests that income differences, on their own, explain 43.0% of racial differences in median wealth. The model-implied ratio of median wealth (white/black), however, is just 1.52 versus a value of 6.71 in the data. Reintroducing bequest motives and direct intergenerational transfers of wealth between family members causes the ratio of median wealth to nearly double, rising from 1.52 to 3.00, and allows the model to fully account for the observed racial wealth gap in levels, with bequest motives accounting for 28.6% of the gap and unequal bequests accounting for 25.8% of the gap. The ratio of median wealth then more than doubles, rising from 3.00 to 6.11, when we reintroduce the income floor as it disproportionately reduces the incentive that low income, low wealth households, the majority of whom are black, have to save. Finally, reintroducing differences in lifespan risk causes the ratio of median wealth to rise from 6.11 to 6.95, which is just 3.5% higher than the value of 6.71 observed in the data.====Our results suggest that one does not need to appeal to racial differences in preferences or returns on investment in order to explain how observed income differences between black and white households lead to large racial disparities in wealth.==== This is not to say that other factors such as legacies of discrimination are unimportant for understanding racial disparities in wealth. Indeed, there is an extensive literature documenting how both statistical and taste-based discrimination may be of primary import for explaining racial differences in labor market outcomes, which our model suggests have important implications for racial wealth inequality.==== What our findings do imply, however, is that the racial wealth gap is a direct result of racial differences in labor income in the presence of bequest motives, intergenerational transfers of wealth, and social insurance. To put it differently, if income differences were eliminated in our model, racial disparities in wealth would eventually disappear.==== Our findings thus underscore the importance of identifying the causes of racial differences in income for understanding racial disparities in wealth.====To our knowledge, there are only two other papers that use a quantitative-theoretic model to explore the racial wealth gap: White (2007) quantifies the extent to which conditions at the time of Emancipation and segregated schooling can explain observed trends in racial income and wealth inequality, and Aliprantis et al. (2018), in a study contemporaneous to ours, compute the transition path of the racial wealth gap as income differences fall at a deterministic rate.==== Instead, we focus on identifying and evaluating potential mechanisms capable of explaining how observed differences in income lead to large disparities in wealth. The model studied by White (2007) features dynastic households that face no idiosyncratic risk. In contrast, the overlapping-generations, life-cycle economy that we develop features a parsimoniously calibrated stochastic income process which produces rich heterogeneity across households, thereby allowing us to more precisely assess the effect of income differences on racial differences in wealth. While the model employed by Aliprantis et al. (2018) is similar to ours along a number of dimensions, it abstracts from racial differences in family structure, the likelihood and persistence of realizing zero labor income, and income volatility, all of which we find are quantitatively important for explaining racial wealth inequality.====Using a quantitative-theoretic model to assess the ability of income differences to explain the racial wealth gap has a number of advantages over the empirical approaches employed to date. In particular, it allows us to (1) isolate the effect of income on wealth and (2) identify and evaluate potential amplification mechanisms capable of explaining how observed income differences between black and white households result in vast differences in median wealth. While exploring the origins of the racial wealth gap is important on its own, our approach also sheds light on the factors driving wealth inequality more generally. Just as explaining the extreme concentration of wealth in the U.S. requires understanding why the rich save so much, we demonstrate that accounting for the racial wealth gap requires understanding why black households save so little relative to their white counterparts.==== Our results highlight the potential role that voluntary bequests, an income floor, and differences in lifespan risk play in driving a wedge between the saving rates of black and white households.====The role of bequests in accounting for the racial wealth gap has been explored extensively in the empirical literature.==== This literature concludes that bequests, on their own, can account for no more than one quarter of the racial wealth gap at the mean and likely matter far less at the median.==== Following De Nardi (2004), we model bequests as luxury goods. This induces the richest households in our model, the majority of whom are white, to save more in order to leave a larger estate to their offspring. This mechanism is consistent with Smith (1995), who argues that differences in intergenerational transfers of wealth arise not because white households have a stronger bequest motive, but because they are more able to “afford” bequests. Simulations of our model suggest that voluntary bequests, through their differential impact on household saving behavior within a generation and their cumulative effects on wealth accumulation over many generations, can explain the entire portion of the racial wealth gap in levels not accounted for by income differences alone.====There is considerable debate in the empirical literature regarding the impact that public assistance programs have on household saving.==== In their influential theoretical analysis, Hubbard et al. (1995) argue that the low saving rate exhibited by the poorest households in the U.S. can be rationalized as a utility maximizing response to the presence of means-tested social insurance. In our model, the income floor disproportionately inhibits precautionary saving by black households since they earn less income and hold less wealth, on average, than white households and are, therefore, more likely to qualify for and benefit from this form of social insurance. As a result, the median wealth of black households declines more in percentage terms in response to the presence of the income floor, which causes the ratio of median wealth to more than double and enter into an empirically plausible range.====Exogenous shocks to family structure and their implications for wealth inequality have been studied previously in overlapping generations life cycle economies.==== For example, Cubeddu and Ríos-Rull (2003) develop a model with exogenous marriage and divorce shocks in which uncertainty over the characteristics of one's future spouse and divorce risk affect saving incentives and, therefore, household wealth. To maintain computational tractability, we assume perfect assortative mating on race, education, income, and wealth, equal sharing of assets upon divorce, and homothetic preferences at the household level over consumption per adult. Marriage and divorce shocks in our model are risky in so far as they determine the properties of the exogenous stochastic income process that households' face and, therefore, affect wealth accumulation over the life cycle mechanically through their impact on household formation and destruction. Despite this simplification, we find that exogenous racial differences in marriage and divorce shocks, and the racial marriage gap that arises as a result, do play an important role in explaining racial wealth inequality.====Finally, we also contribute to that body of the literature which attempts to carefully measure earnings risk and study its implications for consumption and saving behavior.==== In light of well-documented racial differences in incarceration rates and labor force non-participation rates, particularly for men, we augment the canonical permanent-transitory framework for labor income with a Markov process governing discrete jumps between positive and zero labor income.==== In addition, our approach explicitly allows for the possibility that black and white households with identical education and family structure face different income processes, both at the mean and variance levels. In other words, we allow the returns to observable skills, both in terms of average income and income volatility, to vary with race, education, and family structure. This is a key innovation because, as we show here, differential exposure to income risk, which affects both the consistency and reliability of a household's income stream, has important implications for differences in the patterns of wealth accumulation between black and white households.====The remainder of this paper is structured as follows. Section 2 describes our model. Section 3 presents our quantitative results. Finally, Section 4 concludes.",Can income differences explain the racial wealth gap? A quantitative analysis,https://www.sciencedirect.com/science/article/pii/S109420251830601X,16 September 2019,2019,Research Article,219.0
"Jaccard Ivan,Smets Frank","European Central Bank, DG-Research, Germany,European Central Bank, DG-Economics, Germany","Received 9 April 2018, Revised 5 August 2019, Available online 27 August 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.08.002,Cited by (4),"Many southern European economies experience large ==== during periods of expansion that are followed by abrupt capital flow reversals when a recession hits. This paper studies the dynamics of cross-border flows between the North and South of Europe in a two-country ==== with incomplete international asset markets. Over the business cycle, the direction of capital flows between the two regions can be explained by a model in which common shocks have asymmetric effects on debtor and creditor economies. This mechanism explains why aggregate consumption is more volatile in the South than in the North and generates a higher welfare cost of business cycle fluctuations in the region that experiences procyclical net ====. We also study the adjustment to asymmetric financial shocks.","A key characteristic of the Eurozone business cycle is that the trade deficits of Southern European economies vis-à-vis Germany increase during periods of expansion and decline in recessions (see Fig. A.1).==== At the same time, the trade surplus run by Germany vis-à-vis the South of Europe increases in boom times and declines during periods of recession (see Fig. A.2).==== Net exports being a measure net capital outflows, this implies that Southern European economies increase borrowing in boom periods and therefore need to reduce external debt during periods of recession. Since the opposite co-movement is observed in Germany, net capital inflows are thus procyclical in the South and countercyclical in the North.====The second piece of the puzzle is that these large swings in net capital outflows occur between regions that exhibit a high degree of business cycle synchronization. Indeed, at business cycle frequency, output fluctuations in the North and in the South of Europe have remained highly correlated, even during the Subprime crisis (see Fig. A.3).==== What explains the direction of net capital flows between the North and South of Europe? And since net capital inflows are procyclical in the South and countercyclical in the North, what are the welfare implications of these cross-border flows?====Taking the seminal contribution of Glick and Rogoff (1995) as a starting point, this paper addresses these questions by focusing on the issue of cross-country heterogeneity. In a model composed of two symmetric economies that are hit by common shocks, trade balances are always constant and equal to zero. In the presence of common shocks, however, cross-border capital flows can arise if the two economies differ in their economic structure. Given the high degree of cross-country heterogeneity observed across Eurozone countries, this paper studies the implications of diverging economic structures and common shocks for the dynamics of capital flows. Following Glick and Rogoff (1995) and for the sake of parsimony, we start by considering the case in which technology shocks are the only source of common disturbances. The simplifying assumption of common technology shocks, which will be relaxed later on, can also be motivated by the high correlation between measures of total factor productivity observed in the two regions at business cycle frequency (see Fig. A.4).====The degree of structural asymmetry between the two blocks is captured by introducing differences in preferences, in the cost of adjusting capital, in foreign asset demands and in the degree of financial frictions across the two regions. The parameters are then selected by maximizing the model's ability to replicate a set of 12 moments that characterize the Eurozone business cycle. The magnitude of the fluctuations in net capital flows that we obtain is therefore determined by the degree of structural asymmetry implied by the data. The stronger the degree of structural asymmetry between the two regions, the more volatile the fluctuations in net capital flows generated by this mechanism.====Our results suggest that the dynamics of capital flows critically depends on the magnitude of initial net asset positions. In other words, whether a region is a net debtor or a net creditor has a first-order impact on the propagation mechanism of common shocks. In the Eurozone, over the period under consideration, the average trade surplus of Germany with respect to the South of Europe amounted to about 1.6% of German GDP. In the South, the average deficit observed during this period represented about 1.4% of the region's GDP. The North is therefore a net creditor and the South is a net debtor. In our model, it is possible to reproduce this difference in trade balance to output ratios by introducing asymmetries in the demand in each region for the foreign asset issued by the respective trading partner.====Suppose that a favorable shock reduces real interest rates in the entire area, then how do the regions' net asset positions affect the transmission mechanism of a common shock? While the positive shock leads to a synchronized increase in output, the key element is that creditor and debtor regions are differently impacted by a decline in real interest rates. In the debtor region, which in our case is the South, a decline in real interest rates reduces the cost of issuing debt on the international asset market. In the North, by contrast, lower real interest rates reduce the revenue from lending to the South, whereby in net terms lending is achieved by purchasing the asset issued by the debtor region. Over the business cycle, common fluctuations in real interest rates are therefore a source of procyclicality in the South, since the cost of borrowing from the North declines during periods of expansions and increases precisely when the economy is hit by a recession.====The key driver of net capital flows is the differentiated effect of common shocks on creditor and debtor economies. Since in this model real interest rates rise during recessions, the cross-border asset market amplifies the effects of negative common shocks in the debtor region. During periods of expansion and low interest rates, the incentive to build buffers is therefore stronger in the South than in the North because agents in the debtor economy anticipate that borrowing costs increase when a recession hits. In other words, common fluctuations in real interest rates generate a stronger intertemporal smoothing motive in the South, in the sense that marginal utility of consumption is more volatile in the debtor economy. In the creditor region, by contrast, the need to self-insure is less pressing because the cross-border asset market provides a hedge against business cycle fluctuations. In response to a common favorable shock that reduces real interest rates, the decline in marginal utility of consumption is therefore more pronounced in the South than in the North.====We find that this stronger intertemporal smoothing motive in the South is a key determinant of cross-border flows. To understand this result, note that in this model intertemporal smoothing can be achieved via two margins. First, agents can use investment to accumulate physical capital, which is a fixed asset. Second, the intertemporal margin provided by financial markets allows agents to accumulate, in net terms, a foreign asset that can be traded across borders. The cyclicality of net capital flows in turn depends on how agents combine these two margins to achieve consumption smoothing. If net foreign asset accumulation increases then the trade balance improves, and it deteriorates if net foreign asset accumulation declines.====For the set of parameter values that maximizes its ability to match the data, the model predicts that physical capital is the preferred hedge against business cycle fluctuations. Agents in the debtor economy therefore find it optimal to satisfy their desire to accumulate capital during booms by running a trade deficit. In other words, during periods of expansion, investment in the debtor economy is partly financed by net issuance of debt. Since this implies a deterioration of the region's trade balance during periods of expansion, this mechanism therefore provides an explanation for the procyclicality of net capital inflows observed in the South.====Our main mechanism, which implies that the South finances an investment boom during periods of expansion by borrowing from abroad, can be motivated by the strong negative co-movement between investment and the trade balance in the South. As illustrated by Fig. A.5, which shows the investment to output and trade balance to output ratios in the South, the investment boom observed from the end of the 1990s until the crisis period coincided with a sharp deterioration of the region's trade balance. The decline in investment that occurred after the crisis was also accompanied by a marked reduction in the region's trade deficit.====At business cycle frequency, it should be noted that the model only explains around 22% and 27% of the standard deviation of the trade balance to output ratios observed in the North and South of Europe, respectively. One interesting implication of our mechanism is that the model's ability to reproduce the volatility of these ratios improves once medium-term frequencies are taken into consideration. For cycles ranging from 32 to 120 quarters, we find that the model can account for about 40 and 50 percent of the fluctuations in the trade balance to output ratios observed in the North and South of Europe, respectively.====Although common shocks generate fluctuations in capital flows that are much lower than in the data, the magnitude that we obtain is sufficient to generate significant differences in the welfare cost of business cycle fluctuations (e.g. Lucas, 2003) across regions. In the South, the fact that the region needs to reduce its trade deficit during recessions, precisely when marginal utility of consumption is high, exacerbates the effect of common shocks. In the North, by contrast, cross-border capital flows attenuate the effects of common shocks because the net income that creditors receive from the debtor region rises during recessions. Net capital flows facilitate consumption smoothing in the creditor region whereas they make consumption smoothing more difficult to achieve in the debtor economy. The direction of cross-border capital flows therefore matters since it generates a higher cost of business cycle fluctuations in the region that experiences procyclical net inflows.====Given the limited ability of our model to account for the magnitude of net capital inflows observed in the data, we next study the adjustment of our two-region economy to asymmetric financial shocks. In the Eurozone, the introduction of asymmetric financial shocks can be motivated by the disproportionate effect that the 2011-2012 Sovereign Debt Crisis had on the South of Europe. This period was characterized by an increase in spreads between short-term financial assets issued by the South relative to those issued by Northern European economies, such as Germany. The shock also generated a sudden stop in activity, in the sense that it led to a reduction in trade deficits in the South that was accompanied by a reduction in surpluses in the North.====The asymmetric effect of the Sovereign Debt Crisis is captured by introducing a shock that reduces the share of foreign assets in portfolio holdings. We find that this particular sort of asymmetric financial shock reproduces some main empirical regularities observed during this period. In the South, the asymmetric shock increases spreads, generates a deeper recession than in the North and forces the region to reduce its trade balance deficit. Introducing asymmetric financial shocks into the analysis also considerably increases the volatility of trade balance to output ratios.====It is important to emphasize that the present paper studies imbalances at business cycle frequency and abstracts from long-term growth. Introducing endogenous growth into the analysis would be necessary to capture the effects of low frequency fluctuations on cross-border flows. A second main limitation of the analysis is that the paper assumes that technology shocks are the only source of common shocks. Of course, other types of shocks are likely to play an important role in explaining the Eurozone business cycle (e.g. Smets and Wouters, 2003). The next sections also provide a brief overview of the various alternative explanations of capital flows proposed in the literature.====The starting point of our analysis is the basic two-country version of the standard one-sector stochastic growth model with complete asset markets (e.g. Backus et al., 1992). The first key departure from the baseline model is the introduction of restrictions in the extent to which international capital markets permit to pool risk across economies. Following Cole (1988), Baxter and Crucini (1993), Baxter (1995), Kollmann (1996), Arvanitis and Mikkola (1996), Boileau (1999), Heathcote and Perri (2002) and Corsetti et al. (2008) among others, we develop a model in which individuals have incomplete access to international risk-sharing.====Relative to the frictionless neoclassical growth model, the second main difference is that we introduce credit into the analysis by assuming that in each country firms need to pay workers and capital owners in advance of production. Following Jermann and Quadrini (2012), financial intermediation is subject to frictions and credit conditions depend on the tightness of an enforcement constraint.====We also depart from the frictionless benchmark by introducing a specification of habit formation in the composite of consumption and leisure (e.g. Jaccard, 2014), which, when combined with financial frictions and adjustment costs, amplifies the endogenous propagation mechanism of the neoclassical growth model (e.g. Jaccard, 2018). This preference specification was first studied in the context of a two-country business cycle model in Dimitriev (2017), where that author showed that habits in the composite good considerably improve the baseline model's ability to account for the quantity anomaly and international co-movement puzzles.====In Gourio et al. (2013), volatile fluctuations in exchange rates are obtained by augmenting the baseline model with Epstein-Zin-Weil preferences and by introducing disaster risk into the analysis. Fogli and Perri (2015) study the effects of volatility shocks on financial imbalances and show that precautionary saving has a significant impact on net foreign asset positions.====Our main mechanism builds on the work of Kraay and Ventura (2000). Following the intertemporal approach to the current account, they show how favorable shocks can lead to deficits in debtor countries and surpluses in creditor regions, and they also provide empirical evidence in support of the model's implications. Our work is also related to the literature that studies the dynamics and long-run determinants of global imbalances between countries with heterogeneous financial markets. In Mendoza et al. (2009), financial integration between two countries in different stages of financial development reproduces the evolution of imbalances between the United States and China that have been observed over recent decades. In Caballero et al. (2008), the main mechanism relies on differences in ability of a country to generate financial assets from real investments. Prades and Rabitsch (2012) explain the evolution of imbalances in the U.S. in a model where countries differ in their degree of openness to international financial markets.====In line with the stylized facts on emerging markets that are documented in Aguiar and Gopinath (2007), our model predicts that the volatility of consumption should be higher in the region experiencing procyclical net capital inflows.==== In the Eurozone, we also observe a clear relationship between the volatility of consumption and the cyclicality of net exports. Consumption is generally more volatile in economies where the trade balance to output correlation is more negative (see Fig. A.8) and this relationship is robust to the introduction of other European Union economies (see Fig. A.9).====This paper also contributes to the literature initiated by the euro area crisis. Lane (2013) documents the boom-bust cycle in capital flows in the euro area and shows that the reversal in net capital flows was very costly for the high deficit countries. Reis (2013) emphasizes the role played by frictions leading to a misallocation of resources. Gopinath et al. (2017) document a significant loss in total factor productivity in the South of Europe and develop a small open economy model with heterogeneous firms that is able to rationalize these novel empirical findings. In their model, the decline in total factor productivity is due to a misallocation of resources that is caused by a capital wedge. Relative to this strand of the literature, we explain the deeper recession observed in the South of Europe after 2011 by an asymmetric financial shock. Our work is therefore also related to Siena (2018) who studies the effect of anticipated asymmetric financial shocks on current account balances in the Eurozone.====Gilchrist et al. (2018) study imbalances in the Eurozone in a model in which financial frictions affect the pricing behavior of firms. Kollmann et al. (2014) estimate a three country DSGE model and explain the boom-bust cycle in Spain by estimating the model with a large number of exogenous shocks. Rubio (2014) studies the implications of cross-country housing-market heterogeneity for the transmission mechanism of shocks in a monetary union. Kemal Ozhan (forthcoming) investigates the role of the financial sector in explaining boom and bust cycles in capital flows.====Another strand of the literature studies the contribution of expectations about long-run growth in driving current account dynamics (e.g. Hoffmann et al., 2019). In Siena (2018), where this question is studied in the context of a small open economy, the main finding is that anticipated reductions in international borrowing costs are important drivers of current account imbalances. In Bonam and Goy (2019), a home bias in expectations is introduced into the analysis to study imbalances in a monetary union. De Ferra (2018) studies the effect of subsidies on holdings of euro-denominated assets on current account imbalances.====Fernández-Villaverde and Ohanian (2015) argue that the stagnation observed in some European economies can be attributed to sluggish productivity growth originating from political economy distortions (see also Fernandez-Villaverde et al., 2013). Using a large panel of countries, Challe et al. (2016) document that persistent capital inflows are systematically followed by a decline in the quality of domestic institutions, and they develop a model in which government intervention plays a role in allocating resources to the private sector. Martin and Phillipon (2017) develop a model that can be used to study the nexus between fiscal policy, credit and current account dynamics and those authors conclude that stronger fiscal discipline during the boom would have made the recession less severe in some Southern European economies (see also Gourinchas et al., 2016). In House et al. (forthcoming), the positive relationship between austerity and net exports documented by the authors is reproduced in a multi-country DSGE model.====Finally, concerns that diverging economic structures could lead to asymmetries in the transmission mechanism of common shocks were documented in the early stages of the euro area's existence. In Cecchetti (1999) for instance, differences in financial structure across European economies are attributed to their dissimilar legal structure. This argument, which also draws on the work of La Porta et al., 1997, La Porta et al., 1998, is motivated by a series of empirical facts demonstrating the impact of the legal system of a country on the structure of financial intermediation. In the same vein, Cacciatore et al. (2016) focus on product and labor market deregulation and study the implications of asymmetric deregulation for the conduct of optimal monetary policy in the Eurozone",Structural asymmetries and financial imbalances in the Eurozone,https://www.sciencedirect.com/science/article/pii/S109420251830156X,27 August 2019,2019,Research Article,220.0
Gazzani Andrea,"Bank of Italy, International Directorate","Received 16 October 2017, Revised 13 August 2019, Available online 22 August 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.08.001,Cited by (5),"Employing a non-standard VAR technique, I decompose ","The housing market has recently been a powerful source of instability for many economies around the world. The most striking example is the US cycle that ignited the Great Recession. Among the potential drivers of this phenomenon, economists have explored excessive lending, global imbalances, loose monetary policy, and financial innovations.==== Case et al. (2015) point instead to the key role played by long-term expectations, providing survey evidence that these were abnormally optimistic during the recent US housing boom. Despite its intuitive appeal, this possibility has received relatively little attention due to the difficulties involved in measuring expectations and disentangling their structural drivers.====In this paper, I exploit rents to examine the role of expectations and to separate fundamental and non-fundamental shocks to house prices. Rents represent the dividend from owning a house; as such, they are the key determinant of the fundamental house price level in the long term.==== To exploit this relationship, I characterize as “news” shocks those perturbations that affect house prices and anticipate a permanent change in rents at a future date, and label “noise” shocks those that instead affect house prices but are not correlated with the dynamics of future rents. This type of news-noise business cycle framework was pioneered by Beaudry and Portier (2004).====One complication of this framework is that, since both news and noise affect expectations but do not have a contemporaneous impact on rents, current and past rents are not useful for identification. As a result, standard VAR techniques cannot be applied to this identification problem. For this reason, I adopt the methodology of Forni et al. (2017a) (FGLS henceforth; see also Lippi and Reichlin, 1994 and Forni et al., 2017b). The methodology was originally designed to identify news and noise shocks in the stock market, and it consists of two steps. First, shocks to expectations about future fundamentals are recovered using a standard VAR analysis based on the agents' information set (e.g. through short-run zero restrictions). Second, these shocks are decomposed into a news and a noise component by exploiting observations on future fundamentals that do not belong to the agents' information set. In the current application, in particular, the dynamics of future rents are exploited to determine whether a given change in house prices is related to fundamentals.====My empirical results show that news shocks explain between 60% and 80% of the variance of rents and house prices in the long run, and they are aligned with the long-term movements of GDP and stock prices (60% and 70%).==== However, noise shocks can cause significant short-term fluctuations in house prices and spillovers to residential investment and to GDP. The historical decomposition suggests that “noise bubbles”, i.e. boom-bust episodes generated by noise shocks, have been an important driver of housing cycles since the '70s. In particular, the volatility observed in the housing market during the 2000s was significantly driven by noise: non-fundamental shocks account for a deviation of house prices from their unconditional mean of 7.5% at the peak of the boom and at the trough of the bust (and reaches up to 15% in alternative VAR specifications).====These results are consistent with the excess volatility of house prices relative to fundamentals documented in Glaeser et al. (2014). They are also in line with the conclusions of Kaplan et al. (2017) who analyze the housing cycle of the 2000s by means of a fully-fledged heterogeneous agents model. Among different candidate explanations, only a surge in beliefs about future housing demand (or a fall in future expected buildable land), then reversed, explains the housing cycle of the 2000s.==== The noise shocks that I identify are consistent with this explanation as they capture changes in expectations of future rents that do not materialize. Similar findings on the crucial role played by expectations hold in the model of Greenwald (2016).====My empirical analysis can be interpreted through the lens of a standard present value pricing model: if agents receive imperfect information on future rents and are unable to fully separate news and noise in their signals, the noise components can affect their decisions, and consequently market prices, while being completely unrelated to changes in fundamentals observed at future dates.====A crucial feature of FGLS, and hence of my empirical framework, is that it does not rely on rational expectations: the only relevant assumption is that information on future rents matters for house prices. The compatibility with both rational and irrational behaviors is particularly appealing in light of the available evidence on irrational behavior in the housing market (see Shiller 2000 for a review; examples of more recent studies on the formation of house price expectations are Adam et al., 2012 that assume adaptive learning; Glaeser et al., 2014 who model extrapolative house prices formation; Gelain and Lansing, 2014 who assume near rationality).====Shedding light on the drivers of the housing market is important because housing plays a key role in the economy. First, housing is the main asset of households and changes in housing wealth have much stronger wealth effect than other assets, e.g. equity (Case et al., 2005, Case et al., 2012; Berger et al., 2017). Second, private residential investment accounts for 4.6% of GDP.==== Third, boom-bust episodes in the housing market can endanger financial stability and macroeconomic growth (Crowe et al., 2013; Dell'Ariccia et al., 2016). Jorda et al. (2015a, 2015b) find that housing cycles predict financial crises in a wide range of countries and periods, and that leveraged housing bubbles are particularly harmful to the economy.====The remainder of the paper is organized as follows. Section 2 presents a simple present value model of house prices, which can be used to interpret the empirical analysis, and illustrates the implications for VARs of noisy signals. Section 3 describes the data employed in the analysis and the identification strategy. Section 4 illustrates the empirical results. Section 5 reports the sensitivity analysis. Finally, Section 6 concludes.",News and noise bubbles in the housing market,https://www.sciencedirect.com/science/article/pii/S1094202518302916,22 August 2019,2019,Research Article,221.0
Smith Eric,"University of Essex, UK","Received 20 July 2017, Revised 24 June 2019, Available online 5 August 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.07.004,Cited by (6),"This paper demonstrates the way in which stock-flow matching with endogenous seller entry generates hot and cold spells in house sales. Potential sellers know the number of bidders remaining from the last house sale. If two or more bidders remain, the seller obtains the gains to trade through competitive bidding. The market is active. With one monopolistic bidder, the buyer captures the surplus and sellers become unwilling to enter. The market remains inactive until sellers think enough time has passed for buyer entry to have replenished the market and make entry profitable. The resulting pattern of trade matches up with observations from Dane County, Wisconsin.","Housing markets, like labor and other markets with trading frictions, often appear to experience prolonged spells of high and low turnover. Although details vary across markets and over time, the general finding is that during high volume periods, prices are high and the time to trade short. When turnover becomes slack, prices are low, if exchange occurs at all, and traders spend a seemingly long time on the market. As a result, prices and sales become variable, contemporaneously correlated, and persistent.====This paper demonstrates the way in which such distinct hot and cold trading episodes can arise given a stock-flow matching process. The paper characterizes the way in which these cycles vary with trading conditions and then investigates the impact these fluctuating spells can have on observed prices, sales and time to sale. The paper assesses this impact with comparisons to observed measures in the residential property market in Madison, Wisconsin.====Stock-flow matching (see Taylor, 1995, Coles and Smith, 1998, Coles and Muthoo, 1998, Lagos, 2000) assumes that buyers and sellers do not search randomly. Instead, market participants have a good idea about where to look for suitable partners. They check public and private intermediaries such as real estate agencies and websites. Although these trading platforms provide information on a wide variety of opportunities, traders look for very specific characteristics. Stock-flow buyers and sellers trade in precise, distinct markets, differentiated by location and other features in which there are no trading frictions. In the housing market context, multiple markets exist in a local geographic region. Within each small market, buyers look for a combination of rooms, acreage, amenities and so on.====As buyers randomly come and go in each particular market, the populations fluctuate so that traders can be on either the long or the short side of their precise market. If lucky, an entrant is on the short side and finds one or more options immediately available. Trade occurs straightaway. If the entrant is unlucky and on the long side, there are no potential partners immediately at hand. In the event that no partners currently exist, the entrant becomes a part of the stock or queue of traders on their side of the market and must wait to match from the flow of new entrants on the other side.====The innovation introduced into the model here is to allow endogenous seller entry. To maintain a well behaved market over time, the stock-flow literature typically assumes that buyers and sellers independently enter the market one by one at the same exogenous Poisson rate. In this paper, sellers have a higher arrival rate than buyers but they have the option to decline the opportunity to enter the market and save the associated sunk cost of participation.====To illustrate, consider a housing market in which buyers bid for available homes in a public, complete information auction. Suppose a potential house seller contemplating putting a home on the market is “relaxed.” For example, some home owners have the option to wait and consider moving later. If the seller knows that two or more bidders are willing to make offers, it will pay the up-front cost of moving or building the house knowing that Bertrand bidding from the buyers will result in the seller obtaining most of the gains to trade. On the other hand, with one or zero bidders present, the seller will face a monopolistic buyer (either immediately or in the future) who captures the majority of the gains to trade. Entry does not occur in this case as the seller does not recover its up-front costs.====Sellers know the number of bidders, to some extent, from the previous auction. If there were ==== bidders in the last auction, there must be at least ==== for the next. As such, sellers will enter until only one known bidder remains. The market then goes quiet and house sellers forgo the opportunity to enter the market. As time passes, buyer entry will gradually replenish the market. Assuming buyer entry is not revealed until the next auction is held, the market remains dormant until sellers think enough time has passed to make it profitable to enter the market. When the market re-activates, if new buyer entry did not occur, the lone bidder left at the last auction pays a low price and the market becomes dormant yet again, even more so as replenishing now requires not one but two new buyers. If buyer entry occurred, the bidders offer high prices and entry remains active until the queue of buyers dwindles down again.====This pattern of trade is inefficient. When entry gets turned off with one bidder known to be waiting, gains to trade are passed over. A bidder exists but prospective sellers do not respond. In the housing market, a seller first pays a sunk entry cost - the up-front cost of moving, preparing, or building the home. If entry were to occur, the monopolistic bidder would not compensate the seller for this sunk cost. Due to this hold-up problem, the outcome during this dormant period is therefore inefficient.====This pattern of trade (and its associated inefficiency) also varies with the cost of entry and with the desire to own a home. The duration of cold spells increases and a buyer's welfare during these cold spells falls as entry costs rise. On the other hand, as home ownership becomes more desired, cold spells shorten resulting in higher welfare when such cold spells occur.====Simulations reveal that the model can generate cold spells that substantially alter standard measures of the housing market performance. With forgone entry lasting as long as three months, the model performs well when viewed along side the housing market of Dane County, Wisconsin home of the city of Madison. The model without these spells of withheld entry does not perform nearly as well. The implications of hot and cold spells - persistence in price, variation in sales, and correlation of price with sales - are prominent in simulations of the model. Simulated outcomes also exhibit high variation in time on the market, correlation of time on the market with price and correlation with price and lagged inventory. These outcomes are either much smaller or not present in the model with fixed exogenous entry and hence without cold spells.====There is also a more direct link between the model and the data when the observations are decomposed into finer segments. In Madison, small, distinct housing markets exhibit frequent spells with zero sales and with a high number of sales. Models without explicit fluctuations are inconsistent with this finding. On the other hand, simulations of this model with hot and cold spells replicate this finding.====The rest of the paper is as follows. The next section reviews the literature which provides context for the model and the results. The third section presents a baseline version of the stock-flow matching model and specifies the nature of hot and cold trading spells. The following section defines and establishes existence (as well as uniqueness) of an equilibrium, explores its welfare properties, and reports comparative statics. Section 5 examines the implications of the underlying trading assumptions. Section 6 then presents simulated market outcomes for an enhanced version of the model. These outcomes along with similar measures from the model without an entry decision (and hence without hot and cold spells) are then compared with observations from Dane County. Section 7 concludes.",High and low activity spells in housing markets,https://www.sciencedirect.com/science/article/pii/S1094202518302783,5 August 2019,2019,Research Article,222.0
Lepetit Antoine,"Board of Governors of the Federal Reserve System, Division of Research and Statistics, 20th St. and Constitution Ave., NW, Washington, DC 20551, United States of America","Received 19 March 2018, Revised 15 July 2019, Available online 1 August 2019, Version of Record 24 March 2020.",https://doi.org/10.1016/j.red.2019.07.005,Cited by (4),I show that a quantitatively significant trade-off between ,"How much weight should policymakers place on inflation, and how much on employment? In practice most central banks seem to assign a non-negligible role to the stabilization of real activity. Most notably, in the United States, the Federal Reserve pursues the dual objective of promoting price stability and maximum sustainable employment. This behavior of central banks is at odds with the recommendations that have emerged from a literature that seeks to describe optimal monetary policy in dynamic stochastic general equilibrium models featuring nominal and real rigidities. These studies generally find that an exclusive focus on inflation stabilization is close to optimal (Walsh, 2014).====In models incorporating search and matching frictions in the labor market, wage distortions create a trade-off between stabilizing inflation and addressing labor market inefficiencies (Faia, 2009), but this trade-off was found to be quantitatively small in calibrated models (Faia, 2008, Faia, 2009; Thomas, 2008; Ravenna and Walsh, 2011, Ravenna and Walsh, 2012). I revisit this conclusion by showing that a significant trade-off between inflation and unemployment stabilization arises when both steady-state distortions and the elasticity of labor market tightness with respect to labor productivity are large. The first condition implies that linear terms, notably the average unemployment level, show up in second-order approximations to the utility function and matter quantitatively for welfare outcomes. The second condition obtains when the fraction of a job's output that the invisible hand can allocate to vacancy creation—what Ljungqvist and Sargent (2017) call the fundamental surplus—is small, and implies that labor market fluctuations are large. Because of the unemployment asymmetries built in the search and matching framework, it also entails that average unemployment is substantially higher in an economy with business cycles than in steady state. In this environment, the central bank has an incentive to tolerate some inflation volatility in response to shocks since doing so reduces unemployment volatility and average unemployment. The bulk of the welfare gains obtained by the optimal policy derive from this effect on the average level of economic activity.====I use a standard New Keynesian model with search and matching frictions in the labor market. In this setting, inflation volatility is costly as producers face quadratic price adjustment costs. This gives rise to a Phillips curve that relates firms' markups to inflation and gives monetary policy some leverage over job creation. Moreover, positive shocks reduce unemployment less than negative shocks increase it. In the model, fluctuations in technology lead to about symmetric shifts in the job-finding probability. However, because of the negative covariance between the job-finding probability and the unemployment rate, a notable feature of U.S. data, these fluctuations in the job-finding probability have an asymmetric effect on employment (Hairault et al., 2010, Jung and Kuester, 2011). In an expansion, the positive impact on employment of an increase in the job-finding probability is ==== by the decrease in the size of the pool of job seekers. In a recession, the negative impact on employment of the decrease in the job-finding probability is ==== by the increase in the size of the pool of job seekers. This asymmetric nature of unemployment fluctuations implies that aggregate fluctuations lead to a potentially costly increase in average unemployment. In this setting, the central bank may use inflation over the business cycle to influence markups, with the goal of affecting job creation.====The adoption of different monetary policy rules leads to different outcomes in terms of average unemployment. In the baseline calibrated version of the model, average unemployment is higher than steady-state unemployment by 0.21 percentage points when the monetary authority responds to both inflation and output. However, under a policy of price stability, this gap doubles to 0.42 percentage points. More generally, holding the response to output constant, average unemployment is increasing in the central bank's response to inflation. The intuition for this result is as follows. When responding mildly to inflation and/or strongly to output, the monetary authority engineers procyclical markups in response to technology shocks. This behavior of markups limits the procyclicality of firms' real revenues and the volatility of job creation. Under a policy of price stability, markups are constant over the business cycle and real revenues and job creation are accordingly more volatile. This larger volatility of job creation translates in larger unemployment fluctuations, and because the latter are asymmetric, in higher average unemployment. Thus, the model features a long-run relationship between inflation volatility and average unemployment—the central bank can reduce average unemployment by tolerating some inflation volatility.====In order to understand under which conditions the central bank should actually do so, I compute the Ramsey optimal monetary policy and decompose welfare outcomes as a function of the first and second-order moments of inflation and several labor market variables. When steady-state distortions are large and the fundamental surplus is small, business cycle costs under a policy of price stability are substantial and mostly accounted for by a term involving the covariance between the unemployment rate and the job-finding probability. The small fundamental surplus implies that this covariance is sharply negative, and the large steady-state distortions entail that it receives a large weight in welfare. By deviating from price stability, the Ramsey policy can stabilize labor market fluctuations and reduce the covariance between the unemployment rate and the job-finding probability (in absolute value). In doing so, it pushes average unemployment down, which leads to sizeable welfare gains. When one of two above-mentioned conditions fails to hold—that is, when steady-state distortions are small, or when the fundamental surplus is large—business cycle costs under a policy of price stability are reduced. As steady-state distortions become smaller, the weight attached to the covariance between unemployment and the job-finding probability tends towards zero—unemployment is still significantly higher than in steady state but this has a limited impact on welfare outcomes. As the fundamental surplus grows, labor market volatility declines and the covariance between unemployment and the job-finding probability is reduced (in absolute value)—average unemployment is not significantly different from steady-state unemployment. In both cases, the welfare gains that can be achieved by stabilizing labor market activity are limited and the Ramsey policy focuses instead on stabilizing inflation.====  Several papers have showed that the asymmetric unemployment dynamics generated by a simple search and matching model of the labor market can lead to substantial business cycle costs (Hairault et al. (2010), Jung and Kuester (2011) and Petrosky-Nadeau and Zhang (2013)). Benigno et al. (2015) show that the relationship between macroeconomic volatility and average unemployment implied by such a model can be found in U.S. data. Ferraro (2018) also documents that the employment rate fluctuates asymmetrically over the business cycle in the U.S. and proposes an alternative explanation, based on endogenous job destruction and worker heterogeneity in skills. I build on these studies and draw the monetary policy implications of the presence of this asymmetry in unemployment fluctuations. My results contribute to a large literature on the optimal design of monetary policy. The conclusion that monetary policy should focus exclusively on stabilizing inflation is robust in the models generally used for monetary policy analysis, regardless of the different frictions that are included (Walsh, 2014). A large literature has focused on the specific case of labor market frictions. Faia, 2008, Faia, 2009 shows that a trade-off between inflation and unemployment stabilization arises in the presence of search and matching frictions as a central bank can use inflation to correct for an inefficient level of labor market activity. However, just like Thomas (2008) and Ravenna and Walsh (2011), she finds that the gains of adopting this policy rather than a policy of price stability are small.==== Ravenna and Walsh (2012) also find that the incentives to deviate from price stability are small, unless wages are perfectly rigid and set at an inefficient level. Compared with this literature, the contribution of this paper is twofold. First, I provide general conditions under which a significant trade-off between inflation and unemployment stabilization arises. In particular, following Ljungqvist and Sargent (2017), I emphasize that wage rigidity is neither necessary nor sufficient to deliver a large elasticity of labor market tightness with respect to productivity. Such a large elasticity obtains when wages are flexible if the fundamental surplus is small. Conversely, wage rigidity on its own will not be able to generate large labor market fluctuations if the fundamental surplus is large. This implies that a small fundamental surplus—not wage rigidity—is key to obtaining a significant trade-off between inflation and unemployment stabilization. Given these conditions, one can understand why the literature finds that policies of price stability are nearly optimal. In Thomas (2008) and Ravenna and Walsh (2011), the steady state is efficient. In Faia, 2008, Faia, 2009, the elasticity of labor market tightness with respect to productivity is small, although wages exhibit some rigidity in Faia (2008). In three of the four cases considered by Ravenna and Walsh (2012), the steady state of the economy is efficient and/or the elasticity of labor market tightness with respect to productivity is small. In the fourth case, these authors identify a particular situation in which a significant trade-off arises—the steady state is inefficient and both their calibration and the introduction of perfectly fixed wages result in a small fundamental surplus.==== Second, I outline the mechanism through which such a trade-off arises. Because of the presence of labor market asymmetries, business cycles drive a wedge between average unemployment and steady-state unemployment. When this wedge is large and costly from a welfare perspective, monetary policy should deviate from price stability to stabilize labor market fluctuations and bring about a reduction in average unemployment. Finally, this paper is related to a literature that discusses how to properly evaluate the linear terms appearing in second-order approximations to welfare when steady-state distortions are large (Schmitt-Grohé and Uribe, 2004, Benigno and Woodford, 2005, Kim et al., 2008). It provides an example of a model economy in which moving from an efficient steady state to a distorted one drastically changes policy recommendations.====  Section 2 develops the model. Section 3 undertakes a comparative statics exercise to understand the origin of the asymmetry in unemployment fluctuations. Section 4 calibrates the model and studies the inflation volatility and average unemployment outcomes under alternative monetary policy rules. Section 5 examines under which conditions the Ramsey optimal monetary policy should deviate from price stability. Section 6 proposes two robustness exercises involving: i) an alternative wage-setting mechanism; ii) the introduction of alternative shocks. Section 7 concludes.",Asymmetric unemployment fluctuations and monetary policy trade-offs,https://www.sciencedirect.com/science/article/pii/S1094202518301145,1 August 2019,2019,Research Article,223.0
"Battisti Michele,Belloc Filippo,Del Gatto Massimo","University of Palermo, CeLEG LUISS Guido Carli and RCEA, Italy,University of Siena , Italy,“G.d'Annunzio” University, LUISS Guido Carli and CRENoS, Italy","Received 11 October 2017, Revised 20 May 2019, Available online 17 July 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.07.003,Cited by (6),"We investigate the technological dimension of productivity, presenting an empirical methodology based on mixture models to disentangle the labor productivity differences associated with the firm's choice of technology (BTFP) and those related to the firm's ability to exploit the adopted technology (WTFP). The estimation endogenously determines the number of technologies (in the sector) and degree of technology sharing across firms (i.e., for each firm, the probability of using a given technology). By using comparable data for about 35,000 firms worldwide distributed across 22 (two-digit) sectors, we show BTFP to be at least as important as WTFP in explaining the labor productivity gaps across firms. Intra-sectoral and inter-sectoral heterogeneity is substantial and, even in sectors in which BTFP dominates on average, we find a considerable number of firms for which labor productivity is mostly determined by the ability to use the adopted technology. Hence, dissecting the labor productivity gaps is crucial to achieving more targeted innovation policies. The estimated number of technologies ranges from one (in only three industries) to five, being three in most cases. The suggested estimation strategy takes simultaneity into account. The BTFP measure is unaffected by omitted price bias. The presence of BTFP dispersion can be associated with the action of frictions preventing firms from switching to superior and more productive technologies. Eliminating BTFP does not eliminate misallocation.","Labor productivity in OECD countries has grown by 3.7% since 1990 compared with 28.4% in emerging economies (OECD, 2015). Against the 7.5% growth in the United States, the usual productivity benchmark, the rate peaks at 63.8% in China and 38.4% in India. These trends have recently sparked widespread interest in quantifying productivity and analyzing its determinants.====Measured as the estimated residual of a sector-specific production function, total factor productivity (TFP) is usually found to be as important as capital accumulation in explaining the process of labor productivity growth (Kumar and Russell, 2002, Caselli, 2005; Hsieh and Klenow, 2009, Hsieh and Klenow, 2010; Battisti et al., 2018). However, under its conventional representation, TFP is a wide notion encompassing several hard-to-measure factors (i.e., “all the rest”, respect to capital accumulation). As a result, empirical economic analysis explains no more than 50% of the productivity differentials. This fact notwithstanding, and despite the potential impact in terms of policy implications, the literature dealing with the different theoretical conceptions of TFP is unsatisfactory and it is still true that “economists should devote more effort toward modeling and quantifying WTFP” (Easterly and Levine, 2001).====While these considerations apply to both aggregate and firm-level studies, the firm-level perspective has recently come to the fore, with a growing interest (see Gabler and Poschke, 2013, Da Rocha et al., 2017, and, for an early review, Grossman and Helpman, 2015) in the link between the evolution of aggregate productivity and cross-firm technological heterogeneity, as induced by the behavior of new entrants (Sampson, 2016), technology diffusion and imitation (Luttmer, 2007, Perla and Tonetti, 2014, Perla et al., 2015), innovation (Benhabib et al., 2017), and international trade and competition (Bloom et al., 2016).====The importance of dealing with the link between technology and productivity from a firm-level perspective has also entered the policy debate. However, the chance to translate the empirical results into effective policy prescriptions is hampered by the fact that the terms “productivity”, “TFP”, “labor productivity”, “innovation”, and “technology” are often used interchangeably. As a matter of fact, the TFP and/or labor productivity differences across firms are commonly interpreted in terms of technological differentials, but without a clear interpretation.====For instance, OECD (2015) documents the rising (labor) productivity gap between firms at the global (labor) productivity frontier and other firms. According to the report, this raises questions about the obstacles that prevent laggard firms from adopting seemingly well-known frontier technologies.====The reason for this ambiguity is computational. Abstracting from the functional form (e.g., Cobb–Douglas versus translog), the technological dimension of firms' productivity should arguably reflect the estimated production function coefficients (i.e., intercept and input shares). This is a dimension that cannot be extrapolated from the data without clustering firms ex-ante according to the technology they adopt. For example, by referencing the steel industry, Collard-Wexler and De Loecker (2014) study the firm-level and industry-level productivity increases associated with the adoption of a particular technology (the “minimill” technology). Instead, since technologies are not observable in general, the econometrician can only estimate sector-level production function parameters. Under this standard approach, cross-firm differences in output not associated with different input choices flow entirely into the residual TFP term (i.e., the “Solow residual” measured as the difference between actual and predicted output).====In this study, we aim to shed light on the magnitude of the technological dimension of productivity in a world in which different production functions identify different technologies (by differing in terms of intercept and shape parameters). Our empirical strategy allows several technologies to be available in the same sector, with firms potentially adopting any of them.==== The estimation is left free to determine both the number of technologies and the degree of technology sharing across firms (i.e., for each firm, the probability of using a given technology). This approach allows us to disentangle the labor productivity differences associated with the firm's choice to adopt a certain technology, referred to herein as between-technology TFP (BTFP) and the labor productivity differences related to the firm's ability to exploit the adopted technology, referred to as within-technology TFP (WTFP). By applying our methodology to a sample of about 35,000 firms distributed across 22 (two-digit) manufacturing sectors and 76 countries, we find that the technological component is at least as important as the idiosyncratic component in explaining productivity differences. On average, the WTFP productivity of the top 5% of firms in a sector is around 31% higher than that of other firms, while the labor productivity gap between the firms using the frontier technology and those using less productive technologies amounts to around 34%. Within this general result, we document substantial intra-sectoral and inter-sectoral heterogeneity in terms of both measures, particularly when we look at their relative weight. Notably, the estimated number of technologies range from one (in a few industries) to five, being three in most cases. Hence, the hypothesis of a single technology, which is the standard implicit hypothesis in TFP estimation, is likely to hide substantial heterogeneity. In turn, this may suggest that dissecting labor productivity gaps is crucial to achieving more targeted innovation policy.====From a methodological point of view, operationalizing this multiple-technology framework entails tackling several issues.====First, as stated, we need to estimate the technology-specific production function parameters without clustering firms ex-ante. To this end, we rely on a ==== (McLachlan and Peel, 2000). This approach treats the probability distribution of labor productivity as the result of the potential overlapping of several distributions, each reflecting a different technology. Adopting this approach allows us to identify the available technologies by estimating the different production functions characterized by different (cluster-specific) estimated parameters. The estimation provides us, for each sector, with the number of available technologies (i.e., clusters) and, for each firm, with the probability of belonging to each technology cluster.====Second, as a related issue, we have to square things up with the chance to potentially capture as many as possible of the worldwide available technologies. This requires using internationally comparable data with the largest possible coverage, although the country representativeness of firm-level databases tends to shrink with the number of countries included. To deal with this issue, we take advantage of the balance sheet data provided by Bureau van Dijk (2018), which show high information comparability for a large sample of worldwide distributed firms.====Third, as emphasized by the literature (Marschak and Andrews, 1944, Klette and Griliches, 1996), TFP estimation at the firm level incurs simultaneity problems as far as TFP is known to the firm at the stage of the input-level decision, which biases the OLS estimates. A well-known solution relies on the identification of an observable variable reacting to the variations in TFP perceived by the firm. To the extent that the relationship with TFP can be inverted, this proxy variable can be used to estimate the production function semi-parametrically.==== Our multiple-technology framework amplifies this issue by bringing about additional potential simultaneity associated with the choice of technology. To deal with such simultaneity on both counts, we develop an “empirical model” of technology adoption similar in spirit to the semi-parametric approach, but that does not rely on proxy variables. This choice allows us to circumvent the problems related to inverting the relationship between the proxy variable and the idiosyncratic productivity term recently highlighted by Gandhi et al. (2016).====Fourth, another source of bias in TFP estimates is the price dispersion across firms. Indeed, the production function should be estimated in physical terms, that is using firms' output. However, as these data are rarely available in firm-level datasets, it is common practice to base the estimation on firms' sales deflated by an industry-wide price index (individual prices are themselves usually unavailable). In this case, TFP is likely to be misstated to the extent firms apply different markups. Notably, being computed as the difference between the predicted values under different sets of technology-specific production function parameters, our BTFP measure is unaffected by firm-level differences in prices and markups, as long as they are not correlated with the technological clustering.====Finally, our analysis intersects with the recent literature emphasizing the concept of “misallocation” (Alfaro et al., 2008; Banerjee and Duflo, 2005; Bartelsman et al., 2013; Hsieh and Klenow, 2009, Hsieh and Klenow, 2010; Restuccia and Rogerson, 2008, Restuccia and Rogerson, 2013): when market imperfections hamper the flow of factors from less productive to more productive units, they result in lower aggregate TFP than the ideal situation of frictionless factor markets. Our empirical strategy adds to this literature by introducing an additional misallocation mechanism. Indeed, technology dispersion can be seen as one of the possible consequences of the presence of distortions preventing firms from switching to superior and more productive technologies. Two points are noteworthy. First, allowing all firms to use the frontier technology would not eliminate misallocation as long as they are not free to hire the desired amount of capital and labor. Second, in the presence of technology dispersion, it is no longer possible to trace back the whole dispersion of revenue TFP to the presence of misallocation, as suggested by Hsieh and Klenow (2009) and subsequent studies aimed at quantifying misallocation.====The remainder of the paper proceeds as follows. In Section 2, we introduce the basic intuitions behind our notions of the within-technology and between-technology components of TFP. In Section 3, we model the mechanism of technology adoption. In Section 4, we present our dataset and the variables we use in the empirical sections. In Section 5, we describe the mixture model used to estimate the production functions within technology clusters. In Section 6, we present our results and, in Section 7, we analyze possible country patterns and correlations. Finally, Section 8 concludes.",Labor productivity and firm-level TFP with technology-specific production functions,https://www.sciencedirect.com/science/article/pii/S1094202518302898,17 July 2019,2019,Research Article,224.0
Ambrocio Gene,"Bank of Finland, Snellmaninaukio, P.O. Box 160, 00101 Helsinki, Finland","Received 12 July 2016, Revised 17 June 2019, Available online 16 July 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.07.002,Cited by (2),"I provide a theory of information production and learning that can help account for both the excessive optimism that fueled booms preceding crises and the slow recoveries that followed. In my theory, persistence and the size of expectation errors depend on information production about changes in aggregate fundamentals. In turn information production, via credit screening, tends to fall during both very good and very bad times. The former gives rise to episodes of rational exuberance in which optimistic beliefs may sustain booms even as fundamentals decline. I also document evidence from survey forecasts consistent with the model predictions.","Business cycles are typically asymmetric. Although recessions are sharp and short, the recovery process tends to be prolonged and gradual. One theory to account for this is that the speed of learning co-varies with economic activity.==== Expansions are slow to start because they do so from periods of low economic activity which reveal little about the state of the economy. The opposite is true at the height of expansions, when production and investment are high and learning is quick. Although very insightful, this theory has an important limitation. In many instances, expansions that end in crises seem ==== to have been fueled by excessive optimism which sustained economic activity.==== This is of course inconsistent with the theory of learning mentioned above. After all, should not the high levels of investment and production at the height of an expansion produce a very precise signal of any changes in the underlying economic fundamentals? In this paper, I develop a model of information production to show why this may not be the case.====The main feature of my model is that the precision of information about the state of the economy depends on two channels. Agents learn about the state of the economy by observing market outcomes. On the one hand, the informativeness of market outcomes increases in the level of economic activity, a ==== dimension. This is the channel emphasized in the literature. On the other hand, agents in my model can also actively invest in producing information which also improves the informativeness of market outcomes – a ==== dimension. The key result is that this second dimension to information production may decline when the economy is doing well and beliefs are highly optimistic. This is because there is little reason to invest in more information if agents already expect to do well. Therefore, a central prediction of my model is that information produced on the underlying state of the economy may decline as the economy enters a large boom with highly optimistic agents.====Following the literature on imperfect information as an important driver of cycles, I embed these mechanisms in a model where agents use market outcomes to learn about an unobserved aggregate fundamental driving the economy.==== On average, the model generates features similar to those without endogenous learning. However, at the extremes of the cycle, the learning mechanism in the model generates additional persistence. As in previous models of social learning over the business cycle such as Veldkamp (2005); Van Nieuwerburgh and Veldkamp (2006); Ordonez (2013); Fajgelbaum et al. (2017); Dow et al. (2017), and Straub and Ulbricht (2017), a fall in information production from both the quantity and quality channels generates slow recoveries. Pessimistic recessions last too long in cases when fundamentals rapidly improve.====Unlike these models, the inclusion of the quality dimension in my model also allows for information production to fall during highly optimistic booms – what I refer to as rational exuberance booms. Similar to belief traps in Fajgelbaum et al. (2017) and Straub and Ulbricht (2017) which occur during recessions, these rational exuberance booms feature persistently optimistic beliefs. This is because the quality dimension generates a positive feedback between the persistence of highly optimistic beliefs and low information production. Imprecise signals from low information production make agents less likely to perceive a rapid fall in fundamentals and they remain optimistic or rationally exuberant, prolonging the boom.==== In turn, optimism reinforces their decision not to produce more information.====The quality dimension to information production draws on the literature which explain financial boom-bust episodes through informational cascades and herding mechanisms.==== In contrast to these and other models which generate over-optimism through adaptive learning (Bullard et al., 2010) or behavioral biases (Gennaioli et al., 2013, Gennaioli et al., 2015; Thakor, 2015), agents in my model rationally discount warning signs of impending crises during a boom because they are endogenously imprecise. My model makes use of credit markets and screening of borrowers as the vehicle to demonstrate the quality dimension to information production. This is motivated by prior literature on counter-cyclical credit screening over the business cycle (Ruckes, 2004). The particular screening mechanism I employ in the model is adapted from learning about collateral in Gorton and Ordonez (2014). My mechanism differs in that I focus on producing information about the productivity of projects themselves and endogenize the formation of beliefs about the aggregate fundamental. Finally, I abstract from externalities due to imperfect competition in the banking sector and information asymmetries between borrowers and lenders. As such, there are no strategic motives behind screening choice in my model and precludes multiple equilibria.====I also provide evidence which confirms the plausibility of the mechanisms embedded in the model. Direct measures of information precision are hard to come by. Instead, one can observe how agents make forecasts about macroeconomic aggregates and how they revise them. When individual forecasters receive new but noisy information, they can only partially incorporate this information when they revise forecasts as they are unable to distinguish between the news and noise components. However, as econometricians, we can average these forecasters' forecast revisions and forecast errors to filter out the noise component. Then, the correlation between the averaged forecast error and forecast revision would be indicative of the relative fraction of news that was not incorporated in the forecast revisions (and thus made its way into forecast errors). This fraction would be proportional to the relative degree of noisiness in forecasters' information sets. In the absence of noise, forecasters fully incorporate new information into their revised forecast and the average forecast error would be uncorrelated with the average forecast revision. Conversely, if information is very noisy and forecasters only partially incorporate news into their revisions, then we would observe a positive correlation. The larger the noise in new information, the stronger the correlation. This is the strategy employed in Coibion and Gorodnichenko (2015).====The model predicts that the precision of information (degree of information rigidity) available to agents is relatively low (high) in periods of both high optimism and pessimism. I extend the empirical strategy of Coibion and Gorodnichenko (2015) to test this prediction and show that the estimated degree of information rigidities are indeed highest both when prior beliefs are either relatively optimistic and pessimistic.====I use this evidence to calibrate the model and show that endogenous learning mechanisms presented in the model are quantitatively relevant. In particular I show that both the quantity and quality dimensions to information production are necessary to generate skewness in consumption and output that is qualitatively similar to that in the data. Further, I show that the distortions due to imperfect information and endogenous information choices generated by the model are concentrated in the (rational exuberance) boom phase of the cycle when beliefs are highly optimistic.====The model illustrates a new mechanism relating externalities that arise from information rigidities to economic cycles. Individual choices to produce information today do not take into account the consequent improvement in the accuracy of information produced about aggregate fundamentals and the resulting improvement in the efficiency of tomorrow's credit and investment allocations. Agents in the next period are ==== better off if the current period's market outcomes contained more precise information.==== In the model, this externality is most severe at the peak of booms.====The model suggests that policies which subsidize private credit during deep recessions can also help speed up the recovery process through a learning mechanism. On the other hand, booms are periods which can benefit from policies that induce financial intermediaries to act more prudently and produce more private information. However, policies which curtail credit in such booms may end up further reducing the informativeness of market outcomes unless complemented by policies that incentivize information production. Finally, the model cautions against excessive reliance on real-time market-based indicators whose informativeness may decline in rational exuberance booms.====The remainder of the paper is organized as follows. The next section develops a one period version of the model and Section 3 establishes dynamic implications. Section 4 provides supporting evidence which is then used to calibrate the model in a quantitative exercise. Finally Section 5 concludes.",Rational exuberance booms,https://www.sciencedirect.com/science/article/pii/S1094202518301856,16 July 2019,2019,Research Article,225.0
"Cacciatore Matteo,Fiori Giuseppe,Traum Nora","HEC Montréal, Institute of Applied Economics, 3000, chemin de la Côte-Sainte-Catherine, Montréal (Québec), Canada,Bank of Canada, 234 Wellington St, Ottawa, ON K1A 0G9, Canada,North Carolina State University, Department of Economics, 2801 Founders Drive, 4150 Nelson Hall, Box 8110, 27695-8110 Raleigh, NC, USA","Received 9 November 2017, Revised 3 July 2019, Available online 10 July 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.07.001,Cited by (7),We conduct ,"In many OECD countries, changes in average hours per worker are about as large as changes in employment (Ohanian and Raffo, 2012). For instance, in the U.S. economy, the volatility of the intensive margin (average hours per worker) accounts for approximately one-third of the unconditional variability of aggregate hours, while the remainder is attributed to movements in and out of employment (the extensive margin). Moreover, as documented in the next section of the paper, the two margins covary either positively or negatively in specific business cycle episodes, and their relative contribution to aggregate fluctuations is time-varying. What accounts for fluctuations in the labor margins, their comovement with macroeconomic variables, and their contribution to aggregate dynamics? Answering these questions is paramount for both positive and normative analysis, as the economy's response to aggregate shocks and macroeconomic policies depends on what drives the labor margins over time.====This paper takes up the challenge of accounting for and explaining the cyclical properties of hours per worker and employment in a quantitative search-and-matching model—the benchmark theory of equilibrium unemployment and the cornerstone of labor-market policy analysis. Our contribution to the literature is twofold. First, using Bayesian methods, we determine under which conditions the model can account jointly for the relation of hours per worker with employment and their comovement with macroeconomic data. Second, we use the estimated model to provide a structural assessment of the intensive margin's contribution to aggregate fluctuations, shedding new light on the sources of labor market dynamics.====Our baseline specification for the labor market features search-and-matching frictions (Diamond, 1982a, Diamond, 1982b, Mortensen and Pissarides, 1994, and Pissarides, 2000) and adjustment along the intensive margin, as in Andolfatto (1996), Arseneau and Chugh (2008), Christiano et al. (2011), Cooley and Quadrini (1999), Ravenna and Walsh (2012), and Trigari (2009), to name a few. Firms adjust labor inputs either by posting vacancies or changing the number of hours per worker. Households face a standard neoclassical decision, which also is consistent with the business cycle literature. In equilibrium, hours per worker equate the marginal rate of substitution between hours and consumption to the value of the marginal product of hours, i.e., hours maximize the joint surplus of a match. The widespread preference for this setup in the literature stems from its invariance to the Barro (1977) critique, since in this environment wages do not have a direct impact on on-going worker-employer relations (and thus on the adjustment of hours per worker).==== To successfully account for key properties of macroeconomic data, we embed this labor market structure in a state-of-the-art, quantitative business cycle model, following Christiano et al. (2005), Smets and Wouters (2007), and Justiniano et al. (2010). The model features standard ingredients from this literature, including habit formation, investment adjustment costs, variable capital utilization, and nominal rigidities.====In addition, we include standard demand and supply shocks, namely exogenous fluctuations in total factor productivity, household preferences, the relative price of investment, firms' desired mark-ups, and monetary and fiscal policy. We also include two standard labor market shocks—variability in the workers' bargaining power and preference for leisure, which affects hours' supply. This broad set of disturbances allows us to encompass most of the views on the sources of business cycles found in the literature, giving disturbances other than neutral technology shocks a fair chance of accounting for labor market adjustments. In addition, it allows us to differentiate competing forces behind the time variation in the labor margins' comovement. We estimate the model using Bayesian inference on U.S. data, including standard macroeconomic observables, as well as total hours and employment.==== The full information approach allows us to assess the empirical fit of the model against a large set of macro moments, beyond pure labor market outcomes. It also permits a Bayesian counterfactual experiment that quantifies the historical contribution of the intensive margin's adjustment to aggregate fluctuations.====Our analysis yields three main results. While a Bayesian prior-predictive analysis shows that the baseline model in principle can account for individual variance shares of the labor margins, i.e., their relative variability or comovement, conditioning the model on our observables results in posterior estimates that imply counterfactual labor-market dynamics. In other words, likelihood maximization with both U.S. macroeconomic and labor data renders structural parameter estimates that cannot jointly reproduce the comovement of the labor margins with themselves and with macro data. In particular, the model cannot reproduce the positive unconditional covariance between employment and hours per worker, and it generates counterfactual volatilities for both labor margins. The model also cannot account for the covariance between both labor margins and macroeconomic times series; namely it generates counterfactually low (high) comovement of hours per worker (employment) with aggregate output and consumption. The main issue is that the model implies a countercyclical behavior of hours per worker conditional on both technology and demand shocks, a result contrary to existing VAR evidence—see for instance, Ravn and Simonelli (2007). Moreover, the intensive margin's relative volatility is counterfactually high.====Our second result is to reconcile the model with the data. Two candidate explanations for the counterfactual labor market dynamics are the strength of short-run wealth effects on labor supply, which affects the response of hours per worker to TFP and demand shocks, and the asymmetric cost of adjusting the two labor margins, since posting vacancies is costly, while the adjustment in hours per worker is frictionless. For these reasons, we introduce additional flexibility in the model by including two ancillary features: non-separable preferences that feature parametrized wealth effects on labor supply as in Jaimovich and Rebelo (2009) and costly hours' adjustment.==== We re-estimate the model and find strong support in favor of weak short-run wealth effects and positive hours adjustment costs.==== Intuitively, weakening the wealth effect eliminates the negative comovement between hours per worker and employment and increases the comovement of hours per worker with both output and investment. The presence of costly hours' adjustment reduces the variability in hours per worker, the second key dimension for reproducing the cyclical behavior of both margins of labor adjustment.====Finally, we use the estimated model to examine the behavior of hours and employment in post-WWII U.S. recessions and recoveries. The estimated model offers a structural interpretation for the observed time-varying comovement between hours per worker and employment in these episodes. Hours and employment comove positively in response to demand and supply shocks, and comove negatively in response to labor-market shocks—shocks that affect the rigidity of real wages and hours supply. Moreover, a Bayesian counterfactual experiment that holds hours per worker constant shows that adjustment in the intensive margin had sizable effects on the dynamics of total hours; for instance, with the intensive margin fixed, the decline in total hours is halved in the 1982 recession. A notable component stems from the indirect effect of hours per worker on employment (beyond the direct effect of hours per worker on total hours): the intensive-margin adjustment increases employment losses during recessions and delays employment recoveries.====The intuition for these results is the following. With the intensive margin fixed, the expected present discounted value of new matches varies less in response to shocks that induce positive comovement between the margins of labor. Accordingly, firms adjust effective capital more than employment, holding all else constant. Lack of adjustment in hours per worker also implies that firms can no longer substitute across the labor margins in response to labor-market shocks, i.e., shocks that induce a negative comovement between hours and employment. As a result, both effects imply that fluctuations in hours per worker increase the response of employment to aggregate shocks.====Our structural assessment of the role of intensive-margin fluctuations in models with frictional labor markets paves the ground for the design of quantitative search-and-matching models featuring hours-per-worker adjustments for both positive and normative analysis. In addition, while we estimate the model on U.S. data, the results of the paper are broader in scope, as the inability of the baseline model to account for the margins of labor adjustment is not limited to the U.S. economy. For instance, as documented by Ohanian and Raffo (2012), hours and employment positively comove in several economies (for instance, in the U.K. and Canada). In addition, parametrized wealth effects and costly hours' adjustment introduce enough flexibility to match the broad array of empirical covariances of hours per worker and employment observed in the cross-section of countries. Discerning the role of intensive-margin adjustment for other countries is an important avenue for future research.====  This paper relates to different strands of the literature. One branch addresses the variability in the margins of total hours. Some early contributions, including Cho and Cooley (1994), Kydland and Prescott (1991), and Hansen and Sargent (1988), calibrate models in which the supply of total hours adjusts along both the intensive and extensive margins, but abstracts from search-and-matching frictions. A few more recent contributions cast the analysis within the search-and-matching model. In particular, Cooley and Quadrini (1999) build a monetary search-and-matching model with fluctuations in hours per worker to assess the model's ability to replicate correlations between employment and inflation. Cooper et al. (2007) study hours, employment, vacancies, and unemployment at the micro and macro levels. To do so, they consider a model with rich firm heterogeneity while abstracting from capital accumulation dynamics and labor supply considerations. Finally, Kudoh et al. (2019) build a search-and-matching model where firms have the right-to-manage hours that replicates the relative variability of employment and hours per worker observed in Japan. Relative to these studies, our contribution is threefold. First, using Bayesian methods, we determine under which conditions a quantitative search-and-matching model can account for the behavior of hours per worker and employment, including their comovement with macroeconomic data. Second, we do not restrict the analysis to the unconditional volatility of the margins of labor, but address both their unconditional and conditional variability. Third, we provide a structural assessment of the intensive margin's contribution to aggregate fluctuations.====As highlighted above, several contributions in the search-and-matching literature (including Fang and Rogerson, 2009, and Kudoh and Sasaki, 2011) model both labor margins, although their focus is not on the quantitative behavior of the composition of total hours. Moreover, since Shimer (2005), another branch of the literature addresses the ability of the search-and-matching model to replicate the cyclical behavior of vacancies and employment. While the debate has for the most part focused on calibrated versions of the search model, a few recent contributions examine the issue in the context of quantitative, estimated models (e.g., Christiano et al., 2016, Gertler et al., 2008, and Justiniano and Michelacci, 2012).==== In particular, Gertler et al. (2008) show a model with only the extensive margin is able to reproduce the joint dynamics of employment and macroeconomic variables. However, we show that when the intensive margin is introduced in the model, its ability to account for the correlations between labor market variables and aggregate macroeconomic series is significantly impaired.====Finally, this paper is related to the literature addressing the behavior of employment in U.S. cyclical recoveries. In particular, an active strand of research addresses the so-called “jobless recoveries” following the past three U.S. recessions (1991, 2001, and 2009), where aggregate employment continued to decline for years following the turning point in aggregate income and output.==== Our results provide additional insights to the debate by emphasizing the role of hours' adjustment for employment dynamics during these episodes.====  The rest of the paper is organized as follows. Section 2 reviews the empirical relation of U.S. hours and employment. Section 3 outlines the model. Section 4 describes the approach for inference and discusses the cyclical behavior of the margins of labor adjustment in the estimated baseline model. Section 5 studies the performance of the alternative model featuring parametrized wealth effects and hours adjustment costs. Section 6 discusses the cyclical behavior of hours per worker and employment in post-war U.S. recoveries. Section 7 concludes.",Hours and employment over the business cycle: A structural analysis,https://www.sciencedirect.com/science/article/pii/S1094202518302928,10 July 2019,2019,Research Article,226.0
"Andersen Torben M.,Bhattacharya Joydeep","Department of Economics and Business, Aarhus University, Fuglesangs Allé 4, DK-8210 Aarhus V, Denmark,CEPR, UK,CESifo, Germany,IZA, Germany,Department of Economics, Iowa State University, Ames, IA 50011-1070, USA","Received 1 March 2017, Revised 22 April 2019, Available online 18 June 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.06.002,Cited by (3),"Governments, motivated by a desire to improve upon long-run laissez faire, routinely undertake enduring, productive expenditures, say, in public education, that generate positive externalities across cohorts but require investments be made up front. If everyone after the policy is initiated is at least as happy as before and there are some outstanding resources, the Hicks-Kaldor efficiency rule suggests that the present value of these resources could, ====, be distributed to future generations creating the ==== for generational Pareto improvement. The literature recognizes the challenge in constructing a policy that is ","Left on their own, markets typically underinvest in areas such as infrastructure, health, human capital and R&D, activities that generate positive externalities across generations but require up-front investments. Governments routinely get involved in such areas motivated by a desire to improve upon laissez faire. Such intervention, though, may create intergenerational conflict since many of the benefits arrive in the future to generations that did not incur its cost in the present: there may emerge long-run winners and short-term losers.====A standard approach to checking whether a proposed policy intervention is desirable is to employ the Kaldor-Hicks criterion and verify if the policy generates an efficiency gain – a hypothetical potential for the winners to compensate the losers (and leave the latter as happy as before the policy was initiated). If everyone post policy is at least at their pre-policy utility and there are still some resources left standing, then following Auerbach and Kotlikoff (1987), the present value of these new, net resources could, in principle, be distributed in some fashion to future generations; this is how the ==== for Pareto improvement is demonstrated. An important, recent paper that takes this approach is Nishiyama and Smetters (2007) where a hypothetical LSRA (lump sum redistributive authority – Auerbach and Kotlikoff, 1987) compensates the losers from social security privatization. In their words, “the LSRA is not being proposed as an actual government institution. Instead, it is simply a hypothetical mechanism that allows us to measure the standard Hicksian efficiency gains in general equilibrium associated with privatization.” They go on to write, “[C]onstructing a policy that is ==== Pareto improving from a policy that improves Hicksian efficiency is a tougher task.” [emphasis ours] Why is it tougher? Because an actual policy, once implemented, would release its own dynamics and produce general-equilibrium gains and losses spread across generations. All such gains and losses would have to be properly accounted for if the policy is to be deemed genuinely Pareto-improving, generation by generation. It is this “tougher task” we take on.====To that end, we study a standard overlapping-generations model of a closed economy – similar in many aspects to the models in Azariadis and Drazen (1990) and Boldrin and Montes (2005) – with three generations (young, middle-aged, and old), no population growth, and endogenous physical and human capital accumulation. There is no intra-generational heterogeneity, and importantly, no altruism. Human capital accumulation is subject to a standard intergenerational externality: loosely speaking, it is easier for agents to acquire more education if their parents are already well-educated. When young, agents may borrow from perfect capital markets to pay for their educational expenses. When middle aged, they pay off these loans, work, consume, and save for old age. All markets, education-loan, labor and capital, are complete. Just to be clear, define a laissez-faire (LF) economy as the market economy, just described, with no government presence. Assume the LF economy is in a steady state and that it is dynamically efficient.====In the presence of the human capital externality, education spending in the LF is too low and a government could intervene to raise education spending levels beyond LF hoping to improve long-run welfare.==== To fix ideas, start from the LF steady state and consider an incremental but permanent increase in public education spending above its LF level.==== We work with a setting in which private and public spending on education are perfect substitutes which means public spending crowds out private spending one for one. As a result, all education spending becomes publicly financed, relieving the young of the cost of private spending. To finance public education, the government sells bonds to the current middle-aged (who, recall, were schooled under laissez faire). The resulting debt is, therefore, productive – targeted at public education – and is to be serviced at market rates. The debt must be serviced using taxes and, if necessary, new debt may be issued. But how much taxes can the government impose? For us, at all points in time, the government's ability to tax is restrained by the Pareto criterion so as to ensure all generations, including the currently-alive ones, are at least as well off after the intervention as before. Evidently, this sort of public education-debt-tax policy releases multiple general-equilibrium effects downstream.====Under dynamic efficiency, it is possible that debt explodes unless enough taxes are raised to put on the brakes. Anytime a debt is (partly) serviced by means of another debt, the issue arises, is the debt on an explosive path? As is well known, this issue does not arise if the gross return on bonds lies below unity (assuming zero population growth) thereby allowing for Ponzi debt as well as bond seigniorage (Bullard and Russell, 1999), the ability of the government to raise net revenue from the sale of bonds itself. In our setting, debt must bear the same return as capital and ==== we focus on the case where the gross return on bonds exceeds unity (dynamic efficiency), then the path of debt we construct may explode: “debt will grow so large that the government will be unable to find buyers for all of it, facing either default or a tax increase” (Elmendorf and Mankiw, 1999). Our question, then, becomes, is there a sequence of minimum-required, incremental government debt (and associated lump-sum taxes) that leave all generations as well off as under LF and ensures the path of debt is non-exploding?====What sort of welfare gains are available to be taxed? Under the policy initiated at some date ====, the current young accumulate higher levels of human capital (than they would have under LF), a possible source of direct welfare gain to them at ====, during ==== middle age. The inaugural middle-aged, schooled under LF, do not directly benefit from the policy; indirectly, they do. If, on impact, debt crowds out capital – Diamond (1965) – the wage rate falls and the rate of return rises. The former harms the middle-aged at ====, the latter benefits the old, the middle-aged at ==== who took on the debt. This rate-of-return effect helps bring down the opportunity cost of financing education to the middle-aged at ====. On net, there may not be enough welfare gains to tax (Pareto-constrained) and repay the debt entirely in one generation. New debt would have to be issued to pay for past-debt service, debt retirement, and current education spending; this process is repeated here on releasing indirect, general-equilibrium gains and losses over time. Thankfully, the education externality gains vigor as the stock of human capital rises and produces direct income gains downstream that may be taxed to support debt repayment. The challenge is to keep track of all these direct and indirect gains and losses unleashed by public education, debt and taxes – each constrained by the generation-by-generation Pareto criterion – and ensure debt does not explode.====We prove that all competitive equilibria are summarized by a two-dimensional, first-order, non-linear dynamical system in debt and capital. These are linearized near the LF steady state and the dynamics of debt is studied with special focus on the possibility of debt retirement in finite time. The main analytical finding is a strong, possibility result. Starting from a laissez-faire steady state, it is possible for the government to implement an incremental debt-financed, public education scheme that generates more human capital and higher welfare in the long run relative to laissez faire, and most importantly, along the trajectory no transitional generation is harmed relative to laissez faire; additionally, the path of debt does not explode.==== Sufficient conditions for this local result to hold are reasonable: if the LF steady state is locally stable, the policy perturbation is incremental, and public debt increases on policy impact, then debt levels will fall right away for any dynamically-efficient economy with a human capital externality, even when private and public education are perfect substitutes. Our insistence that debt levels go to zero over time has an important implication. Once the debt is entirely paid off (but welfare gains from higher human capital linger) taxes may be cut relative to their levels under the Pareto criterion, making subsequent generations strictly better off relative to LF. Put differently, we prove the existence of a Pareto-==== debt/tax trajectory; for that, it is sufficient (not necessary) that debt levels go to zero.====The findings clearly indicate the main argument developed in the paper does not require the policy transition to start from the LF steady state; indeed, it can start any point along the transition to the LF steady state, anywhere private spending on education is inefficiently low. Similarly, while the availability of lump-sum taxes make the analytics more tractable, they are not necessary for the main intuition to proceed; after all, since there is no within-generation heterogeneity, and hence, no possibility of redistribution (an issue of some significance for research in the Mirleesian tradition, such as Hosseini and Shourideh, 2019) distorting taxes add a tax rate-change in addition to the tax base-change already present with lump-sum taxes, not much else.====Finally, as discussed above, the analytics rely heavily on the fact that policy changes are incremental and can therefore be studied using linearization techniques. Clearly, the incremental approach developed in the paper is limited in what it can teach us about how to design Pareto-improving transitions to steady states that are not in a small neighborhood of the original equilibrium. Not much is known about global properties of generic, non-linear systems which is why analytical progress with discrete policy perturbations is stalled.==== In numerical simulations, we explore the possibility of engineering a discrete policy move, allowing distortionary taxes on wage income, and allowing the start date for the reform to be away from the LF steady state. To that end, we reformulate the model to allow for an additional labor-leisure choice so as to give distorting taxes more to work with. The expanded model economy in the LF steady state is loosely disciplined to replicate some features of the U.S. economy. We restrict attention to an education policy package that delivers higher education spending per pupil in perpetuity and the same utility as in the LF steady state for every current and subsequent generation. The numerical findings help reassure us that the main line of logic remains intact even when policy changes are not incremental, taxes are distortionary, and policy reform starts along the transition to the LF steady state. In the model economy, the debt to GDP ratio rises to about 1.8% on impact and then starts to fall. It takes longer for the debt levels to fall to zero if the human capital externality is weak or the initial policy is quite ambitious.====The rest of the paper is organized as follows. Section 2 offers a quick review of the surrounding literature. Section 3 lays out the main issues and intuition by studying a no-frills, two-period version of our main model. The three-period model itself is laid out in Section 4, and the definition of equilibrium and its stability properties are clarified in Section 5. The possibility of implementing an incremental education-debt-taxes package under the Pareto-criterion is analyzed in Section 6, while Section 7 studies the debt dynamics. Section 8 presents the results from a numerical example that allows for distorting taxes and an endogenous labor-leisure choice. Section 9 offers few concluding remarks. The appendix contains proofs of all major results.",Intergenerational debt dynamics without tears,https://www.sciencedirect.com/science/article/pii/S1094202518302631,18 June 2019,2019,Research Article,227.0
"Chiu Jonathan,Eisenschmidt Jens,Monnet Cyril","Bank of Canada, Canada,European Central Bank, Germany,University of Bern, Study Center Gerzensee, Switzerland","Received 18 March 2017, Revised 28 May 2019, Available online 7 June 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.06.003,Cited by (10),"The market for central bank reserves is mainly over-the-counter and exhibits a core-periphery network structure. This paper develops a model of relationship lending in the unsecured ==== – e.g., the fact that banks sometimes trade below the central bank's deposit rate, as we find using data from the ECB. The model also helps understand how ==== affects the network structure of the interbank market and its functioning.","Major central banks implement monetary policy by targeting the overnight rate in the unsecured segment of the interbank market for reserves – the very short term rate of the yield curve. The textbook principles of monetary policy implementation are intuitive: Each bank holds a reserve account at the central bank. Over the course of a normal business day, this account is subject to shocks depending on the banks' payment outflows (negative shock) and inflows (positive shock) driven by business activities. Banks seek to manage their account balance to satisfy some reserve requirements.==== By changing the supply of reserves, a central bank influences the interest rate at which banks borrow or lend reserves in the interbank market, and as a consequence the marginal cost of making loans to businesses and individuals. In recent years, many major central banks have refined this system by offering two facilities: In addition to auctioning reserves, the central bank stands ready to lend reserves at a penalty rate – the lending rate – if banks end up short of reserves. Symmetrically, banks can earn an interest at the deposit rate if they end up holding reserves in excess of the requirement. As a consequence the interbank rate should stay within the bands of the corridor defined by the lending and the deposit rates. This is known as the “corridor system” for monetary policy implementation.====The reality is more complex than this basic narrative. Bowman et al. (2010) and others report that in many jurisdictions with large excess reserves, banks have been trading below the deposit rate (supposedly the floor of the corridor).==== It is also well known that banks sometimes trade above the lending rate (supposedly the ceiling of the corridor). This is a challenge to the basic intuition that simple arbitrage would maintain the rates within the bands of the corridor. At a time when central banks are thinking of exiting quantitative easing policies, we may wonder whether these apparent details are symptomatic of a dysfunctional interbank market that will hamper exit, or if they are “natural” phenomena with little relevance for the conduct of monetary policy during the exit stage.====Contrary to folk belief, the interbank market is very far from being the epitome of the Walrasian market. For example, every year the ECB money market survey (e.g. ECB, 2013) shows that the majority of the transactions in the European (unsecured) interbank market is made over-the-counter (OTC). Many banks maintain long term relationships with their partners. Smaller banks tend to trade with just a few other banks, if not only one, or they directly access the central bank facilities without even trading with another bank. We review the evidence below, but these facts are now well accepted.====In this paper we analyze the effects of long-term trading relationships and monetary policy on interbank trading volume and rates, the network structure of the interbank market and its functioning. We show that modeling relationships matters for both individual and aggregate demand for liquidity, and can explain why banks trade below the deposit rate or above the lending rate. Our analysis also sheds light on the effect of monetary policy on the network structure of the interbank market. In particular we show that the accommodative monetary policy stance can lower the value of building and maintaining relationships, so that in steady state, no or few relationships exist. Some central bankers have been pointing out this phenomenon and it arises endogenously from our model.====We model the corridor system for monetary policy implementation under zero-reserve requirements.==== At the beginning of each day, banks face a shock to their reserves holdings. Facing the reserve requirement, banks in a deficit position can borrow at the central bank's lending facility at ==== and those in a surplus position can deposit their reserves at the deposit facility to earn ====. A positive spread between ==== and ==== gives banks incentives to borrow and lend directly with each other. These trades can be conducted in two OTC markets, a core market and a periphery market where relationship-lending takes place. Participation in the core market is subject to two frictions. First, banks face matching frictions in finding trading partners. Second, some banks (that we call ==== for “small”) have to pay a cost to access the core market, while others (that we call ==== for “large”) can access it freely.==== In addition, banks can trade reserves with their long-term partners in the periphery market.==== Within a relationship, a large bank can provide intermediation service by accessing the core market on behalf of a small bank; the small bank saves on the access cost, and the large bank extracts some rents from providing this service. Furthermore, a relationship allows long-term partners to trade repeatedly over time without the need to search for a new counterparty everyday. However relationships can end for exogenous reasons. In that case, banks have to search for a new partner.====Within this framework, we can explain why we observe arbitrage opportunities in the data. Small banks value long-term relationships which provide liquidity insurance and save their costs of accessing the OTC market. As a result, they are willing to temporarily lower their surplus from trading a loan as long as the long-term gains from keeping a relationship outweigh the short-term loss. Specifically, if the conditions are right, we show that small banks with surplus reserves agree to lend at a rate below ====. Symmetrically, small banks that need reserves may end up paying a rate above ====. Therefore, in equilibrium some banks trade below the floor or above the ceiling of the corridor. On the surface there seems to be unexploited arbitrage opportunities. There is none really: small banks are willing to trade at a rate outside the corridor only for small loans with their long-term partners, but not for large loans or with other counterparties. Our stylized model shows that the corridor is “soft”, i.e. equilibrium interbank rates can be below ==== or above ====, when trading frictions are present, even though small banks can access the deposit/lending facilities at no cost. Furthermore our model implies that the occurrence of this outcome depends on aggregate liquidity conditions: A soft corridor is more likely when there is a large aggregate liquidity surplus or deficit.====To get a sense for the performance of the model, we use data from the Money Market Survey Report of the ECB. This survey started in July 2016 and contains the universe of money market trades for the largest 52 banks in the Euro area. The data shows that the money market has a core-periphery structure, very much like that in other jurisdictions. In addition, we find that a significant fraction of loans (38%) from small banks to large banks are conducted at a rate below the deposit facility rate. We parameterize our model by matching several moments in the data, in particular the frequency of trades below the floor. We then conduct several experiments. For example, we study the effects of changing the width and position of the interest rate corridor and the supply and distribution of reserve balances on rates and the trading activities in the core and periphery markets.====A lesson for policy makers is that trades outside the corridor are consistent with a well-functioning core-periphery interbank market. Thus, central banks have no need to worry about eliminating deviations due to long-term relationships. However, one should be careful in interpreting the interbank market rate as a reference for overnight cost of liquidity, because it may also incorporate a relationship premium, which at times can significantly distort the observed overnight rate.====Our work is also one of the first attempts at explaining the endogenous response of the network structure of the interbank market to a change in monetary policy. The network structure that emerges endogenously resembles the core-periphery structure we observe in the data, where most of the trading activities is due to a number of banks that appear to intermediate the trades of others. We show how the interest rate corridor and the distribution of liquidity can affect banks' incentives to build relationships and accordingly the terms and patterns of trades.",Relationships in the interbank market,https://www.sciencedirect.com/science/article/pii/S1094202518302679,7 June 2019,2019,Research Article,228.0
Lambrias Kyriacos,"European Central Bank, Sonnemannstrasse 20, 60594, Frankfurt-am-Main, Germany","Received 7 July 2015, Revised 24 May 2019, Available online 5 June 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.06.001,Cited by (1),"We propose a fully flexible, complete-market model of the international business cycle that is consistent with a negative correlation between the ==== and relative consumption (the Backus-Smith puzzle). We allow for imperfect substitutability of capital which significantly reinforces Harrold-Balassa-Samuelson effects, producing more empirically relevant movements in real exchange rates. Most importantly, we introduce changes in expectations (news-shocks) as an explanation to the Backus-Smith puzzle, through shifts in relative demand. However, news-shocks also contribute to lower cross-country correlations of economic aggregates, deteriorating international co-movement.","International cyclical fluctuations exhibit two distinctive features. First, macroeconomic aggregates like output, consumption, investment and hours worked are positively correlated across countries. Second, the correlation between the real exchange rate and relative consumption is low or negative. Standard macroeconomic theory has difficulties reconciling these facts (Backus et al. (1992, 1994), henceforth BKK). Free movement of resources implies a negative co-movement between real quantities, like investment and output, whereas perfect risk sharing and a positive international transmission of productivity shocks result in a strong, positive correlation between the relative consumption and its relative price (Backus and Smith, 1993). In the literature, this is referred to as the Backus-Smith puzzle==== or the consumption-real exchange rate anomaly. The latter might suggest that markets are incomplete and that world residents do not optimally share the risks of country-specific shocks (Kollman 1995). Nevertheless, market-incompleteness alone is not sufficient to break the tight link between consumption and real exchange rates (Chari et al., 2002).====We develop a two-country, complete-market model with non-tradables that is able to reconcile the consumption-real exchange rate correlation in a theoretical economy driven by permanent sectoral productivity shocks that share a common stochastic trend across countries and sectors. The main contribution of our work is to propose an alternative explanation to the Backus-Smith puzzle based on two main channels: strong Harrold-Balassa-Samuelson (HBS) effects and news-shocks. In a world with complete Arrow-Debreu international financial markets, a negative correlation between the real exchange rate and relative consumption can arise only through a relatively strong appreciation of the home non-tradable good. We achieve this by allowing for an imperfect substitutability of capital services across tradable- and non-tradable-goods sectors – as in Mendoza and Uribe (2000) and Arellano et al. (2009). This amplifies HBS effects contributing to a substantially lower correlation between consumption and real exchange rates. At the same time, this provides for a correlation between output and capital services across sectors that is in line with the data. Nevertheless, the B-S correlation is still positive unless shocks to Total Factor Productivity (TFP) are anticipated. News-shocks change agents' expectations about future income without changing current fundamentals. Movements in relative demand between the announcement and the realisation of the shock break the tight link between the real exchange rate and relative consumption. The B-S correlation turns negative and the correlation between the terms of trade and the real exchange rate turns positive, significantly improving the model's fit to the data for relative prices compared to the case of traditional, unexpected disturbances. Our result does not depend on market inefficiency or very low levels of trade elasticity (see Corsetti et al., 2008). Instead, it is the efficient allocation in an economy where preferences exhibit low wealth effects on labour supply, the price of non-tradables plays an important role in real exchange rate fluctuations and innovations to (sectoral) TFP are known one period in advance. Nevertheless, the analysis shows that news-shocks contribute to a lower degree of cross-country co-movement, especially for investment. Allowing for surprise and news shocks to co-exist, as in Schmitt-Grohé and Uribe (2012), improves the cross-country correlations of output and employment, but not sufficiently that of investment.====Our paper lays in the intersection of three different strands of the literature. First is the literature that tries to reconcile empirical business cycle facts with expected innovations to TFP (Beaudry and Portier, 2004, Beaudry and Portier, 2006, Beaudry and Portier, 2007, Beaudry and Portier, 2014, Jaimovich and Rebelo, 2008, Jaimovich and Rebelo, 2009) and in particular to international business cycles (Beaudry et al., 2011). Opazo (2006) and Nam and Wang (2010a) also study the role of news-shocks in explaining international relative prices in models of incomplete markets. Further, we also perform an experiment where surprise and news-TFP shocks are jointly estimated as in Schmitt-Grohé and Uribe (2012, henceforth SGU). Second, our work belongs to the class of multi-sector, international business cycle models trying to explain the Backus-Smith puzzle. Corsetti et al. (2008) show that an incomplete-market model with low trade elasticity is consistent with a low degree of international risk sharing through strong endogenous wealth effects. Benigno and Thoenissen (2008) construct a model where shifts in non-tradables' prices are the main reason behind a negative B-S statistic in a world where a single bond is traded. Raffo (2010) proposes a mechanism via investment-specific technology shocks, as this type of disturbances act like a demand shock.==== Stockman and Tesar (1995) suggest that accounting for taste-shocks can be important to explain several facts of international cyclical fluctuations. Karabarbounis (2010) shows that accounting for a home-sector can potentially explain the observed limited degree of international risk sharing. Differently to these studies, we allow for expected innovations and a common stochastic trend in productivity across the world – as in Rabanal et al. (2011), Mandelman et al. (2011) and Ireland (2013) – and indeed also across sectors as in Akkoyun et al. (2017, henceforth AAK). Lastly, our work is closely related to the open-economy literature that analyses implications of imperfect capital substitutability (Mendoza and Uribe, 2000 and Arellano et al., 2009) which, nonetheless, have not been extensively investigated.==== It's importance for our results suggests that it merits further scrutiny.====The following section outlines the model, and section 3 provides details on the estimation of the underlying exogenous process and statistical evidence of the cointegration properties implied by the model. Section 4 discusses the calibration and the estimation of the model's parameters. The main results are given and discussed in section 5 while section 6 conducts some analysis where the models' underlying shocks, expected and not, are jointly estimated. Section 7 concludes.",Real exchange rates and international co-movement: News-shocks and non-tradable goods with complete markets,https://www.sciencedirect.com/science/article/pii/S1094202518302394,5 June 2019,2019,Research Article,229.0
"Fratto C.,Uhlig H.","Department of Economics, University of Chicago, 1126 East 59th Street, Chicago, IL 60637, USA","Received 18 April 2016, Revised 21 May 2019, Available online 30 May 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.05.005,Cited by (13),"Why was there no deflation and what accounts for ==== after 2008? Is the missing deflation puzzle an indictment of Phillips-curve-type analysis, or is all well and good if one fixes the model with the appropriate features? To shed light on this issue and in order to avoid confounding the answer with post-crisis model adaptations, we provide a “retro analysis” and employ the original benchmark of ==== model. We show that this model implies that shocks to price and wage markups alone are nearly enough to account for ==== before 2008, and that they do so substantially post 2007 as well. While markup shocks account for most of the inflation movements, they do not play a significant role in the accounting of employment: the Phillips curve tradeoff is weak after 2007 according to the model, but also before 2007. We thus argue that the asserted post-crisis model failures were visible pre-crisis already. Extending the retro analysis with features introduced by the most recent part of the literature does little to alter our key insights."," Esp. of fashion, music, or design: characterized by imitation or revival of a style from the (relatively recent) past; (more generally) backward-looking, nostalgic, esp. affectedly so. ====Why was there no deflation after the severe financial crisis and downturn of 2008 and what accounts for the subsequent inflation that occurred? Some have argued that the absence of deflation demonstrates a failure of the Phillips curve and New Keynesian theories linking economic slackness to deflationary pressure. In Hall (2011)'s Presidential Address, he argues that “the inflation rate hardly responded to conditions in product and labor markets, else deflation might have occurred.” He concludes that theories based on the concept of Non-Accelerating Inflation Rate of Unemployment (NAIRU) fail to explain the dynamics of inflation during the recent crisis.====By contrast, Del Negro et al. (2015) argue that a standard New Keynesian Dynamic Stochastic General Equilibrium (DSGE) model, amended with financial frictions features proposed prior to the 2008 crisis, successfully predicts a sharp contraction in economic activity along with a modest decline in inflation. They argue that the missing deflation was due to high expected future marginal costs. Relatedly, Christiano et al. (2015) explain the small drop in inflation during the Great Recession using a fall in the total factor productivity and the rise in the cost of working capital.====However, these authors agree, in essence, that the models prominently used prior to the crisis of 2008 need repair or replacement. Even Del Negro et al. (2015) amend the Smets and Wouters (2007) model considerably. While we agree that substantial and worthwhile research progress has been made based on the experience and lessons of 2008, it is also worth pointing out that examining the missing deflation puzzle with models which have been constructed or amended after the fact is not an out-of-sample prediction exercise. One may see this as akin to arguing that the profession is capable of delivering models that can predict or consider what will or may happen only after we have seen the data. It leaves the question open as to whether the New Keynesian transmission channel is broken or whether it is alive and well, as long as we keep fixing things here and there. Is there hope that this process will converge eventually?====We provide an altogether different perspective in this paper. We first draw on the Smets and Wouters (2007) model for our exercise, as this was perhaps the most popular New Keynesian pre-crisis DSGE model. Various central banks have given it much attention and therefore policy relevance. We shall use that model as originally devised to account for the behavior of inflation. Likewise, we examine movements in employment to shed light on the Phillips curve trade-off, which is central to (New) Keynesian analyses.====On purpose and in the spirit of the out-of-sample prediction exercise, we do not amend the model with greater considerations of the financial sector, nonlinearities and policy changes arising from the Zero Lower Bound (ZLB) considerations, or a time-varying inflation target, as has been done in Del Negro et al. (2015). We provide our benchmark perspective and account for the movements of inflation and employment per decomposing them into their reactions to present and past shocks, according to the linear and original Smets and Wouters (2007) model. We take this “retro perspective” by design.====The original model treats the difference between a negative nominal interest rate predicted by a linear Taylor rule and an actual nominal interest rate of zero as a surprise tightening; we do so as well. The negatives of this approach are discussed in section 8, where we argue that the impact of the ZLB on inflation generally is ambiguous and depends on fine modeling details and assumptions about monetary policy. We provide a numerical example in which the way we model the monetary policy in our paper is either underestimating or overestimating the actual impact effect of the ZLB on output and inflation. The sign of the effect depends on the type of monetary policy followed by the central bank during the ZLB episode. In this light, ours is a reasonable benchmark for the evaluation of the role of the monetary policy at the ZLB.====The key insight from our analysis follows. We find that inflation through the entire sample is accounted for almost entirely by price- and wage-markup shocks, with some (unsurprising) additional impact from the ZLB post-2008. Moreover, price- and wage-markup shocks post-2008 are not unusual in size. The model does not predict much deflation absent in these shocks in any case. The ZLB-implied “surprise” tightening of monetary policy post-2008 only contributes a downward pressure on inflation of about two percent.====Furthermore, we find that the shocks accounting for inflation do not account for much of the movements in employment. The built-in Phillips curve tradeoff is of minor importance for understanding the data over the entire sample, not just post-2008. The Smets and Wouters (2007) model does allow for a financial friction in the form of a wedge, somewhat similar to Chari et al. (2007). This wedge plays some role post-2008.====These results can be read in two ways, but both disagree with the literature perspective discussed above. One may wish to argue that the Smets and Wouters (2007) model was sufficient prior to 2008 in accounting for inflation. But then, one must concede that the model is reasonably fine post-2008 as well in terms of accounting for the inflation. The severe crisis of 2008 does not provide much news here. Conversely, one may argue that the model fails to appropriately account for inflation post-2008, that its mechanisms make little sense in context and that it therefore needs repair or replacement. But since that account does not differ much from the account for pre-2008 inflation, this indictment of the model is just as valid when examining the data prior to the crisis.====While our sympathy is with this second perspective, its message is more fundamental and hopeful than the dismal perspective of always having to fix models ex-post of the current literature. The financial crisis was unnecessary for questioning the inflation accounting capability and the importance of the Phillips curve tradeoff in prominent pre-crisis models, such as Smets and Wouters (2007). Put differently, there is hope in seeking to construct models that can reasonably account for macroeconomic outcomes, even in the face of a future crisis of unknown origin. A crisis such as 2008 is obviously an opportunity and instigation for improving the models further, for greater attention to the role of the financial sector, or for adding vigor to pursuing other agendas. But if the data prior to 2008 did not lead us to invalidate the Smets and Wouters (2007) model for the question at hand of accounting for inflation, the crisis of 2008 shouldn't either.====With our preferred reading of the results — and we concede that an alternative reading is conceivable — the missing deflation puzzle post-2008 (and with our preferred reading of the results) is simply a new version of the insight already available in 2008. There is no good understanding of what drives inflation based on New Keynesian theories such as Smets and Wouters (2007). We therefore sympathize with Hall (2011), when he argues that inflation behaves in a near-exogenous fashion. Importantly, we add that this is even more accurate before 2008. We agree with King and Watson (2012), who challenge the ability of New Keynesian models to explain inflation dynamics, and consider our result to be in line with theirs.====Accounting for or explaining the behavior of inflation is the theme of a large and active body of literature. Coibion and Gorodnichenko (2015) argue that the missing deflation puzzle can be explained by a rise in inflation expectations. Similarly, in Stock's discussion of Ball and Mazumber (2011), he argues that the missing puzzle disappears when we consider PCE-XFE, headline CPI, headline PCE, and median CPI inflation. The borrowing cost channel is explored by Gilchrist et al. (2015). Building on the fiscal theory of the price level, Cochrane (1998), Cochrane (2011), and Leeper and Zhou (2013), Leeper (2013) argue that the expansion of government debt may have led to higher inflation. The inflation-employment-stimulating effects of government spending from a Phillips-curve perspective has been a substantial subject of the literature, such as Cogan et al. (2010), Drautzburg and Uhlig (2011), Shoag (2013), Kaplan and Violante (2014), and Wilson (2012). This has been called into question by Dupor and Li (2015) and Conley and Dupor (2013).====Our findings on the role of price- and wage-markup shocks account for inflation during the Great Recession are related to Bassetto et al. (2013), who document that statistical models predict disinflation during the Great Recession, which does not occur due to offsetting residuals. Related, Linde and Trabandt (2018) stress the importance of nonlinearities in price and wage setting. Gilchrist et al. (2015) argue that the missing deflation puzzle originated from the relation between firm pricing decisions and liquidity constraints.====In section 2, we describe our approach. Section 3 presents the main point of the paper. The following sections provide additional details. Sections 4, shows the results for inflation for the entire sample from 1948 to 2014. In section 5, we “zoom in” on the crisis and post-crisis episode, starting in 2007, and provide some counterfactuals. Section 6 focuses on employment and on the Phillips curve. In section 7, we analyze the interest rate implied by the Taylor rule after the crisis. Section 8 discusses the role of the ZLB. Section 9 examines three alternative models. We allow for a time-varying inflation target, and we study Del Negro et al. (2015) and Christiano et al. (2015) through the lens of our approach. Section 10 offers some final discussion and conclusion.",Accounting for post-crisis inflation: A retro analysis,https://www.sciencedirect.com/science/article/pii/S1094202518302461,30 May 2019,2019,Research Article,230.0
"Havranek Tomas,Sokolova Anna","Institute of Economic Studies, Faculty of Social Sciences, Charles University, Prague, Czechia,Department of Economics, University of Nevada, Reno, United States of America,National Research University Higher School of Economics, International Laboratory for Macroeconomic Analysis, Moscow, Russian Federation","Received 21 August 2017, Revised 7 May 2019, Available online 30 May 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.05.004,Cited by (55),"We show that three factors combine to explain the mean magnitude of excess sensitivity reported in studies estimating the consumption response to income changes: the use of macro data, publication bias, and ","A burgeoning literature investigates the effects of monetary and fiscal policy in a framework where a fraction of households neither save nor borrow, but follow the rule of thumb to consume their current income. Galí et al. (2004) show that the existence of such consumers affects the effectiveness of standard monetary policy rules, while Galí et al. (2007) document how rule-of-thumb behavior can help reconcile model predictions and empirical evidence concerning the effects of government spending on private consumption. Models with a sufficiently high share of rule-of-thumb consumers produce large fiscal multipliers, as illustrated by Leeper et al. (2017). The calibrated or prior value used for this share varies, but is usually substantial: for example, Drautzburg and Uhlig (2015) use 0.25, Leeper et al. (2017) use 0.3, Bilbiie (2008) and Kriwoluzky (2012) use 0.4, Erceg et al. (2006), Galí et al. (2007), Forni et al. (2009), Cogan et al. (2010), Colciago (2011), and Furlanetto and Seneca (2012) use 0.5, while Andres et al. (2008) use 0.65. Models used by policymaking institutions to analyze fiscal stimulus typically assume 0.2–0.5 (Coenen et al., 2012).====Consumers who behave according to a standard incomplete markets model should adjust their consumption in response to unanticipated permanent income shocks, but not anticipated or transitory ones—unless they have limited access to disposable liquid resources. We examine the empirical literature that studies the sensitivity of consumption to the two latter income changes, the literature often cited as the motivation for calibrating shares of rule-of-thumb consumers (we will refer to this evidence as “excess sensitivity estimates”).==== We find that estimates produced by this literature are overall inconsistent with the calibrated values quoted in the previous paragraph. When corrected for the bias due to aggregation and the bias due to publication selection, the literature yields a mean excess sensitivity of merely 0.11. That is outside the 90% probability interval even for the conservative prior used by Leeper et al. (2017). The remaining excess sensitivity, moreover, can be attributed to binding liquidity constraints.====To obtain this result, we collect 3,127 estimates of excess sensitivity reported in 144 published studies and investigate why the estimates vary. In doing so, we take on the challenge put forward by the first survey of the micro literature estimating excess sensitivity, Browning and Lusardi (1996, p. 1833): “It is frustrating in the extreme that we have very little idea of what gives rise to the different findings. (…) We still await a study which traces all of the sources of differences in conclusions to sample period; sample selection; functional form; variable definition; demographic controls; econometric technique; stochastic specification; instrument definition; etc.” To this end we use the methodology of meta-analysis, which has been employed in economics, for example, by Chetty et al. (2013) on the Frisch elasticity of labor supply, Havranek et al. (2015) on the elasticity of intertemporal substitution in consumption, and Card et al. (2018) on the effects of active labor market policy.====We first examine estimates from all 144 studies together, providing a bird's eye view of the literature. We then focus on a more homogeneous subset of micro studies that estimate marginal propensities to consume (MPCs) out of observable payments. Our results indicate that three factors contribute approximately equally to the mean reported excess sensitivity, 0.37: methodology issues (especially the use of macro data), selective reporting of estimates (publication bias), and structural reasons for excess sensitivity (liquidity constraints). The mean coefficient corrected for the three factors mentioned above is zero, which implies little evidence for pure rule-of-thumb behavior—unless liquidity constraints and rule-of-thumb behavior are both symptoms of another characteristic of households, as suggested by Parker (2017). We also find that, for micro studies estimating MPCs, payment amount affects the reported results substantially (an increase in the logarithm of payment size by one standard deviation is associated with a decrease in the reported MPC of 0.08), as does the horizon over which the responses are estimated (an increase in the horizon by one month is associated with an increase in the reported MPC of 0.04). The employed measure of consumption matters for the estimated consumption response (for example, estimated MPCs tend to be 0.06 smaller for food than for the entire set of non-durable goods). In contrast, the choice of estimation techniques does not affect the results in a systematic way.====Our results suggest that publication bias impacts micro studies, but not macro studies: because the underlying excess sensitivity is much smaller for micro data, negative (and thus unintuitive) results tend to appear there much more often, which might lead to selective reporting. We also find indications for a preference in the literature to publish statistically significant results, which is consistent with Brodeur et al. (2016), who collect 50,000 ====-values from various fields of economics and show that insignificant estimates are systematically underreported. In a similar vein, Ioannidis et al. (2017) survey evidence from 6,700 econometric studies and conclude that nearly 80% of the reported effects are exaggerated because of publication bias. In the context of excess sensitivity and rule-of-thumb consumption, however, the bias has received little attention, and we have not found any study that mentions this problem while building a calibration on previous estimates. As we show in the remainder of the paper, the consequences of publication bias are at least as serious as the effects of the widely discussed misspecifications in the estimation of preference parameters.",Do consumers really follow a rule of thumb? Three thousand estimates from 144 studies say “probably not”,https://www.sciencedirect.com/science/article/pii/S1094202518302849,30 May 2019,2019,Research Article,231.0
"Önder Yasin Kürşat,Sunel Enes","Sunel & Sunel Law Firm, Turkey","Received 3 October 2018, Revised 7 May 2019, Available online 28 May 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.05.003,Cited by (1), and are robust to alternative numerical approximation methods as well as sovereign debt maturities.,"This paper emphasizes the decisive role of risk aversion on the interplay between polarization and sovereign default risk in a model of political uncertainty. First, we augment the seminal contribution of Tabellini and Alesina (1990), who study debt accumulation implications of disagreement in the distribution of public spending under political turnovers, with exogenous default. The economy is inhabited by two constituencies of equal size and two respective rival parties that favor one of the constituencies more. Accordingly, polarization is defined as an intratemporal allocation of consumption between constituencies that is not egalitarian. Once a candidate party becomes incumbent, it administers a benevolent government and maximizes aggregate welfare. Political turnovers are assumed to be exogenous for tractability, so that the incumbent party retains office only with a probability between zero and one. Similarly, the economy is in default (with full repudiation of debt) in period two with a finite probability regardless of the incumbent.====There are two competing forces determining the equilibrium level of debt in this environment: (i) An insurance motive concerned with smoothing the intertemporal consumption stream of the favored constituency, which calls for an underborrowing bias. (ii) A strategic debt accumulation motive which front-loads consumption (and hence triggers a borrowing bias) considering the possibility that future distribution of resources might be carried out by a political rival under disagreement.====Using this tractable model, we state a proposition which asserts that when the risk aversion parameter (in the CRRA class of utility functions) is smaller ==== than one, polarization causes over====borrowing. The intuition behind this result follows Tabellini and Alesina (1990) that the incumbent is induced to borrow more in order to ==== of the succeeding party when concavity of preferences decline with consumption.==== This in turn, happens because a social welfare maximizing sovereign treats consumption allocated to the two groups as closer substitutes under decreasing concavity.====We then, analyze an infinite-horizon model, which extends the two-period problem by introducing uninsured income shocks and endogenous default to shed light on how our main result is affected from these realistic ingredients of sovereign debt and default. To that end, we study a quantitative framework that is identical to the Cuadra and Sapriza (2008) (CS, henceforth) environment, which extends a canonical endogenous sovereign default model in the dimensions of political turnovers and polarization. We employ these authors' setup (including their full calibration of the model to the 1980-2001 episode of the Argentinian economy) as a workhorse environment to study the impact of risk aversion on default risk under polarization and instability.====Our quantitative findings strikingly overturn main implications of CS. Particularly, we characterize that polarization and political instability increase borrowing and default risk only when preferences feature a decreasing concavity index. This indeed, necessitates the use of a risk aversion parameter between zero and one should the researcher aim to use a utility function that belongs to the CRRA class. In this sense, we define conditions that fix the calibration in CS, who argue to find increased borrowing and higher default risk while using a risk aversion parameter that is greater than one.====Our results mainly depend on the feature of Eaton and Gersovitz (1981)-type sovereign default models that sovereign debt is priced favorably in response to better macroeconomic conditions under limited commitment to repayment of debt. This induces the incumbent to increase borrowing in good times, driving up consumption and making the sovereign treat the consumption of the two groups as closer substitutes under decreasing concavity. Hence, political instability and disagreement exacerbate the borrowing bias and elevate default risk. Notice that these effects dominate an insurance motive, which is now reinforced since default entails direct output costs. All else equal, default costs make the incumbent refrain from accumulating too much debt, with the precautionary consideration that should a turnover occur in the next period, the rival party might choose to declare default, inflicting output penalties on both constituencies.====In an extension, we establish that for a fixed degree of political instability, an increase in political disagreement elevates sovereign default risk again under risk aversion parameter values that are smaller than one. A parallel finding is obtained when the turnover probability is increased for a given degree of polarization. We also show that these associations are reversed when risk aversion parameters are taken to be greater than one. In this sense, an important part of our contribution emerges as providing a necessary condition that amends the calibration in the CS environment in order to obtain a positive relationship between increased political frictions and default risk.====Our findings are robust to employing alternative numerical approximations of the value functions as well as increasing sovereign debt maturity. This highlights our contribution to the quantitative models of politico-economic equilibrium with allocational disagreement that common parameterizations of these models may not be appropriate for analyzing strategic debt accumulation in the face of exogenous political instability.==== CS argue that political uncertainty elevates default risk while overlooking the Tabellini and Alesina (1990) insight in a model of allocational disagreement. By running multi-dimensional robustness exercises, we establish that in fact, the degree of risk aversion is decisive on the effect of political instability on default risk. Azzimonti (2011) shows that polarization on the preferred composition of public spending under endogenous turnovers hinders investment. She uses logarithmic preferences with a constant drift for the utility derived from public good provision, which is financed by distortionary taxation rather than debt. In Azzimonti (2009), she further reports that her results generalize to CRRA preferences toward public goods with a risk aversion parameter that is smaller than one, consistent with our analysis. Finally, in a model of office rents rather than polarization, Chatterjee and Eyigungor (2019) use a risk aversion parameter that is smaller than one, while linking endogenous turnover probability with economic performance to explain fluctuations in sovereign spreads. These authors note that using a risk aversion parameter that is greater than one requires a large, constant drift in the utility function for their results to hold as in Azzimonti (2011).====The rest of the paper is organized as follows. We relate borrowing bias and political uncertainty to risk aversion using analytical results in the next section. Section 3 extends the basic model to the CS environment with endogenous default and quantitatively demonstrates the robust role that risk aversion plays on the interplay between political risk, polarization and default risk. Finally, Section 4 concludes.",The role of risk aversion in a sovereign default model of polarization and political instability,https://www.sciencedirect.com/science/article/pii/S1094202518304873,28 May 2019,2019,Research Article,232.0
"Cooke Dudley,Damjanovic Tatiana","Department of Economics, University of Exeter, Streatham Court, Rennes Drive, Exeter EX4 4PU, United Kingdom,Department of Economics, Durham University, Mill Hill Lane, Durham DH1 3LB, United Kingdom","Received 12 June 2017, Revised 23 April 2019, Available online 10 May 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.05.001,Cited by (0),"This paper studies firm entry with financial frictions. We motivate our analysis by documenting that a fall in firm entry and a widening of the interest rate spread occur when there is a rise in idiosyncratic uncertainty. We then develop a model of firm entry and financial frictions – with fluctuations in the volatility of firm-level demand shocks – consistent with this empirical evidence. Finally, we study dividend and labor-income taxation. Financial frictions weaken the incentive to support firm entry, and in a calibrated version of our model, accounting for the increase in volatility observed during the 2007-09 recession, optimal fiscal policy raises (lowers) dividend (labor)-income taxes by up to 7 (1.5) percentage points.","Financial market frictions are an important part of the mechanism through which fluctuations in the volatility of firm-level shocks are transmitted to the real economy.==== Similarly, firm entry has been shown to act as a propagation mechanism for business cycle dynamics.==== In this paper, we bring these two ideas together. We first document the relationship between idiosyncratic uncertainty and firm entry. Using a vector autoregression, in which we allow for interaction between financial markets and the broader macroeconomy, we show that a fall in firm entry and a widening of the interest rate spread occur when there is a rise in idiosyncratic uncertainty. We then develop a model of firm entry and financial frictions – with fluctuations in the volatility of firm-level demand shocks – consistent with this empirical evidence.====We use our model to study dividend and labor-income taxation. We find that financial frictions weaken the incentive to support firm entry. Financial frictions generate a trade-off for fiscal policy when firm entry is endogenous. On the one hand, financial frictions lead to a reduction in firm entry, which the policymaker would like to mitigate. On the other hand, the policymaker accounts for the agency costs of default, which firms disregard due to limited liability. In a calibrated version of our model, accounting for the increase in volatility observed during the 2007-09 recession, we show that optimal dividend (labor)-income taxes rise (fall) by up to 7 (1.5) percentage points. Optimal fiscal policy therefore involves a switch away from supporting firm entry and towards supporting employment.====The model of firm entry we develop builds on Bilbiie et al. (2012). New firms enter after paying a one-time cost, and each firm produces a differentiated good, under conditions of monopolistic competition, with a one-period lag.==== We amend this setup in two directions. We suppose each firm receives an idiosyncratic demand shock, which occurs after labor has been hired, and production has taken place. This generates uncertainty over the revenue a firm can generate from the sale of its product.==== We also assume firms finance their labor requirements by borrowing working capital from financial intermediaries who operate a monitoring technology similar to Carlstrom and Fuerst (1997).==== In this environment, firms that produce goods with a relatively low level of ex-post demand default, and agency costs mean that default is costly.====To understand the key mechanism in our model, consider the production decision of an individual firm. Firms produce under limited liability, and they place zero weight on realizations of demand in which profits are negative, since they no longer carry the risk of losses from such realizations. With monopolistic competition, and uncertainty over firm-level revenue, limited liability creates an incentive for each firm to expand production, in an attempt to take advantage of a potentially good realization of demand. Expanding production, however, amounts to committing to a greater level of borrowing, in advance, and increased borrowing requires each firm to generate more revenue to avoid default.====New firms enter the market until their expected profit, conditional on not defaulting, is sufficient to cover the cost of entry. The entry decision is subject to two opposing forces. Due to limited liability, expected revenue is higher, and this encourages entry. However, the minimum level of demand needed to avoid default is also higher, and this discourages entry. Whilst the possibility of default has no negative effect on the production decision of an individual firm, potential entrants weigh the possibility of increased revenues against the probability of default. The latter effect dominates, so, in equilibrium, the mass of firms is lower than when financial frictions are absent.====Now consider an increase in the volatility of firm-level demand shocks. In a more uncertain environment, and with limited liability, firms expand production. The probability of default rises, and this leads to a rise in the interest rate applied to working capital loans. With fewer firms, there is a fall in aggregate output and total employment. Conditional on an uncertainty shock, therefore, firm entry is procyclical, and firm default and the interest rate spread are countercyclical.====Having established the role of financial frictions for firm entry, we study dividend and labor-income taxation. We show that financial frictions weaken the incentive to support entry. To understand this result, it helps to consider the case without financial frictions. Firm entry reduces profit per-firm (a profit destruction effect) but raises product variety (a consumer surplus effect).==== Dividend-income should be subsidized – and firm entry encouraged – because the profit destruction effect is relatively weaker than the consumer surplus effect. In this case, with monopolistic competition, firm entry is inefficiently low.====Since financial frictions reduce the mass of firms in the economy, it would appear likely that optimal policy should further subsidize dividend-income. In the absence of agency costs, it is optimal to increase the subsidy to dividend-income. With agency costs, however, there is a trade-off for fiscal policy, because, whilst firms neglect such costs, the government accounts for the societal cost of default. This means it is optimal to limit firm entry. In general, the socially optimal number of firms declines with agency costs, and so does the optimal subsidy to dividend-income. If we interpret this in terms of the standard result, with financial market frictions, firm entry is instead inefficiently high.====Our model also has a second margin: labor supply. When firm entry is endogenous, labor-income should receive a subsidy equal to price-markup (Bilbiie et al., 2008). In our analysis, the subsidy to labor-income depends on the markup and the interest rate spread, and this introduces a second role for agency costs. As the volatility of firm-level demand rises, the markup falls – reducing the optimal subsidy to labor-income – but the interest spread rises – raising the subsidy. Taken together, the trade-off that characterizes the subsidy to dividend-income applies to labor-income. Therefore, during a recession, optimal fiscal policy involves a switch away from supporting firm entry and towards supporting employment.====We assess the main results of our paper quantitatively. To put our analysis into context, we focus on the Great Recession of 2007-2009 – a period characterized by an unprecedented increase in uncertainty and a drop in firm entry. Keeping dividend and labor-income taxes fixed, our model implies a (maximum) drop in firm entry of around 25 percent and a rise in the default rate of 1.5 percentage points. Optimal policy acts to raise (lower) dividend (labor)-income taxes by up to 7 (1.5) percentage points. As a point of comparison, we also consider a change in the labor-income tax, of 1 percent point, during the second quarter of 2009, consistent with the American Recovery and Reinvestment Act.==== This policy indirectly supports firm entry, and thereby raises the agency costs of default.====Our paper contributes to the literature on the design of fiscal policy when product variety is endogenous and our results on optimal fiscal policy are closely related to Chugh and Ghironi (2015). An important finding, in the context of our analysis, is the result that dividend and labor-income taxes should not respond to aggregate shocks when preferences are of the Dixit-Stiglitz type.==== This allows us to provide an analytical characterization of optimal fiscal policy when there are financial frictions. In general, we find that dividend and labor-income taxes should be time-varying. The design of fiscal policy with endogenous product variety has also been studied in environments with physical capital (Coto-Martinez et al., 2007), long-run risk (Croce et al., 2013), and oligopolistic competition (Colciago, 2016).====The transmission mechanism through which fluctuations in the volatility of idiosyncratic shocks are propagated to the real economy has been discussed in a number of recent papers. For example, in Christiano et al. (2014), a widening in the distribution of productivity shocks increases the fraction of defaults, and in Gilchrist et al. (2014), financial frictions magnify shocks to firm-level uncertainty through movements in credit spreads. Arellano et al. (2018) argue that the majority of the decline in employment during the 2007-09 recession can be explained by an increase in firm-level volatility. Since firm entry plays an important role in aggregate fluctuations, our results provide a potentially different route through which financial frictions and idiosyncratic uncertainty can affect the macroeconomy.====Finally, our paper is related to a large literature on firm entry and exit. Our approach is most similar to Bilbiie et al. (2012). To their model of firm entry, we allow for endogenous default by incorporating ex-post firm-level heterogeneity, a working capital constraint, and financial frictions. A complementary approach to studying firm entry and exit, which amends Hopenhayn's (1992) model with ex-ante heterogeneous firms to allow for investment in physical capital and aggregate shocks, is developed by Clementi and Palazzo (2016). Our modeling choices – which imply a symmetric employment decision by firms in equilibrium – are driven by the desire to generate analytical results. A general point, however, is that, in either setting, firm entry is a form of investment in which up-front costs incurred to start a business generate expected future profits.====The paper is organized as follows. In section 2, we motivate our theoretical work by analyzing the link between idiosyncratic uncertainty and firm entry using a vector autoregression. We study a static general equilibrium model of firm entry and financial frictions in section 3 and derive analytical expressions for the optimal mix of taxes on dividend and labor-income in section 4. In section 5, we develop a dynamic version of our model, and in section 6, we undertake a quantitative analysis, where we revisit the results on fiscal policy, motivated by the increase in uncertainty during 2007-09. A final section concludes.",Optimal fiscal policy in a model of firm entry and financial frictions,https://www.sciencedirect.com/science/article/pii/S1094202518302722,10 May 2019,2019,Research Article,233.0
"Glover Andrew,Short Jacob","University of Texas at Austin, USA,Bank of Canada, Canada","Received 6 October 2018, Revised 11 April 2019, Available online 30 April 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.04.007,Cited by (23), consumption growth.,"Historically, Kaldor's stylized facts have supported a unitary elasticity of substitution between capital and labor due to the near constancy of labor's share of income (Kaldor (1957)). As documented by Karabarbounis and Neiman (2014), this near constancy is no more – labor's share has declined globally since the 1980s. This downward trend, which has occurred alongside rising income and wealth inequality (Piketty (2014), Piketty and Zucman (2014)), has renewed macroeconomists' interest in estimating the aggregate elasticity of substitution between capital and labor (henceforth referenced as ====). This parameter is pivotal: if it is smaller (greater) than 1 then anything that causes an increase in the effective capital-labor ratio will increase (reduce) labor's share.==== Specifically, if it is significantly larger than 1, then the global decline in labor's share can be explained by rising effective capital-labor ratios through physical investment in response to the fall in investment prices observed over the same time period. Using a large cross section of countries, we estimate that ==== and conclude that capital deepening cannot account for the global decline in labor's share.====We identify ==== from the capital demand function of a profit-maximizing firm in the neo-classical growth model. This demand function implies that a 1% fall in the rental rate of capital should reduce labor's share of income by ====. The idea of the cross-country estimation strategy is to correlate country-specific trends in rental rates and labor's share to estimate ====. Unfortunately, rental rates are not readily available for a large cross section of countries, so we use the inter-temporal Euler equation for investment to find a proxy. This condition implies that the rental rate depends on the relative price of investment goods, as well as a ==== that reflects the gradual rise in consumption in response to lower investment prices. We estimate ==== from various data sources, cross-sectional samples of countries, and statistical models. Our estimates are typically smaller than one (although statistically indistinguishable). The average point estimate from our baseline specification is ====, which implies that the global decline in investment prices should have ==== labor's share slightly.====Our results contrast sharply with two recent studies by Karabarbounis and Neiman (2014) (henceforth KN) and the International Monetary Fund World Economic Outlook (2017) (henceforth IMF). These studies also estimate ==== from cross-country correlations between trends in labor's share and investment prices, but omit consumption growth and assume that trends in the rental rate are identical to trends in investment prices.==== For example, KN estimate ==== and conclude that (effective) capital deepening due to falling investment prices can account for half of the global decline in labor's share. Similarly, the IMF estimates that developed countries have an elasticity greater than one (although they do estimate that emerging-market countries have an elasticity less than one). We provide a full account of why our estimates differ from these studies in Section 6.====Our chief conceptual innovation relative to the KN/IMF studies is to proxy for rental rates using their theoretical relationship with investment prices and consumption rather than investment prices alone. While investment prices are a valid proxy for rental rates in a steady state (i.e., after investment prices have been constant for a long time), the data are clearly inconsistent with such an assumption: both consumption and investment prices exhibit trends that are significantly different from zero on average and vary substantially across countries. Our rental-rate proxy, which includes a transitional term reflecting consumption growth, nests the KN/IMF proxies if countries are always in a steady-state, but is also valid along transitions between steady states or growth paths. We show, both theoretically and empirically, that omitting the transitional term can substantially bias estimates of ==== away from one.==== Using our proxy for the rental rate reduces the absolute effect of investment prices on labor's share of income, as mediated by rental rates, by more than 50% in all cases and often reduces the effect to zero, both in terms of statistical significance and economic magnitude.====We differ from KN in two more dimensions. First, we estimate ==== from a larger cross section of countries than KN. We include countries with at least ten years of data on investment prices and labor's share, whereas KN exclude those with less than fifteen years. We prefer this more inclusive cutoff because it substantially increases our sample size, which allows us to split the sample by development status to test for heterogeneity in ====. As we show in Sections 5.4 and 6.1, our sample yields estimates that are consistent with most other samples. Second, they use a robust regression procedure to estimate ====, whereas we employ linear instrumental variables. We prefer the IV estimator because KN's robust regression has lower statistical efficiency, opaquely weights (and in some cases drops) countries, and introduces a free tuning parameter. Furthermore, the IV estimator provides well known formulae for omitted variable and attenuation biases, which are useful for understanding our results, but invalid under robust regression. As explained above, our theoretically consistent proxy for rental rates reduces the effect of investment prices on the labor share by over 50%, while the IV estimator further reduces the effect to essentially zero, independent of sample.====Section 2.1 provides a theoretical basis for our rental rate proxy and the omitted variable bias from excluding it, but a simple example provides intuition. Suppose that ==== is slightly above one and the relative price of investment follows a path of geometric decline, eventually falling by 10% (i.e., the investment price falls by half of the remaining distance each year).==== Households choose a smooth path of consumption to the new steady state, along which consumption growth is declining, which causes rental rates to transition slower than investment prices. The path of consumption implies a slow rise in the investment rate and a gradual increase in the stock of capital. An econometrician whose data start late in the transition will observe an essentially flat path for the investment price but a downward trend in labor's share. If the econometrician ignores the trend in consumption growth, they will underestimate the decline in rental rates and erroneously infer a large value of ====.====The above intuition aligns with our empirical results. Estimates of ==== from our baseline sample of countries are below one and shrink when we omit the transitional term. In samples for which ====, omitting the transitional term leads to larger point estimates, and the opposite holds for samples in which ====. This is true across specifications, estimators, and samples. In short, cross-country data exhibit a weak correlation between labor's share and rental rates (once consistently proxied), which implies that ==== is near one.====Our proxy overcomes the omitted variable bias discussed above, but may introduce additional measurement error since it requires new data on consumption, and requires us to specify a utility function. We address this concern in two ways. First, we use investment prices as an instrument for our rental rate proxy. As long as the additional measurement error in our proxy is independent from the measurement error in investment prices, this cleanses our estimates of any extra attenuation bias relative to using investment prices alone. Second, we estimate the elasticity of substitution from subsamples with higher-quality time series data, such as developed countries or members of the Organisation for Economic Co-operation and Development (OECD). We expect such countries to have less measurement error in investment prices in the first place, as well as better consumption data. Reassuringly, we estimate elasticities for these countries that do not differ significantly from our pooled estimates.====The cross-country approach we employ is one of three empirical strategies used to identify the aggregate elasticity of substitution. One alternative, pursued by Antràs (2004), uses aggregate time series variation in the United States and estimates ====, but requires imputation of effective capital to labor ratios in the presence of factor-augmenting technological growth. There is also a large literature on estimating the elasticity of substitution from micro data. A particularly relevant example is Oberfield and Raval (2014), who estimate these micro elasticities and aggregate them to compute a ==== substantially below one in the United States.==== This approach is the most precise, but also requires high-quality micro data that are unavailable in many countries. The benefit of the cross-country approach is that it requires only aggregate data on quantity flows and relative prices, but has thus far yielded substantially higher estimates of ==== than the other two strategies. Our estimates effectively reconcile the cross-country approach with the other two, while retaining its broad applicability.====Although our estimates of ==== indicate that capital deepening has not caused the global decline in labor's share, the fact that it has fallen so broadly warrants an explanation.==== Our estimates lend support for explanations that do not require strong capital-labor substitutability to generate a decline in labor's share. Some examples include Autor et al. (2017) and Kehrig and Vincent (2017), who document that labor's share has fallen along with a rise in product-market concentration; Elsby et al. (2013), who estimate a strong correlation between industry-level trends in labor share and import competition; Grossman et al. (2017), who build an endogenous growth model with human capital in which labor's share falls due to slower productivity growth; and Glover and Short (2018), who estimate that an aging workforce reduces labor's share at the industry level.====We proceed by outlining the structural theory that relates labor's share to investment prices and show the economic importance of ==== in that framework. We derive the appropriate reduced form model to estimate ====, then present our estimates, perform robustness tests, discuss previous estimates, and conclude.",Can capital deepening explain the global decline in labor's share?,https://www.sciencedirect.com/science/article/pii/S1094202518301479,30 April 2019,2019,Research Article,234.0
"Jin Hao,Shen Hewei","Xiamen University, China,University of Oklahoma, United States of America","Received 7 October 2016, Revised 26 March 2019, Available online 26 April 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.04.006,Cited by (4),"We develop a two-sector, core-periphery country ==== framework with endogenous financial crises to study foreign asset accumulation coordination among emerging market economies. Consistent with the policy prescription described by ====, prefers a different asset position than the national planner. When we calibrate our model to a group of emerging Asian economies, the quantitative analysis shows that in the absence of coordination, national regulation leads to a 3.7% higher average net foreign asset position and a welfare loss relative to the laissez-faire. In contrast, the coordinated level of net foreign assets is 59% of the uncoordinated level and results in a sizable welfare gain.","Financial distress in emerging market economies (EMEs) has stimulated many macroprudential policy proposals, thus promoting the accumulation of foreign assets among EMEs with less developed financial markets in an attempt to self-insure against financial crises. One of the theoretical arguments behind these policy prescriptions is that there exists a pecuniary externality as suggested by Bianchi (2011). This pecuniary externality arises because private agents do not take into account the social benefit of holding liquidity in the presence of financial constraints, hence they “overborrow” during normal times, and macroprudential policy interventions are welfare-improving.====While most existing studies of optimal macroprudential policy are conducted in the context of a single EME and ignore the spillover effect from many EMEs to the global financial market,==== there have been policy reforms that advocate macroprudential policy coordination among EMEs. For example, the Chiang Mai Initiative, which is a multilateral currency swap arrangement among the ASEAN+3 countries that draws from a foreign exchange reserves pool, coordinates the foreign assets accumulation of participating EMEs to jointly insure themselves and seek higher financial returns on their assets.==== In order to analyze the issue of macroprudential policy coordination among EMEs, we revisit the merit of macroprudential policy in the context of a multi-country setup.====In this paper, we quantitatively examine the size of incentives for EMEs coordinating on their net foreign asset (NFA) positions to strategically manipulate the world interest rate in a framework where each individual EME has motives to self-insure itself against financial crises by engaging in precautionary saving. While savings from individual EMEs are small, and thus have little influence on the prices of foreign assets, the aggregate savings from all EMEs are of considerable size and can affect international prices. Bernanke (2005) famously links the “global saving glut” to the low global interest rates observed in the 2000s. Follow-up empirical studies strongly support this hypothesis: aggregate capital flows to international assets have a significant effect on the rates of return of those assets.==== Therefore, it is crucial to assess the extent to which the strategic world interest rate management motive affects country-level macroprudential considerations when EMEs coordinate their foreign asset accumulation.====To formalize our numerical analysis, we develop a two-sector, core-periphery country model to quantitatively evaluate the optimal foreign asset accumulation for EMEs in the presence of credit constraints. The periphery consists of a continuum of financially underdeveloped small open economies (SOEs), each facing a financial friction in which its borrowing is constrained by the market value of its collateral. The borrowing is denominated in the international unit of account, that is tradable goods. The collateral is in the form of current endowment income from both the tradable and nontradable sectors, as in the studies by Mendoza (2002) and Bianchi (2011). The core country is financially developed and does not face any credit constraints. In our framework, two effects are overlooked when the decentralized agents in the peripheral countries make asset decisions: a pecuniary externality and a general equilibrium effect of asset decisions on the world interest rate. A pecuniary externality emerges as decentralized agents fail to internalize the effect of their asset decisions on the market value of their collateral. In contrast to the existing literature, our framework also features an endogenous world interest rate that is determined jointly by the peripheral and core countries. The decentralized agents in the peripheral countries take the world interest rate as given and fail to realize the market power they have in the world financial market as a large group of countries. Our key contribution is to combine these two considerations in a unified framework that is suitable for quantitative analysis.====In our analysis, we compare three equilibrium allocations: (i) the decentralized equilibrium, (ii) the equilibrium of uncoordinated SOE planners who internalize the pecuniary externality and set macroprudential regulations for individual peripheral countries, taking the world interest rate as given, and (iii) the equilibrium achieved by a periphery coordinator who considers both pecuniary externality and the effect of the peripheral countries' asset decisions on the world interest rate and coordinates macroprudential policy for all peripheral countries. Our analytical result shows that the decentralized agents ignore the pecuniary externality and undervalue the liquidity of their asset holdings relative to the SOE planners, as highlighted by Bianchi (2011). The uncoordinated SOE planners thus encourage saving, or equivalently discourage borrowing, to reduce systemic risks. On the other hand, a periphery coordinator chooses a higher (lower) NFA position than the SOE planners when the peripheral countries borrow (save) in the international financial market in order to obtain a more favorable world interest rate. Last but not least, both the pecuniary externality internalization and the world interest rate manipulation motive induce the periphery coordinator to borrow less than the decentralized agents. In contrast, these two considerations work in opposite directions when the peripheral countries are net creditors, and in this case, the relative asset position between the periphery coordinator and decentralized agents is ambiguous.====After calibrating our model to a group of Asian economies that are net savers, our quantitative result suggests that periphery coordination leads to a much lower average NFA level relative to the other two equilibria: the average coordinated NFA level for each peripheral country is 59% of the uncoordinated level and 61% of the decentralized equilibrium level. Our welfare analysis finds that, in the absence of coordination, the unfavorable world interest rate variations dominate the welfare gain from pecuniary externality internalization, and national macroprudential policy yields an average welfare loss. This outcome suggests that the interest rate manipulation motive is quantitatively important such that macroprudential policy proposals that fail to consider the general equilibrium effect on the world interest rate may be welfare-reducing. In contrast, the periphery coordinator who realizes both effects achieves an average welfare gain of 0.024%. In addition, we also characterize a state-contingent tax instrument that can restore the coordinated allocations from the decentralized equilibrium.====Note that our main objective is to quantitatively assess the internal and external macroeconomic and welfare impacts of macroprudential policy coordination within a group of EMEs in the presence of both the pecuniary externality and the world interest rate manipulation motive. We are not pursuing a global welfare objective in this paper despite providing the welfare implications of periphery coordination for the core country. In our model, a group of peripheral countries led by the periphery coordinator exerts their market power to manipulate the world interest rate while the core country acts passively. The coordination thus shifts surplus from the core to the periphery. As a result, the policy coordination we consider in this paper is desirable from the joint perspective of peripheral countries but is undesirable from the core country's perspective.====The rest of the paper is structured as follows. We start with a discussion of the related literature in section 2. In section 3 we introduce the model environment. In section 4 we formally define the three equilibria that we study and provide some theoretical results. In section 5 we apply the model to quantitatively assess the macroeconomic and welfare implications of foreign asset accumulation coordination. Finally, section 6 offers some concluding remarks.",Foreign asset accumulation among emerging market economies: A case for coordination,https://www.sciencedirect.com/science/article/pii/S1094202518302515,26 April 2019,2019,Research Article,235.0
Narita Renata,"University of São Paulo, Department of Economics, Av. Professor Luciano Gualberto 908, São Paulo, SP, 05508-010, Brazil","Received 17 September 2017, Revised 2 April 2019, Available online 17 April 2019, Version of Record 14 January 2020.",https://doi.org/10.1016/j.red.2019.04.001,Cited by (13),"This paper develops and estimates a life-cycle on-the-job search model with self-employment that captures labor market stylized facts typical of middle-income developing economies. Workers flow across unemployment, self-employment, formal and informal wage employment. Individuals differ across and within employment sectors in terms of earnings, self-employment ability and transition rates. Counterfactual analysis shows that a flat reduction in payroll taxation increases the share of formal sector workers mainly due to a drop in self-employment. A proportional reduction in ==== improves total welfare by increasing formal sector wages and profits, and allowing for a better allocation of high education workers. Converting to a progressive payroll tax system, equivalent to a flat reduction, is ineffective in reducing informality and leads to a decline in total welfare.","Self-employment is an important source of employment in developing countries. In Latin America, it comprises more than 30 percent of total employment (Perry et al., 2007). In Brazil, it outnumbers informal wage employment and is also linked to informality as most self-employed do not contribute to social security. In addition, most self-employed individuals run businesses on their own and have low education, all factors associated with lower productivity. While in this sense there is scope to improving formal job creation, for example, via payroll tax reduction, the impacts of such policies on welfare are less clear. On the one hand, bringing individuals to the formal sector may increase welfare as they get better paid and are more productive on average. On the other hand, the gains from reallocating the worst paid informal sector workers or the youngest and least talented self-employed to the formal sector might be limited in comparison to the loss in revenue due to the lower tax rate.====In this study I develop a life-cycle on-the-job search model with four employment states (formal wage earner, informal wage earner, self-employed and unemployed) that captures important empirical patterns found in the Brazilian data. First, I estimate the model on data from the Brazilian Labour Force Survey (PME) using the method of moments and show that the predicted model stocks reproduce well the observed composition of the workforce according to age, gender and education. Then, I use the model to investigate the long-run effects of different labor market policies in the formal sector on welfare and employment. More specifically, I focus on the effects of payroll taxes and severance taxes as these are particularly important in Latin America.====I find that a proportional reduction of 20 percentage points in the payroll tax rate increases the share of formal workers by up to 5% and 4% for low and high education males, respectively. The magnitude of the effect is attenuated as employers appear to compensate workers for the decrease in labor cost, with formal sector wages rising by a considerable amount (elasticity 0.75). As a result, flat reductions in payroll tax rate are welfare improving in the sense that they increase the welfare of workers and firms on average, while keeping tax revenue constant at benchmark level. The increase in total welfare is greater for high education individuals since, in addition to increasing wages, the policy allows for a better allocation of workers across sectors in this case. Interestingly, I find that the increase in labor formality is mostly due to a reduction in self-employment rather than in informal employment. Finally, I show that converting to a progressive payroll tax code, equivalent to a flat reduction of 20 percentage points, is ineffective in increasing formality. It also leads to a decline in total welfare as a progressive tax benefits the low-paid, less-experienced, and low ability among the low education individuals, who are less likely to respond to tax incentives.====I build on the wage posting framework of Burdett and Mortensen (1998) and the literature on equilibrium on-the-job search models with heterogeneous firms.==== I deviate from the standard setup mainly by introducing (i) three different employment states that characterize labor markets in developing countries and (ii) heterogeneity in self-employment skills. I distinguish between informal wage earners and self-employment as there are significant differences between these two categories as documented in the literature for Latin America.==== In particular, I document that most self-employed in Brazil are found in one-worker businesses and work fewer hours, while more than half of informal employees work in medium-large firms. In addition, the main transitions into self-employment increase over the life cycle while entry into the informal sector decreases with age. Although the transition shocks vary exogenously by age in the model, such features help to account for the pattern of stocks in Fig. 1. This shows that at the beginning of their working life, most young workers search for jobs or sort into informal wage work. Later on, they move into the formal or self-employment sectors. After age 30, formal workers migrate to self-employment and the fraction of unemployment and informal wage work remain mostly constant. Such life-cycle pattern holds across gender and education groups.====A few studies incorporate self-employment in a frictional environment. For instance, Albrecht et al. (2009) follow the Diamond-Mortensen-Pissarides search and matching framework and model an economy with workers who can be formal wage earners or informal self-employed. Workers are heterogeneous in the formal sector but have fixed productivity when self-employed, and formal firms hire only from the formal sector or from unemployment.==== Meghir et al. (2015) develop a model based on Burdett and Mortensen (1998) and consider the self-employed and informal wage workers as one group (“informal”). Firms can choose sector location and workers are homogeneous in their set up.==== The present approach allows me to account for potential heterogeneous effects of formal sector policies on informal wage earners, self-employed, and by level of skill, which I show to be relevant.====Regarding the search literature on informality and payroll tax, most related to this paper are the works of Albrecht et al. (2009) and Ulyssea (2010) who calibrate search and matching models to fit the Latin American and Brazilian contexts, respectively. In their counterfactual analyses, both studies find that lower payroll taxes reduce informality without increasing unemployment. Ulyssea (2010) shows that welfare increases by a very small amount which is consistent with a very reduced impact on wages. On the other hand, Albrecht et al. (2009) find that welfare – net output plus total tax revenues – falls as payroll tax is reduced. In their model, there is a continuum of worker types and the policy induces selection of the worst workers into the formal sector, therefore decreasing its average productivity and welfare. Relative to both papers, my approach offers two main advantages. First, I allow for on-the-job search, which explains a larger positive impact on wages and thus on welfare in the same direction. Second, I model self-employment separately from informal sector employees, as opposed to simply bundling them in the model and empirically, and consider direct transitions across all labor market states. Taken together, these features allow for a richer analysis of the effects of labor market policies.====A separate strand of the literature focuses on firms' behaviour, more specifically, on their choice to operate in the formal or informal sector. D'Erasmo and Moscoso-Boedo (2012) show that increasing payroll taxation by 55% more than doubles the informal labor force. Ulyssea (2018) finds that payroll tax reduction of 20 percentage points lowers informal employment by 20%, with no impact on the share of informal firms. There is no search frictions in these models, which helps to explain the large effects they find on labor informality and welfare from changing payroll taxation.====Finally, there are many papers related to entrepreneurship in developed countries (see, for example, Evans and Jovanovic (1989), Evans and Leighton (1989), Holtz-Eakin et al. (1994), Blanchflower and Oswald (1998), Quadrini, 2000, Quadrini, 2009, Cagetti and De Nardi (2006), Hurst and Lusardi (2004) and Dinlersoz et al. (2019)). For developing countries, Allub and Erosa (2017), Bianchi and Bobba (2013), Banerjee et al. (2015), and McKenzie et al. (2008), for example, show that financial capital is not a primary factor determining self-employment entry and suggest that individual heterogeneity is important. This is in line with model proposed in this paper as I focus on self-employment as an occupation choice of agents that are heterogeneous in talent for independent work.====The paper is organised as follows. Section 2 presents the data, some empirical facts about the labor market in Brazil and the background for the policy simulations. Section 3 presents a framework to understand individuals' employment choices. Section 4 describes the estimation procedure. Section 5 presents the results of the estimation. Section 6 uses the model to simulate counterfactual changes in the labor taxes. Finally, section 7 concludes.",Self-employment in developing countries: A search-equilibrium approach,https://www.sciencedirect.com/science/article/pii/S1094202518302874,17 April 2019,2019,Research Article,236.0
"Cooley Thomas,Quadrini Vincenzo","Stern School of Business, New York University, United States of America,NBER, United States of America,University of Southern California, United States of America,Peking University, China,CEPR, United Kingdom of Great Britain and Northern Ireland","Available online 20 June 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.red.2020.06.013,Cited by (0),None,None,The twenty-fifth anniversary of “Frontiers of business cycle research”,https://www.sciencedirect.com/science/article/pii/S1094202520300545,20 June 2020,2020,Research Article,239.0
"Baklanova V.,Caglio C.,Cipriani M.,Copeland A.","Federal Reserve Bank of New York, United States of America,Federal Reserve Board of Governors, United States of America","Received 25 May 2018, Revised 1 March 2019, Available online 21 May 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.05.002,Cited by (25),"We use unique data from U.S. BHC-affiliated securities dealers to study the use of collateral in bilateral repurchase and securities lending agreements. This is the first paper to provide stylized facts about this market, documenting, among other things, that most trades are backed by U.S. Treasuries and equities, and have either an overnight or open maturity. We also focus on how haircuts are determined in this market, highlighting the differences across asset classes. Finally, we document a negative correlation between haircut and ====, a prediction in line with a large set of theories; however this correlation is quite small in magnitude.","Securities dealers use repurchase agreements and securities-lending arrangements to borrow funds on a collateralized basis, to provide funding to others and to borrow or lend specific securities using cash as collateral. Repo transactions are also used by cash investors, including money market funds, for safe and liquid short-term investment. These trades are vital to the functioning and efficiency of financial markets. Indeed, repo is one of the largest single sources of short-term borrowing in the U.S.: in 2018, the average amount of U.S. repurchase agreement outstanding was around $3 trillion; additionally the average amount of outstanding securities lending arrangements against cash was around $700 billion. For comparison, the size of the commercial paper market, a source of unsecured short-term funding for firms, was only around $1 trillion.====Motivated by the recent financial crisis, regulators have begun to collect detailed data on repos to obtain a deeper understanding of these types of trades.==== To date, however, these collections have not been comprehensive, but rather focused on only some of the segments of the overall market, in particular the U.S. tri-party repo and parts of the securities lending market (see for instance Baklanova et al., 2015).====As a result, little is known about the bilateral repo market, even though it represents roughly half of overall repo activity.==== The bilateral repo market is important not only because of its sheer size but also because it is a key source of funding for smaller financial firms without access to tri-party repo (such as hedge funds), as well as a source of specific securities for all dealers (e.g., if they need a specific security to short sell it or an on-the-run Treasury to hedge against interest-rate risk).====In this paper, we fill this gap by using a unique data set to study the bilateral repo segment, with a focus on the role of collateral. Our data are cross-sectional: three snapshots taken in the first quarter of 2015 of all non-matured repo contracts entered into by the U.S. broker-dealer entities of nine bank holding companies that are considered large players in this market. An additional novel feature of our study is the inclusion of data on securities lending transactions where securities are lent against cash; these transactions are economically equivalent to repo although they are executed under different legal contracts.==== We estimate that our sample covers roughly one-half of all bilateral repo activity and so provides a comprehensive perspective of the market. Indeed, as far as we know, these data are the first to provide a representative view of bilateral repo activity, complementing the existing work on the tri-party repo market (see, for example, Copeland et al., 2014b, and Krishnamurthy et al., 2014).====We leverage these data to provide stylized facts about the bilateral repo market. This is a sizeable market; our data cover $1.6 trillion of securities on loan and $1 trillion securities borrowed.==== Trades using Treasuries dominate in terms of value (81 percent of securities on loan and 61 percent of securities borrowed), followed by equities, private-label structured products (collateralized mortgage obligations, mortgage-backed securities (MBS), and asset-backed securities) and corporate debt. Evidently, bilateral repo trades mostly involve the trading of ‘safe’ securities with little credit risk.====The maturity of trades is short. The share of open trades (i.e., able to be terminated at any time by either party to the trade) out of all bilateral trades collateralized by equities and corporate debt is 94 and 79 percent, respectively. The maturity distribution of trades involving Treasuries is less concentrated; nevertheless, 55 percent of them are open or have an overnight maturity.====In terms of counterparties, more than half of the trades by dollar volume involve banks or broker-dealers, followed by hedge funds. This result differs widely from that of tri-party repo, where money market funds and cash reinvestment arms of securities lending agents are the largest participants.====We observe a range of rates both within an asset class as well as for a specific security type, where the rate is a negotiated interest rate on the cash delivered in the trade. This heterogeneity reflects different motivations for trading: typically, positive rates are associated with funding trades and negative rates are associated with trading specials.==== For most of the asset classes, the mean rate is positive. The exceptions are trades backed by equities and corporate debt, where the negative average rate indicates that most trades are motivated by parties seeking to borrow securities in scarce supply. Given their dominant place in this market, we focus on trades involving Treasuries, and observe that there is wide variation in rates across securities as well as for a given security. This evidence likely reflects the heterogeneity of participants, and of their motives.====Finally, we also document the distribution of haircuts within an asset class, where the haircut measures the percentage difference between the cash and the market value of securities exchanged. Although there are a range of haircuts observed for each asset class, each distribution incorporates substantial mass points. We find the majority of Treasuries trades have a zero haircut, reflecting the dealers' use of the Fixed Income Clearing Corporation's bilateral repo netting service.==== For equities (and to a lesser extent, corporate debt), the majority of trades have haircuts of −2, reflecting the fact that securities dealers source equities from securities lending agents, which typically demand securities loans to be backed by cash collateral exceeding the value of the securities by 2 percent.====With these stylized facts in mind, we turn to understanding the determinants of haircuts in this market, with a focus on trades involving Treasuries, equities, and corporate debt securities, the three largest asset classes in the data. We start by considering the haircut as a feature of the security delivered in the repo; in other words, we study whether each specific security in the market is traded with a unique haircut. For Treasuries transactions we find that the vast majority of specific securities are associated with multiple haircuts. In contrast, almost all equities and corporate debt securities are associated with a unique haircut.====We then examine the relationship between haircuts and default. One of the economic features of repos is that securities and cash are simultaneously exchanged, so that both parties have some protection from default: possession of securities protects the cash investor from the default of the securities provider; similarly, the cash held by the securities provider protects her from the failure of the cash investor to return the securities. We first measure whether the party that is over-collateralized in a transaction (that is, the party that holds collateral with a higher market value at inception) has set the haircut to be large enough to fully protect itself in case of a large, but plausible price swing in the security.==== We use historical price data to construct measures of these price swings (i.e. the size of the tail of the expected price distributions). For transactions involving Treasuries and corporate debt securities, we find that haircuts are usually set large enough to fully protect against default. For equities transactions, though, haircuts are mostly set too low to fully protect the over-collateralized party from the costs of default.====We then measure whether the variation in haircuts can be explained by differences in our measure of the potential price swings in the associated securities. We estimate the correlation between these price swing measures and haircuts, with the expectation that securities with large potential price swings would command larger haircuts (higher levels of over-collateralization). Our analysis yields mixed results; the expected relationship is only found for transactions where the cash investor is over-collateralized.====Finally, we explore the relationship between haircut and rates in our data. A negative relationship may be expected since a higher haircut implies a lower default risk for the cash investor, who would therefore require a lower interest rate. We do not find strong evidence of a negative relationship between haircut and interest rate in the data. Although haircuts and interest rates are negatively correlated, the estimated coefficient is quite small and it remains small even if we condition out for a variety of observable trade characteristics, such as counterparty heterogeneity and time effects.====Our analysis contributes to the growing literature focused on understanding the vital but still relatively unknown repo market. Our paper uses unique data and our focus on the bilateral repo market in novel, because most of the current literature focuses on tri-party repo. Gorton and Metrick (2012) illustrate the importance of repurchase agreements in the 2007-09 financial crisis; Copeland et al. (2014b), and Krishnamurthy et al. (2014) analyze activity in tri-party repo, an important funding market for larger broker-dealers, over the crisis. In addition, Hu et al. (2015) study the determinants of tri-party repo pricing. Auh and Landoni (2016) also study bilateral repo trading; however, their paper is a case study of a hedge fund that uses repo trades to borrow cash and finance private-label mortgage-backed securities. This asset class however, accounts for only a small share of the overall bilateral repo market, as shown by our dataset. Our work is focused on understanding the market more broadly and how collateral is used in trades involving securities from the most commonly used asset classes and across many different types of participants.====The remainder of the paper is structured as follows. Section 2 reviews the data, beginning with an overview of the characteristics of a repo, followed by a summary of how the data were collected, and then a description of the data and the U.S. bilateral repo market. Section 3 presents analysis focusing on the determinants of haircuts as well as the correlation of interest rates and haircuts. Section 5 concludes.",The use of collateral in bilateral repurchase and securities lending agreements,https://www.sciencedirect.com/science/article/pii/S1094202518303119,21 May 2019,2019,Research Article,243.0
"Gottardi Piero,Maurin Vincent,Monnet Cyril","University of Essex, United Kingdom of Great Britain and Northern Ireland,University of Venice, Italy,Stockholm School of Economics, Sweden,University of Bern, SZ Gerzensee, Switzerland","Received 24 May 2018, Revised 26 February 2019, Available online 7 May 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.04.009,Cited by (33),"We show that repurchase agreements (repos) arise as the instrument of choice to borrow in a competitive model with limited commitment. The repo contract traded in equilibrium provides insurance against fluctuations in the asset price in states where collateral value is high and maximizes borrowing capacity when it is low. Haircuts increase both with counterparty risk and asset risk. In equilibrium, lenders choose to re-use collateral. This increases the circulation of the asset and generates a “collateral multiplier” effect. Finally, we show that intermediation by dealers may endogenously arise in equilibrium, with chains of repos among traders.","Gorton and Metrick (2012) argue that the financial panic of 2007-08 started with a run on the market for repurchase agreements (repos). Lenders drastically increased the haircut requested for some types of collateral, or stopped lending altogether. This view was very influential in shaping our understanding of the crisis.==== Calls for regulation quickly followed.==== The mere possibility that a run on repos could lead to a financial market meltdown speaks to their importance for money markets. Overall, repo market activity is enormous. Recent surveys estimate outstanding volumes at ====5.4 trillion in Europe while calculations vary from $3.8 trillion to $5.5 trillion for the U.S.==== Repos are simple financial instruments used to lend cash against collateral. Repos allow borrowers to carry out leveraged purchases of assets, which are pledged as collateral to obtain cash, or to borrow securities. The main users of repos are large dealer banks and other financial institutions such as money market funds and hedge funds. For these reasons, repo markets have important implications for market liquidity, as Brunnermeier and Pedersen (2009) illustrate. Dealer banks also play a major role as repo intermediaries between cash providers and cash borrowers. Finally, most major central banks implement monetary policy using repos, thus contributing to the size and liquidity of these markets.====Technically, a repo contract is the sale of an asset combined with a forward contract that requires the borrower to repurchase the asset from the lender at a future date for a pre-specified (repurchase) price. The lender requires a haircut defined as the difference between the selling price in a repo and the asset's spot market price. Besides the haircut, a repo differs from a sale of an asset followed by a re-purchase of the same asset in the spot market at a later date because the repo price is pre-specified. While a repo looks very much like a simple collateralized loan, it has two additional and important features. It is a recourse loan and the borrower sells the collateral rather than merely pledging it.==== The lender thus acquires the legal title to the asset sold and so the possibility to re-use the collateral before the forward contract with the borrower matures.====Repos, as well as the practice to re-use the collateral, known as re-use or re-hypothecation, have attracted a lot of attention from economists and regulators alike.==== However, a proper understanding of the motivation of traders to enter repos and of the implications of collateral re-use is still needed. We propose a theory of repos that accounts for the basic features of these contracts to answer the following questions. Why are repos used over spot sales of the asset? How does collateral re-use impact leverage in the economy? Finally, why are repos intermediated, i.e. why do borrowers trade through dealer banks rather than directly with lenders and how do dealer banks fund their operations? Our model thus provides a basis to understand repos' potential contribution to systemic risk.====In this paper we analyze a simple competitive economy where some investors have funding needs, but are unable to commit to future payments. To satisfy their needs, they can use the assets they own, by either selling them in the spot market or in repo sales. Repo sales are characterized as loan contracts exhibiting the key features of repos described above. We show that in equilibrium investors prefer to trade repos rather than spot. Furthermore, in equilibrium investors exercise the option to re-use collateral. This expands the borrowing capacity of investors in the economy through a multiplier effect. Collateral re-use also affects the structure of the repo market: intermediation by safer counterparties, who use repos to fund their purchase of assets, may endogenously arise.====The model features two types of risk averse investors, cash poor investors (natural borrowers) and cash rich investors (natural lenders). Borrowers own an asset, whose future payoff is uncertain. A large variety of possible repo contracts, characterized by different values of the repurchase price, are available for trade. Due to borrowers' inability to commit, they may choose to default on these contracts. The punishment for default is the loss of the asset sold in the repo together with a penalty reflecting the recourse nature of repos and which varies with the borrower's creditworthiness. Hence there is a maximal amount that borrowers can credibly promise to repay, that depends on the future market value of the asset. The recourse nature of repo contracts implies that this maximal amount may exceed the future spot market price of the asset. This amount and the quantity of the asset held by investors determines then their borrowing capacity with a repo sale.====Lenders can re-use the collateral they acquired via repos, e.g. by selling it in the spot market. By returning collateral to the market, re-use allows borrowers to purchase more assets to pledge them again in repo sales to lenders. We find that allowing re-use, through the iteration of these transactions, generates a collateral multiplier effect, thus augmenting the borrowing capacity of borrowers. Hence, the benefits of re-use materialize when the asset is scarce.====We then characterize the repurchase price of the repo contract that investors choose to trade in equilibrium. Risk-averse investors value the ability to borrow but dislike fluctuations in the future asset price. Hence, two motives – a hedging and a borrowing motive – determine the equilibrium repurchase price. In the states where the market value of the asset is low, the ability to borrow is limited. There, the borrowing motive prevails and the repurchase price equals the maximal amount that borrowers can promise to repay. In the other states, where the asset price is high, their borrowing capacity is also high. Hence investors are not constrained and the hedging motive implies that the repurchase price is set at a level that ensures a constant level of consumption. In the absence of re-use, the repurchase price would thus be set at a constant level. When lenders re-use collateral they effectively sell the asset short in the spot market, and are thus exposed to asset price risk: we show that the equilibrium repurchase price also offsets this price exposure of lenders. These hedging and borrowing motives explain why investors prefer repo contracts over spot trades.====We derive comparative statics properties for equilibrium haircuts and liquidity premia. Haircuts increase when collateral is more abundant or when counterparty quality decreases, because riskier borrowers can credibly promise to repay lower amounts. We also show that riskier assets command higher haircuts and lower liquidity premia, since higher risk entails a worse distribution of collateral value across states relative to collateral needs. The effect of collateral re-use on haircuts and liquidity premia is ambiguous. On the one hand, re-use increases the amount of the asset that can be pledged as collateral and hence relaxes the borrowing constraint. This tends to decrease the liquidity premium and to increase the haircut. However, the fact that the asset can be re-used when pledged as collateral makes each unit of the asset more valuable. This tends to increase the liquidity premium. Collateral re-use also modifies the properties of the equilibrium repo contract, increasing the repurchase price, thereby lowering the haircut. These counteracting effects of re-use on the equilibrium spot price and repo price also explain why its overall impact on leverage is ambiguous.====In addition, our paper sheds light on the way in which dealer banks use repos to lever up and fund their activities. Dealers' leverage is closely related to their role in channeling funds between different investors. Dealer banks obtain funds to purchase assets by using these assets as collateral in repos. As a result, they only need to tap into their cash holdings to pay the repo haircut. Since haircuts are usually small, dealer banks can be highly levered. So using repos, dealers can intermediate between cash poor investors, e.g. hedge funds, and cash rich investors, e.g. insurance companies, or money market funds. As a result, dealer banks make for a significant share of the repo market.====To account for these trading patterns, we extend the model by introducing a third type of investors, to whom we refer as dealers. Dealers have limited cash and no asset, but a higher counterparty quality. We show that in this environment, under some conditions we identify, dealers emerge in equilibrium as intermediaries between natural borrowers and natural lenders. Even though they could trade directly, natural borrowers (say, hedge funds) prefer to sell the asset in the spot market to dealers. Dealers in turn pledge it as collateral in a repo with natural lenders (say, insurance companies) to obtain the funds necessary to purchase the asset. The emergence of dealer banks as leveraged intermediaries hinges on their superior counterparty quality.====Finally, we show that with collateral re-use intermediation may also occur via a chain of repo trades. In a repo chain, a natural borrower enters a repo with a dealer bank who in turn enters another repo with the natural lender. Intermediation via a chain of repos can arise when the dealer bank has both a higher counterparty quality than the natural borrower and is better able at re-deploying collateral than the natural lender. Then, through re-use, one unit pledged to the dealer bank can indeed support more borrowing in the chain of transactions. This explains why a natural borrower chooses to trade with dealers even when there are larger gains from trade with natural lenders.====Recent theoretical works highlighted some features of repo contracts as sources of funding fragility. As a short-term debt instrument to finance long-term assets, Zhang (2014) and Martin et al. (2014) show that repos are subject to roll-over risk. Antinolfi et al. (2015) show that the benefit of an exemption from automatic stay==== granted to repos may be harmful for social welfare in the presence of fire sales, a point also made by Infante (2013) and Kuong (2016). These papers usually take the trade of repurchase agreements and their specific features as given while we want to understand their emergence as a funding instrument.====One natural question is why borrowers do not simply sell the collateral to lenders? A first strand of papers explains the existence of repos using transaction costs (e.g. Duffie, 1996) or search frictions (e.g. Narajabad and Monnet, 2012, Tomura, 2016, and Parlatore, 2018). Bundling the sale and the repurchase of the asset in one transaction lowers search costs or mitigates bargaining inefficiencies. Bigio (2015) and Madison (2016) emphasize the role of informational asymmetries regarding the quality of the asset to explain repos: their collateralized debt features reduce adverse selection between the informed seller and the uninformed buyer as in DeMarzo and Duffie (1999) or Hendel and Lizzeri (2002). We show that investors choose to trade repos in an environment with symmetric information, where markets are Walrasian, but where collateral has uncertain payoff. One limitation of the works mentioned above is that the borrower chooses to sell repo if he can obtain more cash than in a spot sale of the asset, that is if the haircut is negative. Our analysis rationalizes the use of repos with positive haircuts when investors are risk-averse. In addition, we account for the possible re-use of collateral in repos by showing its benefits.====To derive the equilibrium repo contract, we follow the competitive approach of Geanakoplos (1996), Araújo et al. (2000), and Geanakoplos and Zame (2014) where the properties of the collateralized promises traded by investors are selected in equilibrium. Unlike these papers where the only cost of default is the loss of the collateral, our model aims to capture the recourse nature of repo transactions. We thus allow for additional penalties for default, some of them non-pecuniary in the spirit of Dubey et al. (2005). While our results on the characterization of repo contracts traded in equilibrium remain valid also in the absence of these additional penalties, the recourse nature of repos is crucial to explain re-use. Indeed, Maurin (2017) showed in a more general environment that the collateral multiplier effect disappears when loans are non-recourse.====Collateral re-use is discussed by Singh and Aitken (2010) and Singh (2011), who claim that it lubricates transactions in the financial system.==== At the same time, re-use generates the risk that the lender, who receives the collateral, does not or cannot return it when due, as explained by Monnet (2011). Unlike Bottazzi et al. (2012) or Andolfatto et al. (2017), we account for the double commitment problem induced by re-use. The increase in the circulation of collateral obtained with re-use also arises with pyramiding (see Gottardi and Kubler, 2015), where collateralized debt claims are themselves used as collateral. However, the mechanism is different: in pyramiding, no two sided commitment problem arises and the recourse nature of loans also plays no role. We stress the role of collateral re-use in explaining the presence of intermediation in the repo market, as in Infante (2015) and Muley (2016). Unlike in these papers, in our analysis intermediation arises endogenously since direct trade between borrowers and lenders is possible.====The structure of the paper is as follows. We present the model and the set of contracts available for trade in Section 2. We characterize the equilibrium and the properties of repo contracts traded in Section 3, where we also derive the properties of haircuts and liquidity premia. Section 4 shows that intermediation arises in equilibrium. Finally, Section 5 establishes the robustness of our findings to alternative specifications of the repurchase price and Section 6 concludes. The proofs are collected in the Appendix.","A theory of repurchase agreements, collateral re-use, and repo intermediation",https://www.sciencedirect.com/science/article/pii/S1094202518303065,7 May 2019,2019,Research Article,244.0
Lee Tomy,"Department of Economics and Business, Central European University, Hungary","Received 26 May 2018, Revised 15 February 2019, Available online 6 May 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.04.010,Cited by (6),"I examine the impact of cross-venue latency on market quality using a model of informed trader competition in a fragmented market. As cross-venue latency decreases, liquidity and price discovery improve while the expected profits of informed traders decline. Moreover, a fall in the latency of one venue can harm liquidity at the other venue. An extension predicts that, as the informed traders consolidate or outsource trading, benefits of shorter cross-venue latency are attenuated and its harmful effects intensify. My model generates testable predictions about the effects of changes in cross-venue latency on market quality.","Geographic distances separate trading venues and cause information to travel between them with a delay. This delay contributes to ====, “the time it takes to learn about an event, generate a response, and have the exchange act on the response” (Hasbrouck and Saar, 2013, p. 2). Recent innovations in computing and information technologies have drastically reduced the latency across venues. Yet cross-venue latency remains important: In 2010, Spread Networks spent $300 million to lay a fibre optic cable between Chicago and New York exchanges, shaving three milliseconds of latency. Two microwave broadcast connections cut another four milliseconds of latency the following year. Policy-wise, in 2018 US regulators approved the first private low orbit satellite networks, proposed in part to connect exchanges.==== Meanwhile certain trading venues impose artificial delays on incoming orders, called “speed bumps”, to increase latency.====Empirically, the recent decline in cross-venue latency has coincided with greater liquidity and price discovery (Chordia et al., 2011). For instance, trading costs fall and price discovery increases following expanded access to microwave broadcast connections (Shkilko and Sokolov, 2016), faster quote dissemination (Hendershott et al., 2011), entry of high frequency traders (Brogaard and Garriott, 2017; Boehmer et al., 2015) or the introduction of colocation services (Conrad et al., 2015; Brogaard et al., 2015).==== Moreover, institutional traders' profits decrease when cross-venue latency declines, even as measures of liquidity improve (Tong, 2015; Malinova et al., 2018).====In this paper, I explain how a decline in cross-venue latency can improve liquidity and price discovery, while reducing the profits of the institutional traders. A decline in cross-venue latency increases price discovery because the quality of information from other venues improve. Then the price at each venue becomes more sensitive to cross-venue information and less so to its own order flow. The lower sensitivity of prices to own venue order flows implies a smaller price impact, which improves liquidity. As price discovery increases, information rents for informed traders shrink. Given that the institutional traders are more likely to be informed than retail traders, my explanation can reconcile the increase in liquidity with the decrease in the institutional traders' profits documented empirically. Further, I show that a fall in the cross-venue latency of one venue can harm liquidity at other venues. For example, once a venue removes a speed bump or introduces colocation, the venue's price responds more quickly to orders executed elsewhere, the response times of the other venues' prices do not change and, under a certain condition, liquidity declines at those other venues.====My modeling approach is in the tradition of Kyle (1985). Potentially informed agents trade an asset across two venues in two stages. First, informed traders optimally choose the venue to enter. Second, trading occurs: Noise traders submit random order flows into each venue, and the informed traders select one's order size. The net order flow at each venue is competitively cleared by a market maker, who observes both venues' net order flows before setting her price. The market maker observes the order flow sent to her own venue perfectly, and the other venue's order flow noisily. A venue's ==== is the magnitude of the noise in its market maker's cross-venue observation of order flow. Thus the latency in the model captures a source of price inefficiency: The time needed for data and orders to travel between venues limits the amount of cross-venue information reflected in prices at any given moment.====When the latency of one venue falls, subsequent spillover effects can harm liquidity at the other venue. The spillovers are particularly harmful if the second venue has low latency. Once the latency decreases, there is reduced noise in the cross-venue order flow observed at the first venue. Consequently the informed traders expect smaller profits from entering that venue, and informed trading shifts towards the second venue. Two opposing effects result. First, the second venue's order flow becomes more informative relative to the first, so the second venue's price depends less on cross-venue and more on own venue order flow. In other words, the second venue substitutes cross-venue information with own venue order flow, which raises price impact and harms liquidity. This is the ====. Second, the informed orders comprise a larger share of the second venue's order flow, making this order flow more sensitive to the private information of the informed traders. Then the price at the second venue optimally becomes less sensitive to its own venue order flow, improving the venue's liquidity. This is the ====. If the second venue's latency is low, cross-venue information is important, and the substitution effect dominates.====Since my model shows how a decrease in the latency of an exchange can harm liquidity at other exchanges, my results help justify regulatory control over trading venues' latencies. In addition, the model provides a reason to measure the market quality effects of latency changes at one venue on the other venues. Most existing empirical analyses of trading venues' policy and technology shocks focus solely on the shocks' own venue impact.====The potential benefit to market quality of shorter cross-venue latency is attenuated, and its possibly harmful spillovers exasperated, if the informed traders optimally split orders across venues. An alternative set up in which the informed traders strategically submit orders to both venues indicates that price discovery need not improve as cross-venue latency declines. Furthermore, a decrease in one venue's latency always harms the liquidity of the other venue. Combined with the results of the main model, these findings suggest buy side institutions consolidating or outsourcing trades to specialist firms will dampen the benefit of lowering cross-venue latency on price discovery and intensify its negative spillover effects.====In prior models of learning across markets, prices are exogenously determined at some venues or the prices perfectly incorporate information from every venue. Pasquariello (2007) shows financial contagion can arise when market makers observe the total order flow across all venues. Baruch et al. (2007) study the strategic response of informed traders in the presence of correlated assets and without cross-venue observation of order flow. Goldstein et al. (2014) have traders with differential access to assets but each asset has one price. Their model can endogenously generate uninformed order flow. Koudijs (2015) builds a model in which all information arrives at one of two venues, and finds it can explain financial trading patterns between 18th century London and Amsterdam.====The closest paper to mine is Boulatov et al. (2013, hereafter BHL). They develop a two period model in which market makers do not observe cross-venue order flows contemporaneously; rather, the market makers observe the first period cross-venue order flows in the second period. Using this model, Boulatov et al. (2013) explain autocorrelations in portfolio returns. In Boulatov et al. (2013), the lag in the transmission of cross-venue information is fixed (equal to one period) and symmetric. In contrast, my main results examine the impact of asymmetric and symmetric changes in cross-venue latency.====I contribute to the literature in three ways. First, to the best of my knowledge, mine is the first model to explain concurrent liquidity and price discovery improvement observed in recent years. Second, my model can reconcile the decline in institutional traders' profits with the rise in liquidity that follow latency reducing shocks to exchanges. Third, I show how the fragmentation of informed traders across venues can aid price discovery, and diminish harmful spillovers from reductions in cross-venue latency.",Latency in fragmented markets,https://www.sciencedirect.com/science/article/pii/S1094202518303168,6 May 2019,2019,Research Article,245.0
Stacey Derek,"Ryerson University, Department of Economics, 350 Victoria Street, Toronto, Ontario, M5B 2K3, Canada","Received 25 May 2018, Revised 15 March 2019, Available online 2 May 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.04.008,Cited by (5),"A model of a decentralized market is developed that features search frictions, advertised prices and bargaining. Sellers can post ask prices to attract buyers through a process of directed search, but ==== there is the possibility of negotiation. Similarly, buyers can advertise negotiable bid prices to attract sellers. Even when transaction prices often differ from quoted prices, bid and ask prices play a crucial role in directing search and reducing trading frictions. The theory endogenizes the direction of search and provides insight about the prevalence of posted prices in the absence of full commitment by market participants to transact at the advertised price.","When ==== bargaining leads to transaction prices that diverge from original advertised prices, it seems sensible to question the very relevance of price posting. In this paper I develop a dynamic model of a decentralized market to show that even negotiable prices can be advertised strategically as a means of directing search and reducing trading frictions. I allow either side of the market to post prices so that the direction of search is determined endogenously and depends on the details of the bargaining procedure. I consider the implications of bilateral price negotiations for (i) posted and final prices, (ii) the set of traders posting prices in equilibrium, and (iii) market efficiency. Finally, I show that the features and price-related predictions of the model align well with aspects of a decentralized secondary market for a particular security: namely, transferable taxicab license plates in Toronto.====A considerable amount of economic activity transpires in frictional markets that feature transaction prices that do not always equal advertised prices. Examples of financial assets traded in such markets include fixed income securities and mortgage loans. Other non-financial examples include durable goods such as houses, used vehicles and appliances; uncommon or unusual items such as rare wine, fine art or antiques; and services such as those provided by skilled tradespeople and child caregivers. Prices are advertised by supply-side participants in some markets, while other markets feature prices posted by buyers. In the context of the financial markets referenced above, banks post prices/rates that appeal to savers (in the case of fixed income securities) and borrowers (in the case of mortgage loans). In non-financial contexts, platforms for classified advertisements often feature listings/prices from one or both sides of the market (====, “I am offering” versus “I am seeking” ads). To understand these seemingly puzzling observations, I develop a search model with posted prices and bargaining. The theory rationalizes the prevalence of price posting even when transaction prices differ from posted prices, and provides an explanation for why trading opportunities are proposed by one side of the market or the other. The model can even account for price posting on both sides of the market.====These insights and implications are derived from a dynamic model of a decentralized market characterized by a trading process with three key features: pre-match communication, search frictions, and a strategic method of price determination. Traders on either side of the market have the opportunity to post a public advertisement containing an ==== (a seller's quoted price or list price) or a ==== (a buyer's offer-to-purchase price). Next, buyers and sellers meet stochastically according to a bilateral matching technology. Search is ==== in the sense that searching traders first observe all public advertisements and then target a particular price in their search. An assumption of the model is that matches occur between traders that passively post prices and those that instead decide to actively search. Transaction prices are then determined in a bargaining game between a matched buyer and seller, where the posted price is interpreted as the initial offer in an alternating offer bargaining game. The initial advertisement conveys limited commitment to a price because the counterparty that has not engaged in pre-match price publication maintains the ability to trigger ==== negotiation.====In a setting where there is ==== uncertainty about a trader's relative bargaining strength, negotiated premiums/discounts relative to advertised bid/ask prices arise in equilibrium whenever bargaining favors the trading partner that is not constrained by a commitment to a posted price. It follows that the possibility of unfavorable outcomes in the bargaining procedure generates incentives for potential trading partners to direct their search towards a certain bid or ask price. If the seller's expected bargaining strength is sufficiently high, for example, an ask price effectively limits the seller's share of the surplus and can therefore be chosen strategically as a means of attracting a buyer. The strategic role of an ask price is therefore somewhat related to that in Chen and Rosenthal, 1996a, Chen and Rosenthal, 1996b and Arnold (1999), where a seller sets an asking price to effect a price ceiling which encourages a buyer to incur the cost of inspecting the item for sale.==== In an environment where sellers compete for buyers, Lester et al. (2017) show that an asking price mechanism provides both an appropriate means of attracting buyers and sufficient motivation to incur the inspection cost. Even in the absence of idiosyncratic values (observable or otherwise) and costly inspection, I show that an appropriately chosen ask price should appeal to buyers if it insures them against detrimental outcomes in price negotiations. Moreover, the opportunity to post a bid price permits a buyer to implement an analogous technique for seducing sellers. The details of the ==== bargaining procedure determine which traders post prices and which traders actively search in equilibrium. In other words, the direction of search is determined endogenously and depends on the expected division of bargaining power.====Constrained efficiency in this environment can be described by an optimal ratio of buyers to sellers given search frictions and the cost of market participation. Decentralizing the constrained efficient allocation requires a surplus-splitting rule that assigns to each trader their marginal contribution to the match (in expectation).==== Absent agreement to this division of surplus, ==== bargaining generically assigns too much or too little of the surplus to buyers, which distorts their entry decisions and reduces the joint surplus. Models of directed search typically provide incentive for agents to post and commit to mechanisms that maximize the joint surplus.==== The mechanism I consider allows for ==== negotiations arising from limited commitment to posted offers,==== but affords enough flexibility to deliver the constrained efficient level of surplus provided the appropriate side of the market engages in price posting (====, the side of the market that would otherwise capture too much surplus). The constrained efficient direction of search emerges endogenously in equilibrium. Bid prices are advertised in settings where, absent posted prices, ==== negotiations tend to favor the demand side of the market which creates incentives for too many market participants. The inverse is also true: ask prices are posted when bargaining favors the supply side of the market leading to less than the optimal ratio of buyers to sellers.====In the context of frictional financial markets, it is worth noting again that banks post prices/rates for financial products and services that appeal to both savers (====, fixed income securities) and borrowers (====, mortgage loans) even though customers regularly negotiate premiums/discounts. The model presented here offers a search and bargaining related explanation premised on banks having an advantage over consumers in price/rate negotiations: a plausible assumption if the frequency of negotiations is a source of bargaining prowess. A straightforward extension of the theory to an environment with ==== trader heterogeneity can account for the coexistence of bid and ask prices in equilibrium. An implication that follows is that ask prices exceed the average transaction price, whereas bid prices lie below the average sale price. As an interesting illustration of these outcomes, I consider the secondary market for transferable standard taxicab licenses (STLs) in Toronto. These are license plates that can not only be used by an owner-driver to operate a taxicab vehicle in Toronto, but can also be leased, rented to shift drivers or transferred to a new owner by means of a transaction in the secondary market. The assets being exchanged in this market (====, the standard taxicab licenses) are homogeneous, but are nevertheless traded in a decentralized manner; buyers and sellers publish negotiable prices and search for each other by means of an online platform for classified advertisements. The advertised prices and transaction data support the theoretical implications insofar as advertised ask prices are typically higher than transaction prices, while advertised bid prices are lower. The qualitative characteristics of the price distributions lend support to the mechanism proposed in the theory based on strategically chosen bid and ask prices that direct search but are subject to bilateral negotiation.====The remainder of the paper is organized as follows. Section 2 describes the model environment. The equilibrium strategies for bargaining and price posting are characterized in Section 3. Section 3 also defines a steady state directed search equilibrium with posted prices. The positive implications of the model are illustrated in Section 4 by means of simulated examples and special cases. Section 5 extends the model to include variation in both the size and division of the match surplus by allowing for ==== heterogeneity in holding costs. Section 5 also presents evidence from the secondary market for STLs and argues that aspects of this market align well with the features and predictions of the theory. Section 6 concludes.","Posted prices, search and bargaining",https://www.sciencedirect.com/science/article/pii/S1094202518303107,2 May 2019,2019,Research Article,246.0
"Bethune Zachary,Sultanum Bruno,Trachter Nicholas","University of Virginia, United States of America,Federal Reserve Bank of Richmond, United States of America","Received 24 May 2018, Revised 11 March 2019, Available online 18 April 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.04.003,Cited by (10),"We model asset issuance in over-the-counter markets. Investors buy newly issued assets in a primary market and trade existing assets in a secondary market, where trade in both markets is over-the-counter (OTC). We show that the level of asset issuance and its efficiency depend on how investors split the surplus in secondary market trade. If buyers get most of the surplus, then sellers do not have incentives to participate in the primary market in order to intermediate assets and the economy has a low level of assets. On the other hand, if sellers get most of the surplus, buyers have strong incentives to participate in the primary market and the economy has a high level of assets. The decentralized equilibrium is inefficient for any splitting rule. The result follows from a double-sided hold-up problem in which it is impossible for all investors to take into account the full social value of an asset when trading. We propose a tax/subsidy scheme and show how it restores efficiency. We also extend the model in several dimensions and study the robustness of the inefficiency result. Finally, we explore the effects of the inefficiency using numerical examples. We study how bargaining power and trading speed in the secondary market affect the efficiency result, and we notice some interesting implications for policy interventions aimed to restore efficiency to OTC markets.","Many assets, both real and financial, are traded in secondary over-the-counter (OTC) markets after their initial issuance (e.g. real estate, municipal bonds, treasuries, asset-backed securities, etc.). Further, many of these markets experienced severe volatility during the 2008 financial crisis, and several policies were enacted that aimed to directly support the issuance of new assets. For example, the Federal Reserve created the Term Asset-Backed Securities Loan Facility (TALF) to support the issuance of asset-backed securities collateralized by different types of private loans, and the Commercial Paper Funding Facility (CPFF) to support the issuance of commercial paper.==== While there is a large literature studying OTC markets (see Duffie et al. (2005), Duffie et al. (2007), Lagos and Rocheteau (2009), and Hugonnier et al. (2014), among others), most studies in this literature assume a fixed supply of assets, a nonstarter in understanding the effects of policies aimed to spur issuance. In this paper, we study how the trading of seasoned assets in secondary OTC markets affects their primary issuance and, in turn, aggregate asset supply and welfare.====We emphasize two frictions in OTC trade: (i) searching for counterparties to trade, which takes time, and (ii) conditional on a trade opportunity, the terms of the trade are determined by bargaining. We explore the canonical economy of Duffie et al. (2005) but for two differences: (i) we abstract from competitive market-makers in order to make the model more tractable (as in Duffie et al. (2007)), and (ii) we introduce the notion of issuers—agents who have a technology to issue new assets. In the model, trade occurs in pairwise meetings and these meetings are subject to frictions. We interpret meetings between an investor and an issuer as occurring in the primary market since they involve the potential issuance of a new asset. Likewise, we interpret meetings between two investors as occurring in the secondary market as they involve a transfer of a previously issued asset. When two agents meet, either in the primary or secondary market, the terms of trade are determined by Nash bargaining.====We solve for the decentralized equilibrium in the economy and compare it with the constrained efficient allocation, which is the welfare-maximizing allocation constrained by the search frictions. Surprisingly, we find that the decentralized equilibrium allocation is never constrained efficient. This conclusion holds even though trade in the secondary market is constrained efficient when the asset level is fixed, consistent with the literature cited above. Under search and bargaining, the prices at which investors trade in the secondary market do not reflect the social return of assets. When asset supply is fixed, this mispricing is irrelevant in determining the allocation—assets flow from low-valuation agents to high-valuation agents—and equilibrium is constrained efficient. When we introduce issuance, this mispricing affects investors' incentives to buy assets in the primary market, which distorts the asset allocation across investors. This distortion of asset allocation further affects the mispricing of assets in the secondary market and in turn affects issuance. In the end, we show that the allocation of assets across investors is inefficient in any decentralized equilibrium—regardless of investors bargaining weights when trading.====The inefficiency we find can be interpreted as the result of a double-sided hold-up problem. A hold-up problem, as first described by Willianson (1975) and Klein et al. (1978), arises when one party must bear the entire cost of an investment while others share in the payoff. In markets with search frictions, hold-up problems arise often because investments must be made ex-ante, before agents meet. For instance, in monetary search models (e.g., Lagos and Wright (2005); Rocheteau and Wright (2005); Aruoba et al. (2007)) agents acquire money balances before trading with sellers, and in labor search models (e.g. Acemoglu (1996); Masters (1998); Acemoglu and Shimer (1999)) firms or workers invest in capital before negotiating wages.====In our environment, the hold-up problem is two-sided, faced by both buyers and sellers in the secondary market. This occurs as both buyers and sellers must make specific “investments” before trade. Sellers in the secondary market create surplus when they buy assets from issuers and resell them to high valuation investors in the secondary market. That is, these agents create surplus by intermediation. However, because bargaining in the secondary market happens ex-post – after the agent acquires the asset – sellers only receive a share of the gains from trade, resulting in a hold-up problem. In fact, this is the standard hold-up problem. To fix this hold-up problem, the bargaining outcome needs to assign all the trade surplus to the seller in the secondary market.====While sellers in the secondary market create surplus when they buy assets from issuers in the primary market, buyers in the secondary market can destroy surplus when they buy assets from issuers instead of waiting to buy them from sellers in the secondary market. This occurs because, when buying the asset from issuers, this agent is destroying the surplus that could be created by intermediation. Since bargaining in the secondary market happens ex-post, these buyers only receive a share of the gains from trade, resulting again in a hold-up problem. This is a less standard hold-up problem. Here, unlike with more standard hold-up problems, the sunk cost underlying the inefficiency does not come from making an early investment – such as acquiring an asset from issuers, but from ==== making it. Although it looks different, the inefficiency is essentially the same. Fixing this inefficiency requires to assign all the trade surplus to the buyer in the secondary market.====The difference between a one-sided hold-up problem and our two-sided hold-up problem is that the inefficiency that follows from the former can be solved by an appropriate choice of the trade surplus sharing rule, while the inefficiency that follows from the latter cannot be solved by any sharing rule of the trade surplus: While the one-sided hold-up problem requires to assign all trade surplus to sellers, the double-sided hold-up problem that we discuss requires to assign full trade surplus to both buyers and sellers, which is clearly not feasible. In other words, when faced with the double-sided hold-up problem, there is no trade surplus sharing rule that makes the decentralized equilibrium constrained-efficient.====Trade is inefficient for any bargaining rule, however the direction of the inefficiency crucially depends on the way buyers and sellers split the surplus in the secondary market. We show that when the secondary-market sellers have all the bargaining power, investors overvalue assets, and issuance and intermediation are inefficiently high; and, when the secondary-market buyers have all the bargaining power, there are little gains to buy newly created assets in order to resell, and issuance and intermediation are inefficiently low.====Our result provides a rationale for the types of intervention in OTC markets that were observed during the 2008 financial crisis. This rationale is independent of additional frictions, such as private information (see Camargo and Lester (2014), Chiu and Koeppl (2015), Chang (2017), and Bethune et al. (2016), among others). To highlight the role of intervention, we propose a simple government policy that individually corrects the double-sided hold-up problem and decentralizes the constrained efficient solution. Since low-value investors do not fully internalize the gain in intermediating assets, the government subsidizes their asset holdings. Likewise, since high-value investors do not fully internalize their outside option value of waiting to buy assets in the future, the government taxes their asset holdings. The budget is balanced through lump-sum taxation.====We find it natural to interpret low-valuation investors in our economy as intermediaries, and high-valuation investors as customers. Viewed through this lens, the model is set up to examine the efficiency properties of markets in which customers do not have direct access to primary issuance. This interpretation of the market's participants also helps us to better understand the nature of the double-sided hold-up problem. We show that the decentralized equilibrium can be made efficient by an appropriate choice of bargaining power in the secondary market in economies where, for some technological reason, only low-valuation investors – that is, intermediaries – access the primary market. This follows because, once only the low-type investors access the primary market, the double-sided hold-up problem reduces to a single-sided hold-up problem, which can be handled by an appropriate design of the institutions governing bilateral trade.====We explore the effects of the inefficiency using numerical examples. First we study how the magnitude of the inefficiency, or the gains from intervention, depend on the way the surplus is split between buyers and sellers in secondary market trade. We find a U-shaped pattern; the double-sided hold-up problem is the most severe when the gains from trade are shared unevenly in the secondary market. For intermediate values of the bargaining power, the inefficiency is reduced, but does not vanish. We then study how the gains from intervention depend on trading speed in secondary markets. We find that the inefficiency is hump-shaped in trading speed; there is no role for intervention when secondary market trade is shut down or goes to infinity and the inefficiency is most severe in slow markets. However, importantly, we show that the gains from intervention converge slowly to zero as trading speed goes to infinity. Hence policies solely focused on increasing the speed of trade may have limited effects as a result of bilateral trade and bargaining.====The paper is structured as follows. Section 2 introduces the environment. Section 3 defines a decentralized equilibrium and discusses how bargaining determines equilibrium asset allocations. Section 4 describes the constrained efficient benchmark and compares it with the decentralized equilibrium. Section 5 discusses a government intervention to decentralize the constrained efficient outcome. Section 6 studies efficiency in a version of the model in which the primary market is only open to intermediaries. Section 7 examines a numerical exploration of the model. Finally, Section 8 concludes. We provide proofs of all results in the paper in Appendix A.====  Following Duffie et al. (2005), the OTC literature has mostly focused on studying trading dynamics in decentralized markets with no meaningful issuance margin and an exogenous supply of assets. For example, Lagos and Rocheteau (2007) and Gârleanu (2009) feature unrestricted asset holdings but leave the aggregate asset supply constant. He and Milbradt (2014) include debt maturity but assume that firms reissue assets to replace maturing debt. Recent work has started to include a meaningful role for asset issuance in an OTC setting. Arseneau et al. (2015) examine similar questions as we do in a three-period model in which assets are created in a primary market subject to a costly state verification problem. Alternatively, we characterize equilibrium dynamics in infinite time in which assets are allocated in a frictional primary market. Geromichalos and Herrenbrueck (2016) also consider an environment with asset issuance and decentralized secondary markets, but their focus is on the determination of liquidity and not on efficiency or policy.====Our efficiency results have a similar flavor to those in the OTC literature that introduce some endogenous extensive margin in trade. For instance, Lagos and Rocheteau (2007) consider an environment in which traders' asset holdings are unrestricted and assets are reallocated between traders through a competitive inter-dealer market.==== They find that under free entry by dealers, the decentralized equilibrium cannot implement the constrained-efficient solution. Efficient entry requires that dealers' bargaining power equal their impact on matching, a la Hosios (1990). However, positive dealer bargaining power leads to a hold-up problem by traders as a result of ex-post bargaining.====Gofman (2014) also considers the efficiency of OTC markets but in an environment in which agents can trade bilaterally according to an exogenous network structure. If the network is complete, in that all traders can trade directly with each other, then the equilibrium is efficient for any set of bargaining powers. However, if the network is incomplete and bargaining powers are strictly between zero and one, a hold-up problem arises. Traders do not internalize the full gain of transferring the asset on valuations further along the network and, as a result, assets may not end up with the highest valuation traders, lowering welfare. Efficiency can be restored for any connected network when sellers possess all the bargaining power. That happens because only sellers face a hold-up problem. We show, however, that introducing asset issuance necessarily introduces an inefficiency that cannot be restored for any set of bargaining powers because both buyers and sellers face a hold-up problem.====Double-sided hold-up problems have been studied in the context of the labor market in which firms and workers make investment decisions before matching and determining wages. Acemoglu (1996) shows that random search and ex-post bargaining lead to social increasing returns in the production technology that create an externality since the gains from trade must be split. This leads to a similar result that efficiency cannot be restored by choosing the right bargaining power. Masters (1998) shows this type of inefficiency always leads to underinvestment in physical and human capital. Alternatively, in the context of our asset market there could be under- or over-investment since there are not generally increasing returns to investment on both sides of the market. One-side of the market tends to under invest in assets (sellers in the secondary market) and the other side tends to over invest (buyers in the secondary market). We show that bargaining power has an important role in determining the shape of inefficiency.====Recent work has highlighted how the presence of intermediaries in OTC markets with random search can sometimes lead to inefficiency. Farboodi et al. (2017) endogenize contact rates in Duffie et al. (2005) and also find that the equilibrium is, in general, inefficient. Traders inefficiently invest in contact rates as a result of a search externality: they do not internalize that increasing their contact rate affects the distribution of other traders' contacts. A Pigouvian tax that charges traders when they make contact and uses the revenue to supplement the gains from trade decentralizes the Pareto optimum. Our efficiency result is similar in that the planner would like to increase the size of the surplus in any trade, however the tax/transfer scheme in Farboodi et al. (2017) only achieves the optimum in the case with symmetric bargaining weights. Further, Farboodi et al. (2017) do not consider issuance or endogenous asset supply.====In Menzio et al. (2016), traders meet randomly but differ with respect to their ability to commit to take-it-or-leave-it offers, and as a result, their bargaining power. If types, or bargaining powers, are exogenous, then equilibrium is efficient since (i) bilateral trade is efficient and (ii) all traders will meet each other, almost surely.==== However, if bargaining types are endogenous and commitment requires a sunk cost, equilibrium is inefficient.====Also related to our work, Nosal et al. (2014) and Nosal et al. (2016) study an environment with endogenous intermediaries where efficiency only arises if bargaining weights satisfy a version of the Hosios (1990) condition.",Asset issuance in over-the-counter markets,https://www.sciencedirect.com/science/article/pii/S1094202518303041,18 April 2019,2019,Research Article,247.0
"Afonso Gara,Armenter Roc,Lester Benjamin","Federal Reserve Bank of New York, United States,Federal Reserve Bank of Philadelphia, United States","Received 29 May 2018, Revised 29 March 2019, Available online 18 April 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.04.004,Cited by (26), and trading volume in the federal funds market as the supply of aggregate reserves shrinks. We find that these outcomes are highly sensitive to the dynamics of the distribution of reserves across banks.,"In response to the Great Recession, the Federal Reserve resorted to a number of unconventional policies that drastically changed the landscape of the federal funds (FF) market. Prior to 2008, depository institutions actively relied on the FF market, borrowing to satisfy their reserve requirements and payments needs, or lending to avoid holding unremunerated excess reserves. Trading volume in the FF market was robust, averaging more than $250 billion per day, and the majority of trades occurred between banks.==== In this environment with ====, monetary policy implementation was fairly straightforward: The Open Market Trading Desk at the Federal Reserve Bank of New York would implement the desired target for the effective federal funds rate (EFFR) by adjusting the supply of reserves via open market operations.====In the wake of the 2008 financial crisis, the large-scale asset purchase programs left most depository institutions awash with excess reserves. As a result, trading activity between banks became rare, and volume in the FF market dropped substantially, to $75 billion or less per day. With few trades occurring between banks, activity in the FF market has been dominated by government-sponsored enterprises (GSEs) looking for some yield on their overnight balances. In this environment with ====, the Fed came to rely on two new policy levers to implement its desired target range for the EFFR: the rate of interest on reserves (IOR), offered exclusively to eligible depository institutions, was set at the top of the target range; and the rate of return at the overnight reverse repurchase (ON RRP) facility, which is available to an expanded set of counterparties including GSEs and some money market funds, was set at the bottom of the range.==== The interest rate at the discount window—where depository institutions are able to borrow—became essentially irrelevant.====In addition to the large-scale asset purchase programs and the advent of these new policy levers, the FF market has also changed as a result of enhanced regulatory requirements. In particular, FDIC insurance fees have made banks even more reluctant to borrow funds, and liquidity requirements have created incentives for banks to maintain substantial buffers of reserves (and other high-quality liquid assets). Hence, these regulations have only reinforced the shift in the FF market away from its pre-2008 landscape, in which robust bank-to-bank lending prevailed at rates above the interest rate available on overnight reserves (which was zero at the time).====In September 2014, however, the Federal Open Market Committee (FOMC) presented a strategy to shrink or “normalize” the Fed's balance sheet from its current exceptional levels. Given the tight link between the asset holdings of the Fed and the supply of reserves held by banks, a number of crucial questions emerge. For example, as the Fed's balance sheet shrinks and reserves become more scarce, will bank-to-bank lending in the FF market resume? If so, how much will the balance sheet have to shrink before this happens? What policy tools will be needed to ensure interest-rate control, both in the long run and during the transition? How do changes in regulatory requirements affect the answers to all of these questions?==== ====We develop a simple model that is capable of reproducing the main features of the federal funds market in regimes with either scarce or abundant reserves, as observed before and after 2008, respectively. We use this model as a laboratory to quantitatively evaluate the future conditions in the federal funds market in response to changes in the supply of reserves, policy rates, and regulatory requirements.====We capture the over-the-counter nature of the FF market using a random-search model with two types of market participants: depository institutions (or “banks”) and non-depository institutions (or “GSEs”). We assume that GSEs are homogeneous and always looking to lend in the overnight market. However, we allow for relatively rich heterogeneity across banks, and ascribe a central role to the decision of each bank to approach the FF market as a lender or a borrower.====In an environment with scarce reserves, banks with excess balances look to lend out funds to those banks with temporary shortfalls in their reserve holdings. Naturally, banks with large balances are willing to lend out funds at a rate above their outside option—the IOR rate—and banks with temporary shortfalls are willing to borrow at a rate below their outside option—the discount window rate. Hence, in an environment with scarce reserves, there are gains from trade between banks. In equilibrium, this implies robust trading volume in the FF market, driven by bank-to-bank trades; the median traded rate exceeds the IOR rate; and the EFFR is sensitive to small adjustments to the aggregate supply of reserves.====In contrast, when reserves are abundant, few (if any) banks find it profitable to lend, as there are little gains from trading with other banks. Instead, banks look to borrow from a GSE and realize arbitrage profits between the ON RRP rate and the IOR rate. Since there is little or no trade between banks, volume in the FF market is almost reduced to the funds provided by GSEs, and the EFFR typically trades below the IOR rate.====Naturally, each bank's decision to borrow or lend in the FF market depends on its own level of reserves, along with the supply of liquidity coming from GSEs and the spread between policy rates, which determines the potential profits from arbitrage. Moreover, because of the over-the-counter nature of the FF market, a bank's decision to borrow or lend also depends on the distribution of reserves across other banks looking to lend and looking to borrow. Banks' decisions, in turn, determine the market composition as well as the traded rates and market volume. This reinforcing mechanism is what allows the model to reproduce ==== the varying landscape of the FF market as a function of the aggregate supply of reserves, policy rates, and other factors.====However, the important questions posed above ultimately require ==== answers. To meet this challenge, we start with a careful calibration of our model in an environment with abundant reserves. We use publicly available data from Call Reports for the period 2015-2016 to estimate the empirical distribution of excess reserves across banks, and a host of other observations and existing estimates to discipline the remaining parameters. We find that the model is able to match the observed distribution of market rates and trading patterns quite well. We also check the calibrated model's implications for an environment with scarce reserves by shifting the aggregate supply of reserves to the levels observed in 2002-2006.==== We confirm that the model reproduces the hallmarks of the scarce-reserves or classic “corridor” regime: FF rates, largely determined by bank-to-bank trades, lie between the IOR rate and the discount window rate; trading volume is elevated; and small open market operations are an effective instrument for controlling market rates.====For our main policy exercise we trace the evolution of the FF market as we reduce aggregate excess reserves from its current levels down to $200 billion. Doing so requires us to specify the complete dynamics of the distribution of excess reserves across banks along the path. In our baseline analysis, we find that the banks with the largest balances return to lending funds when aggregate excess reserves reach about $800 billion, kick-starting a resurgence in FF volume. As excess reserves continue to decline, the EFFR quickly rises above the IOR rate—somewhere between $700 billion and $800 billion—as bank-to-bank trades necessarily execute at rates above the IOR rate. This is an important event, as it marks the end of the implementation framework with abundant reserves that equates the IOR rate to the top of the target range for the EFFR. However, the level of aggregate excess reserves must decrease an additional $350 billion or so before the EFFR rises more than 5 basis points above the IOR rate and becomes responsive to small open market operations—what would be the hallmark of a classic corridor system.====Importantly, we find that the evolution of the FF market is sensitive to the dynamics of the distribution of excess reserves across banks, which are difficult to anticipate. In particular, the extent to which the largest banks hoard reserves is crucial to determine when the EFFR rises above the IOR rate. By varying the rate at which banks with higher balances reduce their holdings of reserves, relative to those banks with lower balances, the EFFR can first drift above the IOR rate with as few as $400 billion in aggregate excess reserves, or as much as $1 trillion.====As far as we know, Kim et al. (2017) is the only other work to attempt a similar exercise, though their approach is exclusively theoretical and based on a centralized market. As we do, Kim et al. (2017) carefully model the borrowing costs imposed by the FDIC fees. However, they choose to emphasize the possibility that interbank trading never returns. Such a scenario is actually possible in our model, but appears extremely unlikely: It would require either very high balance sheet costs, such that no bank wants to borrow, or a near-degenerate distribution of reserves across banks, such that there are no gains from trade among them.==== ====There is a long tradition of research on the FF market starting with Poole (1968). Most existing work models the FF market as a centralized, Walrasian market and studies regimes with scarce reserves. Recent contributions with a focus on the interbank market include Furfine (1999) and Whitesell (2006), among many others. Assuming a centralized market also allows for embedding the FF market into a general equilibrium model, as in Martin et al. (2013), Ennis (2014), and Bech and Keister (2017).====Starting with Ashcraft and Duffie (2007), several recent models have aimed to capture the over-the-counter nature of the FF market. Given the historical precedence, however, most of these papers focus on regimes with scarce reserves; see, among others, Berentsen and Monnet (2008), Ennis and Weinberg (2013), and Afonso and Lagos (2015).==== The current regime with abundant reserves, and its implications for the federal funds rate, has only recently been studied in Bech and Klee (2011), Armenter and Lester (2017), and Williamson (2018). Given this context, the current paper can be viewed as bridging existing work on the FF market as an over-the-counter market under both scarce and abundant reserves.","A model of the federal funds market: Yesterday, today, and tomorrow",https://www.sciencedirect.com/science/article/pii/S1094202518303223,18 April 2019,2019,Research Article,248.0
Neklyudov Artem,"University of Lausanne and SFI, Switzerland","Received 26 May 2018, Revised 4 April 2019, Available online 15 April 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.04.005,Cited by (34),"This paper studies heterogeneity in search technology among dealers in an over-the-counter (OTC) market. Empirical evidence suggests that in many OTC markets dealer networks have a core-peripheral structure with customer bid-ask spreads depending on whether a customer trades with a more central dealer or a more peripheral dealer. The paper develops a baseline search and matching model, which produces a robust centrality discount—lower bid-ask spreads offered by more central dealers. When dealers are allowed to move sufficient amounts of trading capital between their idiosyncratic states the same model parameters may produce a centrality premium. Numerical analysis of the model shows the effects of different underlying distributions of the heterogeneity in search technology on dealer networks and welfare.","In over-the-counter (OTC) markets for corporate bonds, municipal bonds, securitizations, etc., customers do not trade with one another directly. Instead, customers trade bonds with broker-dealers, registered with a specific regulatory authority, such as Financial Industry Regulatory Authority (FINRA) and Municipal Securities Rule-making Board (MSRB). There are 3,746 dealers registered with FINRA alone.==== Some dealers have larger trading capital and better infrastructure than others, pertaining to a very heterogeneous search technology with an impact on the overall market quality. Customers in corporate bond markets, municipal bonds markets, and securitizations markets face significantly different bid-ask spreads when trading with more central and more peripheral dealers. On one hand, one of the first empirical evidence on this topic in Li and Schürhoff (2019) shows that there is a centrality premium in municipal bonds markets: More central dealers charge up to 80% higher bid-ask spread for medium-size customer trades. Di Maggio et al. (2017) document a centrality premium in corporate bond markets. On the other hand, the empirical evidence in Hollifield et al. (2017) shows that there is a centrality discount in the more sophisticated asset-backed securities and securitizations markets: More central dealers charge smaller bid-ask spread to customers.====In this paper, I present a search-and-matching model of an OTC market with heterogeneous search technology among dealers and a decentralized interdealer market. Agents in the model are risk-neutral and their asset holdings are limited to zero or one unit. Trading gains are due to idiosyncratic shocks to agents' preferences for holding the asset. Idiosyncratic preference shocks occasionally reshuffle agents between a high or low liquidity state. In the high-liquidity state the agent receives a high flow utility from holding the asset, an agent without the asset in the high-state wants to buy one from somebody else. In the low-liquidity state the agent receives a low flow utility from holding the asset, an agent without the asset in the low-state wants to sell one. As in Duffie et al. (2005), Vayanos and Wang (2007), Weill (2007), Shen and Yan (2015), such shocks capture changes in individual liquidity needs or hedging motives of a customer or a dealer. Preference shocks result in a temporary asset misallocation.====Customers trade only with dealers, dealers trade with each other and customers. Search technology of dealers reflects the amount of trading capital and other related infrastructure in the possession, and the preference shocks reflect liquidity needs, hedging motives, rebalancing portfolios that in one way or another motivate agents to trade the asset with someone else in the population who they happen to meet randomly from time to time. Dealers with a more efficient search technology become central dealers in the trading network, dealers with a less efficient search technology form the periphery. Differences in search technology among dealers create additional trading gains between central and peripheral dealers even when they are in the same liquidity state, a novel feature of my model as compared to e.g. Duffie et al. (2005), Vayanos and Wang (2007) where trading is limited to agents in opposite liquidity states.====Decentralized interdealer market and heterogeneity in search technology interact together to produce a robust finding of a centrality discount in equilibrium of a baseline model. There are two channels: the effect of search intensity on 1) the asset allocation in the population, and 2) on the reservation values of dealers. Dealers with a higher search intensity tend to have a mismatch between their asset holding and their liquidity state. This implies better average terms of trade for customers who trade with more central dealers. More peripheral dealers offer higher ask prices and lower bid prices because their outside options to search again for the counterparty are weaker. Thus trades with peripheral dealers associate with relatively low gains for customers. Together, the two channels imply a centrality discount. The magnitude of the centrality discount is zero when the interdealer sector is too small; when dealers have full bargaining power with customers; when dealers do not relocate their individual search intensity between liquidity states. A notable feature of my model is its ability to produce a centrality premium for the same model parameters after a slight modification in the environment. I show how a modified environment with dealers having extra flexibility to relocate their trading capital among states is capable of producing a centrality premium.====Customers in my model are agents who are exposed to the search friction of the market the most, and have no ability to trade with each other directly. I assume the rate of bilateral meetings among agents is proportional to the sum of the two counterparties' individual search intensities, and that the search intensity of customers is zero. Literally it means customers never contact anybody in the population, instead they get contacted by others. However, the key predictions and findings are robust to such normalization. In real markets customers rebalancing bonds or derivatives often pick up the phone first to call the dealer, however, by any measure customers are less active market participants than dealers. Notably, in my model all central and peripheral dealers have the same bargaining power with customers.====The paper contributes to the body of search literature about OTC markets. Duffie et al. (2005) develop the seminal search-and-matching model of an OTC market, and derive bid-ask spreads charged by dealers who have access to a frictionless interdealer market. Hugonnier et al. (2016) develop a generalized model, where agents' preferences differ continuously within the population, and agents with a median preference type arise endogenously as intermediaries. In my model agents differ continuously in their search intensity. Üslü (2016) develops a model with both preference and search intensity dimensions together with unrestricted asset holdings, and delivers predictions about centrality discount and centrality premium as well. In his model agents can trade infinitely small amounts of the asset, so almost any pair of agents have gains from trading. In my model agent can only trade one unit of the asset, and an endogenous measure of agents have gains from trading. Dunne et al. (2015) characterize dealers' intermediation role and inventory management between the monopolistic customer market and a frictionless interdealer market. In contrast, the interdealer market in my model is not frictionless, and dealers with different search intensities have different reservation values in equilibrium. Atkeson et al. (2015) characterize a single-period trading among banks with heterogenous exposures to the aggregate default risk in credit-default swap (CDS) contracts. They show how an interdealer market in equilibrium features a lower price dispersion than the customer-to-dealer market, which has similarities with some implications of my model. Babus and Hu (2017) develop a model of the endogenous formation of a central broker-dealer when agents are allowed to invest in trading relationships. My model is related to Gofman (2014) in that trade prices are outcomes of a bilateral bargaining and are affected by dealers' private asset values. However, in my model dealers match with their counterparties randomly, and the realized trading network is random. Zhu (2012) explores pricing implications of a ringing phone curse when sellers contact buyers sequentially with a possibility of a repeat contact. In my model, the market there is no repeat contact and the reputation effects do not occur. Babus and Kondor (2018) develop a model where a centrality discount arises due to adverse selection, while in my model there are no information asymmetries among agents.====The organization of the paper is as follows. Section 1 describes the environment with a heterogeneous search technology. Section 2 develops two tractable special cases and presents an algorithm for solving the general model. Section 3 presents the analysis of customer bid-ask spreads, numerical analysis of dealers' degree centralities and welfare effects of heterogeneous search technology. Section 4 concludes.",Bid-ask spreads and the over-the-counter interdealer markets: Core and peripheral dealers,https://www.sciencedirect.com/science/article/pii/S1094202518303156,15 April 2019,2019,Research Article,249.0
Peng Qiusha,"Fanhai International School of Finance and School of Economics, Fudan University, China","Received 6 May 2017, Revised 25 March 2019, Available online 4 April 2019, Version of Record 12 April 2019.",https://doi.org/10.1016/j.red.2019.03.010,Cited by (8),"With a focus on the entry channel, this paper investigates the role of business deregulation and financial reform in China's credit and stock markets in explaining the rapid economic growth of China over the past twenty years. A dynamic general equilibrium growth model with heterogeneous consumers and firms is developed. Quantitative results using firm-level data show that the structural reforms that facilitated business formation and growth led to significantly higher aggregate output. This was driven by resource reallocation resulting from stronger market competition, in particular caused by the massive influx of new firms. Policy analysis shows that further reform could also have a large impact.","China's economy has grown incredibly rapidly in recent years. The average annual growth rate of real GDP per capita between 1990 and 2010 was 9.5% while average global growth in the same period was just 3.6%. Furthermore, from 1952 through 1990, the Chinese economy grew at an average rate of only 4.5% per year.==== A great deal of effort has been aimed at understanding this striking and important phenomenon. In particular, Hsieh and Klenow (2009) and Song et al. (2011) have focused on the reduction of misallocation among manufacturing firms in China. Specifically, Hsieh and Klenow (2009) empirically documents this pattern, and Song et al. (2011) models the resource reallocation induced by the reduction of credit frictions.====Along this line of research, this paper uses a structural macroeconomic framework to investigate the role of structural reforms—business deregulation and financial development in both the credit market and the stock market—in advancing reallocation and thus explaining China's rapid growth. Quantitative exercises conducted in this paper allow us to trace the reallocation and the growth back to precise policies.====The paper is motivated by China's remarkable development, as shown in Fig. 1, panels (i)-(iii). Since the market-oriented reforms in China were extended to the whole country in 1992, entry barriers in terms of costs of business start-up procedures have declined dramatically.==== Meanwhile, the stock market has grown rapidly since it was established at the end of 1990, as reflected in the increasing number of public firms. In addition, the credit market (as measured by domestic credit to private sector (%GDP) has improved gradually.==== Based on these facts, I construct a structural model in order to assess the effects of reforms including business deregulation in the form of reduced entry costs for private enterprises (PEs) and endogenous and exogenous exits of state-owned enterprises (SOEs), the establishment of a stock market, and the improvement in the credit market reflected in the relaxation of credit constraints.====The paper stresses the importance of entry. Structural reforms lead to a massive influx of new firms and thus a higher business density. As a result, market competition becomes fiercer and firms with lower efficiency have to exit. This induces resource reallocation from inefficient firms to efficient ones and thus results in a higher aggregate output. The model's implication on the productivity improvement at the extensive margin is supported by the firm-level data in China. As shown in Fig. 1, panels (iv)-(vi), in the post-reform stage, we observe a sharp increase in the number of manufacturers and in the meantime, a gradual improvement in the bottom percentiles of the productivity distribution for all the manufacturers, but not among manufacturers that survived throughout the sample period, 1998-2007. The importance of entry is the insight I share with Brandt et al. (2012), which empirically documents that net entry accounts for over two thirds of the TFP growth.====To quantify contributions of different reforms to the economic growth in China through this entry channel, a tractable growth model integrating costly entry, credit frictions, IPOs and dynamics of heterogenous firms and consumers is presented. In the model, on the consumer side, individuals can work subject to idiosyncratic labor income shocks as in Aiyagari (1994), or they can set up their own businesses, while on the firm side, the model is built on Hopenhayn (1992), in which private enterprises differ in productivity and net worth, and operate with credit constraints and IPO options. Moreover, to capture certain basic features of the Chinese economy, I model a separate sector consisting of state-owned enterprises.====I model the structural reforms considered in this paper as follows. First, I capture the promotion of the growth of the private economy by reduced entry costs. Second, I study the reform of SOEs, consisting of: 1) breaking up large monopolies while maintaining state control; 2) privatizing small enterprises; and 3) closing down unprofitable firms. These steps are characterized by introducing endogenous exit for SOEs (which never exited before the reform) and reducing the measure of SOEs exogenously. Third, I model the effect of the establishment of a stock market. In the pre-reform stage, there was no stock market. In the post-reform stage, however, firms can gain access to the stock market with some equity financing cost. Fourth, I consider the relaxed credit constraints on PEs. According to Song et al. (2011), when the private economy started to grow, PEs relied mainly on retained earnings and on family and friends to finance their investments. Therefore, in the model, the pre-reform status of the credit market for PEs is completely self-financing, but after the reform, PEs are less constrained.====I calibrate model parameters by matching certain key moments from the old and new balanced growth paths (BGPs) in the model with those from the Chinese firm-level data and aggregate data statistics in 1990 and 2010, respectively. The change from the old BGP to the new BGP captures the cumulative effect of the structural reforms. In terms of such reforms, this paper addresses four specific policy changes. Given that the earliest one, the establishment of the stock market, happened at the end of 1990, I set that year as the beginning of the reforms. After that, the market share of PEs has gradually grown from nearly zero in 1990 to almost one in 2010. Observing this, I set 2010 as a dividing line between the transition and the post-reform stage. Policy parameters governing different aspects of the reform have first-order effects on business density, domestic credit to the private sector and revenue share of public firms. Thus, I determine the value of those parameters by matching corresponding moments.====The quantitative results indicate that these reforms had strong effects. Decomposition of the total effect shows that, of the 9.26% annual GDP growth rate in the model, reallocation resulting from structural reforms generates 4.73%, labor migration adds 1.11%, and technological progress produces 3.19%. Within the reallocation effect, while each step affects the market competition and thus the TFP, the output growth is mainly driven by the relaxation of credit constraints for PEs and the reform of SOEs since these two policy changes generate weaker general equilibrium effects. Moreover, within the model, most of the output change is due to change at the extensive entry-and-exit margin, which indicates the importance of the entry channel.====Policy analysis indicates that further reforms could also have strong effects. For instance, if credit constraints were completely removed, we would expect output to increase by 48.37%, and half of this change would be attributable to the TFP growth induced by reallocation.====This paper contributes to the literature on the Chinese economy. Many researchers have conducted growth accounting exercises for China (examples include Bosworth and Collins (2008), Perkins and Rawski (2008), Brandt and Zhu (2009) and Zheng et al. (2009)) and find that aggregate TFP and capital per capita each explain about half of the growth of real GDP per capita. Unlike the accounting approach, this paper tries to provide a micro-foundation for the rapid growth. Hsieh and Klenow (2009) and Song et al. (2011) are closely related to this paper. As discussed above, this paper quantifies the effect of specific policies. In addition, by modeling the behavior of individual firms, the framework endogenizes the productivity distributions of SOEs and PEs and thus endogenizes the difference in productivity between these two sectors. Moreover, the structural reforms induce reallocation mainly through the entry/exit channel in my quantitative results. This echoes Brandt et al. (2012), which empirically documents the importance of entry.====This paper also adds to the recent quantitatively-oriented literature on financial frictions and economic development (examples include Giné and Townsend (2004), Jeong and Townsend (2007), Amaral and Quintin (2010), Buera et al. (2011), D'Erasmo and Boedo (2012), Greenwood et al. (2013), Buera and Shin (2013) and Midrigan and Xu (2014a)). In particular, Midrigan and Xu (2014a) examines the size of misallocation induced by financial frictions and emphasizes the potential importance of the entry channel. In addition, Buera et al. (2011) conducts cross-country comparisons on the growth effect of financial frictions, and Buera and Shin (2013) studies the transition dynamics that occurred after the financial liberalization. My model's framework is closely related to that of those three papers, but there are important differences. For example, I study the effect of both business deregulation and financial reform, I directly examine the effect at the entry margin, and, unlike previous studies that consider the credit market, I also model the stock market to assess its effect.====Additionally, this paper is related to the literature on entry barriers and market selection. Jovanovic (1982) explicitly models selection and associated industry dynamics, and Hopenhayn (1992) discusses the relationship between entry costs and selection. More recently, some studies have made cross-country comparisons of entry costs and aggregate economic performance using structural models (e.g., Barseghyan and DiCecio (2009), Boedo and Mukoyama (2012)), and some other papers focus on specific forms of entry costs such as the difficulty of imitating incumbents by entrants modeled in Luttmer (2007). Among these papers, Aghion et al. (2007) conducts a cross-country empirical study and finds that business creation is highly influenced by applicable financial systems. In addition, several papers have discussed various types of selection processes (e.g., Acemoglu et al. (2013), Bonfiglioli and Gancia (2014), Hsieh et al. (2016), Adamopoulos et al. (2017)). Relative to the literature, this paper focuses on the economic growth of one specific country and investigates the impacts of various types of entry barriers in the industrial sector.====The rest of the paper proceeds as follows. Section 2 introduces the institutional background. Section 3 describes the model and characterizes the equilibrium. Section 4 conducts quantitative analysis. Section 5 concludes.","Financial frictions, entry and growth: A study of China",https://www.sciencedirect.com/science/article/pii/S1094202518303375,4 April 2019,2019,Research Article,250.0
"Beaudry Paul,Fève Patrick,Guay Alain,Portier Franck","Vancouver School of Economics, University of British Columbia and NBER, Canada,Toulouse School of Economics and University of Toulouse I-Capitole, France,Université du Québec à Montréal and CIREQ, Canada,University College London and CEPR, United Kingdom","Received 26 September 2018, Revised 24 March 2019, Available online 3 April 2019, Version of Record 10 April 2019.",https://doi.org/10.1016/j.red.2019.03.011,Cited by (10),"In ====, identification of structural shocks can be subject to nonfundamentalness, as the econometrician may have an information set smaller than the economic agents' one. How serious is that problem from a quantitative point of view? In this paper we propose a simple diagnostic for the quantitative importance of nonfundamentalness in structural VARs. The diagnostic is of interest as nonfundamentalness is not an either/or question, and its quantitative implications can be more or less severe. As an illustration, we apply our diagnostic to the identification of TFP news shocks and we find that nonfundamentalness is of little quantitatively importance in that context.","Since Sims (1980), Structural Vector AutoRegressions (SVARs) have become a popular tool for macroeconomists, as they allow to identify the structural shocks that affect the macroeconomy as well as the response to those shocks (see Ramey (2016) and Kilian and Lütkepohl (2017) for a complete review). In a comment on Blanchard and Quah's (1989) SVAR exercise, Lippi and Reichlin (1993) raised the question of the nonfundamentalness of some structural moving average representations. When the econometrician has less information than the agents in the economy, she might not recover the structural shocks from the present and past observations of the economy regardless the identification strategy. In such a case, the moving average representation is nonfundamental. The example given by Lippi and Reichlin (1993) and further developed by Lippi and Reichlin (1994) is the one of a technological diffusion process, for which economic agents act knowing the future development of technology while the econometrician does not have such an information.====If one believes that Dynamic Stochastic General Equilibrium (DSGE) models are a good approximation of the true data generating process, then nonfundamentalness might be more than a theoretical curiosity. Indeed, Fernández-Villaverde et al. (2007) have shown that DSGE models may not have a fundamental moving average representation in the structural shocks, so that a SVAR cannot recover the structural shocks. From a quantitative perspective, Sims (2012) then shown that nonfundamentalness is not so much of a either/or problem: there are models in which one can pretty well, if not perfectly, recover structural shocks even with nonfundamentalness, as the information of the econometrician “almost” includes the one of the economic agents.====The questions then becomes an empirical one: can we test whether or not a structural representation of the data is fundamental? Forni and Gambetti (2014) and Forni et al. (2014) have suggested to answer this question by testing for the orthogonality of SVAR residuals to a large information set that is well captured by the main factors of a Factor Augmented VAR (FAVAR) model ==== a VAR model to which is added the main factors of a large model with hundreds of macroeconomic variables, that is likely to contain all the information possessed by economic agents. The “sufficient information” test can detect wether or not the SVAR suffers from nonfundamentalness (under the assumption that the factors contain all the information that is used by the economic agents).====But as for theory, an either/or test for nonfundamentalness is of limited interest, as it does not tell whether the consequences of nonfundamentalness are severe or not. The paper first proposes an empirical diagnostic of the nonfundamentalness severity. We show that the coefficient of determination (hereafter ====) of the projection of innovations of SVARs on factors is indeed a proper measure on that severity. The problem is to judge how large the ==== has to be for the consequence of nonfundamentalness to be severe. Simulation experiments conducted in Sections 2.2 and 2.3 show that autoregressions yields accurate dynamic responses for a ==== below 0.25. Interestingly, this ==== has some tight connections with some previous literature on VARs and identification, as we will show that it is a measure of the “anticipation rate” discussed in linear rational expectations models by Ljungqvist and Sargent (2004) and Mertens and Ravn (2010). It is also directly related to the “Poor Man's Invertibility Condition” of Fernández-Villaverde et al. (2007), more specifically to the largest eigenvalue of ==== matrix (using the “ABCD” language of these scholars). An additional contribution of the paper is to explicitly characterize the bias in estimated dynamic responses obtained from a misspecified SVARs (in the sense that it omits the relevant state variables, represented as a set of factors) in terms of ====. We analytically determine the link relation between the ==== and the upper bounds of the relative bias in estimated impact responses obtained from a bivariate SVAR setup with the very commonly used Cholesky decomposition.====We then implement our ==== diagnostic in the case of the identification of technological news shocks. The connection between news shocks and nonfundamentalness is tight: if agents receive some information about future technological improvements, this information might not be embedded in the current information set of the econometrician. The running example of Lippi and Reichlin (1994) when they illustrated nonfundamentalness was indeed a technological diffusion. A key insight of the “news” VARs of Beaudry and Portier (2006) is that the use of asset prices might overcome the nonfundamentalness problem, as they are likely to react strongly to agents' changing views of the future. As Forni et al. (2014) have questioned this property and shown that such identified technological news might be tested as nonfundamental, it is of interest to implement our ==== diagnostic in this case. As we will show, relevant ==== range between 3% and 21% depending on the specification, and the consequences of nonfundamentalness appear to be of relative minor importance in practice.====Two modeling issues deserve additional comments. First, we consider that factors (estimated by the econometrician) span the true state of the economy, as usual in that literature (see Stock and Watson, 2016). Under this assumption, we are armed with a simple diagnostic about potential misspecification of the SVAR model. As shown in Forni et al. (2009), this augmented setup is less (if not) affected by the nonfundamentalness problem, because it includes a sufficient amount of information. Second, our procedure assumes that the omitted factors are known and one may wonder why they are not directly included in the VAR model. Our approach has the advantage of maintaining a parsimonious (small scale) VAR model and thus does not require estimating a large number of parameters. A small scale VAR model allows minimizing the root mean square errors of the estimated impulse responses. This is why it is preferred by applied researchers. However, it can suffer from an omitted variables problem and thus yield inconsistent estimates of shocks (see Canova, 2007). With our pre-test procedure, we avoid this problem because our diagnostic allows to check if a small-scale SVAR model is a proper approximation of the true dynamic structure. In addition, the ==== diagnostic could be employed as an information criterion to properly select a limited set of relevant variables in the VAR model and thus to recover the structural shocks of interest.====As related literature, Soccorsi (2016) develops a global measure of nonfundamentalness with respect to DSGE models. The measure is a distance based in covariances between the true nonfundamental innovations and the innovations resulting from the unique fundamental representation obtained by flipping the problematic roots of the MA (Moving Average) structural representation. Forni et al. (2016) also develop a measure of nonfundamentalness for a specific structural shock by projecting this shock onto the VAR innovation. Both measures are implemented to evaluate the severity of the nonfundamentalness problem respective to specific theoretical macroeconomic models and have the property to disentangle the nonfundamentalness bias from the lag truncation bias resulting from a finite VAR. However, the application of these measures of nonfundamentalness necessitates the knowledge of the DSGE model. We develop a similar population measure of the severity of the nonfundamentalness for the whole system or for a single structural shock for macroeconomic models that can be expressed in state-space representation. The original contribution of the paper is to show that our ==== diagnostic can be directly implemented on observable data using SVARs without specifying any DSGE models, under the assumption that the factors well capture the state variables of the economy. The ==== diagnostic can then detect empirically the severity of nonfundamentalness and/or lag truncation problems of the finite VAR model.====The paper is organized as follows. In a first section, we expound the ==== setups and the ==== diagnostic. We also illustrate the merits of this diagnostic using a simple Lucas' tree model. In a second section, we perform quantitative and simulation experiments. A third section connects the bias that arises from a misspecified VAR model to the ====. In the fourth section, we implement the ==== diagnostic in the case of the identification of technological news shocks with US data. A last section concludes. Proofs are reported in appendix.",When is nonfundamentalness in SVARs a real problem?,https://www.sciencedirect.com/science/article/pii/S1094202518305106,3 April 2019,2019,Research Article,251.0
"Fella Giulio,Gallipoli Giovanni,Pan Jutong","Queen Mary University of London, CFM and IFS, United Kingdom of Great Britain and Northern Ireland,University of British Columbia, CEPR, HCEO and RCEA, Canada,Analysis Group, Canada","Received 4 May 2017, Revised 21 March 2019, Available online 3 April 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.red.2019.03.013,Cited by (7),"Non-stationary income processes are standard in quantitative life-cycle models, prompted by the observation that within-cohort income inequality increases with age. This paper generalizes ====, ====, and ===='s (====) discretization methods to non-stationary AR(1) processes. We evaluate the performance of these methods in the context of a canonical life-cycle, income-fluctuation problem with a non-stationary income process. We also examine the case in which innovations to the persistent component of earnings are modeled as draws from a mixture of Normal distributions. We find that the generalized Rouwenhorst method performs consistently better than the others even with a relatively small number of states.","Life-cycle models featuring idiosyncratic risk are used extensively to quantitatively examine a wide range of issues, such as the determinants of consumption (Storesletten et al., 2004a) and wealth (Huggett, 1996; De Nardi, 2004; Cagetti and De Nardi, 2006), optimal tax progressivity (Conesa and Krueger, 2006; Krueger and Ludwig, 2013; Heathcote et al., 2017) and educational choices (Abbott et al., 2019), just to name a few.====Idiosyncratic labor income risk is most often a crucial ingredient in this class of models. The stylized fact, first documented by Deaton and Paxson (1994), that both income and consumption inequality increase with age implies that the non-stationarity of income must, a fortiori, be driven by a non-stationary persistent component. For this reason, most quantitative life-cycle analyses assume a persistent labor income component whose (unconditional) variance increases with age. Typically this is obtained by positing that the persistent component has a stationary ==== distribution (i.e. the persistence parameter and the distribution of innovations are age-independent) but either (a) the process has a unit-root (Storesletten et al., 2004a); or, if the persistence parameter is less than unit, (b) the variance of its initial conditions is small relative to the variance of subsequent shocks==== (Huggett, 1996; Storesletten et al., 2004b; Kaplan, 2012). More recently, a number of papers (Karahan and Ozkan, 2013; Blundell et al., 2015; Guvenen et al., 2016; De Nardi et al., 2019) have documented that even the ==== distribution of the persistent component of labor income is non-stationary; namely, both its persistence and the variance of innovations change with age. As shown by Karahan and Ozkan (2013) and De Nardi et al. (2019) these latter features are important to account for the pass-through of persistent income shocks onto consumption and for the evolution of cross-sectional consumption dispersion in the data, as well as for the welfare costs of labor income risk.====To sum up, non-stationarity of the (persistent component of the) labor income process is an important feature of any life-cycle model that aims to account for the distribution of consumption and wealth and other forms of heterogeneity in individual outcomes.====Introducing such a process into a quantitative model usually involves approximating the continuous stochastic process through a Markov chain with a finite state space. As one would expect, the accuracy of such an approximation affects quantitative predictions. Different methods are available to perform such approximation for ==== AR(1) processes. Among these, Tauchen (1986) and its variant Tauchen and Hussey (1991), Rouwenhorst (1995), and Adda and Cooper (2003) are the most commonly used in economics. Yet, there is currently no standard, off-the-shelf method for discretizing a ==== AR(1) process. The quantitative implementations in the extensive literature on life-cycle, heterogeneous-agent models use a variety of approaches and we review some well-known examples in Section 2.4. In most cases these methods are only partially documented, hence we know little about their performance.====Our work is meant to provide a more systematic analysis of this approximation problem. We show how to extend standard discretization methods for stationary AR(1) processes—namely Tauchen (1986), Rouwenhorst (1995), and Adda and Cooper (2003)—to non-stationary AR(1) processes, and we evaluate their performance. As in the original methods, our extensions keep the number of states in each time period constant. The main difference is that both the state vector and the transition matrix are allowed to change over time in accordance with changes in the moments of the original process. In all cases, the defining properties of the original stationary method are preserved.====The properties of alternative discretization methods to approximate ==== AR(1) processes in the context of stationary infinite horizon problems have been studied in some detail by Kopecky and Suen (2010). They find that: (a) the choice of discretization method may have a significant impact on the model simulated moments; (b) the performance of Rouwenhorst's (1995) method is more robust, particularly for highly persistent processes. Like in the analysis of Kopecky and Suen (2010) for the stationary case, we compare the respective performance of the three methods both in approximating the original continuous process ==== in generating accurate model solutions. The latter is the relevant metric to assess the impact of alternative discretization methods on the variables of interest to the researcher. Our baseline analysis is carried out within a standard life-cycle, income-fluctuation model with a canonical labor income process featuring both an AR(1) persistent component and a transitory white noise component (Abowd and Card, 1989).====In our numerical implementations we consider three sets of assumptions for the AR(1) component. In the first, more standard, case the conditional distribution is stationary, while the unconditional is not; i.e., both the persistence coefficient and the variance of the shocks are age-independent. In the second case, both the persistence coefficient and the shocks' variance are functions of age, as in Karahan and Ozkan (2013). In both these specifications the innovations to the AR(1) component are assumed to be normally distributed. Finally, in a third case, we maintain the assumption that the persistence coefficient and the shocks' variance are age-independent, but we assume that shocks have a non-normal distribution following Guvenen et al. (2016). In both the first and third case, we report results under different degrees of persistence of the AR(1) process, including the unit root case, as it is well known that the performance of standard discretization methods worsens as the degree of persistence increases.====We find that Rouwenhorst's method tends to perform better even with a relatively small number of grid-points.====The remainder of the paper is structured as follows. Sections 2.1-2.3 discuss how to extend Tauchen (1986), Adda and Cooper (2003) and Rouwenhorst's (1995) methods to non-stationary AR(1) processes. Section 2.4 reviews some well-known implementations of the life-cycle model with idiosyncratic labor income risk, with a focus on their respective approaches to discretizing non-stationary AR(1) processes. Section 3 presents the quantitative framework we use to assess the accuracy of the various methods. Section 4 compares the accuracy of our three discretization methods. Section 5 concludes.",Markov-chain approximations for life-cycle models,https://www.sciencedirect.com/science/article/pii/S1094202519301565,3 April 2019,2019,Research Article,252.0
Dogan Aydan,"University of Barcelona, Department of Economic Theory, Barcelona, Spain","Received 26 July 2018, Revised 28 March 2019, Available online 3 April 2019, Version of Record 10 April 2019.",https://doi.org/10.1016/j.red.2019.03.012,Cited by (8),"This article explores the role played by investment-specific technology (IST) shocks in emerging market business cycle fluctuations. The analysis is motivated by two key empirical facts; the presence of IST change in the post-war US economy combined with the importance of US investment goods in the emerging market imports. The goal is to quantify the contribution of US IST change for the business cycles of an emerging country in the context of a two-country, two-sector international real business cycle framework with investment and consumption goods sectors. Specifically, I estimate the model using Mexican and US data and find that a permanent US-originating IST shock is important in explaining Mexican business cycle dynamics. Shocks to investment sector technology explain around 60% of the investment, 44% of the consumption and 52% of the output variability. I argue that both a shock that captures financial frictions and a permanent US-originating IST shock are necessary to account for the key business cycle features in the data.","A well-established empirical regularity in international macroeconomics is that investment goods are produced by a few countries only and that advanced countries are, as such, less dependent on imported investment goods (see Eaton and Kortum (2001), Mutreja et al. (2014) and Serven (1995)). Indeed, the import to production ratio of investment goods is 66% on average in emerging markets, while it stands at an average 31% in the world' main producers.==== Given the high import content of investment goods in emerging markets, here I quantitatively study the role of investment price fluctuations in advanced economies on the emerging markets.====Specifically, I explore the role of investment-specific technology (IST) shocks arising in the US on Mexican business cycle dynamics. I focus on the dynamics between these countries for two reasons: First, the US is Mexico's biggest trade partner and, more importantly, US investment goods represent on average 55% of Mexico's total imports of such goods between 1990 and 2016. In contrast, on average, only 13% of US imported investment goods originate from Mexico.==== Second, the relative price of investment goods presents a falling trend in the post-war US economy while the real investment rate has increased, indicating an IST change (see Greenwood et al. (1997)).==== A fall in the relative price of investment goods in the US is likely to generate international spill-overs that affect the Mexican economy via its trade linkages.====I quantify the effect of this trade channel in a two-country, two-sector international real business cycle (IRBC) model with consumption and investment goods sectors. In the model set-up, I allow country size to differ so that I can reproduce a small open economy with the calibration and I introduce sector-specific total factor productivity (TFP) shocks in both countries. The model presented here is similar to that developed in Boileau (2002), but it differs from that author's theoretical set-up in terms, primarily, of its shock structure. The IST shock is assumed to be non-stationary so as to capture the trend in the relative price of investment goods. Moreover, I allow this shock to be persistent in growth rates. I assume that the shock originates in the developed economy and that it then slowly diffuses into the emerging market economy in a slow process of adjustment. To check whether this formulation is consistent with the data or not, I test the cointegration relationship between the relative import price of investment goods in Mexico and the relative price of investment goods in the US. I find that both series are non-stationary and cointegrated, thus confirming the modelling of the IST shock.==== Intuitively, the formulation of the IST shock captures the fact that technological improvements are usually made in advanced economies and that it is some time before they are acquired and implemented in emerging markets.==== The technology shock in the consumption sector, on the other hand, is temporary. I also take into account the potential role of financial frictions in the theoretical set-up by incorporating a risk premium shock, which is a residual of the uncovered interest rate parity (UIP) condition.====I estimate the model and evaluate the importance of IST shocks for business cycle fluctuations in Mexico. I find that shocks to the investment technology account for 52% of output, 44% of consumption and 60% of investment dynamics. Disturbances in consumption sector productivity are also important for output and consumption fluctuations, with around 38% of the former and 24% of the latter being driven by the consumption sector TFP shock. In contrast, technology shocks play almost no role in relation to international variables. The UIP shock accounts for 97% of the trade balance variability and 87% of the real exchange rate fluctuations. These findings are important as this paper includes two key drivers of emerging market business cycle fluctuations as identified in different strands of the literature. The first strand builds on the hypothesis, forwarded by Aguiar and Gopinath (2007), according to which the business cycle characteristics of these countries can be replicated in a standard real business cycle (RBC) framework when the main driving source of fluctuations is a shock to trend growth. The other strand of the literature – including the studies of Neumeyer and Perri (2005), Uribe and Yue (2006), Garcia-Cicco et al. (2010) and Chang and Fernandez (2013) – emphasizes the importance of shocks to these countries' risk premia and financial frictions. Here, I show, on the one hand, that permanent technology shocks are important for the business cycle dynamics of emerging markets as Aguiar and Gopinath (2007) suggest, but I also show that this permanent shock takes the form of a slowly diffusing permanent IST shock that originates in the US as opposed to a domestic TFP shock. Moreover, on the other hand, I show that risk premium shocks are still key in explaining trade balance dynamics.====Given the importance of technology shocks in driving Mexican business cycle dynamics, I further analyse the dynamic transmission of technology shocks into the Mexican economy and find that an IST shock originating from the US triggers a recession in the Mexican economy, while a domestic consumption sector TFP shock triggers a boom. The intuition underpinning this result is as follows: The permanent IST shock originates in the US and is eventually acquired by the Mexican economy. To take advantage of the productivity improvement in the investment sector abroad, the Mexican economy imports investment goods from the US. The high degree of openness in the investment sector causes the trade balance to deteriorate for a short period and, thus, the US-originating IST shock leads to an increase in investment and a fall in consumption and output. The deterioration in the trade balance is short lived as the technologies share a common trend in the long run. On the other hand, a persistent but temporary consumption-specific technology shock leads to a fall in the price of the consumption goods produced in Mexico; hence, world expenditure of consumption goods switches towards home produced consumption goods boosting the domestic output and consumption. Meanwhile, agents in Mexico use the rise in income to invest and accumulate capital. As investment has high import content, the increase in demand for investment goods cause a fall in trade balance.====I also assess the model's performance in terms of second moments of the data. Above all the two-country structure allows the process of IST in the rest of the world to be modelled and the covariation of the variables in Mexico and US to be analysed. The model performs significantly well in matching the cross-country investment, output and consumption correlations and, consistent with the data, predicts a cross-country output correlation that is greater than that of consumption. This improvement in the model's performance is a consequence of a rich shock structure and, more specifically, of the diffusion of the IST shock. A further major advantage purported by the model is the positive correlation it generates between consumption and investment in line with the data. Although the main driving source of both consumption and investment fluctuations are IST shocks, consumption sector TFP shocks are still very important for consumption dynamics, thus resulting in a co-movement between the two variables. The model also addresses the positive relationship between investment and consumption with output. Yet, it fails with respect to one key dimension: The model generated consumption volatility is less than the output volatility, which is at odds with the observed business cycle characteristics of emerging market economies. Although the permanent IST shock explains a large part of the dynamics of consumption, the wealth effects resulting from this permanent shock are not large enough to produce the correct relative consumption volatility. In the absence of technology shocks in the consumption sector, the model indeed predicts a relative consumption volatility that is larger than one.====The analysis in this paper contributes primarily to the literature on emerging market business cycles. In addition to the papers mentioned above, many other studies of relevance have investigated the business cycle dynamics of emerging markets – see, among others, Mendoza (1995), Boz et al. (2011) and Boz et al. (2015).==== As I present a two-sector structure with investment and consumption goods, this paper is more closely related to Alvarez-Parra et al. (2013). These authors build a small open economy model with durable and non-durable goods augmented by shocks to trend and country risk premium. They calibrate the model to the Mexican economy and emphasize the importance of shocks that capture financial frictions – a UIP shock – in explaining some key features of emerging market business cycles. This paper confirms their findings regarding the importance of a UIP shock in an estimated two-country, two-sector setting, but, as a reflection of the empirical evidence, I emphasize the role of a permanent IST shock and show that the latter is important for the model's overall performance. In line with the approach adapted herein, the literature has exploited the implications of IST shocks for small open economies – see, for example, Letendre and Luo (2007) and Araujo (2012). However, these papers assume that an IST shock is an exogenous variable determined solely domestically. In a recent paper of some relevance, Boileau and Normandin (2017) consider the impact of changes in the price of imported investment goods from the US on the consumption dynamics of emerging markets. However, while in their paper, the IST improvement in the US is entirely exogenous to the small open economy, here the technological improvement is acquired by Mexico gradually, consistent, that is, with the evidence on cointegration.====This paper is also closely related to the literature that studies the implications of IST shocks in order to address questions of an international dimension. Papers here, include Boileau (2002), Raffo (2010), Mandelman et al. (2011) and Jacob and Peersman (2013).==== Finally, since I show that a US-originating IST shock is important for fluctuations in Mexican output and investment, my findings relate to the literature that explores the importance of US shocks for the business cycle fluctuations of emerging market countries (see, for instance, Canova (2005) and Mackowiak (2007)). Unlike these papers, I analyse the importance of US shocks within a structural model of IRBC and highlight the role of investment-specific technical change.====The rest of the paper is structured as follows. I begin with a description of the model in Section 2, while in Section 3, I outline the Bayesian estimation of the model. I then present the results in Section 4 by discussing the variance decomposition, second order moments, and the impulse response analysis. Finally, I conclude in Section 5.",Investment specific technology shocks and emerging market business cycle dynamics,https://www.sciencedirect.com/science/article/pii/S1094202518304095,3 April 2019,2019,Research Article,253.0
"Dinlersoz Emin M.,Hyatt Henry R.,Janicki Hubert P.","Center for Economic Studies, U.S. Census Bureau, 4600 Silver Hill Road, Suitland, MD 20746, United States of America,Center for Economic Studies, U.S. Census Bureau and IZA, United States of America,Center for Economic Studies, U.S. Census Bureau, United States of America","Received 15 January 2018, Revised 22 March 2019, Available online 2 April 2019, Version of Record 12 April 2019.",https://doi.org/10.1016/j.red.2019.03.009,Cited by (4),"Compared to more established firms, young firms tend to hire younger workers and provide them with lower earnings. To understand these facts, a dynamic model of entrepreneurship is constructed, where individuals can become entrepreneurs, or work in either a corporate or an entrepreneurial sector. Sectoral differences in production technology, financial constraints, and labor market frictions lead to sector-specific wages and worker sorting into the entrepreneurial sector by productivity and assets. Individuals with lower assets tend to accept jobs in the entrepreneurial sector, an implication that finds support in the data. The analysis indicates that sector-specific labor market frictions are critical to the model's ability to generate worker sorting and to match the key features of the entrepreneurial sector.","Job creation by entrepreneurs is an important component of employment dynamics in the United States. In a typical year, new firm startups account for about 3% of total employment but almost 20% of gross job creation.==== The jobs entrepreneurs create, however, may not always be the most desirable ones. Entrepreneurial firms, which are generally thought of as startups or young and small firms, provide lower earnings on average to their workers compared with older or larger firms.==== They also tend to hire disproportionately from the pool of workers who are young and have lower education.==== Jobs in entrepreneurial firms may therefore play an important role in the labor market by providing employment opportunities for those who would otherwise be nonemployed or wait for a higher-paying job offer from an established firm. Despite the increasing attention to differences in worker characteristics and earnings across entrepreneurial versus more established firms, the mechanisms by which workers sort into these two types of firms, and how this sorting is influenced by various labor market and financial frictions, remain relatively less understood.====The long-run decline in business startups and diminished business dynamism also call for a better understanding of the connection between the supply of entrepreneurial firms and the market for the type of labor these firms attract.==== The share of young employers in the population of firms has been falling, and workers are increasingly employed in older firms.==== Businesses that have formed recently tend to create fewer jobs and pay lower wages, and the decline of business startups explains part of the decline in worker reallocation rates.==== As a result of the decline, those individuals who tend to work for entrepreneurs may face an increasingly lower supply of new entrepreneurial jobs. Conversely, changing dynamics of labor markets may have consequences for entrepreneurs' ability to hire and retain workers, and hence, their cost of doing business. This connection between the supply of entrepreneurs and the supply of labor to their firms leads to several questions. What kind of individuals choose to work for entrepreneurs, and why? How do financial and labor market frictions affect the decision to become an entrepreneur and to work for one? Which type of frictions is critical in generating the observed allocation of workers to entrepreneurial businesses? These questions demand a framework where individuals face not only the decision to become entrepreneurs, but also the decision to work for entrepreneurial versus other firms.====This paper develops a model to study jointly the questions of who becomes an entrepreneur and what kind of workers sort into entrepreneurial firms in the presence of search frictions in the labor market and financial constraints for entrepreneurs. The calibrated model's equilibrium exhibits worker sorting: individuals with lower assets on average tend to take jobs in the entrepreneurial sector. However, workers with even moderate amount of assets accept entrepreneurial sector employment if their labor productivity is sufficiently large. This results in slightly higher average worker productivity in the entrepreneurial sector. The analysis also explores in detail the model's mechanisms that generate worker sorting and entrepreneurship in order to isolate the roles of financial constraints, labor market frictions, and entrepreneurial uncertainty. Labor market frictions – and sector-specific job finding rates, in particular – are central to generating differences in levels of worker assets between sectors. The analysis of a novel matched data on workers' assets and their employers provides some support for the implications of the model on worker sorting.====In the model, individuals differ in wealth, entrepreneurial ability, and worker productivity. Each individual can become an entrepreneur, or work in one of the two sectors: entrepreneurial and corporate–a label for the set of firms that don't face the constraints entrepreneurial firms do. The constraints entrepreneurs face are of two types. The entrepreneurial production is subject to diminishing returns that arise from the limits to entrepreneurs' span-of-control. In contrast, firms in the corporate sector can scale up production without such restrictions. In addition, entrepreneurs can borrow only up to a limit to operate their businesses–a constraint that does not apply to corporate sector firms.====The choices to become an entrepreneur and to work for one are endogenously determined, along with the price of labor entrepreneurs face. The match between workers and firms is subject to frictions in the labor market. Not all nonemployed individuals who look for a job can find one, and workers can be separated from their employers involuntarily, in addition to voluntary separations. These labor market frictions, however, are allowed to vary across the two sectors. Job offers arrive at different rates, and involuntary separations occur with different probabilities. Workers can also switch sectors without a spell of nonemployment through inter-sectoral “job-to-job” transitions. The differences across the two sectors in various frictions lead to divergence in sectoral wages per unit of worker efficiency. This wage differential, combined with the heterogeneity in worker productivity and wealth, results in worker sorting across the two sectors based on both productivity and assets.====The model outlined above is related to recent models of entrepreneurship.==== What distinguishes it from these models, however, is the presence of sector-specific labor market frictions and wages. The labor market frictions prevent costless movement of workers across sectors, and in and out of nonemployment. Because of these frictions, an equilibrium with two distinct wage rates emerges, where some workers take low-wage jobs in the entrepreneurial sector even though higher-wage corporate jobs are available. In particular, differences in labor market frictions are needed for the model's equilibrium to replicate observed facts. The labor market frictions, together with the differences in production technology and financial frictions across the two sectors, enable the model to generate employment shares, worker earnings, and worker flows to and from nonemployment that are consistent with their observed counterparts. At the same time, the model's equilibrium accounts for the observed fraction of entrepreneurs in the population, as well as the distributions of wealth for entrepreneurs and workers.====The model provides an answer to the central question of who works for whom. A key property of the model's equilibrium is that workers in the entrepreneurial sector tend to have fewer assets, lower earnings, and higher labor productivity compared to those in the corporate sector.==== The asset differential is not only driven by the fact that individuals who work in the higher-wage corporate sector can accumulate on average more wealth over time than their counterparts in the entrepreneurial sector. There is an important selection effect: individuals who take jobs in the entrepreneurial sector tend to be less wealthy even ==== they take these jobs. In other words, the wealth and productivity differences across the two sectors also apply to individuals who have just taken jobs in these sectors. Nonemployed individuals with a job offer from the entrepreneurial sector have to decide whether to reject this offer and wait for an offer from the higher-wage corporate sector. Individuals with lower levels of savings prefer to take jobs in the entrepreneurial sector rather than waiting. This sorting emerges in the absence of any inherent preference for working in entrepreneurial firms, or any form of compensation other than the equilibrium wages these firms provide.====Worker sorting by assets turns out to be a robust feature of the model. The presence of sector-specific labor market frictions that favor job draws in the corporate sector is critical in obtaining worker sorting. In addition, the presence of sector-specific separation rates is important for the model to match all targeted moments. To understand the mechanisms behind sorting and equilibrium allocations, additional analysis is carried out to isolate the roles of the model's key elements. In particular, the analysis explores the specific roles of the distinct sectoral wages for labor, the differential labor market frictions, the extent of the borrowing constraint for entrepreneurs, and the uncertainty about the entrepreneurial ability at the time of the entrepreneurship decision. Worker sorting prevails to varying degrees under alternative assumptions about the nature of these key elements.====The model's prediction that workers with lower assets sort into entrepreneurial firms is taken to data. The test of this prediction requires data not only on individuals' assets, but also on their employment choices and the characteristics of their employers. While data on worker assets is available from a variety of sources, measuring workers' assets by employer type (e.g. employer size or age), and especially at the time when they start a job, is more challenging. The analysis uses a novel combination of data on workers' net worth from the Survey of Income and Program Participation (====) and data at the worker-job level from the Longitudinal Employer-Household Dynamics (====) program that captures employer characteristics and workers' job transitions. The empirical counterpart of the model's entrepreneurial sector are taken to be the set of young firms.==== The findings suggest that individuals who work in younger firms tend to have fewer assets than their counterparts in older firms. Furthermore, individuals who take jobs in young firms also tend to be less wealthy around the time they take these jobs, compared to those who take jobs in older, more established firms.==== These findings support the predictions of the model on worker sorting based on assets.====The rest of the paper is organized as follows. The next section documents some key motivating facts about entrepreneurial firms. Section 3 introduces the model, followed by its baseline calibration in Section 4. The properties of the baseline model are discussed in Section 5. Section 6 explores the role of the model's key ingredients in determining equilibrium allocations and worker sorting. Sensitivity analysis with respect to some of the key assumptions of the model is carried out in Section 7. Section 8 offers empirical evidence on the predictions of the model on worker sorting by assets. Section 9 concludes.",Who works for whom? Worker sorting in a model of entrepreneurship with heterogeneous labor markets,https://www.sciencedirect.com/science/article/pii/S1094202518303193,2 April 2019,2019,Research Article,254.0
Wang Haomin,"University of Konstanz, Universitätsstraße 10, 78464 Konstanz, Germany","Received 1 May 2017, Revised 30 November 2018, Available online 1 April 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.red.2019.03.008,Cited by (20),"This paper studies the extent to which working couples can insure one another against cyclical fluctuations in the labor market and examines the implications of joint household decision-making for cyclical fluctuations in the unemployment rate. For this purpose, I provide a dynamic life-cycle model of households that make joint savings and job search decisions in the presence of aggregate shocks. I show that two key mechanisms are at play. The first is the added-worker effect, which leads to counter-cyclical search intensity because workers increase search intensity when their spouse becomes unemployed. The second is the comparative advantage effect, according to which couples' job search efforts are coordinated based on the relative returns to search of each spouse. I estimate the model using data from the US Current Population Survey, and find that joint household decision-making contributes to the counter-cyclicality of women’s unemployment rate, but not for men. Moreover, joint household decision-making lowers the welfare costs of cyclicality.","Transitions between non-participation and unemployment display strong cyclical patterns, suggesting that non-employed workers are more likely to search for a job in economic downturns.==== These transitions also vary significantly across spousal labor force status, which suggests that the job search decision is made at the household level rather than the individual level.====Existing literature such as Guler et al. (2012) shows that, when couples are risk-averse and imperfectly insured, the interactions between spouses can lead to labor market behaviors that are significantly different from those of singles. Over the business cycle, job search of married individuals can be affected by their spouse's employment prospects and outcomes. Since job search decisions affects the job finding rate the determines whether a non-employed individual is classified as an unemployed worker, making search decisions jointly with a spouse has empirical implications for cyclical fluctuations of the unemployment rate. In the meantime, as joint decision-making enables spouses to share risks with each other, couples may experience reduced welfare costs of cyclical fluctuations compared to individuals who make decisions independently.====Cyclical implications of joint household decision-making are not well explored in the literature. The goal of this paper is to study the extent to which working couples can insure one another against cyclical fluctuations in the labor market and to examine the implications of joint decisions for cyclical fluctuations in the unemployment rate. With over half of the adult population in the U.S. being married, a better understanding of the role of joint decision-making offers valuable insights for policymakers in assessing labor market slacks and designing appropriate policy measures.====To this end, I construct and estimate a life-cycle model of married-couple households in the presence of cyclical fluctuations in the labor market. Specifically, each household is formed by a married couple that acts as a unitary decision maker.==== Couples are risk-averse and imperfectly insured due to incomplete financial markets and borrowing constraints. When seeking employment, couples choose search intensity: Higher search intensity increases the job finding rate but also incurs greater costs. The job finding rate per unit of search intensity fluctuates over the business cycle.====I distinguish unemployment from non-participation using a threshold rule on search intensity: those who search intensively are classified as unemployed workers and the others as non-participants. This allows me to simulate labor force stock and flow statistics from the model, which are matched to their data counterparts in order to estimate the model parameters using the indirect inference technique. Estimation of the model is based on a sample of married-couple households drawn from the US Current Population Survey (CPS) between 1994 and 2014 with the sample stratified by the education levels of the couples.====In the model, there are two main mechanisms through which joint decision-making affects cyclical fluctuations of search intensity. The first is the added-worker effect: Individuals search for jobs more intensively when their spouse is not employed; therefore, counter-cyclicality spousal non-employment leads to counter-cyclical search intensity. The second mechanism is the comparative advantage effect: The spouse facing less cyclical volatility in the labor market would be incentivized to search more counter-cyclically, and vice-versa.====Gender-asymmetry is allowed for in several dimensions in the model. Both the level and the cyclicality of the job finding rate per unit of search intensity may be different between men and women. In addition, the marginal cost of job search may vary differentially with search intensity for the two genders. The gender-specific parametrization captures differences in employment opportunities across industries and occupations, the usage of job search methods, and, possibly, taste for job search between men and women. Estimation results reveal substantial gender-asymmetry among households across all education levels. Compared to men, women face a less convex search cost function and thus their search intensity is more responsive to changes in the aggregate state and the spousal employment status. In addition, the job finding rate per unit of search intensity of married women is relatively less cyclical.====To assess the role of joint household decision-making over the business cycle, I compare the baseline model of married couples to a counterfactual one in which individuals make independent decisions. The results show that, for women, making joint decisions with their spouse significantly amplifies the counter-cyclicality of the unemployment rate. The effect is stronger among less-educated women. In contrast, the effect of joint decision-making on the cyclicality of men's unemployment rate is ambiguous.====In addition to the cyclicality of the unemployment rate, the simulation results also indicate that joint decision-making by couples reduces the welfare cost of cyclical fluctuations, computed as a percentage increase in the lifetime welfare if there were no cyclical fluctuations. Married-couple households face an average welfare cost of 0.6% due to cyclical fluctuations. If spouses did not make joint decisions, the welfare cost would rise by around 0.3 percentage points, suggesting that joint decision-making is an effective way of insuring married couples against cyclical fluctuations. This is consistent with Shore (2010)'s finding that marriage makes the riskiness of incomes less counter-cyclical. Furthermore, echoing Ortigueira and Siassi (2013)'s finding that joint decision-making is most valued by wealth-poor households, my simulation results show that couples that are unlucky in the labor market and unable to accumulate wealth find joint decision-making most valuable in reducing the welfare cost of cyclical fluctuations. This is true for all households in which at least one spouse do not have a college degree.====The paper is closely related to the growing literature that examines the economic implications of joint decision-making by couples facing unemployment risk and wage uncertainties.==== Compared to singles, couples have different motives in making job search decisions because they can share risks with each other. Guler et al. (2012), Flabbi and Mabli (2018), and Blundell et al. (2016) show that intra-household risk sharing has empirical ramifications for lifetime incomes, labor force transitions, lifetime inequality, and consumption inequality. As couples can self-insure through risk sharing, Haan and Prowse (2017) show that the optimal generosity of social assistance for couples is significantly lower compared to singles. Besides risk sharing, Dey and Flinn (2008) study employer-provided health insurance as a public good within married-couple households. Guler et al. (2012) consider a multiple-location model of couples facing costs of living apart.====Relative to this literature, the contribution of this paper is to examine the role of joint decision-making over the business cycle. A closely related work is Mankart and Oikonomou (2017), who show that incorporating joint decision-making allows for a better match of cyclical volatilities in aggregate labor market measures, which is a challenge for single-agent models with endogenous participation decision (Tripier, 2004; Veracierto, 2008). Although Mankart and Oikonomou (2017) also focus on the business cycle, there are a number of differences between our papers, one of which is the treatment of gender-asymmetry. In their model, the two genders are ex-ante identical. In my paper, because of the gender-specific parameterization, I find joint decision-making to be a more important driving force behind the counter-cyclicality of women's unemployment rate.====Differences across genders in the cyclicality of labor supply and unemployment have been shown to be important. Hoynes et al. (2012) and Albanesi and Sahin (2018) find that women's labor supply and unemployment are less cyclical compared to men, with industry and occupation being important factors explaining this observation. Doepke and Tertilt (2016) show that, although female labor supply may have higher wage elasticity at the individual level, there is less cyclical volatility in women's aggregate labor supply even after controlling for industry effects. They argue that women's greater role in household self-insurance may have a dampening effect on the cyclicality of their labor supply. The findings in this paper support their view.====This paper's specification of the job search decision is also different from Mankart and Oikonomou (2017). They assume a binary job search decision, while I consider a multi-level search intensity decision that is motivated by the empirical evidence that there is rich heterogeneity in labor force attachment among the non-employed (Jones and Riddell, 2006; Faberman et al., 2017). In my estimated model, there is a positive correlation between the level and the counter-cyclicality of search intensity. The correlation is particularly pronounced among women, which has empirical ramifications. In the Online Appendix, I study a version of the model in which there exist only binary search decisions and show that such a model performs poorly in matching cyclical fluctuations in women's labor force participation rate.====The rest of the paper is organized as follows. In Section 2, I introduce the life-cycle model of married couples and its implications for the cyclicality of search intensity. I discuss the estimation method in Section 3 and present estimation results in Section 4. In Section 5, I demonstrate the empirical implications of joint decision-making by comparing the married-couple model with a counterfactual individualist model. Finally, I conclude the paper in Section 6.",Intra-household risk sharing and job search over the business cycle,https://www.sciencedirect.com/science/article/pii/S1094202518302692,1 April 2019,2019,Research Article,255.0
"Chu Angus C.,Cozzi Guido,Fan Haichao,Furukawa Yuichi,Liao Chih-Hsing","Management School, University of Liverpool, Liverpool, United Kingdom,China Center for Economic Studies, School of Economics, Fudan University, Shanghai, China,Shanghai Institute of International Finance and Economics, Shanghai, China,Department of Economics, University of St. Gallen, St. Gallen, Switzerland,Institute of World Economy, School of Economics, Fudan University, Shanghai, China,School of Economics, Chukyo University, Nagoya, Japan,Department of Economics, Chinese Culture University, Taipei, Taiwan","Received 23 June 2018, Revised 6 March 2019, Available online 26 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.red.2019.03.006,Cited by (21),This study develops a monetary Schumpeterian growth model with heterogeneous households and heterogeneous firms to explore the effects of ,"The seminal study by Tobin (1965) initiated an influential literature in macroeconomics that explores the relationship between inflation and economic growth. Studies in this literature have focused on how inflation affects economic growth via the accumulation of physical capital and/or human capital.==== However, an important insight from the seminal study by Solow (1956) is that economic growth is ultimately driven by technological progress. Therefore, it is important to also understand the effects of inflation in a growth model with endogenous technological progress. Marquis and Reffett (1994) explore the effects of inflation in the R&D-based growth model developed by Romer (1990). However, this early study by Marquis and Reffett (1994) and many subsequent studies in this branch of the literature have mostly focused on a representative-household setting with homogeneous firms. In this study, we find that the interdependence between heterogeneous households and heterogeneous firms leads to novel results.====Specifically, we develop a monetary Schumpeterian growth model with heterogeneous firms and heterogeneous households. We model firm heterogeneity in the Schumpeterian quality-ladder model by assuming that the step size of quality improvements is randomly drawn from a Pareto distribution. Then, to allow for endogenous firm entry, we assume that R&D entrepreneurs need to pay an entry cost to enter the market after observing the step size of their quality improvements. As a result, an entrepreneur would enter the market if and only if her quality improvement is sufficiently large, which in turn generates an endogenous distribution of quality improvements that are implemented. Motivated by the empirical evidence in Piketty (2014), we consider an unequal distribution of wealth as an important source of income inequality. Therefore, we model household heterogeneity in the Schumpeterian model by assuming that households have different levels of wealth in order to generate an endogenous income distribution. Within this growth-theoretic framework, we explore the effects of monetary policy on innovation and income inequality. In summary, we find that inflation has an inverted-U effect on economic growth and income inequality under endogenous firm entry.====The inverted-U effect of inflation on economic growth under endogenous entry of heterogeneous firms can be explained as follows. Inflation increases the cost of R&D via the cash-in-advance (CIA) constraint on R&D and decreases the arrival rate of innovation, which is a negative effect of inflation on economic growth.==== The lower rate of creative destruction however increases the expected value of future profits and the market value of inventions, which in turn lowers the entry threshold for quality improvements. With more inventions being implemented, inflation also has a positive effect on economic growth. These positive and negative effects together generate an inverted-U effect of inflation on economic growth if the entry cost is sufficiently large.====Interestingly, this inverted-U effect of inflation on economic growth also leads to an inverted-U effect of inflation on income inequality in the Schumpeterian model. In our model, income inequality is increasing in the ratio of wealth income to wage income. Therefore, either an increase in the real interest rate or an increase in the value of financial assets would increase income inequality. Given the Euler equation under which the real interest rate is increasing in the growth rate of consumption, the above-mentioned inverted-U effect of inflation on economic growth causes an inverted-U effect on the real interest rate and hence also an inverted-U effect on income inequality. Furthermore, inflation has both positive and negative effects on the value of financial assets. On the one hand, by slowing down the rate of creative destruction, inflation increases the market value of monopolistic firms, which in turn increases the value of financial assets. On the other hand, by lowering the entry threshold for quality improvements, inflation reduces the average step size of quality improvements implemented in the market and decreases the average markup ratio, which in turn decreases the market values of monopolistic firms and financial assets. Combining all these effects yields an overall inverted-U effect of inflation on income inequality, which exists only under endogenous entry of heterogeneous firms. We also calibrate the model to perform a quantitative analysis and find that our model is able to match a growth-maximizing inflation rate of 5% and an inequality-maximizing inflation rate of 12% that are estimated using cross-country panel data. Finally, we simulate the utility-maximizing inflation rate and explore how it is affected by relative household wealth.====This study relates to the literature on innovation and economic growth. Romer (1990) develops the seminal R&D-based growth model in which economic growth is driven by the invention of new products. Segerstrom et al. (1990), Grossman and Helpman (1991) and Aghion and Howitt (1992) develop the Schumpeterian quality-ladder model in which economic growth is driven by the innovation of higher-quality products. For tractability, these seminal studies and many subsequent studies assume a constant step size of quality improvement. Important exceptions include Klette and Kortum (2004) and Minniti et al. (2013). Minniti et al. (2013) develop a Schumpeterian growth model with random step sizes of quality improvements drawn from a Pareto distribution.==== This study extends the elegant model in Minniti et al. (2013) by allowing for a Hopenhayn-Melitz-type entry cost to generate endogenous entry of heterogeneous firms==== and introducing heterogeneous households with different asset holdings. In other words, this study contributes to the literature by developing a Schumpeterian growth model with two dimensions of heterogeneity among households and firms.====This study also relates to the literature on innovation and inflation. In this literature, the seminal study by Marquis and Reffett (1994) analyzes the effects of inflation on innovation in a variant of the Romer variety-expanding model. Subsequent studies analyze the effects of inflation in the Schumpeterian quality-ladder model; see for example Chu and Lai (2013), Chu and Cozzi (2014), Chu et al. (2015), He and Zou (2016), Huang et al. (2017), Neto et al. (2017), He (2018) and Lin et al. (2019).==== However, all these studies feature a constant step size of quality improvement. As a result, these studies predict a monotonic relationship between inflation and economic growth, which is different from the inverted-U relationship between inflation and economic growth often found in empirical studies.==== As a result, Chu et al. (2017) develop a monetary Schumpeterian growth model with endogenous entry of heterogeneous firms,==== and they show that their model can generate an inverted-U relationship between inflation and economic growth and match empirical estimates of the growth-maximizing inflation rate under plausible parameter values. However, all the above-mentioned studies feature a representative household; therefore, they cannot be used to analyze the implications of monetary policy on the income distribution. Therefore, this study introduces heterogeneous households into this literature in order to analyze the effects of monetary policy on income inequality in addition to innovation and economic growth.====This study also relates to the literature on innovation and income inequality. Representative studies include Chou and Talmain (1996), Li (1998), Zweimuller (2000), Foellmi and Zweimuller (2006), Kiedaisch (2017), Grossman and Helpman (2018), Jones and Kim (2018) and Aghion et al. (2019). These studies focus on the relationship between income inequality and innovation. Our study complements these interesting studies by exploring the effects of monetary policy on innovation and income inequality. Chu and Cozzi (2018) explore the effects of R&D subsidies and patent policy on income inequality, but not monetary policy. More importantly, Chu and Cozzi (2018) focus on a Schumpeterian growth model with a constant step size of quality improvement. We show that endogenous entry of heterogeneous firms is necessary for the emergence of an inverted-U effect of inflation on income inequality that is consistent with our empirical finding.====In the New Keynesian literature, recent studies such as McKay and Reis (2016) and Kaplan et al. (2018) introduce heterogeneous agents into the standard New Keynesian model to explore the effects of government policies on inequality via nominal rigidity. In the New Monetarist literature, studies have also introduced heterogeneous agents into the search-theoretic monetary model to explore the effects of inflation on inequality via search and matching frictions; see Rocheteau et al. (2018) for a recent study and a discussion of earlier studies. Our study differs from these interesting studies by exploring the effects of inflation on inequality in a monetary Schumpeterian growth model with heterogeneous agents in which inflation affects inequality via R&D and innovation. In other words, we focus on the long-run effects of monetary policy on the macroeconomy, which complement the interesting effects, emphasized by the New Keynesian model and the search-theoretic monetary model, at different time horizons.====The rest of this study is organized as follows. Section 2 presents the model and solves the market equilibrium of the aggregate economy. Section 3 explores the distributions of wealth and income. Section 4 analyzes the effects of monetary policy. Section 5 provides a quantitative analysis. Section 6 concludes. Proofs are relegated to the appendix.",Innovation and inequality in a monetary Schumpeterian model with heterogeneous households and firms,https://www.sciencedirect.com/science/article/pii/S1094202518303570,26 March 2019,2019,Research Article,256.0
"Minamimura Keiya,Yasui Daishin","Kansai Gaidai University, Japan,Graduate School of Economics, Kobe University, Japan,Kyoto University, Japan","Received 10 May 2018, Revised 12 March 2019, Available online 21 March 2019, Version of Record 28 March 2019.",https://doi.org/10.1016/j.red.2019.03.005,Cited by (5),"This paper develops a growth model à la ==== that captures the replacement of physical capital accumulation by human capital accumulation as the prime engine of growth. We show that (i) decreased mortality promotes this replacement, and (ii) the effect of a decrease in mortality on per-capita income differs across the phases of the development process. A notable prediction of our theory is that the higher the level of education in an economy, the more likely it is that a decrease in mortality will increase income per capita. Using finite mixture models, we show that this prediction is supported by the data. Our results provide a new interpretation on the negative effect of life expectancy on growth reported by ====.","Does a decrease in mortality raise income per capita? An influential paper by Acemoglu and Johnson (2007) provoked a controversy over this question. In contrast to studies based on cross-country variation (e.g., Lorentzen et al., 2008), Acemoglu and Johnson (2007) use instruments for mortality changes to solve the endogeneity problem, and obtain a surprising result: improved life expectancy has a positive effect on population growth, but a negative effect on GDP per capita. This result is somewhat surprising because it implies a trade-off between health improvement and economic growth. Theoretically, their result is justified by the dilution effect in traditional growth theories: population growth decreases land per capita (the Malthus channel) and physical capital per capita (the Solow channel), thus reducing income per capita. On the other hand, recent growth literature suggests the increasing importance of human capital over land and physical capital as an engine of growth, particularly in developed countries (e.g., Jones and Romer, 2010). Here, we develop a growth model in which the replacement of physical capital accumulation by human capital accumulation is endogenous, and mortality changes affect the replacement process. Furthermore, we investigate, theoretically and empirically, how a decrease in mortality affects income per capita.====We extend the model of Galor and Moav (2004), which captures the replacement of physical capital accumulation by human capital accumulation as a prime engine of growth, in order to consider the effect of mortality changes on the development process. Our model derives two key results: (i) decreased mortality promotes this replacement; and (ii) the effect of a decrease in mortality on per-capita income differs across the phases of the development process. As in Galor and Moav (2004), we shed light on the portfolio choice between physical and human capital. However, our focus differs from theirs. We focus on an asymmetry between physical and human capital: human capital is inherently ==== in people and, thus, it yields a return only when the investor survives, whereas physical capital is ==== and, thus, its returns belong to other people (e.g., a spouse, siblings, offspring), even if the investor dies. This asymmetry implies that in environments with high mortality, it may not be profitable for households to concentrate their assets in human capital. A decrease in mortality stimulates the incentive to invest in human capital, and promotes the replacement of physical capital accumulation with human capital accumulation.====Many studies have established the theoretical result that a decrease in mortality promotes human capital investment, based on the Ben-Porath effect (e.g., De la Croix and Licandro, 1999; Kalemli-Ozcan et al., 2000; Boucekkine et al., 2002; 2003; Soares, 2005; Cervellati and Sunde, 2005). The conventional channel through which the Ben-Porath effect operates is as follows. Human capital continues to yield a return as long as agents are able-bodied workers once they form human capital, while physical capital investment yields a once-off return. Thus, a lower mortality prolongs the working life over which investments in human capital pay off, thereby positively affecting human capital investment.==== The novelty of this study is to shed light on the ====, which depreciates fully with the death of the individual, and the ====, which remains after death. Physical capital can be used to improve the living standards of surviving family members after the death of the original investor because it is left to them in the form of bequests. On the other hand, human capital cannot be used in such a way because it ends with deceased person. The ==== and the ==== result in physical capital having an advantage over human capital as intergenerationally-transferred resources. This advantage is large when mortality is high and, thus, a decline in mortality induces agents to substitute human capital for physical capital.====In our model, an economy switches from a regime where only physical capital is accumulated to a regime where both physical and human capital are accumulated during the development process. In the former regime, a decrease in mortality depresses income per capita by the dilution effect on physical capital, especially in the short run. In the latter regime, however, two additional effects improve the average amount of human capital in the labor market. First, a decrease in mortality induces agents to substitute human capital for physical capital, by the mechanism mentioned in the previous paragraph. This shift in assets not only improves human capital, but also weakens the dilution effect, because physical capital is more vulnerable to the dilution effect than is human capital.==== Second, a decrease in mortality raises the average age of the working population. Since older workers have more human capital, because of human capital accumulation, such a change in the age composition of the workforce increases the average amount of human capital in the workforce. In the determination of per-capita output, the human capital improvement effect acts in the opposite direction to the physical capital dilution effect. Our model predicts that a decrease in mortality is more likely to increase GDP per capita in an economy with more education (and more resources) because the human capital improvement effect dominates the dilution effect, while the opposite is true in an economy with less education (and fewer resources).====Fig. 1 depicts the wage profiles of workers in high- and low-education countries, illustrating the intuition behind our mechanism.==== If the wage rate reflects the amount of human capital, the wage profile represents human capital accumulation (through education, experience, and training, etc.) over a lifetime. The wage profile for high-education countries is steeper and has a higher peak than that for low-education countries, implying that the gap in human capital between young and adult workers is larger in high-education countries. A decrease in mortality has two effects: a direct effect and a composition effect. First, it lifts the profile from the low-education type to the high-education type by increasing educational investment. This directly improves the average amount of human capital because each agent has more human capital. Second, it shifts the weight on the workforce from young to adult workers and, thus, has the effect of improving the average amount of human capital. This composition effect is larger in higher-education countries because they have a larger gap in the amount of human capital between older and younger workers.====A notable implication of our theory is the heterogeneity in the effects of mortality on per-capita income among countries. The higher the level of education in a country, the more likely it is that a decrease in mortality will increase income per capita. To test this hypothesis, we use the same data and identification strategy as Acemoglu and Johnson (2007), though our approach differs in that we relax the assumption that all countries are in the same growth regime. Using a finite mixture model (FMM) approach, we investigate whether the effects of mortality on per-capita income differ between countries in high- and low-education regimes.====Cervellati and Sunde (2011a; 2011b) share our motivation to reinterpret the analysis of Acemoglu and Johnson (2007). Cervellati and Sunde (2011b) split the sample of Acemoglu and Johnson (2007) between pre- and post-demographic-transition countries based on the classification criteria used in demography literature. They show that improved life expectancy has a negative effect on per-capita GDP in pre-transition countries, but a positive effect in post-transition countries.==== Cervellati and Sunde (2011a) use an FMM approach without imposing any ex-ante classification, obtaining similar results to those of Cervellati and Sunde (2011b). Cervellati and Sunde (2011a; 2011b) base their studies on demographic-transition theory in order to investigate the role of population growth in determining the effect of mortality on per-capita income. Here, we focus on the role of education in determining the effect of mortality on per-capita income based on human-capital theory. Since the demographic-transition and human capital theories are two pillars in the literature on long-run growth, our study is a natural development of this literature. The empirical part of our study shows that both education and population growth are important in determining the effect of mortality on per-capita income.====The remainder of this paper is organized as follows. Section 2 presents the model and analyses the effect of mortality changes on income per capita. In Section 3, we confront the testable implications of the model with empirical evidence. Section 4 concludes the paper.",From physical to human capital accumulation: Effects of mortality changes,https://www.sciencedirect.com/science/article/pii/S1094202518302175,21 March 2019,2019,Research Article,257.0
"Pensieroso Luca,Sommacal Alessandro","IRES, Université catholique de Louvain, Belgium,Department of Economics, University of Verona, Italy,Dondena Centre (Welfare State and Taxation Unit), Bocconi University, Italy","Received 29 August 2017, Revised 13 March 2019, Available online 21 March 2019, Version of Record 25 March 2019.",https://doi.org/10.1016/j.red.2019.03.007,Cited by (5),"We show that the structural change of the economy from agriculture to ==== was a major determinant of the observed shift in intergenerational coresidence. We build a one-good, two-sector overlapping generation model of the structural change out of agriculture, in which the coresidence choice is endogenous. We calibrate the model on U.S. data and simulate it. The model can match the decline in U.S. intergenerational coresidence both qualitatively and quantitatively.","In this paper, we provide a macroeconomic model in which the structural change out of agriculture determines a shift from intergenerational coresidence to the nuclear family, and we quantify the importance of this mechanism by means of numerical simulations for the United States between the 19th and 20th century.====The family structure in the United States has changed significantly since the nineteenth century. One of the major changes has been the shift from intergenerational coresidence to independent living arrangements for the elderly: according to data, the percentage of elderly persons residing with their adult children plummeted from 69% in 1850, to almost 17% in 1990 (see Fig. 1).====Fig. 1 also shows that a companion fact to the change in intergenerational coresidence was the structural change out of agriculture. If we compare the intergenerational coresidence rate with the employment rate in agriculture between 1850 and 2010, we observe that the two time series show similar behaviour, suggesting the existence of a link between the two phenomena. In Section 2, we delve more deeply into this empirical evidence, and show that there actually exists a robust correlation between employment in agriculture and intergenerational coresidence.====To rationalise the evidence, we propose a formal model based on technical change and the relative income of the different generations. Higher technical change in the industrial sector with respect to the agricultural sector causes a progressive reallocation of labour from agriculture to industry and affects the relative income of the different generations. This in turn changes the decision power of the different generations, and therefore the incentive to coreside.====More specifically, we build a one-good, two-sector overlapping generation model with agriculture and industry à la Hansen and Prescott (2002). We assume that the old own all the land, and receive a rent from it, while the young provide the labour force. The young can work in both the agricultural and the industrial sector, their choice being driven by a no-arbitrage condition on wages in the two sectors. We model the endogenous choice of coresidence as in Pensieroso and Sommacal (2014). In that framework, coresidence is deeply influenced by the relative income of the young with respect to the old. In particular, coresidence decreases when the relative income of the young increases. As productivity in the industrial sector relative to productivity in the agricultural sector takes off, employment shifts from agriculture to industry. The functional distribution of income changes: the wage earned by the young increases, while the rent earned by the old decreases. Therefore, the industrial take off implies a lower coresidence rate.====We calibrate the model to U.S. data and simulate it to quantify the relevance of the proposed mechanism. Our model can reproduce the qualitative behaviour of the intergenerational coresidence rate for the whole period. Furthermore, the model matches the decline in intergenerational coresidence well from a quantitative point of view: it accounts for 71% of the overall observed drop between 1870 and 2010.====This article is linked to three strands of the literature: the literature on the structural change out of agriculture, the literature on intergenerational coresidence and the literature on family patterns and economic growth.====The structural change out of agriculture, whose explanation is still debated, is a defining feature of the industrial revolution. Its role in determining economic development is hardly controversial, as witnessed by a long standing literature in economic development.====For what concerns the decline in intergenerational coresidence, different theories have been advanced in the literature to explain it. A group of authors maintains that the introduction of Social Security is the engine behind the observed shift in the coresidence pattern.==== According to this perspective, also known as the “affluence hypothesis”, intergenerational coresidence was imposed on the elderly by the lack of alternatives. Others take the opposite view, also known as the “economic development hypothesis”, and attribute the shift to the increased income of the young.==== The two perspectives can actually be viewed as complementary rather than alternative, and the pre-eminence of one over the other possibly depends on the period under exam. In particular, since the Social Security Act dates to 1935, the economic development hypothesis seems more relevant for the period before WWII, when there was virtually no Social Security; while the affluence hypothesis might be of some importance for the period after 1950, when Social Security payments became widespread in the United States. As our story focuses on the link between the structural change out of agriculture during the industrial take-off and intergenerational coresidence, much of the action takes place before 1950. In facts, the employment rate in agriculture was already down to 13% in 1950, from 62% in 1850; while the coresidence rate dropped from 69% in 1850 to 40% in 1950 (see Fig. 1).==== Accordingly, drawing inspiration from Pensieroso and Sommacal (2014), we build a quantitative macroeconomic model of the economic development hypothesis to assess how much of the observed decline in intergenerational coresidence can be explained by the structural change out of agriculture, abstracting from other possible explanations like Social Security and cultural change.====Finally, this article contributes to the literature exploring the link between family patterns and economic growth.==== With respect to this literature, we are the first to introduce a quantitative macroeconomic model to study how the structural change out of agriculture during the industrial revolution might have affected intergenerational coresidence.====The rest of the paper is organised as follows. Section 2 discusses the definition of intergenerational coresidence, the empirical link between employment in agriculture and intergenerational coresidence and the relevance of our proposed mechanism with respect to possible competing explanations. Section 3 presents the model. In Section 4 we calibrate the model to U.S. data and simulate it. Section 5 concludes.",Agriculture to industry: The end of intergenerational coresidence,https://www.sciencedirect.com/science/article/pii/S1094202518302862,21 March 2019,2019,Research Article,258.0
"Meza Felipe,Pratap Sangeeta,Urrutia Carlos","CAIE and Dept. of Economics, ITAM, Camino a Sta. Teresa 930, 10700, Mexico D.F., Mexico,Dept. of Economics, Hunter College and Graduate Center, City University of New York, 695 Park Ave, New York, NY 10065, United States of America,CIE and Dept. of Economics, ITAM, Camino a Sta. Teresa 930, 10700, Mexico D.F., Mexico","Received 20 November 2017, Revised 5 March 2019, Available online 14 March 2019, Version of Record 20 March 2019.",https://doi.org/10.1016/j.red.2019.03.004,Cited by (9),"We study the effect of credit conditions on the allocation of inputs, and their implications for aggregate TFP growth. For this, we build a new dataset for Mexican manufacturing merging real and financial data at the 4-digit industrial sector level. Using a simple misallocation framework, we find that changes in inter-industry allocative efficiency account for 41 percent of changes in aggregate TFP. We then construct a model of firm behavior with working capital constraints and borrowing limits which generate sub-optimal use of inputs, and calibrate it to our data. We find that the model accounts for 38 percent of the observed variability in efficiency. An important conclusion is that heterogeneity in credit conditions across industries is key in accounting for efficiency gains. Despite overall credit stagnation, more access to credit and lower ==== to distorted industries contributed substantially to the recovery from the 2009 recession, suggesting a plausible mechanism for credit-less recoveries.","Total factor productivity is an important driver of economic growth and short run fluctuations. A recent literature highlights the role of heterogeneous distortions, i.e., implicit taxes, barriers and constraints that affect economic units differently, as a source of misallocation of resources and lower total factor productivity (TFP) levels (Restuccia and Rogerson, 2008, Hsieh and Klenow, 2009, and Bartelsman et al., 2013). However, the economic forces behind the distortions observed in the data are still not well understood. The objective of this paper is to assess the role of credit and financial frictions in the allocation of resources, and through that, on changes in TFP over time.====Previous studies have found an important role for financial frictions in explaining productivity changes in the context of aggregate models.==== Our analysis uses a rich industrial structure within Mexican manufacturing and exploits the variation in real and financial variables over a ten-year period to revisit this question at a more disaggregated level. This is important since, as Buera and Moll (2015) show, credit shocks and financial frictions that are firm-specific can be mapped into measured distortions obtained using the business cycle accounting methodology of Chari et al. (2007), and through them, into measured aggregate TFP.====An important contribution of our work is the construction of a novel data set linking manufacturing activity in Mexico with credit flows and interest rates at a disaggregated level from 2003 to 2012. This is a particularly interesting time frame to study, since it includes the period of rapid growth of 2003-08, the economic crisis of 2008-09 and the subsequent recovery. We have detailed data on output, input expenditures, investment, capital stock, new credit flows and interest rates for 82 industries. The data allow us to construct measures of input distortions for each industry-year and, in the context of a simple static framework, to map the evolution of these distortions into a measure of inter-industry allocative efficiency. Moreover, the availability of credit data makes it possible to investigate the extent to which input misallocation between industries can be explained by financial factors.====Using data on output and input expenditures by industry, a simple accounting exercise shows that efficiency gains accounted for 41 percent of aggregate TFP movements over time. This number encompasses significant differences between subperiods. More than half the pre-crisis growth and 75 percent of the recovery came from improvements in efficiency. In contrast, the drops in output and TFP in the recession of 2008-2009, which was triggered by an external shock, are not associated with a corresponding decline in efficiency. The accounting framework also suggests that the identity of the industries changing their distortions is important. Improvements in allocative efficiency occur only if highly distorted industries are able to reduce their distortions.====We then build a static model that maps financial frictions into distortions to the use of inputs. Firms face a working capital constraint and a borrowing limit. The former implies that firms have to finance their variable input purchases with bank credit or the more expensive trade credit, while the latter dictates the amount of bank credit available to firms. Taken together, the constraints determine the availability and costs of credit that, in turn, influence input use and determine the degree of allocative efficiency. We calibrate the borrowing constraint for each industry from the data and feed it into the model, along with the observed interest rates. We find that the model accounts for 38 percent of the changes in allocative efficiency over time, and almost 80 percent of the efficiency gains in the recovery.====Our results emphasize the role of heterogeneity across industries. More than half of the efficiency gains in the recovery can be attributed to heterogeneous changes in credit conditions across industries. This suggests a potential mechanism by which “credit-less recoveries”, as documented by Calvo et al. (2006) may occur. Low aggregate levels of credit after the recession mask substantial heterogeneity in credit and interest rates between industries. If credit and/or lower interest rates are available to highly distorted industries, aggregate TFP and output will respond positively. In other words, ====.====The first part of our analysis borrows from Hsieh and Klenow (2009) and several studies that apply their quantitative methodology to infer misallocation from input expenditure shares. There are, however, three important differences. First, our analysis is carried at the industry level, instead of at the firm level, so it focuses only on misallocation between industries. Second, we explicitly include distortions to intermediate goods in the analysis, as in Jones (2011). Third, our focus is on changes in efficiency and TFP over time, rather than on the comparison of productivity levels across countries.====More closely related to our paper is a recent literature on misallocation and TFP changes in episodes of financial crisis. Using firm level data from the Chilean manufacturing sector, Oberfield (2013) argues that a deteriorating allocation of resources between industries explains about one third of the TFP decline. Sandleris and Wright (2014) show that more than half of the drop in TFP in Argentina during the 2001 crisis can be accounted for by efficiency losses, although the salient factor is the worsening of misallocation within industries. Chen and Irarrazabal (2015) focus on the aftermath of the 1982 Chilean debt crisis and show that efficiency gains account for about 40 percent of the recovery in manufacturing TFP. Although we do not find evidence of a worsening in misallocation between industries during the 2009 recession (which did not lead to an internal financial crisis in Mexico, unlike the 1982 Chilean crisis), our results are consistent with Chen and Irarrazabal's for the recovery period. Also, while these studies use more disaggregated, firm-level data, our contribution is to provide direct evidence of a relation between changes in inter-industry allocative efficiency and credit conditions.====The important role of credit conditions for efficiency gains stands in contrast to the results in Gilchrist et al. (2013) and Midrigan and Xu (2014), who find small losses to TFP from financial frictions. However, both studies focus on differences in TFP levels, whereas our goal is to study how changes in credit conditions affect efficiency and TFP growth.==== Still, for comparability we calculate the static TFP loss due to misallocation in our model to be 8.5 percent, which is larger than the upper bound in these two studies. This is partly because the dispersion in interest rates that we observe in our data is larger than in the U.S., and partly due to the amplifying effect of distortions to intermediate goods, from which they abstract. We discuss these differences in detail in Section 5.2.====The paper is organized as follows. In Section 2 we describe the data used to analyze the relationship between economic activity and credit. Section 3 develops an analytical framework to identify industry-level distortions and account for aggregate TFP changes through efficiency gains in Mexican manufacturing. Section 4 presents a static model with financial frictions linking credit variables and industry distortions, that we use to account for observed efficiency gains. In Section 5, we perform some counterfactual experiments and analyze the robustness of the results. Section 6 concludes.","Credit, misallocation and productivity growth: A disaggregated analysis",https://www.sciencedirect.com/science/article/pii/S109420251830293X,14 March 2019,2019,Research Article,259.0
Robatto Roberto,"University of Wisconsin-Madison, United States of America","Received 9 March 2017, Revised 28 February 2019, Available online 7 March 2019, Version of Record 13 March 2019.",https://doi.org/10.1016/j.red.2019.03.001,Cited by (15),I present a ,"The 2007–2008 US financial crisis and the Great Depression were characterized by a flight to liquidity (i.e., a dramatic increase in the private sector's willingness to hold liquid assets) and runs on several financial intermediaries. In 2007–2008, the crisis affected mostly shadow banking institutions not covered by deposit insurance—runs took place on the repo market (Gorton and Metrick, 2012), on asset-backed commercial paper programs (Covitz et al., 2013), and on money market mutual funds (Schmidt et al., 2016).==== During the Great Depression, the US economy experienced three main waves of runs on traditional banks (Friedman and Schwartz, 1963). The Federal Reserve reacted aggressively in 2007–2008 by implementing unconventional monetary policies, whereas its response was more muted during the Great Depression. Indeed, Friedman and Schwartz (1963) argue that Federal Reserve interventions were inadequate and helped to cause and exacerbate the Depression.====Motivated by these events, this paper provides a general equilibrium model of banking with multiple equilibria. In the good equilibrium, all banks are solvent and there are no runs. In the bad equilibrium, several banks are insolvent and subject to runs at the same time, capturing the systemic nature of financial crises. The distress in the banking sector is associated with deflation, a drop in nominal asset prices and money velocity, and a flight to liquidity (that is, depositors hold more money in nominal terms and fewer deposits at banks, in comparison to the good equilibrium). To describe the results of the baseline model with fixed money supply, I present a quantitative example calibrated to the Great Depression. I choose to focus on the Great Depression because the limited policy interventions make that episode a natural benchmark to study the model with a fixed money supply. I then use this framework to study whether the central bank can eliminate a banking panic using monetary injections.====The two key ingredients that give rise to multiple equilibria are liquidity risk ==== Diamond and Dybvig (1983) and debt deflation in the spirit of Fisher (1933).==== During crises, banks do not function well, and thus households fly to money and away from less liquid assets to self-insure against liquidity risk. This flight to liquidity depresses the price of illiquid assets in comparison to that of money (i.e., the nominal price of illiquid assets), including the price of the assets held by banks. Banks suffer losses because of the mismatch between the denomination of assets and liabilities; that is, in nominal terms, assets are valued at (dislocated) market prices, whereas liabilities are fixed. The losses worsen the conditions of banks, reinforcing the flight to liquidity and making the process self-fulfilling. Thus, the channel that gives rise to multiple equilibria is different from that of Diamond and Dybvig (1983), and is instead based on a strategic complementarity in the decision to fly to liquidity.====I build on a growing literature that studies bank runs in environments with money and nominal contracts, such as Allen and Gale (1998), Allen et al. (2013), Diamond and Rajan (2006), and Carapella (2012). In comparison to these papers, my model improves the match with some key stylized facts, and the same modeling assumptions that improve such a match generate novel policy results. For instance, in Allen and Gale (1998), Allen et al. (2013), and Diamond and Rajan (2006), crises are not associated with flights to liquidity or deflation. In Carapella (2012), all banks become insolvent, whereas a crisis in my model does not completely shut down the financial sector because of heterogeneity across banks (i.e., only some banks become insolvent). This approach not only is more consistent with the data but also creates novel policy results because monetary injections generate non-trivial general equilibrium effects on the amount of resources intermediated by the financial sector. Models in which all banks become insolvent and deposits drop to zero in the bad equilibrium can only highlight general equilibrium effects of policy interventions that are strong enough to eliminate such an equilibrium. My model can also highlight general equilibrium effects of monetary injections that increase deposits without eliminating the bad equilibrium, and even effects that reduce banks' deposits, which in turn weaken the effectiveness of such monetary injections.====In the policy analysis, I ask whether and how monetary injections eliminate the bad equilibrium. In particular, the central bank can inject money into the economy using two tools: (i) ==== (i.e., buying assets on the market) or (ii) ====. The focus on these tools is motivated by the monetary injections of the Federal Reserve during the 2007–2008 financial crisis. Using numerical simulations of the model, I show that both tools eliminate the bad equilibrium provided that the intervention of the central bank is sufficiently large, in line with the Friedman-Schwartz hypothesis.====The key policy result is that loans to banks are more effective than asset purchases at eliminating the bad equilibrium, in the sense that they require a smaller monetary injection.==== Under the loans-to-banks policy, the losses of insolvent banks are borne not only by the depositors of those banks but also by the central bank, which in turn redistributes the losses lump-sum to all depositors in the economy. I show that a policy of loans to banks is equivalent to a combination of asset purchases and a lump-sum transfer to banks subject to runs (which can be interpreted as partial deposit insurance or an equity injection) financed by lump-sum taxes on depositors. The direct effect of the implicit transfer embedded in the loans-to-banks policy is to increase the return on deposits at insolvent banks, thereby increasing the incentive to deposit more at banks. In addition, this result creates an indirect general equilibrium effect. Since everybody deposits more at banks, the strategic complementarity that generates the flight to liquidity is weakened, which in turn increases the effectiveness of the loans-to-banks policy even further.====In addition, I provide an example in which a moderate temporary monetary injection gives rise to a bad equilibrium, even if the bad equilibrium does not exist without policy intervention. While the direct effect of the intervention is to increase current nominal asset prices—including the prices of banks' holdings of illiquid assets, thereby boosting banks' equity values—an indirect effect arises as well. The temporary nature of the intervention implies that future asset prices are unchanged. As a result, higher current asset prices reduce the return earned by banks on their investments and, thus, the return that banks pay on deposits. This effect reduces deposits in the first place and thus might create or amplify the flight to liquidity that is responsible for the existence of the bad equilibrium. Even though large monetary injections always eliminate the bad equilibrium, the unintended consequence can arise with moderate monetary injections.====  I use a three-period model in which households are subject to liquidity shocks and banks offer deposits, in the spirit of Diamond and Dybvig (1983). There is an exogenous supply of two assets: fiat money and a productive asset (capital). Money is the liquid asset that allows the consumption expenditure to be financed, but it pays no return, whereas capital is illiquid but pays a positive return. The model can be naturally applied to the Great Depression, but a reinterpretation of its elements allows a comparison with the 2007–2008 financial crisis as well.====Some elements of banks' balance sheets are crucial. First, banks are endowed with assets (i.e., capital and money) and liabilities (i.e., deposits) at ====, which are motivated by preexisting banking relationships. Second, banks' endowment are heterogeneous, and each bank has private information about its endowments. This is motivated by Gorton (2008), who emphasizes the uncertainty regarding the identities of the weakest financial institutions during the Great Recession.==== Third, the preexisting deposits are nominal, that is, specified in terms of money.====The logic of the good equilibrium is similar to that of Diamond and Dybvig (1983), but the logic of the bad equilibrium is very different. In the bad equilibrium, at ====, households become concerned about the possibility of runs on their own bank at ====. As a result, at ====, they fly to liquidity, withdrawing some of their preexisting deposits and holding more money than in the good equilibrium; nonetheless, they leave some deposits in the banks because of the opportunity cost of holding money. Households' pricing kernel determines asset prices in the economy; the combination of liquidity risk and runs at ==== implies that illiquid physical capital loses value in comparison to money, and thus its nominal price drops. To repay the withdrawals at ====, all banks sell some of their endowment of capital at this low nominal price, whereas liabilities are fixed in nominal terms. As a result, the weakest banks become insolvent at ====, but they are subject to runs only at ====, when the asymmetric information about banks' balance sheets is resolved. In addition, the price level drops in the bad equilibrium because part of the money supply is held for precautionary reasons and not spent.====The drop in asset prices and banks' sale of capital in the bad equilibrium are similar to the real model of Gertler and Kiyotaki (2015), but with two crucial differences.==== First, asset prices drop in Gertler and Kiyotaki (2015) because banks sell assets to agents who have an exogenously lower ability to manage such assets, as in Shleifer and Vishny (2011). In contrast, illiquid asset prices are depressed in my model because runs and liquidity risk affect the households' pricing kernel. Thus, the friction that justifies short-term, demandable debt—namely, liquidity risk—also affects the price of illiquid assets. Second, nominal bank liabilities play an essential role in my model because the drop in asset prices is in nominal terms.====Despite the role of banks in my model being similar to that of Diamond and Dybvig (1983), my model differs from theirs in a number of ways. First, there is only one real asset in Diamond and Dybvig, so it is difficult to use their model to analyze monetary injections.==== Second, the model of Diamond and Dybvig has exogenous asset returns and thus is typically interpreted as a partial equilibrium model of one bank. In contrast, my analysis is based on a general equilibrium model with endogenous returns, and runs are systemic events that involve a fraction of the banking system.==== Third, the coordination failure in Diamond and Dybvig is related to the decision to run, whereas in my model it is related to the decision to fly to liquidity.====Monetary injections during banking panics are also studied by Cooper and Corbae (2002) in a model with multiple equilibria driven by increasing returns to scale in intermediation. They focus on steady states in which banks are either perpetually malfunctioning or well functioning, whereas I analyze a crisis that eventually ends. As a result, the policy implications are different. Monetary policy eliminates the bad equilibrium by permanently increasing the growth rate of money in their model and by temporarily increasing the level of money in my model.","Systemic banking panics, liquidity risk, and monetary policy",https://www.sciencedirect.com/science/article/pii/S1094202518302643,7 March 2019,2019,Research Article,261.0
"Lama Ruy,Medina Juan Pablo","International Monetary Fund, United States of America,Universidad Adolfo Ibáñez, Chile","Received 4 November 2015, Revised 23 February 2019, Available online 6 March 2019, Version of Record 2 April 2019.",https://doi.org/10.1016/j.red.2019.02.007,Cited by (2),"This paper studies the joint dynamics of fiscal deficits and unemployment in a ==== rigidities. First, we show that a tax increase or a reduction in government spending can improve the fiscal balance at the expense of a higher unemployment rate. However, under a scenario of rigid wages and productivity gains, it is possible to achieve a simultaneous reduction of the fiscal deficit and the unemployment rate. Second, we analyze the Swedish ==== episode of the 1990s through the lens of the model. The model is capable of reproducing the simultaneous reduction in fiscal deficits and unemployment observed during this episode. Counterfactual simulations show that in the absence of TFP gains and rigid wages, fiscal consolidation measures alone would not have eliminated fiscal deficits, and the unemployment rate would have reached double digit levels.","Fiscal consolidation programs have been implemented in recent years in several advanced economies with the objective of reducing fiscal deficits and ensuring sustainable path for public debt. While there are tangible benefits from preserving the sustainability of public finances, fiscal consolidation programs might have negative short-run effects on economic activity and employment.==== Moreover, critics of consolidation plans argue that these can be self-defeating, as a fiscal tightening may reduce output and worsen the overall fiscal position as tax revenues decline. In this context, a key policy question is how to implement a fiscal consolidation that ensures a reduction in fiscal deficits while simultaneously minimizing output and employment losses. To shed light on this question, using a neoclassical growth model with distortionary taxation and labor search frictions, we analyze the extent to which the combination of productivity gains and wage rigidity can contribute to a simultaneous reduction of fiscal deficits and the unemployment.====Fig. 1 motivates our quantitative analysis by illustrating the dynamics of fiscal deficits, unemployment, and total factor productivity (TFP) around major fiscal consolidation episodes in advanced economies.==== While governments introduced discretionary changes in taxes and spending aimed at reducing fiscal deficits in all those episodes, one salient feature of the data is that countries that experienced improvements in their primary fiscal balances also had significant gains in TFP. The association between TFP gains and improvements in the primary fiscal balance occurs through two channels: (i) an increase of fiscal revenues; and (ii) a reduction of the ratio of government spending to GDP. As TFP improves, there is a boost in output that expands the tax base, resulting in higher fiscal revenues and lower deficits for a given level of tax rates. Moreover, for a given path of government spending, higher TFP and output reduces the ratio of government spending to GDP, generating an improvement in the fiscal balance as percentage of GDP.====Furthermore, as shown in the second chart in Fig. 1, countries that experienced higher TFP gains had larger declines in unit labor costs (measured by the ratio of labor compensation to labor productivity). This implies that labor productivity increased at a faster pace than labor compensation during these consolidation episodes. Since wage increases were outpaced by productivity gains (a manifestation of wage rigidities), the relative cost of hiring declined, resulting in a higher labor demand and a lower unemployment rate, as shown in the third chart in Fig. 1. To summarize, Fig. 1 illustrates that the productivity gains and wage rigidities (reflected in declining unit labor costs) are associated with simultaneous reductions in fiscal deficits and unemployment.====With the goal of analyzing the quantitative implications of productivity gains and wage dynamics on fiscal deficits and unemployment, we develop a neoclassical growth model with distortionary taxation, unemployment, and real wage rigidities. In a calibrated version of the model, we first illustrate the implications of fiscal adjustment strategies (a reduction in government spending and a tax rate increase) under different scenarios of productivity and wage dynamics, and show that under the assumption of productivity gains and wage rigidities, it is possible to achieve simultaneous reductions in the fiscal deficit and the unemployment rate.==== Then, we empirically evaluate the model in a specific episode–the Swedish fiscal consolidation episode (1994-2000).==== In our simulations, we consider three types of shocks in accounting for the macroeconomic outcomes during this period: (i) government consumption; (ii) tax rates; and (iii) productivity shocks.====The calibrated version of the model is capable of reproducing the simultaneous decline in fiscal deficits and unemployment observed in the Swedish economy during the 1990s. The paper also evaluates the contributions of different factors in accounting for the dynamics of the fiscal balance and unemployment by conducting counterfactual simulations. First, we find that fiscal consolidation measures alone contributed to a fiscal deficit reduction, but at the cost of increasing the unemployment rate. The decline in the unemployment rate during the period is explained by the combination of sustained TFP gains and wage rigidities. In the absence of TFP gains and high wage rigidities, fiscal consolidation measures alone would have resulted in persistent fiscal deficits and a double digit unemployment rate.====This paper is related to the extensive literature on the macroeconomic effects of fiscal policies. Our starting point is a neoclassical model with fiscal policy, as in Ohanian (1997), McGrattan and Ohanian (2010), and Uhlig (2010). However, we depart from the standard neoclassical model by incorporating two features. First, we include frictional unemployment, following Shimer (2005), Mortensen and Pissarides (1994), Merz (1995), and Andolfato (1996). Second, we consider real wage rigidities, as in Shimer (2012) and Gorodnichenko et al. (2012). These two features are key in allowing the model to reproduce the dynamics of unemployment rate during the fiscal episode. This paper contributes to the literature by quantifying the potential role of productivity gains and wage dynamics in offsetting the negative effects of a fiscal adjustment on output and employment.====The remainder of the paper is organized as follows. Section 2 lays out a neoclassical model with labor search frictions and distortionary taxation. Section 3 summarizes the calibration strategy. Section 4 presents the implications of fiscal adjustment strategies under different scenarios of productivity gains and wage dynamics. Section 5 discusses the model-based analysis of the Swedish fiscal consolidation episode. Section 6 concludes.",Fiscal austerity and unemployment,https://www.sciencedirect.com/science/article/pii/S1094202519301140,6 March 2019,2019,Research Article,262.0
"Shimer Robert,Werning Iván","University of Chicago, USA,MIT, USA","Received 25 May 2018, Revised 31 December 2018, Available online 6 March 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.02.004,Cited by (4),"We study pairwise trading mechanisms in the presence of one-sided or two-sided ==== and two-sided limited commitment, whereby either trader can walk away from a proposed trade when he learns the trading price. We show that when one trader's information is relevant for the other trader's value of the asset, optimal trading arrangements may necessarily conceal the traders' information. While limited commitment itself may not be costly, it shapes how prices transmit information.","How does dispersed information get transmitted and aggregated by financial markets? What are the impediments to information revelation? Is information passed on from informed to uninformed traders? Is information transmission always desirable?====Economists have important paradigms to address these classical questions. Grossman (1976) first showed that rational expectation models aggregate dispersed information through prices, formalizing the ideas in Hayek (1945). Indeed, under some conditions, prices may reveal information perfectly. This leads to a paradox: if private information is instantly revealed then there are no incentives to gather it. In response, Grossman and Stiglitz (1980) and Kyle (1985) introduced noise traders, alongside informed and uninformed traders. Informed traders do their part to reveal information, but their effect on the price is distorted by the presence of noise traders. Another reason prices might not reveal private information is if traders have market power (Glosten, 1989), in contrast to the price-taking assumption in rational-expectations equilibrium. An alternative recent approach considers decentralized settings where information takes time to percolate through the entire market (see e.g. Duffie and Manso, 2007; Duffie et al., 2014). Traders learn from each other in each meeting, but it takes time for this information to get around. In all these theories, information transmission enhances the efficiency of the market, but it may be hindered by noise traders or delayed by illiquid markets.====The purpose of this paper is to propose and explore a different paradigm for thinking about these questions. We also focus on decentralized over-the-counter markets; however, we take a step back from the slow diffusion of information across the market in order to zoom in on how information is transmitted within a bilateral meeting. To avoid ad hoc assumptions on trading arrangements, we use a mechanism design approach (Kennan and Wilson, 1993). Any specific trading arrangement, such as a particular bargaining protocol, is a special case of the mechanisms we consider. We focus on environments where traders are unable to commit to trade before they know the terms of trade. Our main results question the assumption that information is revealed in bilateral trades. We find that when at least one trader's value of the trade depends on the other trader's information and the uninformed party cannot commit to trade, efficient trading arrangements prevent the revelation of information in order to enhance the bilateral gains from trade. Information revelation is imperfect by design, not because of external impediments or constraints.====Our approach is firmly rooted in classical microeconomic information theory and recent advances in mechanism design. Indeed, we will leverage several useful concepts and results from this literature. However, our focus and perspective is quite different, since the existing literature has focused on the impediments to efficient trading due to private information, rather than addressing questions about information transmission in trade.====We focus on a bilateral trade situation under incomplete information. A single buyer and a single seller meet and there are gains from trade. Each trader may have some private information that is possibly relevant to both traders' valuations. The two traders negotiate over a trading price, but each retains the option to walk away from the negotiation at any time. We ask whether the negotiation process, including the trading price, reveals each trader's information to the other. We find that the answer depends on how negotiations are structured, by which we mean the mechanism that links the structure of the model, including the traders' private information, to the terms of trade. There are mechanisms that fully reveal each trader's information to the other. However, sometimes Pareto efficient mechanisms must hide some information, which enables one trader to profit at the expense of the other. This is true even though there exist feasible (but Pareto inefficient) mechanisms where all information is revealed and no individual ever regrets a trade. If existing arrangements strive for trading efficiency, then our results imply that information transmission may be purposefully prevented in the marketplace.====We believe this insight may be useful for understanding trading patterns and information revelation in over-the-counter securities markets. A key attribute of these markets is that many buyers and sellers are financially sophisticated traders who, under certain circumstances, may have private information about an asset's cash flow. The information may not be directly about the quality of the asset they are trading, but instead might be about the underlying economic environment. For example, traders may disagree about how to model the stochastic process for a new security's cash flow. Over time, ample historical data may limit the scope for disagreement, but then a highly unusual stream of cash flows—a rare event—may lead traders to conclude that the old cash flow model is misspecified. Sophisticated traders will respond to this uncertainty by undertaking research to improve their model. They will also recognize that other sophisticated traders will perform their own research, obtaining their own model. These competing models are the traders' private information.====This discussion also suggests that both sides of the market may be privately informed with useful information. While it may be reasonable that a car's owner has unique insight into the car's quality to the extent a car is an experience good, this assumption seems less plausible for many financial assets.==== Any trader can try to model an asset's cash flow, whether he owns that asset or not. We start with the simpler one-sided private information scenario and show that our key results hold in that environment. However, we also consider a situation with two-sided private information because we believe this case is realistic.====The critical issue is the winner's curse: a rational buyer will fear that if a seller is willing to sell him an asset at a low price, the seller's model suggests that the assets's value is low, reducing the buyer's willingness to pay. The more information is conveyed through prices, the more acute this fear will be and the greater will be the reduction in the willingness to pay. Similarly a seller will rationally worry that if a buyer is willing to purchase an asset at a high price, the buyer's model suggests that the asset's value is high. These concerns can lead to a breakdown in trade, even if it is common knowledge that there are gains from trade at some price. If less information is conveyed through prices, the response of willingness-to-pay to price is weaker, potentially allowing for more trade.====Trading mechanisms that obscure private information mitigate this problem. If a buyer knows only that his own signal is low, his willingness-to-pay is higher than if he knows both signals are low; and similarly a seller who knows only that her own signal is high has a lower reservation price than a seller who knows both signals are high. In fact, under certain conditions, a constant sales price will ensure that trade always occurs yet will reveal no information between the trading partners, while a fully revealing trading mechanism will necessitate a complete breakdown in trade. This illustrates the tension between trading efficiency and information revelation.====This tension might seem counterintuitive. A reduction in asymmetric information alleviates the lemons problem in Akerlof (1970), so one might expect that Pareto optimal trading mechanisms would induce traders to reveal their information to each other. This conclusion would be warranted if there were policies that could costlessly force traders to reveal their information. In our environment, however, information revelation is endogenous. It is possible to construct trading mechanisms that induce traders to reveal their information to each other, but we find that Pareto optimal mechanisms generally do not have this property.====In the standard mechanism design approach to bargaining (Myerson, 1979), each trader observes his own signal and makes a report to the mechanism, which then instructs the traders on whether and at what price to trade as a function of those reports. Mechanisms must satisfy participation constraints, so both traders are willing to enter the mechanism ====, before observing their signal, or at the ==== stage, after observing their signal.====A distinguishing feature of our approach is that we also impose ==== participation constraints: after the mechanism recommends that trade takes place at a particular price, both traders have the option to walk away, given all the information available at that time. For example, if a buyer is only willing to buy an asset at some price in the event that the seller would refuse to sell at that same price, trade will not occur at that price. We assume this lack of commitment because we believe it is natural in over-the-counter markets. In these settings, we find it difficult to imagine market participants being committed to trade before they know the price at which trade takes place. It is also an immediate implication of any standard bargaining protocol.====The lack of commitment assumption is critical to our results and puts information transmission at center stage. The key issue is that ex post constraints depend on the information revealed by the price. If traders are committed to trade before the mechanism gives its instructions, it is costless for the mechanism to reveal each trader's information to the other. Ex post participation constraints capture how learning from prices constrains the set of feasible trades. Formally, we look at the set of veto-incentive-compatible mechanisms (Forges, 1999). Veto incentive compatibility constraints allow for the possibility that a trader misreports his type and then decides not to trade upon learning the recommendation of the mechanism.====Interestingly, the lack of commitment not to walk away may or may not affect the efficiency of trade. In some cases the maximum feasible gains from trade is unaffected by whether traders have a commitment technology. That lack of commitment can be costless may explain why real-world trading arrangements allow traders to back away from a trade and do not introduce institutional arrangements to create more commitment. Even if the lack of commitment does not affect attainable allocations or the gains from trade, it is crucial for our main conclusion, that information revelation is costly. Thus, it potentially affects how we implement allocations, hiding information.====Conversely, lack of commitment does reduce allocative efficiency in some cases. An interesting and open question is whether blockchain or similar technologies may help relax ex post participation constraints by providing an additional commitment technology. Such a technology might then allow for better information transmission and improved trading efficiency, alleviating the problems that we highlight in this paper.====We do not view our paper as offering actual proposals for a mechanism that will improve the efficiency of trade, although it may be possible to use our approach to construct such a mechanism. Instead, we are interested in understanding what a pair of traders can accomplish on their own and what features real-world trading outcomes might have. Our main conclusions are (i) we should not expect that pairwise optimal trading mechanisms will induce information revelation; and (ii) we should not be surprised by the ability of either party to walk away from a trade based on the information they learn through bargaining.====Also important for our results is that traders hold information that the other party cares about. We prove in Section 4 that revealing information is costless in pure private values environments, such as Myerson and Satterthwaite (1983). This is because traders never learn anything that changes their willingness to trade at a particular price. The trade-off between information transmission and trading efficiency is only important when at least one of the traders has information that is valuable to the other. Nevertheless, we believe that this common values case is the most natural one in asset markets, since everyone would like to have a better model of an asset's cash flow.====This paper is related to a growing literature that examines information diffusion in over-the-counter markets. To focus on information diffusion, these papers often assume that traders share all of their information in every meeting. Some treat this assumption as a primitive (Duffie et al., 2009), but others generate this from the trading mechanism. For example, Duffie and Manso (2007) and Duffie et al. (2010) assume that traders observe each others' bids in a second price auction. They can invert those to infer each others' beliefs, which they use to update their own beliefs in future meetings. Duffie et al. (2014) assume pairwise meetings with prices determined by a double auction. Under the assumption that bids are monotone in beliefs, each trader can again invert the other's bid to update their own beliefs.====In other papers, not all information is transmitted in every meeting. The classic example of such a situation is Wolinsky (1990). A large number of individuals meet in pairs to bargain over an indivisible asset. If they reach an agreement, they trade and each exits the market. If they fail to reach an agreement, they wait one period and are matched with another randomly selected trader in the following period. Wolinsky (1990) proves that, even in the limit as the delay between trading rounds disappears, trading prices do not reflect all of the available information. In contrast, Green (1991) shows that it is feasible for a patient uninformed trader to elicit the private information held by informed competitive traders. In a more recent paper, Golosov et al. (2014) relax the assumption that assets are indivisible and that traders exit after trade. They conclude that information gradually diffuses through the economy, with the value of information converging to zero.====Our environment is much simpler than the ones in these papers, since there is only one buyer and one seller. But in contrast to the existing literature, we study optimal trading arrangements and explore the tension between information diffusion and trading efficiency. Any particular trading protocol may not reveal information fully, but this leaves open the possibility that other better mechanisms reveal more information. Our results show that the efficient trading mechanisms purposefully do not reveal information fully. We leave the issue of merging these two approaches to future research.====An important recent theoretical contribution exploring ex post participation constraints and veto incentive compatible mechanisms is Gerardi et al. (2014), who consider an environment with one-sided private information. They compare the set of allocations attainable in this environment to those attainable in the standard mechanism design problem with commitment. Although we use the same theoretical concepts, such as veto incentive compatibility, their focus on allocations is quite different from our focus on information revelation, indeed, in some sense the opposite. For example, even in cases where the no-commitment and commitment allocation coincide, the implications for information revelation are different. Their main result provides a characterization of veto incentive compatibility constraints showing these can be reduced to a set of linear constraints expressed directly in terms of the allocation, casting information transmission entirely to one side.====There are other approaches to mechanism design with limited commitment which show how this can create an incentive to hide information. For example, Green and Laffont (1987) study a mechanism design problem where two agents send messages that can depend on their private information. The mechanism then selects an outcome as a function of their messages. They impose an additional restriction on the message space and the mechanism, that messages must also be a best response to each other. Whenever messages reveal private information, this tightens the incentive compatibility constraint. It follows that hiding information by restricting the message space is generally optimal in this environment as well. A crucial difference between our environments is that, rather than working with a refinement of the set of admissible mechanisms, our model directly captures a feature of the environment: traders lack commitment and can walk away after a proposed outcome is determined by the mechanism and revealed to them. See also Laffont and Tirole (1988) and Bester and Strausz (2001) for dynamic principal-agent problems with short-term contracts. These papers show that it may be necessary to hide the agent's private information from the principal in the first period in order to achieve better outcomes in the second period, when the principal will try to exploit that information.====The remainder of this paper proceeds as follows. Section 2 considers the one-sided private information case. Section 3 sets up the two-sided information case. Section 4 then discusses the special case of private values while Section 5 treats the case of common values. We conclude briefly in Section 6.",Efficiency and information transmission in bilateral trading,https://www.sciencedirect.com/science/article/pii/S1094202518303132,6 March 2019,2019,Research Article,263.0
"Foley-Fisher Nathan,Gissler Stefan,Verani Stéphane","Federal Reserve Board, 20th & C Street, NW, Washington, DC 20551, United States","Received 25 May 2018, Revised 14 February 2019, Available online 1 March 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.02.005,Cited by (12),"This paper studies how over-the-counter market liquidity is affected by securities lending. We combine micro-data on corporate bond market trades with securities lending transactions and individual corporate bond holdings by U.S. insurance companies. Applying a difference-in-differences empirical strategy, we show that the shutdown of AIG's securities lending program in 2008 caused a statistically and economically significant reduction in the market liquidity of corporate bonds predominantly held by AIG. We also show that an important mechanism behind the decrease in corporate bond liquidity was a shift towards relatively small trades among a greater number of dealers in the interdealer market.","The financial crisis of 2007-2009 kindled a wider interest in studies of liquidity in over-the-counter (OTC) markets, in which participants trade without centralized exchanges.==== In the absence of such exchanges, buyers and sellers in OTC markets must devote time and resources to trade, which impedes market liquidity, the ability to transact efficiently (Duffie et al., 2005; Lagos et al., 2011). Intermediaries, such as broker-dealers, may emerge to match buyers and sellers and to maintain an inventory of securities (Hugonnier et al., 2014; Chang and Zhang, 2015; Neklyudov and Sambalaibat, 2015; Wang, 2016). Nevertheless, costs associated with dealing securities, such as inventory holding costs or time to search for and bargain with counterparties, mean that frictions remain an important feature of OTC markets.====Securities lending markets offer dealers a way to mitigate the consequences of frictions inherent to OTC markets. As illustrated in Fig. 1, a dealer that receives a buy order from a client will try to find another client with a matching sell order. While searching, the dealer can fill the buy order using its own inventory, by locating the security in the interdealer market, or by borrowing the security from a securities lender. In exchange for paying a fee and posting collateral, dealers borrow securities from other financial institutions with large portfolios of securities, such as insurance companies. To be sure, the lending market itself may not be frictionless and impediments to the ability to borrow securities could impinge on market liquidity. Such frictions may arise from high lending fees for borrowing certain securities (Duffie, 1996; Krishnamurthy, 2002; D'Avolio, 2002) or from search and bargaining in the securities lending market (Duffie et al., 2002; Vayanos and Weill, 2008; Sambalaibat, 2017). Nevertheless, when the costs of interdealer trading and inventory are relatively high, securities lending can improve market liquidity by reducing the costs of dealing securities.==== However, identifying and quantifying the importance of securities lending to OTC market liquidity is far from trivial due to the many confounding determinants of market liquidity.====In this paper, our objective is to better understand the causal effect of securities lending on OTC market liquidity. We exploit a shock to corporate bond securities lending during the 2007-2009 financial crisis to identify this causal effect. Fig. 2 illustrates the size of the shock that occurred during the financial crisis. The figure shows that, in the period before the financial crisis, around $100 billion of corporate bonds were lent against cash collateral on any given day. The corporate bond securities lending market collapsed by about half towards the end of 2008, largely due to insurance companies, which at that time accounted for more than three-quarters of all loans.==== While AIG was not solely responsible for the market-wide collapse in late 2008, its narrative is the canonical example for the shock (Peirce, 2014; McDonald and Paulson, 2015). Amid concerns about the quality of cash collateral reinvestment that were unrelated to liquidity in the market for corporate bonds, securities borrowers demanded the return of their cash and precipitated the collapse in securities lending.====To analyze the interaction between corporate bond market liquidity and securities lending, we construct a new dataset by combining micro-level data on corporate bond transactions with individual corporate bond loans. We obtain a comprehensive overview by matching the Trade Reporting and Compliance Engine (TRACE) records of OTC corporate bond transactions with bond-level securities lending transactions from Markit Securities Finance, which provides the most extensive coverage of the securities lending market. Lastly, we add information on the bond-level holdings and securities lending activity of U.S. insurance companies from their annual statutory filings. Our basic empirical strategy is to study the impact of the effect of insurance companies lending programs on the dynamics of corporate bond market liquidity.====The main empirical challenge is to obtain a shock to the supply of corporate bonds in the securities lending market that is independent of the demand for corporate bond borrowing and liquidity in the spot market. Insurers, who make their bonds available to the securities lending market, naturally respond to both supply and demand factors. The availability of corporate bonds in the securities lending market reflects conditions in the spot market that simultaneously affect bond borrowing demand and bond lending supply. Thus, to measure the effect of securities lending on bond liquidity requires an identifying shock.====We exploit our institutional setting to address this identification challenge. In particular, we distinguish between AIG and other U.S. life insurers that had securities lending programs. AIG experienced a complete shutdown of the securities lending program operated by its life subsidiaries, falling from over $80 billion to almost nothing in less than one year.==== By contrast, while other life insurers' securities lending programs shrank somewhat during the crisis, they continued to be active. For example, MetLife remained a significant corporate bond securities lender and has since become the largest lender of corporate bonds.====Fig. 3 illustrates the size of the shock to AIG relative to other securities lenders. We divide our data into three mutually exclusive subsamples. The first subsample contains those corporate bonds that are held by insurance companies excluding AIG. The second subsample contains those corporate bonds that are held predominantly by AIG.==== The third and final subsample contains those corporate bonds that are not held by any insurer. Figs. 3a and 3b show coefficient plots from regressions of corporate bond availability for securities lending and actual lending, respectively, using quarterly dummies to reveal the time-series dynamics.==== The evolution of the gap between the blue diamonds and green squares is the variation underpinning our empirical strategy. Fig. 3a shows how the availability of those corporate bonds held predominantly by AIG fell relative to those bonds held by other insurers. To be sure, the availability of corporate bonds held by ==== insurance companies falls in the second half of 2008 as their securities lending programs shrink. However, as indicated by the gap between the blue diamonds and green squares, the decrease in availability was especially pronounced for AIG. This gap opens slowly and becomes significant as early as 2008Q3 because AIG's securities lending program was shut down gradually. Fig. 3b shows that the actual lending of corporate bonds also fell for AIG relative to other insurers. The gap between the blue diamonds and green squares opens sharply in 2008Q4 and widens only slightly in subsequent quarters, which could, of course, potentially reflect both supply and demand factors.====Our empirical analysis exploits the shutdown of AIG's securities lending program to implement a difference-in-differences strategy. The dependent variable is an individual bond's market liquidity, measured as the bond's monthly average realized bid-ask spread.==== The first difference in our strategy is between those bonds that AIG held a high fraction of industry holdings in 2006 and those bonds in which AIG held a low fraction. The second difference is between the period before and the period after the shutdown in AIG's lending programs at the end of 2008. These differences together identify the effect of an exogenous reduction in corporate bond securities lending on market liquidity.====Our identification strategy relies on three key assumptions. First, we assume that AIG did not reinvest their cash collateral mostly in corporate bonds. Regulatory filings confirm that AIG indeed reinvested less than 20 percent of its cash collateral in corporate bonds. Second, we assume that the shutdown of AIG's securities lending program was not due to concerns about corporate bond market liquidity. We show that the shift in corporate bond market liquidity occurred ==== the shutdown of AIG's securities lending program. In addition, anecdotal evidence suggests that the run by securities borrowers was driven by losses on AIG's cash collateral reinvestment portfolio (Peirce, 2014). Our third and final assumption is that those corporate bonds held and lent by AIG and those bonds held and lent by other insurance companies differ only along observable dimensions, for which we include all available control variables. We compare holdings and lending portfolios of AIG and other insurance companies with securities lending programs and find no evidence of differences, in particular with respect to the key outcome variable, market liquidity.====Our results suggest that securities lending markets have a significant statistical and economic effect on corporate bond market liquidity. We find that the shutdown of AIG's securities lending program lowered the market liquidity of those bonds that were held by AIG. Specifically, a one standard deviation increase in the share of a bond that was held by AIG is associated with a reduction in that bond's liquidity of about 2 basis points in the period after AIG's securities lending program was shut down. This point estimate can be compared with typical corporate bond trading costs, which are in the order of 10 basis points for smaller traders, to gauge the economic importance of securities lending to OTC market liquidity (Schultz, 2001; Hong and Warga, 2000).====We further exploit our empirical setting and identification strategy to study how dealers responded to the shutdown of AIG's securities lending program. As suggested by Fig. 1, in the absence of an available securities lender, dealers resort to other markets to meet their clients' orders. Applying our difference-in-differences identification strategy to analyze the dynamics of interdealer trading in those bonds predominantly held by AIG, we find that, after an adjustment period, the interdealer market partly compensated for disruption to the securities lending market. For a bond predominantly held by AIG, the ratio of interdealer trading volume to total trade volume increased by 1 percentage point.==== As this ratio is on average 10 percent across all bonds, the increase in interdealer trading volume was about 10 percent. We also show that the increased cost of trade—measured using price dispersion—was eventually passed on by dealers to their clients.====Our paper contributes to several broad research topics in the literature. We provide the first evidence that OTC market liquidity is vulnerable to run risks arising in the securities lending market, particularly in corporate bond securities lending by non-bank financial institutions. The financial crisis of 2007-2009 initiated a surge of interest in the activity of so-called shadow banks and the risks those activities may pose to the broader financial system.==== While many studies have sought to understand the determinants of market liquidity, few have explored the important contribution of the shadow banking system.==== Our finding helps to understand the determinants of corporate bond market liquidity and, especially, the connection between market liquidity and the shadow banking system.====Our paper also contributes empirical evidence to the literature studying OTC markets through the lens of search-based models (Duffie et al., 2005; Lagos and Rocheteau, 2009). Although our empirical analysis is not a direct test of search-based models, it is closely related to the frontier of this literature that studies the liquidity feedback from the securities lending market to the corresponding spot market (Vayanos and Weill, 2008; Sambalaibat, 2017). We use a shock to the securities lending market to identify a causal link between bond availability in the securities lending market and liquidity in the spot market. We then use the same shock to estimate the dynamics of price dispersion and trading between dealers and between clients and dealers. Our findings offer new insight into the mechanics of the relationship between the two markets.====Lastly, our paper contributes to a growing literature on corporate bond market liquidity during and after the financial crisis. Dick-Nielsen et al. (2012) find evidence of short-run illiquidity, potentially as a consequence of (i) distress at lead underwriters e.g., Bear-Sterns and Lehman Brothers, (ii) investor flight towards more highly rated securities, and (iii) information asymmetry. Other studies examine long-term corporate bond market liquidity in the aftermath of the financial crisis. As surveyed by Adrian et al. (2017), the literature has found little to no evidence of a long-lasting decline in corporate bond market liquidity.==== We offer a nuanced view that long-term corporate bond market liquidity did decline for those bonds that were both held in large amounts by insurance companies and were made available to market participants through securities lending programs that were shut down. In addition, we show that dealers eventually passed the trading cost increase on to their clients.====The remainder of the paper proceeds as follows. In Section 1 we provide an overview of the market for corporate bond securities lending and the experience of insurance companies during the financial crisis. Section 2 describes our data and summary statistics. Sections 3 and 4 present our empirical strategy and main results. Section 5 investigates one mechanism behind our results. We conclude in Section 6.",Over-the-counter market liquidity and securities lending,https://www.sciencedirect.com/science/article/pii/S1094202518303120,1 March 2019,2019,Research Article,264.0
"Furlanetto Francesco,Robstad Ørjan","Norges Bank, P.O. Box 1179 Sentrum, 0107 Oslo, Norway,Norges Bank, Bankplassen 2, P.O. Box 1179 Sentrum, 0107 Oslo, Norway","Received 22 June 2017, Revised 22 February 2019, Available online 1 March 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.red.2019.02.006,Cited by (31)," and household credit, and a negative effect on productivity driven by a large decline in capital intensity.","During the past decades immigration flows have increased significantly in most advanced economies. This is certainly the case for Norway, where the population share of immigrants has surged from approximately 3.5 percent in 1990 to 14.9 percent in 2016, a share higher than the corresponding number for France (11,8 percent), Germany (13,3 percent) and the United Kingdom (13,3 percent) provided by Eurostat. While a large literature has studied in detail the effects of immigration flows on employment and wages using mostly disaggregate data, the impact of immigration on standard macroeconomic variables has not been investigated systematically. This paper aims at filling this gap. We conduct our analysis using Norwegian data as Norway is one of the few countries for which a quarterly net immigration series is available from the early 1990s.====Our goal is to include a net immigration variable into a Structural Vector Autoregression (SVAR) model, which is the most widely used empirical model for macroeconomic analysis. Notably, immigration is a fully endogenous variable in our set-up and responds to exogenous immigration shocks but also to a series of macroeconomic disturbances driving the business cycle. Following Canova and De Nicoló (2002), Faust (1998), Fry and Pagan (2011), Peersman (2005) and Uhlig (2005), our identification strategy is based on imposing a limited number of sign restrictions on macroeconomic variables to disentangle immigration shocks from other sources of business cycle fluctuations. In a second step we complement traditional sign restrictions with a few narrative restrictions, as proposed by Antolin-Diaz and Rubio-Ramirez (2018).====We estimate several versions of our baseline model introducing one alternative unrestricted variable in each experiment. This strategy enables us to investigate the macroeconomic effects of immigration shocks on variables such as unemployment, public finances, house prices, household credit, prices, exchange rates and productivity. Furthermore, we are able to quantify the relative importance of immigration shocks for macroeconomic dynamics and evaluate the strength of the endogenous response of immigration to the other shocks identified in our system. The analysis of the drivers of immigration and the effects of immigration shocks on macroeconomic variables constitute our key contributions.====As already anticipated, our analysis is feasible since Norway is one of the few countries for which a quarterly net immigration series is available from the early 1990s. The series, plotted in Fig. 1, is provided by Statistics Norway. We include in our analysis only the net flow of workers immigrating to Norway from EU/EFTA countries, North America, Australia, New Zealand and Eastern Europe in percent of the population aged 15-74. We exclude from our analysis immigrants from Africa, Asia (including Turkey) and South and Central America since our identification assumptions are most likely violated for immigrants that do not enter rapidly into the labor force (as is the case for asylum seekers, for example). The case of Norway is particularly interesting as immigration was a marginal phenomenon in the 1990s (cf. Fig. 2), whereas it became the dominant driver of population growth in the aftermath of the EU enlargement to include Eastern European countries (cf. Grangård and Nordbø, 2012). In addition, Norway is an interesting laboratory to disentangle the immigration shock from two other labor market shocks. The first is a domestic labor supply shock that turns out to be particularly important, as participation is cyclical and volatile in Norway. The second is a wage bargaining shock that may have a structural interpretation in Norway given the centralized nature of the wage negotiation system (cf. Aukrust, 1977), in which the wage norm is determined in the sector exposed to international competition and is then used to guide wage increases in the other sectors of the economy.====We disentangle the three labor market shocks (wage bargaining, domestic labor supply and immigration shocks) from business cycle shocks by assuming that they imply a negative co-movement between output and an aggregate measure of real wages which combines wages of native and foreign-born workers. Our assumption finds theoretical support in standard dynamic stochastic general equilibrium (DSGE) macroeconomic models with an explicit role for labor market shocks (cf. Galí et al., 2011, and Foroni et al., 2018) and also in the most used theoretical framework in the immigration literature that features a national labor market with a production function structure which combines workers of different skill and experience levels (cf. Borjas, 2003, and Ottaviano and Peri, 2008, Ottaviano and Peri, 2012). To identify the wage bargaining shock we use data on the participation rate following the previous literature. To separate domestic labor supply shocks from immigration shocks we rely on a restriction on the ratio of immigrants over participants which is naturally pro-cyclical in response to an expansionary immigration shock and countercyclical in response to an expansionary domestic labor supply shock.====In terms of impulse responses, several results emerge from our analysis. First, an exogenous increase in immigration lowers the overall unemployment rate and even the unemployment rate for native workers. Second, a positive immigration shock increases public spending in the medium run, but the response of fiscal revenues follows the same path and the net effect on public finances turns out to be even positive in the short run and neutral in the long run. Third, the immigration shock has no effect on house prices, which are mainly driven by business cycle shocks, but also by domestic labor supply shocks that generate a negative conditional correlation between house prices and immigration. Thus, if anything, immigration has had a mitigating effect on the housing boom that Norway has experienced in our sample period. The same result is confirmed when we consider household credit growth. Fourth, an expansionary immigration shock has no effect on domestic prices but results in an increase in the CPI in the medium run through an exchange rate depreciation. Fifth, labor productivity falls in response to an immigration shock. Notably, the decline in productivity is driven by a large fall in capital intensity, thus suggesting that immigration induces the adoption of less capital intensive technologies (cf. Lewis, 2011).====In terms of variance decompositions, our main result is that immigration shocks are non-negligible (although not major) drivers of the Norwegian business cycle, explaining on average around 15-20 percent of output fluctuations. Immigration responds little to the state of the business cycle in Norway, whereas it reacts more to factors that are specific to the Norwegian labor market: when participation by native workers is low (i.e. in response to a negative domestic labor supply shock), immigration increases significantly.====The literature on immigration in the context of macroeconomic models is limited, perhaps due to the absence of reliable quarterly series for net immigration over a sufficiently long period for many countries. Mandelman and Zlate (2012) propose a DSGE model with immigration focusing on the role of remittances for business cycles in Mexico. Earlier contributions include Canova and Ravn (1998), who study the macroeconomic impact of a flow of unskilled migrants in the neo-classical growth model, and Bentolila et al. (2008), who show how immigration flattens the slope of the New Keynesian Phillips curve for prices in Spain. More recent contributions presenting stylized DSGE models with net migration include Bandeira et al. (2018), Braun and Weber (2016), Lozej (2018) and Smith and Thoenissen (2018) with a focus on Greece, Germany, Ireland and New Zealand respectively, while Hauser and Seneca (2018) and Weiske (2017) study the case of the US. In the SVAR literature, Kiguchi and Mountford (2017) provide an analysis on US annual data using the penalty function approach in which a shock to the working population (that could be due to immigration but also to domestic factors) leads to a temporary reduction in GDP and consumption per capita. D'Albis et al. (2015) use monthly data for France over the sample period 1994-2008 in a SVAR identified with a recursive scheme. They find that immigration responds significantly to France's macroeconomic outlook and at the same time immigration itself increases GDP per capita, particularly in the case of family immigration. Two interesting analyses focus on New Zealand, a country featuring detailed data on immigration flows. In the first, McDonald (2013) studies the effect of an immigration shock on house prices in a SVAR identified with a recursive scheme. He shows that an immigration shock has a strong positive effect on house prices and construction activity, thus boosting aggregate demand even more than aggregate supply. The second paper, Armstrong and McDonald (2016), extends the previous set-up to include a second immigration shock associated with fluctuations in Australian unemployment. They find that higher net immigration due to a higher Australian unemployment rate leads to a higher unemployment rate in New Zealand, whereas higher net immigration for other reasons reduces unemployment in New Zealand.====While the literature studying the macroeconomic effects of immigration is still in its infancy, studies using more disaggregate data are numerous (for a survey cf. Card, 2005, Kerr and Kerr, 2011, and Hagelund et al., 2011). Selected issues of interest are the assimilation of immigrants into the host-country labor market in terms of wages and employment, the identification of long-run displacement effects on native workers in terms of wages and employment (cf. Borjas, 2003, and Ottaviano and Peri, 2012, among others), the impact of immigration on public finances (cf. Borjas, 1999, Storesletten, 2000, among others), on house prices (cf. Saiz, 2003, and Sá, 2014, among others), on prices and the composition of demand (cf. Lach, 2007, Cortes, 2008, and Frattini, 2008) and on productivity (cf. Peri, 2012). While we impose as an identification assumption that an immigration boom has a short-run dampening effect on aggregate wages (thus without taking any stand on the long-run-impact on wages for native workers), our set-up can shed light on all the macroeconomic issues listed above in the context of an aggregate time-series approach that is complementary to analysis based on more disaggregate data and with a microeconomic focus. Notably, the main advantage of a SVAR approach based on sign restrictions over alternative approaches is the ability to disentangle the exogenous and the endogenous component of immigration in a system that fully takes into account feed-back effects between different variables.====The paper is structured as follows. Section 2 presents the SVAR model and describes the identification strategy. In Section 3 we propose results for our baseline case with unemployment introduced as an unrestricted variable in the system. Section 4 presents several extensions to discuss the effects of immigration shocks on public finances, house prices, household credit, prices, exchange rates and productivity. In Section 5 we include narrative sign restrictions into our baseline set-up. Finally, Section 6 concludes.",Immigration and the macroeconomy: Some new empirical evidence,https://www.sciencedirect.com/science/article/pii/S1094202518302746,1 March 2019,2019,Research Article,265.0
"Biais Bruno,Green Richard","HEC, France,Carnegie Mellon University, United States of America","Received 25 May 2018, Revised 21 December 2018, Available online 19 February 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.01.003,Cited by (36),"Bonds are traded in opaque and fragmented over-the-counter markets. Is there something special about bonds precluding transparent limit-order markets? Historical experience suggests this is not the case. Before WWII, there was an active market in corporate and municipal bonds on the NYSE. Activity dropped dramatically, in the late 1920s for municipals and in the mid 1940s for corporate, as trading migrated to the over-the-counter market. Average trading costs in municipal bonds on the NYSE were half as large in 1926-1927 as they are today over the counter. Trading costs in corporate bonds for small investors in the 1940s were as low or lower than they are now. The difference in transactions costs likely reflects the differences in market structures, since underlying technological changes have likely reduced costs of matching buyers and sellers.","Bonds are mostly traded on fragmented and opaque over-the-counter (OTC) markets.==== Stocks, in contrast, are for the most part traded in transparent centralized limit order books. Why is there such a contrast? Is it inherently problematic to trade bonds on a transparent limit-order book? Could mandate changes in trading mechanisms lower costs for investors? Or would regulatory interference simply suppress a natural diversity in institutional arrangements benefiting all investors?====Answers to these questions are difficult to obtain through cross-sectional comparisons of existing markets because liquidity, transactions costs, and trading mechanisms are all jointly endogenous. Perhaps corporate and municipal bonds have low liquidity and high trading costs because they are traded in opaque and decentralized dealer markets. Alternatively, perhaps they trade over the counter because the infrequent need for trade renders the continuous maintenance of a widely disseminated, centralized limit-order book wasteful and costly.====We believe the historical experience can shed light on these questions, because it has not always been the case that equities and bonds were traded in such different venues. Until the late 1940s, there was an active market in corporate bonds on the NYSE. In the 1930s, on the NYSE, the trading volume in bonds was between one fifth and one third of the trading volume in stocks. In earlier periods, there was also an active market for municipal bonds and government bonds. Indeed, the first organized exchange in New York, from which the modern NYSE traces its descent, was established by a group of brokers “under the buttonwood tree” to trade U.S. government bonds. Municipal bond trading largely migrated from the exchange in the late 1920s, and NYSE volume in corporate bonds dropped dramatically in the late 1940s.==== Since this collapse, bond trading on the NYSE has been limited.====This historical evidence shows that an active bond market with a centralized and transparent limit-order book was feasible. This, in turn, raises other questions. Why did liquidity dry up on the NYSE? Why has it been so difficult for the exchange to regain volume despite its periodic attempts to do so? What were the consequences for transactions costs of the migration of bond trading to the OTC market?====To answer these questions we first provide institutional information on the microstructure of the bond market in the twentieth century. We then consider possible explanations for the drop in the liquidity of the bond market on the NYSE. First, we ask whether decreases in liquidity could have been associated with changes in the role of bond financing generally. Based on data assembled from different sources (Federal Reserve, NBER and Guthmann, 1950) we show that bond financing actually grew during the periods when trading volume collapsed on the NYSE.====Second, we ask whether the drop in liquidity could have resulted from SEC regulations increasing the cost of listing on the NYSE, e.g., the Securities Act of 1933 and the Securities Exchange Act of 1934 required firms offering securities to the public and listing on an exchange to disclose detailed information and file reports. We show that the decline in bond liquidity on the NYSE was not correlated with a decline in listings. Furthermore, while NYSE trading disappeared in securities that were exempt from the 1933 and 1934 acts (such as municipal bonds), it remained active in securities which were subject to this regulation (most notably stocks).====A third possible explanation focuses on the interaction between classes of traders with different preferences. It is widely recognized that there are positive externalities in liquidity (see for example Admati and Pfleiderer, 1988 and Pagano, 1989). Traders prefer to route their orders where they expect to find liquidity, i.e., where they expect the other investors to have sent their orders. Complementarities can give rise to multiple equilibria. While each of these equilibria can be locally stable, it can be upset by an exogenous shock, or a change in the characteristics of the players. Different equilibria will vary in terms of their attractiveness for different categories of market participants. Intermediaries benefit when liquidity concentrates in venues where they earn rents, such as opaque and fragmented markets. For reasons we will show were quite evident to observers at the time, large institutions fare better than small investors in a dealership market.==== This was especially true on the NYSE until 1975, because commissions were regulated by the Constitution of the NYSE, while intermediary compensation was fully negotiable on the OTC market. We find that liquidity migrated from the exchange to the OTC market at times when institutional investors and dealers became more important relative to retail investors. As institutions and dealers became more prevalent in bond trading, they tipped the balance in favor of the over-the-counter markets.====To evaluate the impact on trading costs, we collected high frequency data on transactions and quotes for 6 corporate bonds from 1943 through 1947 and 6 municipal bonds between 1926 and 1930. We chose these dates because they bracket the periods during which liquidity vanished from the NYSE for municipal bonds and then for corporate bonds. Average transactions costs were substantially lower in the late 1920s for municipal bonds than they are today. In the 1940s, despite fixed commissions, costs for retail investors trading corporate bonds were as low or lower than they are today in OTC markets. We believe this is quite striking. The natural or potential liquidity of corporate and municipal bonds is unlikely to have been higher historically than it is today, and the availability of counterparties is likely to have improved, since a much larger portion of the population invests and the population is much larger. More obviously, the cost of finding counterparties and processing trades is likely to have decreased, given the improvements in communication and data processing technology. These technological changes have dramatically reduced the costs of trading in other sectors of the economy. For example, Jones (2002) shows that, starting in the 1980s, bid-ask spreads and commissions declined persistently and significantly.====Municipal bonds are a particularly interesting security to study in this context. The interest on the bonds is tax-exempt, and retail investors are therefore a significant presence in the market, as they are with equities. Migration of liquidity from the NYSE to the OTC market is most costly for retail investors.==== Our high-frequency data shows there was a striking drop in municipal bond trading on the NYSE in the late 1920s. At that time trading volume in equities was soaring. The NYSE was desperately short of capacity. (See Davis et al. (2005).) The NYSE decided to reallocate capacity from relatively inactive bonds towards stocks, which were more profitable for the floor traders. For example, and telephones and trading space on the floor were reallocated from bonds to stocks. Simultaneously, retail investors, attracted to equities by the large recent returns, lost their appetite for municipal bonds, leaving investment in this market to institutions. At this point, trading activity in municipal bonds rapidly migrated to the OTC market. This experience illustrates how shocks can lead to shifts in the focal point for trading. The difficulty of reversing such shifts once they have occurred (even after the disappearance of the conditions triggering the shift, e.g., the opportunity cost of trading munis in the late 1920s) is illustrated by the inability of the NYSE to regain volume in municipal bonds, even when equity trading dropped relative to bonds during the years of the Great Depression.====Several papers have shown empirically that, in the OTC bond market, investors can incur large transactions costs, in particular for smaller trades. Green et al. (2007b) show that, when municipal bonds are issued, there is a large amount of price dispersion and retail investors systematically buy at relatively high prices. Harris and Piwowar (2006) write: “Our results show that municipal bond trades are significantly more expensive than equivalent sized equity trades.” That bonds command transactions costs, larger than for stocks, is surprising. Risk is one of the main components of the cost of supplying liquidity. Bonds are less risky than stocks. They should have lower spreads. The large transaction costs paid by final investors in the bond market could reflect dealers' market power. Indeed, Green et al. (2007a) find that dealers in the municipal bond market exercise substantial market power. In the corporate bond market, Di Maggio et al. (2016) find that, when dealers trade with clients (as opposed to other dealers), they charge an extra markup of about 50 basis points. But could transactions costs be lowered by trading bonds in transparent centralized limit order market instead of a fragmented dealer market? It is difficult to answer that question without a counterfactual. While modern bond markets don't offer such a counterfactual, historical experience does.====An important source of exogenous variation showing the importance of market structure for bond trading costs has been the introduction of post-trade transparency. Edwards et al. (2007), Goldstein et al. (2007) and Bessembinder et al. (2007) show that the improvement in post-trade transparency associated with TRACE lowered trading costs.==== For example, Goldstein et al. (2007) find that, for small and intermediate trade sizes, transparency caused spreads to decrease by more than 22 basis points per $100 face value. Note that, while TRACE introduced post-trade transparency, the limit order book used for bond trading on the NYSE also involved pre-trade transparency and enabled final investors to place their own limit orders. These two additional features potentially could further reduce investors' transactions costs.====More recent, but also very important, regulatory reforms affecting the microstructure of the bond market have been the “Volcker Rule” of the Dodd Frank act, whose implementation was finalized in 2014, and the Basel accords implemented in 2012 and 2013. Both reforms made it costlier and more difficult for bank-affiliated dealers to trade (and hence to provide liquidity) in the bond market. As shown by Bessembinder et al. (2018) and Trebbi and Xiao (2016) this did not lead to reduced liquidity, as non-bank dealers stepped in to supply liquidity, and some trading moved to more transparent and competitive trading platforms. Their findings and ours are consistent with the view that i) OTC bank-affiliated dealer markets are not the only possible trading arrangement for bonds, but ii) strong exogenous shocks play a key role in triggering changes in the microstructure of that market.====In the next section we review the organization of the bond market in the 20th century. Section 3 describes our data sources. Section 4 reviews long term trends in bond financing and investing and discusses their possible link with the migration of bond market liquidity off the exchanges. Sections 5 and 6 consider trading and trading costs for corporate bonds in the 1940s and municipal bonds in the 1920s, respectively, using transactions data from the NYSE. Section 6 offers additional remarks on convertible bonds and stocks. Section 8 concludes.",The microstructure of the bond market in the 20th century,https://www.sciencedirect.com/science/article/pii/S1094202518303077,19 February 2019,2019,Research Article,266.0
Osotimehin Sophie,"Université du Québec à Montréal, C.P. 8888, succ. Centre-ville Montréal H3C 3P8, Canada","Received 5 May 2017, Revised 22 January 2019, Available online 15 February 2019, Version of Record 28 February 2019.",https://doi.org/10.1016/j.red.2019.02.003,Cited by (13),"This paper proposes a novel decomposition of aggregate productivity to evaluate the role of allocative efficiency for the cyclical dynamics of aggregate productivity. The decomposition, which is derived from the aggregation of heterogeneous firm-level production functions, accounts for changes in allocative efficiency, as well as for changes in entry and exit. This approach thereby extends ===='s (====) growth accounting exercise to a framework with firm heterogeneity and frictions in the allocation of resources across firms. I apply the decomposition to a comprehensive dataset of French manufacturing and service firms, and I find that entry and exit contribute little to the year-on-year variability of sectoral productivity, between-sector allocative efficiency plays a limited role for the volatility of aggregate productivity, whereas within-sector allocative efficiency is countercyclical and tends to reduce the volatility of sectoral productivity.","Recessions are often considered as a time when the economy is “cleansed”; as the least productive firms are forced to exit the market, resources are reallocated towards more productive uses. This Schumpeterian view of recessions suggests that recessions improve the efficiency of resource allocation. Despite the high interest in this view, little is known about how business cycles affect the efficiency of resource allocation. Moreover, the literature typically focuses on the cyclicality of the firms' exit rate and on the productivity of exiting firms.==== Resource reallocation, however, mainly involves incumbent firms, as shown by Davis and Haltiwanger (1998), among others. A potentially important contribution to changes in the efficiency of resource allocation has therefore been neglected. Does the efficiency of resource allocation across incumbent firms vary over the business cycle? Is allocative efficiency an important determinant of aggregate productivity over the business cycle?====To answer these questions, I propose a novel decomposition of aggregate total factor productivity (TFP) changes that separates out the variations that are due to firm-level productivity from those due to entry and exit and to the efficiency of resource allocation. I apply this decomposition at the sectoral and economy-wide levels and hence obtain the contribution of allocative efficiency both within and between sectors. The decomposition requires deriving the link between firm-level and aggregate productivity in a framework in which resources are potentially inefficiently allocated. Exploring the micro determinants of aggregate TFP raises a methodological problem: how should we aggregate firm-level TFP? There is no consensus on this question.==== In this paper, I advocate the use of a measure of aggregate productivity, computed from the aggregation of firm-level production functions, and which is therefore consistent with the measure used at the firm level. This paper thus extends Solow's (1957) growth accounting exercise to a framework with firm heterogeneity and allocative inefficiency. Deriving the contribution of allocative efficiency to aggregate productivity changes is not only of theoretical interest, it also has important implications for our understanding of the dynamics of aggregate productivity. I estimate the decomposition on French firm-level data from the manufacturing and service sectors over 1991–2006. I find that between-sector allocative efficiency shows little cyclicality and plays a limited role in the dynamics of aggregate TFP. Entry and exit and within-sector allocative efficiency both tend to be countercyclical but only within-sector allocative efficiency plays a substantial role for the dynamics of aggregate TFP. I find that the movements in within-sector allocative efficiency dampen the fluctuations in aggregate TFP.====To measure changes in allocative efficiency, I follow Chari et al. (2007) and Restuccia and Rogerson (2008) and do not specify the frictions that may distort the allocation of resources. Rather, the distortions are modeled as wedges between the firms' marginal products. The wedges, which measure the distance from the frictionless allocation of resources, encompass various sources of distortions such as adjustment costs, markups, search frictions, financial constraints or distortionary regulations, provided they create a wedge between the firms' marginal products. Distortions that modify the marginal products of all firms in the same proportion do not affect aggregate productivity; only cross-sectional inefficiencies affect the total output that can be produced out of a given quantity of inputs. Within this framework, I show how to aggregate firm-level production functions into an aggregate production function. In the aggregation literature, the traditional approach consists in defining the aggregate production function as the efficient frontier of the production possibilities set (e.g. May, 1946; Fisher, 1969; Houthakker, 1955). This approach is not suited for my purpose since it gives a measure of aggregate productivity under the assumption that resources are efficiently allocated. To derive a measure of aggregate productivity when resources may be inefficiently allocated, I use Malinvaud's (1993) insight and define the aggregate production function as the relation between aggregate output and inputs for a given allocation of resources. This aggregation method can be applied to any production function and only requires a framework that relates firm-level inputs to aggregate inputs. The aggregate production function is specific to the framework and, in particular, to the link between firm-level inputs and aggregate inputs. In this framework, which can be viewed as an accounting framework, the allocation rule is a function of firm-level productivity and distortions, the change in allocative efficiency is measured as the effects of changes in distortions. I measure the contribution of distortions, holding fixed aggregate inputs and productivity, and the contribution of firm-level productivity and aggregate inputs, holding fixed distortions. I show how to derive an exact decomposition of sectoral productivity changes into productivity changes at the firm-level, changes in allocative efficiency among incumbent firms, and changes in entry and exit. Applied across sectors, the decomposition separates the economy-wide aggregate productivity changes into within-sector productivity changes and between-sector allocative efficiency. This approach is, however, silent on the factors behind the change in allocative efficiency and cannot disentangle allocative efficiency changes due to variation in the severity of frictions from those due to aggregate inputs or firm-level productivity changes.====I use the decomposition to document the dynamics of allocative efficiency over the business cycle and quantify its role in the fluctuations of aggregate TFP. I estimate the decomposition on a comprehensive firm-level dataset of French manufacturing and service firms over the period 1991–2006. The data are collected annually by the tax administration and combined with survey data in the INSEE unified system of business statistics (SUSE). I find that, in most sectors, both the contribution of entry and exit and the efficiency of resource allocation within sectors are countercyclical. On the other hand, the efficiency of resource allocation between sectors appears to be acyclical. In line with the cleansing effect of recessions, these results indicate that within-sector allocative efficiency contributes to raising productivity during downturns. Whereas the literature emphasizes the role of entry and exit, I find that the efficiency of resource allocation between incumbent firms is more important to understand the dynamics of aggregate productivity over the business cycle. Entry and exit play a limited role in the cyclical dynamics of aggregate productivity. The countercyclicality of within-sector allocative efficiency is robust to changes in the specification of the firms' production functions. I find that allocative efficiency is countercyclical also when accounting for overhead labor costs and heterogeneity in the firms' factor elasticities. Overall, these results suggest new directions for future theoretical work since little is known on the mechanisms behind the cyclical patterns of allocative efficiency.====A vast literature has documented the importance of resource reallocation for aggregate productivity growth. Following Baily et al. (1992) and Foster et al. (2001), the most common approach is to decompose the weighted average of firm-level TFP into changes in output (or input) shares and changes in firm-level TFP. Although they provide useful insights on the patterns of reallocation, these shift-share decompositions are based on ==== firm-level TFP and may therefore not give information on the role of resource reallocation for ==== TFP. In fact, aggregate TFP depends on how inputs are reallocated across firms with different marginal productivities rather than on the correlation between changes in output shares and firm-level TFP. Several recent papers, building on Basu and Fernald's (2002) work, have advocated the use of a measure of aggregate TFP to study the impact of resource reallocation (e.g., Basu et al., 2009; Petrin et al., 2011; Petrin and Levinsohn, 2012). In the vein of the earlier contribution by Jorgenson et al. (1987), these papers propose a decomposition of the aggregate Solow residual in a framework that allows for distortions and firm heterogeneity.==== My decomposition mainly differs from these papers in its objective. As the shift-share decompositions, these papers focus on measuring the effects of changes in resources allocation. The recent paper by Baqaee and Farhi (2018) follows this approach as well. By contrast, my objective is to provide a decomposition that accounts for changes in the ==== of resource allocation. In addition, I propose an alternative aggregation method that relies on the aggregation of firm-level production functions rather than on the Solow residual. With the objective of measuring the effect of allocative efficiency, I derive the link between aggregate output and aggregate inputs, taking distortions as given, whereas the Basu–Fernald decomposition takes the firms' input shares as given. The difference between my decomposition and the Basu–Fernald decomposition is best shown by an example. Consider an economy with a fixed input allocation: firms never vary their inputs, even in the face of productivity shocks. In that economy, the contribution of firm-level inputs to aggregate productivity, measured by the Basu–Fernald decomposition, is equal to zero since there is no change in inputs. The efficiency of resource allocation, however, will vary over time depending on the distribution of firm-level productivity shocks. In particular, allocative efficiency would improve if firms with low marginal productivity experienced an increase in their TFP.====To underline the differences between my approach and the existing literature, I compare the results of my decomposition to those obtained with Foster et al.'s (2001) shift-share decomposition and with a reallocation decomposition in the style of Basu and Fernald (2002). I find that neither the shift-share decomposition nor the reallocation decomposition is a good proxy for changes in allocative efficiency. Whereas changes in allocative efficiency tend to reduce the volatility of aggregate productivity, the contribution of reallocation to aggregate productivity is negligible, and the reallocation component of the shift-share decomposition tends to raise the volatility of average productivity. In the end, these decompositions give complementary information on the patterns of reallocation and its role for the dynamics of aggregate productivity.====This paper is also related to the growing literature that highlights the role of firm-level allocative distortions as a determinant of aggregate productivity.==== In particular, my paper is related to Oberfield (2013), Sandleris and Wright (2014), and Gopinath et al. (2017) who study the dynamics of allocative efficiency.==== As these papers, I build on Hsieh and Klenow (2009) and measure allocative efficiency from the wedges between the firms' marginal productivity. My paper contributes to the literature by deriving the contribution of entry and exit and by extending Hsieh and Klenow's (2009) approach to more flexible production functions.==== I propose a method to aggregate firm-level production functions in the presence of distortions in the allocation of inputs across firms. The aggregation method does not require a specific functional form and therefore permits evaluating the robustness of the results to alternative functional forms. With distortions likely to be sensitive to the specification of the production function, evaluating the robustness of the results to alternative specifications is crucial. I show that the results are robust to allowing for heterogenous factor elasticities and for fixed labor costs of production. Furthermore, while Hsieh and Klenow (2009) focus on within-sector allocative efficiency, the aggregation method allows me to derive the change in the allocative efficiency between sectors.====The paper is organized as follows. Section 2 describes, in a simplified framework, how to aggregate heterogeneous production units and how to derive the decomposition of aggregate productivity. That section also compares the decomposition to the existing literature. Section 3 presents the decomposition of aggregate productivity in the general framework. Section 4 presents the estimation method and the results obtained on French firm-level data. Section 5 concludes.",Aggregate productivity and the allocation of resources over the business cycle,https://www.sciencedirect.com/science/article/pii/S1094202518302709,April 2019,2019,Research Article,267.0
Braun Christine,"European University Institute, Italy,University of Warwick, UK","Received 30 November 2017, Revised 4 February 2019, Available online 13 February 2019, Version of Record 19 February 2019.",https://doi.org/10.1016/j.red.2019.02.002,Cited by (15),"How does the minimum wage affect crime rates? Empirical research suggests that increasing a worker's wage can deter him from committing crimes. On the other hand, if that worker becomes displaced as a result of the minimum wage, he may be more likely to commit a crime. In this paper, I describe a frictional world in which a worker's criminal actions are linked to his labor market outcomes. The model is calibrated to match labor market outcomes and crime decisions of workers from the National Longitudinal Survey of Youth 1997, and shows that the relationship between the aggregate crime rate and the minimum wage is U-shaped. The results from the calibrated model, as well as empirical evidence from county level crime data and state level minimum wage changes from 1995 to 2014, suggest that the crime minimizing minimum to median wage ratio for 16 to 19 year olds is 0.91. However, the welfare maximizing minimum to median wage ratio is 0.87, not equal to the crime minimizing value. The median wage of 16 to 19 year olds in the United States in 2018 was $10, suggesting that any federal minimum wage increase up to $8.70 may be welfare improving.","The minimum wage has once again made it to the front lines of political discussion in the United States. Both the Democratic and Republican Party have come out in favor of substantial increases. An unprecedented number of cities have proposed legislation for higher local minimum wages and for the first time ever, a majority of states have minimum wages higher than the federal level. California and New York City have passed laws raising the minimum wage to $15 within a few years, bringing about some of the largest real increases since 1949. Economists have long debated the labor market effects of a minimum wage, dating back to Stigler (1946) who first drew attention to possible employment effects after a 21% erosion of the real wage floor induced a public outcry for a higher minimum. While nearly all of the arguments hinge on employment, in this paper I ask how changes in the minimum wage affect criminal activity. Given that the policy is primarily aimed at improving labor market conditions for young and unskilled workers, who are also most at risk in terms of criminal activity, see Fig. 1, potential changes in crime should be part of the policy debate.====Many economists have tested how the decision to commit crimes changes with respect to the probability or severity of punishment.==== However, it was not until Schmidt and Witte (1984) and Grogger (1998) that they began to test the effects of labor market changes on people's criminal actions. The conclusions drawn by these studies are as economic theory suggests: people choose to commit more crimes when unemployment increases and less when they receive higher wages.==== Therefore, economic theory alone can not determine how an increase in the minimum wage will affect the crime rate. Increasing the minimum wage can raise wages for workers, thus deterring them from crime. However, there exists empirical evidence that the minimum wage will displace some workers from jobs,==== thus enticing them to commit more crimes. The employment effects from the minimum wage on specifically teen employment (the focus of this paper) is mixed; Allegretto et al. (2010) find no significant employment effects and Neumark et al. (2014) find significant employment effects on teens with estimated elasticities around −0.3. Although the literature is mixed, I show that the model presented below exhibits small employment effects, similar to those estimated in the empirical literature.====To find the direction of the effect, I use a search-theoretic framework to describe a world in which people make crime and labor market decisions jointly. I calibrate the model to match aggregate statistics of crime and the labor market to analyze the quantitative implications of changing the minimum wage. The existing literature trying to identify and quantify the effect of the minimum wage on crime rates is sparse. Hashimoto (1987) finds evidence of a positive relationship using national time-series data of the minimum wage and teenage arrest rates relative to adults.==== In a recent micro-level study, Beauchamp and Chan (2014) find a positive effect of minimum wage increases on crime for people employed at a binding wage. I analyze a setting in which the minimum wage can change all workers' crime decisions and examine the effect on the aggregate crime rate. Both increased schooling and work have been proposed as methods for reducing youth crime rates. Grogger (1998) finds the elasticity of crime with respect to wages for teens to be −0.18 while Lochner and Moretti (2004) find that an increase in one year of schooling decreases teen arrest rates by ==== implying similar returns. Therefore, increasing the minimum wage could be a policy tool that is as effective as education for decreasing teen crime rates, as long as the negative employment effects are outweighed by the wage effects. To effectively implement the minimum wage as a policy tool for deterring teens from crime it is crucial to know for what level of minimum wage each effect dominates, which is, the goal of this paper.====The basic structure of the model is as follows: in the labor market, workers receive job offers at an exogenous rate and wages are determined by strategic bargaining between workers and firms. Workers are heterogeneous in ability, which influences their labor market outcomes; heterogeneity among workers is essential for analyzing the effects of a minimum wage policy on labor market outcomes, since not all workers are affected equally.==== For the minimum wage to have positive welfare effects, firms must have some monopsony power; search frictions and match specific productivity create monopsony power for the firms, which shifts the gains from trade toward the firm. In the model, the minimum wage will act as a policy tool that can be used to shift some of the gains from trade back to the worker.====The crime market is as in Burdett et al. (2003); workers receive random crime opportunities while employed and unemployed. I add two levels of heterogeneity to capture two important interactions between changes in the labor market and the crime market. First, in contrast to other models of crime and the labor market, workers are ex-ante heterogeneous in ability, making the stock of criminals endogenous and allowing changes in the labor market to have an extensive effect on crime. This extensive effect is also modeled in Huang et al. (2004), where workers specialize in criminal activities, however among those that commit crimes, their propensity for criminal behavior is identical. In contrast, in Burdett et al. (2003) all workers are criminals and have the same propensity for criminal behavior, therefore changes in labor market conditions will not have an extensive effect on crime. In Engelhardt et al. (2008) all workers commit crimes with propensities differing across employment states, again changes in labor market conditions will not have an extensive effect on crime as everyone is a criminal. Second, matches are ex-post heterogeneous with respect to productivity, allowing the “quality” of a job to enter into the worker's crime decision, and creating a range of wages for which he commits crimes, in contrast to a single criminal wage as in Burdett et al., 2003, Burdett et al., 2004. Therefore, changes in labor market outcomes can have an intensive effect on crime, changing the propensity for criminal behavior differentially across individuals. Including this intensive effect on crime creates the wage effect: when the minimum wage increases, wages increase and the criminal propensity for those committing crimes while employed decreases. In the model, the minimum wage will also act as a policy tool used to deter workers from crime and decrease the prison population. Indeed, the minimum wage has multiple roles, lessening the effects of monopsony power, as well as deterring the worker from crime. To the best of my knowledge, this is the first study to investigate both roles in a general equilibrium model.====Using the benchmark model, I introduce a minimum wage by imposing a constraint on the bargaining problem faced by firms and workers. The model is calibrated to match the crime decisions and labor market outcomes of 16-19 year olds from the National Longitudinal Survey of Youth 1997 in 1998. I vet the model by simulating data and estimating the elasticity of crime with respect to wages and the elasticity of employment with respect to the minimum wage, finding that the model generated elasticities, although not targeted in the calibration, are similar to those found in the empirical literature. Increasing the minimum wage within the calibrated model reveals a non-monotomic, U-shape relationship between the minimum wage and the crime rate. The results from the calibrated model and empirical evidence from county level crime data and state level minimum wage changes from 1995 to 2014 suggest that the crime minimizing minimum to median wage ratio for 16–19 year olds is 0.91. However, welfare is not maximized when crime is minimized. The welfare maximizing minimum to median wage ratio is 0.87, which leaves crime at 0.02 crimes per person per month higher than the crime minimizing minimum to median wage ratio. If policymakers abstract from the effect of a minimum wage on crime, the welfare maximizing minimum to median wage ratio is 0.7.====The median nominal wage of all 16 to 19 year old across the United Stated in 2018 was $10. The model implies that the crime minimizing nominal minimum wage is $9.10 and the welfare maximizing nominal minimum wage is $8.70 in 2018. Both values are above the current nominal federal minimum wage of $7.25, suggesting that when taking changes in crime into consideration, a federal increase in the minimum wage may be welfare improving. However, the median wage for 16 to 19 year olds in 2018 in California was $11.10, implying that without substantial real wage growth for young workers, the impending minimum wage increase toward $15, may be welfare reducing and may increase crime rates within the state. Flinn (2006) also attempts to find a welfare maximizing minimum wage, however, ignoring the impact on individuals' crime decisions. The welfare maximizing nominal minimum wage in 1997 that Flinn (2006) estimates is $3.36 per hour. The median nominal wage of 16 to 19 year olds in 1997 was $6.36; the model presented in this paper implies the welfare maximizing nominal minimum wage in 1997 was $5.53. The difference between the welfare maximizing minimum wage estimated by Flinn (2006) and the model proposed in this paper is consistent with the result that ignoring the effects of a minimum wage on individuals' crime decisions leads policymakers to underestimate the optimal minimum wage.",Crime and the minimum wage,https://www.sciencedirect.com/science/article/pii/S1094202518302941,April 2019,2019,Research Article,268.0
"Araujo Luis,Cao Qingqing,Minetti Raoul,Murro Pierluigi","Michigan State University, United States of America,Luiss University, Italy","Received 4 December 2017, Revised 28 January 2019, Available online 7 February 2019, Version of Record 22 February 2019.",https://doi.org/10.1016/j.red.2019.02.001,Cited by (4),"We investigate the effects of a ==== in an economy where firms can retain a mature technology or adopt a new technology. We show that firms' collateral eases firms' access to credit and investment but can also inhibit firms' innovation. When this occurs, a contraction in the price of collateral assets squeezes collateral-poor firms out of the credit market but fosters the innovation of collateral-rich firms. The analysis reveals that the credit and asset market policies adopted during recent credit market crises can boost investment but slow down innovation. We find that the predictions of the model are consistent with the innovation patterns of a large sample of European firms during the 2008-2010 credit crisis.","During the financial crisis that started in 2008, a major drop in the value of collateral assets, especially real estate, triggered a credit contraction, prompting firms to reduce their investments. The literature offers well-established arguments for interpreting these effects of a credit crunch. When entrepreneurs cannot commit to repay lenders, collateral eases their access to credit.==== Thus, aggregate shocks that erode collateral asset values depress total investment by hindering firms' access to external finance (Kiyotaki and Moore, 1997, Lorenzoni, 2008, Holmstrom and Tirole, 1997, Den Haan et al., 2003, Buera and Moll, 2015).====While useful to explain key transmission mechanisms of a credit market crisis, these arguments only yield partial insights into the effects of a credit crunch on technological change. Financial crises appear to have contrasting effects on technological change. The OECD (2009) reviews several pieces of evidence and concludes that credit market crises can certainly damage innovative firms but also be “times of industrial renewal”. Field (2011) documents that the Great Depression was a period of major innovations for the U.S. economy.==== These innovations ranged from Teflon in petrochemicals industries to household appliances, such as the radio and refrigerator, and formed the basis for the post-World War II economic expansion. In South Korea and in Finland, the number of innovative firms boomed during and in the immediate aftermath of the 1990s financial crises (OECD, 2009). And firm-level surveys reveal that while the credit crisis that started in 2008 depressed the innovation efforts of firms with difficult access to credit, such as young businesses with scarce collateralizable wealth, it also stimulated the innovation activity of other firms, especially more established businesses with more pledgeable assets and easier access to credit (see, e.g., Hall, 2011, European Commission, 2012, Voigt and Moncada-Paternò-Castello, 2009, Cincera et al., 2012, and references therein). In Section 2, we will uncover evidence in support of these conclusions by exploring survey data on the innovation activity of about 15,000 European firms during the 2008–2009 credit crisis. The results will suggest that, while the majority of firms responded to the 2008–2009 credit crunch and drop in asset prices by postponing innovation, a non-negligible number of firms responded to shrinking credit and asset values by investing in innovation. Such firms were concentrated in the segment of businesses with large assets, whereas firms with little assets scaled down their innovation activity.====These observations elicit fundamental questions: Can we build a model economy that captures these contrasting effects of credit market crises on innovation? In such an economy, under what conditions does a credit crunch triggered by a drop in collateral values depress or stimulate innovation? And what policies are more innovation-friendly during a credit crunch? This paper takes a step towards addressing these questions. Building on the above observations, we focus on the innovation propensity within incumbent businesses that rely on collateral-based funding. We posit an economy where entrepreneurs operate a mature technology or innovate and adopt a new technology. Lenders, in turn, acquire information that is essential for repossessing and liquidating productive assets pledged as collateral when entrepreneurs default (as in Diamond and Rajan, 2001, for example). Lenders' information on collateral assets eases entrepreneurs' access to credit but can make lenders reluctant to finance entrepreneurs' innovation. In fact, the assets of the new technology are more illiquid and firm specific and, hence, less pledgeable as collateral than the assets of the mature technology. Furthermore, lenders' information on the assets of the mature technology is (partially) specific and non-transferable to the assets of the new technology. Therefore, expecting that the information they have accumulated on mature collateral assets will go wasted if entrepreneurs switch to the new technology, lenders may hinder entrepreneurs' innovation efforts.====The distribution of firms across collateral values replicates salient features of that obtained in previous general equilibrium models of the credit market (e.g., Holmstrom and Tirole, 1997). Collateral-poor firms lack access to credit because they cannot pledge enough expected returns to lenders. Firms with medium and rich collateral assets, instead, obtain credit. The novelty consists of firms' technology adoption. When lenders' technological inertia arises, while firms with a medium value of collateral potentially innovate, collateral-rich firms retain the mature technology. In fact, their lenders expect a large depreciation in the value of their information if the mature technology is abandoned in favor of the new technology.====We study the effects of a contraction in the price of collateral assets (e.g., as in Holmstrom and Tirole, 1997, and Kiyotaki and Moore, 1997). Following a drop in the asset price, marginal firms with collateral assets just sufficient to obtain credit are squeezed out of the credit market because they can no longer pledge enough expected returns to lenders. This tends to reduce total investment and innovation. Consider next collateral-rich firms. The asset price drop erodes the value of the information acquired by their lenders on mature collateral assets, mitigating lenders' potential technological inertia. This can foster the innovation of collateral-rich firms. Overall, while the asset price drop depresses total investment, its impact on innovation depends on the relative magnitudes of the drop in the innovation of firms squeezed out of the credit market and the increase in the innovation of collateral-rich firms.====The analysis delivers novel policy implications. We investigate the effects of two unconventional policies implemented by central banks and governments during the recent credit crisis: an intervention in the collateral asset market aimed at bolstering the asset price and a policy of direct lending to collateral-poor firms. We find that both policies boost total investment but may dampen the increase in the innovation of collateral-rich firms after the shock. Notably, the asset market policy turns out to be more taxing for innovators than the direct lending policy.====In the last part of the paper, we revisit the mechanisms of the model under richer structures of the credit sector and the corporate sector. The goal is to assess under what structures of the credit and corporate sectors a credit crunch is more likely to depress or stimulate innovation. In the credit sector, we allow for information-intensive credit relationships between firms and lenders in which lenders obtain more information on firms' collateral assets (Diamond and Rajan, 2001). We find that, following a contraction in the price of collateral assets, the increase in the innovation of collateral-rich firms can entail a reduction in the number of credit relationships. In turn, this can dampen the stimulus to output triggered by innovation. In the corporate sector, we allow for managerial firms and managers' technological inertia due to higher riskiness of the new technology. We then investigate how managers' inertia interacts with lenders' potential technological inertia.====This paper especially relates to two strands of literature. The first strand investigates the impact of a disruption in the financial structure on aggregate investment (e.g., Gertler and Karadi, 2011; Gertler and Kiyotaki, 2010, Gertler and Kiyotaki, 2015). In this literature, we borrow some properties of our modeling strategy, such as the focus on a finite horizon economy, from Holmstrom and Tirole (1997). Other related papers in this literature include Den Haan et al. (2003), Dell' Ariccia and Garibaldi (2001), Jermann and Quadrini (2012), Khan and Thomas (2013), Buera and Moll (2015) and Catherine et al. (2017). Buera and Moll (2015) study the effects of a tightening of collateral constraints on investment wedges in a model with credit frictions and heterogeneous firms. Catherine et al. (2017) structurally estimate the impact on investment and output of collateral shocks in a model with heterogeneous firms.==== While in these studies a credit tightening depresses investment, in our economy it depresses investment but may foster technological change.====The second related strand of literature analyzes the impact of recessions on firms' innovation through the credit market (e.g., Caballero and Hammour, 2005, Ramey, 2004, Garcia-Macia, 2017, Wang, 2017). Caballero and Hammour (2005) show that, because of credit frictions, production units can be destroyed at an excessive rate during a recession and the subsequent recovery can occur more through a slowdown in the destruction rate than through an increase in the creation rate. Ramey (2004) shows that if financial managers have empire-building incentives, during downturns they can discard efficient projects to preserve the size of their portfolios. This paper puts forward a view opposite to these studies: while it depresses investment, a credit tightening can mitigate lenders' technological inertia. In this regard, our analysis shares some features with the Shumpeterian view of recessions as moments of creative destruction. For example, in Caballero and Hammour (1994) during recessions innovative production units can more easily enter and displace outdated ones. However, we do not focus on the innovations of newly formed start-ups, but rather on changes in the innovation propensity within incumbent businesses, showing how changes in the value of their collateral assets can alter the incentives of their lenders to support innovation. Accordingly, we focus on collateralized lending by financial institutions, rather than early-stage private equity or growth funding. Recent studies in this literature focus increasingly on the design of effective policies for promoting innovation. Garcia-Macia (2017) investigates an economy where heterogeneous firms invest in physical and intangible capital. Intangible capital is harder to seize by creditors and hence incurs into higher financing costs. In a financial crisis, these costs rise and the resulting fall in intangible investment amplifies the crisis. Garcia-Macia (2017) finds that a policy of transfers conditional on firm age may speed up the recovery from a crisis more than credit subsidies. Wang (2017) studies R&D investment and knowledge capital accumulation in an economy with credit frictions. Unlike physical capital, knowledge capital cannot be pledged as collateral. Wang (2017) shows that firms with initially high knowledge capital engage in precautionary savings to raise their collateralizable financial assets. He also examines the impact of industrial policies, finding that tax credits for R&D investment can boost output more than policies that encourage the use of intellectual property as collateral.====The remainder of the paper unfolds as follows. In Section 2, we provide background evidence for key mechanisms of the model. In Section 3, we present the baseline model. Section 4 solves for the equilibrium. In Section 5, we conduct experiments aimed at mimicking a credit market crisis. Section 6 considers policies. Section 7 studies extensions while Section 8 concludes. Appendix A contains additional details on the data and the empirical analysis while Appendix B contains the main proofs (baseline model, Sections 3–5). Further details of the derivations and additional results are relegated to the online Supplement.","Credit crunches, asset prices and technological change",https://www.sciencedirect.com/science/article/pii/S1094202518302965,April 2019,2019,Research Article,269.0
"Jo In Hwan,Senga Tatsuro","National University of Singapore, 1 Arts Link, Singapore 117570, Singapore,Queen Mary University of London, ESCoE and RIETI, Mile End Road, London, E1 4NS, UK","Received 6 November 2017, Revised 23 January 2019, Available online 5 February 2019, Version of Record 8 February 2019.",https://doi.org/10.1016/j.red.2019.01.002,Cited by (10)," effects dominate the former direct productivity gains in a model with the standard AR(1) process, as compared to our non-Gaussian process, under which both welfare and aggregate productivity increase by subsidy policies.","Government policies that attempt to alleviate credit constraints faced by small and young firms are widely adopted across countries; provision of subsidized credit is a prime example of this.==== Despite the popularity of such targeted industrial policies, quantitative studies on their macroeconomic effects are scarce.==== Subsidized credit helps small and young firms achieve efficient and larger scales of production, thus resolving misallocation of resources and enhancing aggregate productivity. However, increased factor prices in equilibrium reduce the number of firms in production, which in turn depresses aggregate productivity. The relative magnitudes of each channel—the direct productivity gains and the indirect general equilibrium effects—depend on the underlying distribution of firms and their financial status. Thus, whether long-run effects are productivity enhancing or not is ambiguous.====In this paper, we offer a general equilibrium analysis of such targeted credit subsidy policies by extending a heterogeneous firm model with collateral constraints and endogenous entry and exit. In particular, we employ a Pareto-distributed firm productivity process to capture the skewed firm size distribution in the Business Dynamics Statistics (BDS). From our policy experiments, we have two main findings. First, aggregate productivity rises by at most 1.35 percent, following a credit subsidy policy for small or young firms. While the policy promotes the reallocation of resources among firms, associated indirect effects offset much of the potential gains in productivity. These indirect effects arise from adjustments in the extensive margins which reduce the equilibrium number of firms. Second, credit subsidy policies ==== aggregate productivity when a model is inconsistent with the empirical firm size distribution. This is the case when we instead use a standard AR(1) process of firm productivity, a common approach in the literature.==== It follows that the skewness of the firm size distribution is crucial in quantitatively evaluating the aggregate effects of such policies.====Why do both extensive margin adjustments and firm size distribution matter? Models with a standard AR(1) process cannot capture firms located at the right tail of the skewed distribution of firms. Therefore, these models lack highly productive-small firms that could grow substantially if collateral constraints are lifted, and for whom there are large potential gains from credit subsidy policies. In the absence of such firms with substantial growth potential, the direct effect from credit subsidy policies is relatively small and is dominated by the negative indirect effects in equilibrium. This indirect effect is amplified when we also match the left tail of the firm size distribution. In equilibrium, many small-unproductive firms endogenously respond to changes in the exit margin. When the extensive margins are held fixed, on the other hand, the number of firms remains constant by construction. A credit subsidizing policy then leads to a positive gain in aggregate productivity, even when the assumed firm productivity follows an AR(1) process.====Our model builds on a standard heterogeneous firm model. The model has three key ingredients. First, as discussed above, we employ a non-Gaussian process for firm-level productivity which follows a bounded Pareto distribution. It is well-known that the empirical distribution of firm employment is highly skewed. That is, small firms dominate the business population, while large firms account for the largest fraction of aggregate employment.==== Our specification of firm productivity successfully replicates the empirical firm size distribution in the model economy. Second, we allow both endogenous entry and exit by firms.==== This element is essential to reproducing the empirical patterns of firm dynamics, including substantially lower survival rates among young firms. In addition, it allows us to examine the role of extensive margin adjustments in affecting aggregate productivity. Third, forward-looking collateral constraints are added, in the spirit of Kiyotaki and Moore (1997). The presence of collateral constraints restricts investment decisions at the firm level, thereby hindering immediate firm growth upon entry.==== This, together with the second ingredient, characterizes the firm lifecycle aspects with external financing and the corresponding firm age distribution.====We use this model to study the aggregate implications of targeted credit subsidy policies in a general equilibrium environment. In the absence of policy interventions, we require the model to match the key macroeconomic aggregates, firm size and age distributions, and firm dynamics patterns in the US. We then introduce a policy into the model which provides credit to financially constrained firms economy-wide. In this regard, our approach is in line with that of Buera et al. (2017), who studied the macroeconomic impact of large-scale microfinance in developing economies and highlighted the importance of considering the general equilibrium effects of such policies. To avoid potential policy distortions, we assume that credit subsidies are lump-sum cash transfers from households to targeted firms.====From our policy experiments, we show that there are four effects at work, one direct and three indirect, following a targeted credit subsidy. The direct effect emerges from the fact that small and young firms can achieve an efficient scale of production through subsidies. In the aggregate, the policy alleviates the capital misallocation that exists across firms due to credit constraints. In line with the findings in previous studies (e.g., Buera et al. (2011), Khan and Thomas (2013), Midrigan and Xu (2014)), we demonstrate the quantitative importance of misallocation in explaining aggregate productivity. In the meantime, the policy also brings indirect effects in general equilibrium. First, increased demand for capital and labor—by the recipients of the subsidy—raise factor prices. Higher factor prices depress the scale of production of untargeted firms. The second indirect effect is from adjustments in the extensive margins. Increased factor prices affect both entry and exit thresholds, together with the policy that encourages more entry by small firms. This is a ==== that replaces less-productive incumbents with productive entrants.==== The last effect is that there are fewer firms in operation because of higher costs of production, thus depresses aggregate productivity.====Our policy experiments focus on both small and young firms. It is widely known that these firms are likely to experience more difficulties in financing their investment externally. For example, 61 percent of small firms reported that they faced financial challenges and 76 percent among them used personal funds, according to the ====.==== Young firms in the survey faced relatively tougher borrowing conditions due to a lack of credit history or limited access to credit. In addition, the recent literature has documented that credit constraints faced by young firms compound adverse shocks, which leads to high exit rates among them.==== Given its pervasiveness, we focus on a subsidized credit policy that alleviates difficulties in external borrowing for small and young firms, as in Buera et al. (2013).====In practice, there are a variety of government support schemes for small businesses.==== These actual policies are mostly in comprehensive packages which include legal and financial assistance, education of managerial skills, and so forth. Moreover, such policies are in small-scale for a limited number of eligible firms. In contrast, we attempt to isolate the macroeconomic impact of policies that are specialized to ease credit constraints among firms. One such policy is the SBA loan program in the US, noted above.==== We also provide a plausible and useful model counterpart of this policy in this paper. The corresponding outcome from the model is consistent with our main findings though the magnitudes are smaller, and we verify that the indirect effects described above are robust.====Across our policy experiment results, there are two main mechanisms affecting the allocative efficiency of resources, one across incumbents (e.g., Restuccia and Rogerson (2008)), and one across entering and exiting firms.==== The role of financial frictions in generating resource misallocation and its aggregate implications have been studied by Khan and Thomas (2013), Buera and Moll (2015), Buera et al. (2011), and Midrigan and Xu (2014), among others. More recently, Catherine et al. (2018) studied misallocation in a model of collateral constraints and found a positive productivity gain from investment subsidies. Our policy exercises are distinct as we allow endogenous firm entry and exit, which leads to our finding that the long-run effect on aggregate productivity from alleviating credit constraints may be negative.====Our focus here is on the role of policies in reducing micro-level distortions. Previous studies, in contrast, have looked at policies that distort the allocation of resources. Guner et al. (2008) considered a model with size-contingent regulations and used their calibrated version of the model to show sizable welfare losses. Gourio and Roys (2014) also studied the aggregate implication of a French policy that distorts the size distribution of firms, showing the important role of such policy distortion in explaining aggregate productivity.====We present our theory in Section 2. We calibrate the theoretical model and examine the equilibrium before policy intervention in Section 3. In Section 4, we analyze the impact of industrial policies, and Section 5 investigates the role of extensive margins and skewed firm size distribution. Section 6 concludes the paper.",Aggregate consequences of credit subsidy policies: Firm dynamics and misallocation,https://www.sciencedirect.com/science/article/pii/S1094202519300584,April 2019,2019,Research Article,270.0
"Lagos Ricardo,Zhang Shengxing","Department of Economics, New York University, 19 W. 4th St., New York, NY 10012, United States of America,Department of Economics, London School of Economics, Houghton Street, London, WC2A 2AE, United Kingdom","Received 25 May 2018, Revised 31 October 2018, Available online 4 February 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.01.004,Cited by (10), where it is traded. These liquidity considerations imply a positive correlation between the real yield on such assets as stocks and housing and the nominal yield on Treasury bonds—an empirical observation long regarded anomalous. We provide novel theoretical implications and empirical evidence regarding the effect of monetary policy on the liquidity of these markets.,"We develop a monetary model of financial exchange in bilateral over-the-counter (OTC) markets and use it to show how the details of the market microstructure and the quantity of money (====) shape the performance of OTC markets (e.g., as gauged by standard measures of market liquidity), (====) generate speculative premia, and (====) explain the positive correlation between the nominal yield on Treasury bonds and the real yield on stocks (the basis for the so-called “Fed Model” of equity valuation popular among financial practitioners).====We consider a setting in which a financial asset that yields a dividend flow of consumption goods (e.g., an equity or a real bond) is traded by investors who have time-varying heterogeneous valuations for the dividend. In order to achieve the gains from trade that arise from their heterogeneous private valuations, investors participate in a bilateral market with random search. In the bilateral market, which has the stylized features of an OTC market structure, investors trade the financial asset using fiat money as a medium of exchange. Periodically, investors are able to rebalance their portfolios in a frictionless (Walrasian) market.====First, we use the theory to study the role that the quantity of money plays in shaping asset prices and the performance of OTC markets more generally. Since money serves as means of payment in financial transactions, the quantity of real balances affects the equilibrium allocation of the asset. Anticipated inflation reduces real balances and distorts the asset allocation by causing too many assets to remain in the hands of investors with relatively low valuations.====Second, we show that in a monetary equilibrium the asset price is larger than the expected present discounted value that any agent assigns to the dividend stream. This difference is a “speculative premium” that investors are willing to pay because they anticipate capital gains from reselling the asset to investors with higher valuations in the future. We show that the speculative premium and the asset price depend on the market structure where the asset is traded, e.g., both the premium and the asset price are decreasing functions of the expected execution delay. This theoretical result is broadly consistent with the behavior of illiquidity premia in response to variations in measures of trading activity documented in the recent empirical literature.====Third, we show how monetary policy affects speculative motives and the resulting speculative premium. An increase in anticipated inflation reduces the real money balances used to finance asset trading, which limits the ability of high-valuation traders to purchase the asset from low-valuation traders. As a result, the speculative premium and the real asset price are decreasing in the expected rate of inflation. This mechanism rationalizes the positive correlation between the real yield on stocks and the nominal yield on Treasury bonds—an empirical observation long regarded anomalous and that, for lack of an alternative theory, has been attributed to money illusion since the 1970s. We also use the model to study the effects of monetary policy on measures of financial liquidity of OTC markets, such as trade volume and price dispersion.====The remaining sections are organized as follows. Section 2 presents the model. Section 3 describes the efficient allocation. Equilibrium is characterized in Section 4. Section 5 analyzes the effects of monetary policy and OTC frictions on asset prices. Section 6 shows how monetary policy and OTC frictions influence measures of market liquidity, such as trade volume and price dispersion. In Section 7 we show asset prices typically exceed the expected present discounted value that any agent assigns to the dividend stream by an amount that reflects a speculative premium. In that section we also show the size of this speculative premium declines with inflation and the degree of OTC frictions. In Section 8 we show our theory offers a novel theoretical foundation for the “Fed Model”—a construct popular among practitioners and policymakers that is based on the documented positive correlation between nominal bond yields and real equity yields. Section 9 concludes. The appendix contains all proofs.",A monetary model of bilateral over-the-counter markets,https://www.sciencedirect.com/science/article/pii/S1094202518303144,4 February 2019,2019,Research Article,271.0
"Kuhn Florian,George Chacko","Department of Economics, Binghamton University, PO Box 6000, Binghamton, NY 13902-6000, United States of America,FDIC, Center for Financial Research, 550 17th St NW, Washington, DC 20429, United States of America","Received 26 March 2017, Revised 20 January 2019, Available online 31 January 2019, Version of Record 13 February 2019.",https://doi.org/10.1016/j.red.2019.01.001,Cited by (7),"When capacity constraints limit the production of heterogeneous firms, demand shocks can endogenously generate a number of important business cycle regularities: recessions are deeper than booms, economic volatility is countercyclical, the aggregate Solow residual is procyclical and the fiscal multiplier is countercyclical. The model's main mechanism is that the share of firms at their production limit is strongly procyclical. A baseline calibration of a basic New Keynesian ==== with capacity constraints delivers more than 25% of the empirically observed asymmetry in output, 18% of the additional cross-sectional dispersion in recessions and around 25% of the additional aggregate volatility, and more than 50% of the fluctuations in the Solow residual. The model implies fluctuations in the fiscal multiplier of around 0.12 between expansions and recessions.","This paper studies how to reconcile within a simple framework four disparate business cycle facts: the asymmetry of business cycle fluctuations, the countercyclicality of aggregate and cross-sectional volatility, the acyclicality of utilization-adjusted total factor productivity, and counter-cyclical fiscal multipliers. Together, these empirical findings characterize recessions as times when output is especially low, volatility is high, and fiscal policy is particularly effective.====While previous work has considered mechanisms that can account for each fact in isolation, these potential explanations are generally at odds with other facts. For example, one can appeal to asymmetric business cycle shocks to explain the asymmetry in business cycles, but this would not, by itself, account for the observed countercyclicality in the dispersion of cross-sectional firm productivity. Rather than trying to combine all of the mechanisms that could potentially account for each fact individually into an unwieldy model, we instead show that a single mechanism — occasionally binding capacity constraints — can endogenously generate each of these business cycle facts when introduced into an otherwise standard business cycle model.====In the model, firms choose their capital capacity before the realization of idiosyncratic and aggregate demand shocks. After learning about these, they may vary their utilization of capital in a way that is increasingly costly as the utilization rate increases. When the economy experiences positive shocks to the demand for firms' products, they increase their capital utilization and output. With capital predetermined, this endogenous choice of utilization gives rise to procyclical measured total factor productivity even when business cycles are driven by shocks other than TFP. At the same time utilization-adjusted factor productivity may remain acyclical, as documented by Basu et al. (2006).====The combination of predetermined capital and convex utilization costs yields an upper bound on any individual firm's production. Large, positive aggregate shocks, then, increase the number of firms at their capacity constraint. This adds extra concavity to aggregate production as a function of demand and helps explain the three remaining business cycle facts. First, booms are “smaller” than downturns, in the sense that average deviations of output from trend are smaller in absolute value when the economy is far above trend than far below trend. In the calibrated model, capacity constraints generate around one quarter of the observed asymmetry of U.S. business cycles.====Second, capacity constraints provide a channel through which fiscal multipliers can be countercyclical. Higher government spending that increases demand for firms' products will have larger effects when the economy is in a downturn than in an expansion. During downturns, few firms are capacity constrained and they can therefore readily expand production. During a boom, on the other hand, firms are already producing at their capacity constraint which reduces the expansionary effects of fiscal policy. Quantitatively, while the extent of countercyclicality of fiscal multipliers remains a point of contention empirically, the model here suggests a difference of about 0.12 between the multiplier in recession and expansion, respectively.====Third, the upper limit to production reduces cross-sectional and aggregate volatility when many firms have high capacity utilization. Idiosyncratic demand shocks generate a non-trivial distribution in the measured productivity of firms. The share of firms at their capacity constraint affects the variance of this distribution: since all constrained firms look very similar in terms of their productivity, a higher share of constrained firms implies a lower variance in the distribution of productivity. Recessions, during which few firms are capacity constrained, are then periods of high cross-sectional productivity dispersion. Occasionally binding capacity constraints therefore provide a previously unexplored channel through which cross-sectional productivity dispersion can endogenously move in a countercyclical manner even in the absence of second-moment shocks. Similarly, when the economy-wide utilization is already high, additional demand shocks do not move aggregate output much. Firms at their constraint are in the flat part of their production function and hence many of them do no respond to changes in demand. This explains why aggregate volatility, as measured by the conditional growth rate of aggregate output, is higher in recessions.====Understanding the properties of recessions matters in the assessment of their welfare costs. For example, while symmetric fluctuations reduce welfare, this loss is more severe if fluctuations exhibit asymmetry and the cost of a downturn is concentrated in a short period of time. Increased volatility in recessions can similarly reduce the welfare of risk-averse agents, and, as recent literature has shown, can have adverse economic effects of its own. The question of how economic fluctuations originate and are transmitted also has important implications for fiscal policy because the efficacy of government spending in general depends heavily on the cause of downturns. For example, the government multiplier is generally acyclical in standard models, whereas in models of uncertainty shocks, government spending can actually be less effective in recessions than in normal times.====The main contribution of this paper is to show that capacity constraints can explain several important features of the behavior of output under few additional assumptions. Second, capacity constraints suggest a novel explanation as to why productivity dispersion among firms is countercyclical. Third, while the traditional Keynesian literature has long emphasized idle capacities as one likely source of high fiscal multipliers when aggregate demand is low, there has been relatively little work on integrating this mechanism into modern DSGE models. This paper provides such a model. Fourth, we document how much this model, in addition to being qualitatively consistent, can contribute quantitatively to the explanation of the four business cycle facts. Finally, we add some empirical evidence to previous work on output asymmetry and find that large recessions on average deviate 30% more from trend output than large booms.====A number of papers study the effects of variable capacity utilization in general equilibrium frameworks. Work by Fagnart et al. (1999), Gilchrist and Williams (2000), Álvarez-Lois (2006) and Hansen and Prescott (2005) investigates capacity constraints with heterogeneous firms. The main difference to the present paper is that they consider shocks to aggregate TFP under putty-clay technology or irreversibilities, whereas we focus on fluctuations in aggregate demand under standard Cobb–Douglas production in which capacity constraints arise endogenously rather than as an assumption on production technology. The closest models are Fagnart et al. (1999) and Álvarez-Lois (2006), who explicitly model the pricing decision of monopolistically competitive firms. Fagnart et al. (1999) focus on the amplification of TFP shocks under putty-clay technology and flexible prices, whereas Álvarez-Lois (2006) looks at the response of firm mark-ups when prices are set one period in advance as well as the internal propagation of the putty-clay mechanism. Gilchrist and Williams (2000) emphasize the asymmetric effects on output following large TFP shocks and the hump-shaped response that is generated through the effects of vintage capital. Hansen and Prescott (2005) generate asymmetries by including a choice along the extensive margin of operating or idling plants.====A strand of papers considers variable capacity utilization in a representative-agent framework (Greenwood et al., 1988, Cooley et al., 1995, Bils and Cho, 1994, Christiano et al., 2005). In contrast, the environment with heterogeneous firms allows us to consider occasionally binding capacity constraints, as well as price setting and demand shocks in the monopolistic competition framework. This firm heterogeneity in turn is driving several of the results in our model, as we show in section 5.====A recent paper that also looks at the interplay of cross-sectional and aggregate asymmetries is Ilut et al. (2016), albeit under a different mechanism. They show that under ambiguity aversion (or more generally any concave reaction of employment growth to expected profitability), news shocks can tightly link countercyclical volatility at the micro and macro level. Their explanation involving firms' decision making offers a complementary alternative to the approach in this paper focusing on firms' production technology.====The key mechanism in our model is that the number of capacity constrained firms is procyclical. While the degree to which capacity constraints bind is hard to measure directly, we show below that three empirical observations are consistent with our mechanism: We see significant procyclicality in a) aggregate and sectoral capacity utilization, b) firms' investment in new capacity as documented by Bachmann and Zorn (2016) and c) firms' backlogs of unfilled orders. Additionally, testing a prediction of the mechanism, we also find that sectors with higher variance in capacity utilization exhibit stronger business cycle asymmetry.====The paper is structured as follows: In the next section 2 we review the stylized facts established by recent literature and add evidence on business cycle asymmetry. In section 3 we illustrate in a stylized example how capacity constraints can generate these facts qualitatively. We embed this mechanism in a full DSGE model in section 4, and discuss quantitative results in section 5. Section 6 concludes.",Business cycle implications of capacity constraints under demand shocks,https://www.sciencedirect.com/science/article/pii/S1094202519300572,April 2019,2019,Research Article,272.0
Lee Sang Yoon (Tim),"Queen Mary University of London, UK,CEPR, UK","Received 25 June 2018, Revised 23 December 2018, Available online 11 January 2019, Version of Record 21 January 2019.",https://doi.org/10.1016/j.red.2018.12.006,Cited by (3),"Income concentration in the U.S. rose sharply since the 1970s. But the share of ==== held by the top 1 percent increased less. This can be partially accounted for by a quantitative model of ==== from 1970 to 2000 replaces top entrepreneurs with top managers, which can account for 65% and 30% of the increase in the share of wages and income earned by the top 1 percent, respectively. At the same time, the share of wealth held by the top 1 percent remains stable, as entrepreneurs decumulate but managers accumulate wealth.","Wealth is much more concentrated than income in the United States. For most of the latter half of the 20th century, the top 1% of the wealthiest households held approximately 30% of aggregate wealth. In contrast, the top 1% of the highest income households earned approximately 10% of aggregate income.==== At the same time, income has become much more concentrated toward rich households since the 1970s compared to the previous post-war era, with the highest income percentile earning 7.8% of aggregate income in 1970 and 16.5% in 2000. Most of this can be accounted for by wages: Both the share of wages earned by the top wage earners, and the wage component of the highest income households, have risen throughout the same time frame (Piketty and Saez, 2003). In contrast, wealth concentration has either been rather flat according to the Survey of Consumer Finances.====Two questions beg explanation: What drove the dramatic increase in wage and income concentration? And why was this pattern not mirrored in wealth concentration? To answer these questions, I develop a model where firms can remain in the hands of an entrepreneur, or be sold and run by managers. I show that less progressive taxation improves the market for managers, replacing top entrepreneurs with top managers. Because their sources of income and savings behavior differ, this ultimately shifts the distributions of wage, income and wealth. Feeding observed tax policies in 1970 and 2000 into the model can account for 65% of the increase in the share of wages earned by the top 1% and about 30% of the increase in the share of income earned by the top 1%. Moreover, the concentration of wealth at the top 1% first drops before starting to rise again.====The novel component of the model is the distinction between entrepreneurs and managers. Most models treat them identically, and by doing so miss a market for managers, or “talent.” To incorporate the missing market, I take a simple approach where entrepreneurs need to use their own assets to run a firm, while managers are hired by firms in which they need not invest. This generates a trade-off between becoming an entrepreneur or manager: Entrepreneurs must save to put capital into the firms they run in the face of collateral constraints, while managers are hired in a frictional market with agency costs. Occupational choices are affected by tax policy because managers earn most of their income in the form of wages, while entrepreneurs earn relatively more from their business.====The manager market is assumed to be competitive. This implies that managerial compensation is proportional to the size of the firms they run (up to a constant). Therefore, the wages of the managers who run the largest firms rise with the total mass of firms in the economy (Tervio, 2008; Gabaix and Landier, 2008). When managers replace entrepreneurs at the high end of the income distribution, income becomes more concentrated because managers have higher earnings.==== This is in line with the explosive increase in managerial compensation since the 1980s and the rise in earnings concentration being the main culprit for the rise in income concentration (Piketty and Saez, 2003).====The collateral constraints create a strong concentration of wealth by inducing individuals to save up, both in anticipation of becoming an entrepreneur, and to expand their business when they are an entrepreneur. When it becomes better to be a manager, this savings motive declines, leading to a drop in wealth concentration. But because becoming a top manager is a scarce opportunity, individuals save disproportionately more when they finally make it. Moreover, top managers save out of a higher income than top entrepreneurs (on average). So as managers continue to crowd out entrepreneurs at the top, wealth concentration eventually rises again. These two forces keep the concentration of wealth stable.====My quantitative results show that such occupational shifts at the top can be induced by a decline in tax progressivity. Taxes are fed into the model exogenously, and I numerically compute the response of the economy to historical changes. Federal income taxation has become much less progressive, with the highest income groups paying as much as 70% of their income in taxes in the 1970s as opposed to 35% today. While other forms of taxation such as the capital gains tax have also fell, in Appendix C I present evidence that the major shift at the top will have come from how income taxation affects top labor earnings, of which as much as half may be managerial compensation.====In the model, lower taxes on high levels of managerial compensation increase both the supply and demand of managers in equilibrium, and more importantly, increases the relative measure of managers vs entrepreneurs at the high end of the income distribution. This occupational shift occurs ====: A rich, high ability entrepreneur does not instead choose to become a manager because ==== managerial compensation is high, but because the ==== compensation becomes higher when there is a larger mass of managers.====The main message of my model is that wealth concentration need not follow income concentration, since it depends on rich individuals' savings incentives. This highlights a mechanism that is important to consider when designing tax policies. My model lets progressive taxation affect the production side of the economy by changing the equilibrium composition of entrepreneurs and managers, who produce all output. Since the producers also save, the wealth distribution depends on the interaction of their capital demand and supply. This result contrasts with previous studies that weigh the benefits of public insurance against the individual labor supply incentives of workers.==== Much of the literature attempts to explain the high degree of wealth concentration observed in the data. Aiyagari (1994) shows that incomplete markets alone come far from accomplishing this. Krusell and Smith (1998) find that adding aggregate uncertainty is also insufficient.==== Castañeda et al. (2003) find that a model with endogenous labor supply and taxes can almost exactly match empirical earnings and wealth inequality moments, by assuming an extremely high labor productivity shock that occurs with very small probability.====In these models, all individuals are wage workers and the main source of risk is an exogenous labor productivity shock. But while the typical progressive taxation debate pits the workers' wages against firm revenues, firms in these models play little role besides pinning down equilibrium prices. In my model, all workers are hired by entrepreneurs or managers. And since occupational choices are made endogenously, a change in tax policy affects the equilibrium allocation of producers vis-à-vis workers. This also implies that individuals' income processes themselves respond endogenously to policy changes.====This insight is borrowed from the span-of-control model of Lucas (1978), which endogenously generates high income for top managers. In a parsimonious overlapping generations framework, Cagetti and De Nardi (2006) shows that a span-of-control mechanism with collateral constraints can generate a realistically high concentration of wealth. Quadrini (2000) shows that collateral constraints and entrepreneurial risk can explain income class mobility as well as the U.S. wealth distribution. In practice, these models add a small fraction of entrepreneurs into a model of what they call the “corporate sector,” which is more or less identical to Aiyagari (1994). Such models are suitable for analyzing the behavior of entrepreneurs and how they interact with the macroeconomy, but do not capture the large role played by the corporate sector, which accounts for more than 70% of U.S. output. The manager market in my paper can be viewed as a model of the corporate sector in such papers.====More important, neither line of literature can explain rising income concentration not accompanied by large shifts in wealth concentration.==== The mechanisms used generate a strong concentration of wealth because high income earners have an unusually large savings motive, whether it be because they face a different income process or because they have a different occupation. Consequently, an increase in income concentration is necessarily linked to an even higher concentration of wealth.====To break this link, I combine elements from both strands of the literature. Starting from an entrepreneurial model similar to Buera and Shin (2013), I add another high income earning occupation, a manager, that competes with the entrepreneurs. Managers are hired in a market to run businesses sold by potential entrepreneurs who chose to not run the business themselves. This can be viewed as technology transfer à la Holmes et al., 1990, Holmes et al., 1995, which Silveira and Wright (2010) generalize by adding various frictions to the transfer process. In contrast, I interpret such transfers as a mechanism that brings a business to the disposal of a competitive investment market, and simplify the process so that it can be embedded in a general equilibrium framework. This simplification is done by borrowing from managerial assignment models, e.g. Tervio (2008), Gabaix and Landier (2008).====In equilibrium, high income groups display different characteristics depending on the ratio of entrepreneurs versus managers that comprise them. When there are more entrepreneurs, the model behaves similarly to an entrepreneurial model with collateral constraints. When there are more managers, it behaves more similarly to a competitive assignment model with superstar earnings; the equilibrium income process then resembles that of Castañeda et al. (2003). Thus, a shift in occupational choices alters the savings behavior of different income groups and their sources of income.====It is important that I allow policy to affect such occupation shifts. Feeding in a rich set of fiscal policies into a model similar to Castañeda et al. (2003), Kaymak and Poschke (2016) find that policy changes can explain about half of the rise in the concentration of wealth by making savings more attractive, but that rising wage dispersion is the main driver of inequality trends.==== At first glance, this may seem to contradict my results. But it is still true in my model that wage dispersion drives inequality, and that top earners save more than before. The main difference is that in my model, wage dispersion is not exogenous, but endogenously determined by equilibrium occupation choices. Because less progressive taxation leads to more managers in equilibrium, top wages rise, and top earners save more to take advantage of their luck.====Since what triggers the occupation shifts in the model is tax progressivity, the paper is also related to the empirical public finance literature, which I discuss in the next section while summarizing the relevant empirical facts. Section 3 presents the theoretical model and its properties. Section 4 describes the calibration strategy and the numerical experiment. Section 5 discusses the results and the quantitative mechanisms of the model, and Section 6 concludes. Supplementary data and results are included in the Appendix.","Entrepreneurs, managers and inequality",https://www.sciencedirect.com/science/article/pii/S1094202518303600,April 2019,2019,Research Article,273.0
Gelfer Sacha,"Bentley University, Waltham, MA, USA","Received 17 December 2017, Revised 20 December 2018, Available online 2 January 2019, Version of Record 4 January 2019.",https://doi.org/10.1016/j.red.2018.12.005,Cited by (9)," and ====, I estimate two DSGE models in a data-rich environment. The two models estimated in this paper include close variations of the ==== (====; ","Modern day macroeconomic theory has greatly leaned on structural dynamic stochastic general equilibrium (DSGE) modeling. These models give policymakers a workshop in which co-movements of aggregate macroeconomic time series can be evaluated over the business cycle. The Smets and Wouters (2003; 2007) model (henceforth, SW) in particular is widely considered the “workhorse” of the DSGE literature. However, Del Negro and Schorfheide (2013) have found this model to be limited in identifying the financial crisis for most of 2008, including the 4th quarter of 2008, when the crisis was in full swing.====A model that was better equipped to identify the Great Recession a few months earlier than the SW model is a variant of the SW model with financial frictions (henceforth, SWFF). The SWFF model introduces a Bernanke et al. (1999) financial accelerator mechanism and closely follows the entrepreneurial sector of the FRBNY model outlined by Del Negro et al. (2013). Del Negro and Schorfheide (2013) compared the SW and SWFF models' forecasting performance over the past two decades when the models were estimated under a standard set of seven or eight macroeconomic data series. They found that during the Great Recession the modified SWFF model was better at forecasting output and inflation when compared to the original SW model, however, this forecast model ranking was not consistent in time frames outside of the Great Recession.====In this paper, I compare the forecasting performance of both the SW and SWFF DSGE models when they are estimated in a data-rich environment using the techniques of Boivin and Giannoni (2006) and Kryshko (2011). To my knowledge, this is the first time the SWFF model has been estimated in this fashion. Given the construction of traditional DSGE model estimation (henceforth, DSGE-Reg) Del Negro and Schorfheide (2013) were only able to compare the two models along a few key macroeconomic series. However, the estimation technique of Boivin and Giannoni (henceforth, DSGE-DFM) allows DSGE models to be estimated using a large data vector of macroeconomic time series and it provides an avenue through which these two models can be used to study the dynamics of such series as the unemployment rate, unemployment duration and employees by sector; even when no such series are directly incorporated into the structural model. Instead, the series that are not directly incorporated inside the DSGE model are allowed to load on economic variables and structural processes that are inside the DSGE model.====In addition, the DSGE-DFM estimation technique may provide different estimates of some structural parameters in the model that could produce better forecasts of variables that are directly modeled inside the DSGE model, such as GDP, consumption, investment growth, inflation and interest rates.====After conducting the DSGE-DFM and DSGE-Reg estimations on both the SW and SWFF models, I compare the forecasts of the four models, SW-Reg, SW-DFM, SWFF-Reg, SWFF-DFM. Comparing these four models helps answer three important questions. First, do DSGE-DFM models help in forecasting core macroeconomic variables before and during the Great Recession when compared to DSGE-Reg models? Second, when I compare the SW-DFM and SWFF-DFM models do we see similar results for other key macroeconomic variables not directly incorporated in either model as Del Negro and Schorfheide (2013) found for the variables of output growth and inflation? Third, does DSGE-DFM estimation help eliminate the time varying forecast dominance of the SW and SWFF models?====I first closely examine the period surrounding the Great Recession and its recovery and find compelling evidence that the answer to the first two questions is ====. I find that both DSGE-DFM models are better equipped to replicate the dynamics of the Great Recession than their DSGE-Reg counterparts are, when it comes to output, consumption, hours worked and investment. In addition, the SWFF-DFM model is able to foresee the downturn in output and investment as early as February of 2008. I also find that the SWFF-DFM model was able to foresee the decrease in the number of overall jobs, number of jobs in the manufacturing and construction sectors and the rise in the unemployment rate beginning in the fall of 2008.====When comparing the four models along a wider time frame horizon (1998–2011), I find that both DSGE-DFM models predict the dynamics associated with the core macroeconomic growth variables more accurately when compared to the two DSGE-Reg models. I also find that many of the in-sample forecasts generated by the SW-DFM and SWFF-DFM models do not differ much in tranquil economic times. It is only in times of financial volatility that I see the simulated paths from the two DSGE-DFM models begin to differ. As a result of this the SWFF-DFM model out ranks the SW-DFM model in terms of forecasting for the entire sample period not just in times of financial volatility as was the case with the SWFF-Reg and SW-Reg models as illustrated in Del Negro and Schorfheide (2013).====These results suggest that the SWFF model estimated in a data-rich environment (SWFF-DFM) would have predicted the labor market and production dynamics associated with the Great Recession and its proceeding recovery. In addition to generating the most accurate dynamics, the SWFF-DFM model also has similar or smaller root mean squared errors (henceforth, RMSEs) when compared to the Survey of Professional Forecasters' (henceforth, SPF) forecasts of output, consumption and investment growth. This implies the SWFF-DFM not only wins the horserace amongst the four models but that it should also be taken seriously as a forecasting and policy analysis tool.====Lastly, I examine both DSGE-DFM models for clues on why they could have foreseen the Great Recession earlier than their DSGE-Reg counterparts. I conduct an historical decomposition of the SWFF model and find that the Great Recession can mainly be attributed to negative investment, negative preference and negative finance shocks (corresponding to an increased spread between the risk and risk-free interest rates inside the model). When I look at the impulse response functions (IRFs) generated for the SWFF-Reg and SWFF-DFM models, I find that the different structural parameter estimates help generate more persistent declines in output from these types of shocks and I see that the investment-consumption tradeoff that can occur from these shocks diminishes in the SWFF-DFM model. This diminishing tradeoff results in slower recoveries in both real investment and real consumption and thus, a slower recovery in real GDP, as was seen with the recovery from the Great Recession.====The macro-financial time series I use to estimate both the SW and SWFF models is a near replica of the Stock and Watson (2003) dataset used in estimating their dynamic factor model. It includes labor and financial data series that are usually not utilized in DSGE-Reg estimation. These include employment by sector, stock price indexes, housing starts and many price and wage indexes beyond the standard CPI index and GDP deflator.====This DSGE-DFM method has been most recently used by Gali et al. (2012), Brave et al. (2012), Justiniano et al. (2013), and Barsky et al. (2014); who have all expanded the observable vector to improve the identification of unobservable and observable states and thus improve the estimation of the structural parameters. Gali et al. (2012) and Justiniano et al. (2013) promotes the use of multiple series for the measurement of wages, while Brave et al. (2012) and Barsky et al. (2014) uses multiple measures of inflation to estimate their perspective models. However, these papers used the method to allow for multiple data variables measuring the same model concepts and I will use the methodology to allow a large vector of macro-financial data to load on all DSGE model states.====In addition to these papers, my paper also fits into the structural DSGE literature of labor market dynamics around the Great Recession. Gali et al. (2012), Christiano et al. (2015; 2016) incorporate a more advanced labor market in their perspective DSGE models than either the SW or SWFF model. The models of Gali et al. (2012) and Christiano et al. (2015) are able to simulate and/or forecast the dynamics of employment, unemployment and other aggregate labor market statistics quite nicely, as does the SWFF-DFM model of this paper. However, the SWFF-DFM model in this paper is able to also capture the labor market and output dynamics of less aggregate statistics, such as employment and production by sector.====The remainder of this paper is structured as follows. Section 2 briefly explains the features of each DSGE model and outlines the estimation technique used to incorporate the large set of economic and financial series. Also included in this section is a description of the priors for the state-space and structural parameters as well as an overview of the data series. Section 3 presents the simulated output growth paths of all four models around the Great Recession and the paths for both the SW-DFM and SWFF-DFM models for various production growth, labor, output and finance series around the trough and recovery of the Great Recession. Also included in this section are Diebold Mariano test statistics of out-of-sample forecasts for all four models around the 1998–2011 time frame. Section 4 discusses the important dynamics of the SWFF-DFM model and discusses why the SWFF-DFM was able to predict a slow recovery in employment and production markets. Section 5 concludes and discusses future extensions.",Data-rich DSGE model forecasts of the great recession and its recovery,https://www.sciencedirect.com/science/article/pii/S1094202518302989,April 2019,2019,Research Article,274.0
Reichlin Pietro,"Department of Economics and Finance, LUISS G. Carli, Viale Romania 32, 00197 Rome, Italy","Received 3 April 2018, Revised 23 August 2018, Available online 13 December 2018, Version of Record 17 December 2018.",https://doi.org/10.1016/j.red.2018.12.002,Cited by (0),"Equilibria where altruistic generations are linked via positive bequests are indeterminate and subject to sunspot variables when each individual's utility in non-separable in her own age contingent consumption and sufficiently biased towards old age. This result does not require strong income effects and it applies if individuals select their own savings and bequests by taking the decisions of their ==== and successors as given. In this case, the equivalence with the Dynastic Equilibria of a Ramsey-type model envisaged in ==== fails. To help interpreting my results I compare the olg cum ==== model with a canonical non-altruistic olg economy with two goods and age dependent per period utility functions.","Real indeterminacy of equilibria in pure exchange overlapping generations (olg) economies is a consequence of a backward bending offer curve (strong income effect). In this contribution I show that, by introducing some however small degree of parental altruism, indeterminacy can be obtained under more general conditions, and in the absence of a strong income effect.====In order to get direct comparisons with the existing literature, I consider a model very similar to Calvo (1978), ====, a standard olg economy with two-period lived individuals using a long-run real asset as a store of value, a type of Lucas tree with positive dividends. This specification of the model is motivated by the intention to stress that indeterminacy is not a consequence of nominal assets o asset bubbles. In the canonical version of this type of economy, individuals are totally selfish and real indeterminacy can only arise if the savings function is decreasing in the real interest rate. In the reformulation of the model considered here, individuals are characterized by some degree of altruism with respect to their offsprings. In particular, their welfare is defined by a utility function of their own age-contingent consumption (which I call ====) and the welfare of their immediate successors (====, the next generation of individuals) multiplied by a discount rate ====. The notion of equilibrium adopted in this paper stands in contrast to the one that is implicitly adopted in Barro (1974), where parents act as Stackelberg leaders versus their children. Instead, I follow a Cournot-type of equilibrium concept, whereby parents select their own savings and (====) bequests by taking the decisions of their offsprings and successors as given. At equilibrium, parents' decisions about bequests and their children's decisions about their savings are best responses to each others actions (so that no individual has an incentive to move).====I show that local indeterminacy, sunspot equilibria and even global indeterminacy (when altruism is one sided and the non negativity constraints on bequests are occasionally binding) may arise under the assumption that the selfish utility is non-separable in young and old age consumption. The basic requirement for indeterminacy under this set of assumptions is that the selfish utility is sufficiently biased toward old age. Although this results holds with quite general characterizations of non-separable utility functions, I provide the key analytical results under the assumption that the selfish utility is linearly homogeneous and such that the elasticity of substitution between young and old age consumption is greater than one. This specification has the advantage of simplifying the dynamic structure of the model and guaranteeing that the corresponding canonical olg model obtained (from my model) by setting ==== has a unique equilibrium. In appendix A I provide a generalization of the key propositions to the case of non-linear homogeneity and show that the conditions for indeterminacy can be relaxed under more general preferences.====In the final section of the paper I present a different variation of the canonical olg model in Calvo (1978), where individuals are selfish (====), but the old derive utility from an extra good that is produced through the labor effort of the young. Similarly to the case of parental altruism, the competitive equilibria of this model display a large degree of indeterminacy despite the absence of a strong income effect (backward bending offer curve) in the canonical olg model. Hence, bequests play a role that is somewhat similar to the presence of an extra good that is offered by the young and demanded by the old.====Beside providing some additional conditions for the existence of indeterminate equilibria in olg models, I view my contribution as having consequences on at least three other issues that are important in the macroeconomics literature.====First, we may have to admit that the dynamic patterns of households' wealth with ==== bequests may be very different irrespective of individuals' characteristics and degree of altruism. In the past thirty years many economists have tried to understand the households' old age consumption and bequests based on some detectable characteristics, such as preferences, occupation, education and the number of children (see Hurd, 1987, Hurd, 1989, Kopczuk and Lupton, 2007). Based on the indeterminacy results of this paper it may be possible to argue that the pattern of bequests may also be defined by expectations about future market variables.====The second issue is related to the possibility of understanding the standard infinitely lived Ramsey model as a dynastic economy with successive generations of finitely lived individuals. It is commonly argued that, when parental altruism (in the form just described) is allowed in the canonical olg model with no market frictions, each individual behaves as though it is a single infinitely lived individual and the set of competitive equilibria (====) inherit some of the typical properties of an economy with a single representative individual. In particular, they are Pareto optimal and locally determined.==== More precisely, equilibria correspond to the Pareto optimum derived from a social welfare function assigning zero weights to all generations except for the initial one. The present contribution shows that this conclusion is unwarranted, in the sense that it requires a notion of equilibrium whereby the initial generation choses her own consumptions and bequests as well as the consumptions and bequests of all future generations. More precisely, equilibrium uniqueness (and the equivalence between the olg model with parental altruism and the standard Ramsey type model) would be restored under an alternative notion of equilibrium, corresponding to the unique sub-game perfect equilibrium of a sequential moves game where parents chose bequests by responding to the children's reaction function (and to the children's response to the grandchildren's reaction function, and so on). Although this is a natural and relevant equilibrium concept, I see no reason to view the assumption that parents are able to act on the children's reaction function as the only possible representation of the game. In my alternative formulation of the game, parents observe the bequests received from the previous generation and select their own savings and bequests by making a guess about the savings and bequests that their children will select in the future. These guesses are correct at the Nash equilibrium of the game, so that individuals are acting rationally, as they have no incentive to deviate from their actions at equilibrium. On the other hand, the old are not sophisticated (or “super-rational”) in the sense that they have no system of beliefs that would induce them to eliminate actions based on sub-optimal responses from their children off the equilibrium path. My view is that this is a reasonable way of modeling parents' behavior. In other words, there are good reasons to consider the consequences (on the structure of equilibria) of departing from a model where agents are “super rational” (====, they are able to compute the impact of their choices on their children's actions on and off the equilibrium path). Violations of super-rationality have already been considered in the literature on wealth transmission. One example is the large literature on intergenerational wealth transmission based on the assumption that bequests provide direct utility benefits to the donors (as in the ==== introduced by Andreoni (1989) and Andreoni (1990) which has been widely used in literature on bequests). Although this way of modeling parents' bequests may not be considered a consequence of limited rationality, it is certainly true that making bequests an argument of the utility function (joy of giving) is a reduced form of some deeper assumptions about parents' concern for their children's welfare and, then, it implies a sort of behavioral decision-making. The latter assumption is not the way I am interpreting my model (the Cournot game implies individual rationality), but one may interpret my model as a “middle ground” between pure altruism ==== sub-game perfection and a reduced form representation of altruistic concerns. In summary, the Nash equilibrium of the game adopted in this paper stands out as an alternative to the “sequential rationality model” and the “joy of giving approach”: it shares with the first model the idea of pure altruism and full rationality, although it does not impose that parents are fully internalizing the impact of all of their decisions on the children's actions.====The final issue emerging from my analysis is the role of time (additive) separable preferences in macroeconomics and growth theory, employed in most of the papers in this field. My contribution shows that this assumption is not only important for computational and analytical reasons, but also because it guarantees the existence of a unique equilibrium. On the one hand, this could be an extra motivation for adopting this type of preferences in macroeconomics, on the other hand, we cannot deny that non-separability between current and future consumption in individuals' utility is, by all accounts, a reasonable assumption and, conversely, that time separability is highly special, as claimed by many important economists starting from Fisher (1930) and Hicks (1965). For instance, Lucas and Stokey (1984) argue that ==== (Lucas and Stokey, 1984, p. 169)”. The literature on consumption theory has experimented a lot with non-separable time preferences, which appears to be appropriate to account for the well documented observation that past consumption is affecting current consumption patterns. Habit formation is one leading case proving that non-separability in inter-temporal decisions is widely accepted (Abel, 1990 and Constantinides, 1990).====The remainder of the paper is organized as follows. Section 2 presents the model, section 3 discusses the structure of equilibria, section 4 provides conditions for indeterminacy, section 5 discusses the relation of this model with the model by Barro (1974), section 6 presents the olg model with two goods and section 7 concludes.",Equilibrium indeterminacy with parental altruism,https://www.sciencedirect.com/science/article/pii/S109420251830142X,January 2019,2019,Research Article,276.0
Williamson Stephen D.,"University of Western Ontario, Canada","Received 21 January 2018, Revised 4 December 2018, Available online 12 December 2018, Version of Record 21 December 2018.",https://doi.org/10.1016/j.red.2018.12.003,Cited by (9),"How do low real interest rates constrain ==== are useful in bringing inflation down and relaxing financial constraints, not for forward guidance reasons. The ZLB may be suboptimal under tight collateral constraints.","It is now widely-accepted that the real rate of interest on safe assets is historically low, and that this phenomenon is likely to persist. While the reasons for a low real rate of interest are typically ascribed to non-monetary factors, the consensus view among central bankers is that monetary policy may need to be conducted differently in a low-real-interest-rate world. The typical central banking view is that a low real interest rate implies that the zero lower bound (ZLB) (or effective lower bound – ELB), on nominal interest rates will be encountered more frequently. The symptoms of a frequently-binding ZLB/ELB constraint are thought, in this view, to be low inflation and low aggregate output. As a result, it is argued, monetary policy tools formerly thought to be unconventional will find a place in the conventional toolbox. In particular, forward guidance, in the form of promises by the central bank to keep the nominal interest rate low for an extended period at the ZLB/ELB, is thought to be effective.====In this paper, we construct a tractable analytical model of low real interest rates and monetary policy. The goal is to study optimal monetary policy using a model which is explicit about the reason for low real interest rates – a scarcity of safe collateral – and which can incorporate alternative sources of inefficiency so as to understand how those interact with collateral scarcity. The model can incorporate sticky prices, financial frictions, monetary exchange, and open market operations, in a straightforward way. In a number of respects, the results contradict the conventional wisdom of central bankers. (i) If a binding ZLB constraint presents a problem for inflation targeting, it is that inflation will be above target, not below target. (ii) Keeping the nominal interest rate low for an extended period in the face of a low real interest rate may be optimal. But this acts to bring inflation down quickly, not to increase current inflation through promises of high inflation in the future. (iii) It may be optimal for the central bank to increase the nominal interest rate when the real interest rate is low, rather than going to the ZLB.====Interest in the ZLB in macroeconomics is at least as old as Keynes's ==== (Keynes, 1936). Keynes emphasized the importance of the liquidity trap – the neutrality of open market operations at the ZLB. However, Keynes's arguments seem to have been viewed as of little practical importance, except perhaps for explaining what was going on in the Great Depression. Indeed, in the period after the 1951 Treasury/Fed Accord until 2002, monetary policy in the United States rarely got within shouting distance of the ZLB. For example, between 1951 and 2002, the 3-month Treasury bill rate dipped below 1% only twice, and only for short periods of time, in 1954 and 1958.====The ZLB did not become a widely discussed monetary policy issue until the late 1990s and early 2000s. Interest at that time appears to have been sparked by experience in Japan, as well as in the United States, where the federal funds rate target stood at 1% from mid-2002 to mid-2003. This experience led to macroeconomic research using modern macroeconomic theory to understand the importance of the ZLB for macroeconomic activity and for monetary policy. Krugman (1998) used a dynamic cash-in-advance model to show how temporarily-sticky prices could lead to a situation in which current monetary policy would be thwarted by the ZLB, but a promise to raise future prices through future monetary expansion would permit escape from a liquidity trap. This idea was later fleshed out in a more complete New Keynesian framework by Eggertsson and Woodford (2003), who argued that a binding ZLB constraint could cause deflation and low output. This was then characterized as a policy problem that could be mitigated by commitment to future policy actions. Once the underlying factor that had been causing the ZLB to bind – a low “natural real rate of interest” – dissipates, the future inflation-targeting central banker will revert to a policy that the present-day policymaker would view as suboptimal. According to Eggertsson and Woodford, a forward guidance policy by which the policymaker could commit to a future policy of low nominal interest rates and higher inflation - after the natural real rate rises - would mitigate the ZLB problem.====An influential policy view from this period (see Bernanke et al., 2004, page 1) is neatly summarized as follows: ====This policy view is in some ways consistent with Eggertsson and Woodford (2003), but differs in two ways. First, there is no emphasis on forward guidance. Second, Bernanke thinks that a ZLB monetary policy leads to unstable dynamics and the potential for a deflationary black hole. This idea is hard to trace in the academic literature, but seems to have been part of the public policy discussion (see for example Krugman, 2002).====Since the early 2000s, macroeconomic shocks and the policy responses to those shocks have provided macroeconomists interested in ZLB issues with more information. The global financial crisis and the resulting worldwide recession in 2008–2009 ultimately resulted in ZLB (or negative interest rate) monetary policy in many countries, including the US, the UK, Canada, the Euro area, and Sweden. In Japan, the Bank of Japan has now pursued ZLB policies, or negative interest rate policies, for about 23 years. As well, there has been extensive use of the unconventional monetary policies specified in Bernanke et al. (2004) for example, including large central bank balance sheets, increases in the average maturity of central bank asset portfolios, and forward guidance.====More recent research by Werning (2012) reinforces the New Keynesian analysis of Eggertsson and Woodford (2003). Werning's contribution is to analyze optimal monetary policy in the context of a temporarily low natural real rate of interest. As in Eggertsson and Woodford (2003), the ZLB period is characterized by low inflation and low output. Forward guidance, in the form of commitment to low nominal interest rates and high inflation in the future, mitigates the problem. Werning also argues that the ZLB problem worsens as price flexibility increases.====Williams (2014) summarizes mainstream monetary policymakers' views of the state of the art in ZLB monetary policy. What appears to have disappeared from the policy discussion (relative to 2003) is concern with the deflationary-black-hole potential formerly thought to be inherent in ZLB episodes. This change in the consensus policy view has occurred for good reasons. In particular, it would be hard to characterize the 23-year low-nominal-interest-rate regime in Japan as unstable, and Japan has not experienced a persistent deflation. On average, the inflation rate in Japan since 1995 has been close to zero. As well, during the seven-year period (2008–2015) in the United States when the fed funds rate target range was 0–0.25%, the inflation rate (headline personal consumption deflator) in the US averaged 1.3%. This rate was lower than the Fed's target of 2%, but hardly a deflationary black hole.====From Williams (2014) and the New Keynesian literature on the ZLB discussed above, a current consensus policy view emerges:====There are good reasons to be skeptical of this consensus policy view. First, under the consensus view, the ZLB is sometimes discussed as an inevitable response to a set of economic shocks. But the ZLB binds in practice as the result of policy choices. Benhabib et al. (2001) tells us that adherence to the Taylor rule by a central bank, following the Taylor principle (increase the nominal interest rate more than one-for-one in response to an increase in the inflation rate) can lead the central bank into a policy trap. At low nominal interest rates, the Fisher effect sets in, inflation is low, and the central banker lowers the nominal interest rate to zero, expecting that inflation will go up. But this just leads to persistently low inflation, and the central bank is stuck at the ZLB.====Second, some recent research on the properties of New Keynesian models raises doubts about how the results in Eggertsson and Woodford (2003) and Werning (2012), for example, should be interpreted. Work by Cochrane, 2016, Cochrane, 2017, Rupert and Sustek (2016), and Williamson (2018c) comes to ==== conclusions concerning the properties of New Keynesian models, and of the broader set of mainstream macroeconomic monetary models. That is, low (high) inflation tends to be caused by low (high) nominal interest rate settings by the central bank. While there may be nonneutralities of money that result in liquidity effects (an increase in the central bank's nominal interest rate target raises the real interest rate in the short run), such effects are dominated by Fisher effects, even in the short run. As a result, higher (lower) nominal interest rates are associated with higher (lower) inflation. This alternative view of inflation dynamics might make us question the consensus policy view of the ZLB problem. Maybe inflation is low when the ZLB binds because of monetary policy, not because of what monetary policy is responding to.====Third, as Werning (2012) notes as a caveat in his work: ====A good case can be made that such constraints and frictions are indeed relevant in such situations – i.e. situations in which the ZLB may bind, or should bind, according to policymakers. That is, macroeconomists seem to agree that real interest rates are currently low for reasons independent of monetary policy. In particular, low productivity growth and demographic factors may cause the real interest rate to be low. And theory and evidence suggest that a key factor leading to low real rates of interest on safe assets is the high demand and low supply of safe and liquid assets. For theoretical support, see Caballero et al. (2016) and Andolfatto and Williamson (2015), and for empirical evidence, see Krishnamurthy and Vissing-Jorgensen (2012), and Del Negro et al. (2017). Thus, it seems that it would be useful in analyzing the implications of low real interest rates to model a safe asset shortage explicitly.====To explore these issues, we need a model, and many monetary policymakers think it important that policy modeling include price and/or wage rigidity. But following the conventional New Keynesian (NK) route – Dixit–Stiglitz monopolistic competition, Calvo pricing, etc., leads to complicated reduced-form linearized derivations, which can obscure what is going on. And a key simplification in such models is to eliminate central bank balance sheets and the details of retail transactions from the analysis, as in Woodfordian “cashless” models (see Woodford, 2003). But once we enter the realm of financial frictions, looking out for the details of central bank asset swaps and retail transactions can be critical (Andolfatto and Williamson, 2015, Williamson, 2016, Williamson, 2018a, Williamson, 2018b). If we want to introduce other frictions than sticky prices, and include details of central bank assets and liabilities and their roles in the economy, it seems unwise to try to build on a conventional NK framework, which was designed with something else in mind.====The tractable model constructed here will permit us to put in and take out particular frictions, so that we can understand where the results come from. In general, the model can include sticky prices, a safe asset shortage reflected in binding collateral constraints, monetary exchange, and explicit open market operations. To keep things simple, production and consumption are carried on at the household level, and when prices change they are determined competitively, not be price-setting firms. Every period, some prices are flexible, while sticky prices remain at their previous-period levels. In markets in which prices are sticky, output is demand-determined, just as in mainstream NK setups.====We start with a cashless model, in the spirit of Woodford (2003) (though without monopolistic competition and Calvo pricing). All goods are purchased with secured credit, and the available collateral in the model is government debt. In this version of the model, there is a natural inflation target, which is zero whether the collateral constraint binds or not. If the collateral constraint binds, the real interest rate is low, and there is a liquidity premium associated with government debt. In the cashless model, the ZLB may bind when the central bank is conducting policy optimally. But, a binding ZLB is always reflected in inflation that is above the natural inflation target – i.e. above the inflation rate that eliminates the relative price distortion caused by sticky prices. Further, future monetary policy is irrelevant for current inflation, so there is no role for forward guidance. That is, all of the effects of policy flow from present to future. Current monetary policy can matter for the future liquidity premium on government debt, which can indirectly constrain future monetary policy at the ZLB and affect future inflation.====In the cashless model, a collateral constraint that is tight on average, and tight enough so that the ZLB binds on a regular basis, implies that, under optimal monetary policy, the inflation rate is above the natural inflation target when the ZLB binds, and below the target when it does not bind. This is the opposite of the consensus policy view about how to manage a binding ZLB constraint.====In the cashless model, a scenario similar to Werning (2012) is considered, where the economy experiences a temporary period with a low real interest rate, followed by an indefinite period with a “normal” real interest rate. That is, the collateral constraint is tight for a specified period of time, and then the supply of safe assets increases permanently, relaxing the collateral constraint. Under this scenario, while the collateral constraint is tight the ZLB constraint binds at the optimum for the central bank, output is low, and inflation is high. Once the quantity of safe assets falls permanently, there is a temporary period when it is optimal for the central bank to keep the nominal interest rate low. But that is because this policy provides the fastest disinflation path to the natural target inflation rate, at the same time relaxing the collateral constraint in the quickest fashion. In the Eggertsson and Woodford (2003) and Werning (2012) analyses, a prolonged period of low nominal interest rates, after the low “natural rate” period has passed, acts to increase welfare during the low-natural-rate period.====The model is then extended to permit retail transactions using currency and open market operations by the central bank to support its interest rate policy. So that we can understand how all the frictions in this setup fit together, we first consider the case with flexible prices. In this case there are two potential sources of inefficiency: a traditional Friedman rule inefficiency according to which high inflation and a positive nominal interest rate make exchange in the cash market inefficient; and a binding collateral constraint, whereby scarce collateral makes exchange inefficient in the cash-and-credit market. If the collateral constraint does not bind, then a Friedman rule is optimal and the nominal interest rate should be zero. However, if the collateral constraint binds under any conditions, it binds when the nominal interest rate is low, and at the optimum the nominal interest rate should be greater than zero. That is, an open market sale of government bonds by the central bank at the ZLB raises the nominal interest rate and relaxes the collateral constraint. This raises welfare. The results for the cashless model are just the opposite. The nominal interest rate should be greater than zero when the collateral constraint does not bind, and the ZLB is optimal when the collateral constraint is sufficiently tight.====But what happens in the case in which we include sticky prices, retail payments using currency, and secured credit? Then, there are potentially three inefficiencies at work: a sticky price inefficiency that distorts the relative price of flexible-price and sticky-price goods, a Friedman-rule inefficiency which causes inefficiency in the market for goods purchased with currency, and a safe asset scarcity, which causes inefficiency in the market for goods purchased with secured credit, and generally constrains the demand for goods.====In this full-blown model, if the collateral constraint does not bind, then the nominal interest rate should be positive, and optimal policy trades off the costs of Friedman rule inefficiency and sticky price inefficiency. Optimal inflation falls somewhere between Friedman-rule deflation and zero inflation. In a region of the parameter space, a tighter collateral constraint and a low real interest rate implies that the optimal nominal interest rate increases as the collateral constraint gets tighter. That is, sticky prices are insufficient to induce an optimal ZLB policy when the collateral constraint is tight. For the ZLB to be optimal, it is necessary, but not sufficient, that the demand for sticky-price goods purchased with credit be highly interest-elastic. Like the cashless version of our model, this version has the implication that forward guidance in monetary policy is irrelevant.====So, the conclusions here differ markedly from the consensus view of the ZLB “problem,” and the results have a Fisherian flavor. A safe asset shortage that causes the real interest rate to be low implies that inflation tends to be high – perhaps higher than desirable. If monetary factors are deemed to be irrelevant for the problem, this implies an extended period with low nominal interest rates, and this serves to bring inflation down quickly. Once we take monetary factors into account, the ZLB may be suboptimal when the collateral constraint is tight. In contrast to arguments in Woodford (2003), the cashless economy behaves differently – in important ways – from the economy with currency transactions and open market operations.====In terms of the theoretical approach to monetary exchange and monetary policy, this paper is closest to Andolfatto and Williamson (2015), and shares an approach to safe asset scarcity with Williamson, 2016, Williamson, 2018a, Williamson, 2018b. The modeling of sticky prices is new here, and certainly different from standard New Keynesian frameworks, e.g. Woodford (2003) and Gali (2015).====The remainder of the paper is organized as follows. The baseline cashless model is constructed and analyzed in the second section. Then, in the third section, an extended model that includes retail exchange with currency, along with open market operations, is developed and analyzed. The final section is a conclusion.",Low real interest rates and the zero lower bound,https://www.sciencedirect.com/science/article/pii/S1094202518300176,January 2019,2019,Research Article,277.0
"Boucher Vincent,Goussé Marion","Department of Economics, Université Laval, CRREP and CREATE, Canada,Department of Economics, Université Laval and CRREP, Canada","Received 19 January 2018, Revised 30 November 2018, Available online 6 December 2018, Version of Record 12 December 2018.",https://doi.org/10.1016/j.red.2018.12.001,Cited by (5),"We present a flexible model of wage dynamics where information about job openings is transmitted through social networks. We show that the individuals' wages dynamic is positively associated across time and that this result holds outside the stationary distribution, and under observed and unobserved heterogeneity. We present an empirical application using the British Household Panel Survey by exploiting direct information about individual's social networks. We find that having more employed friends leads to more job offers, but to slightly lower offered wages. We also find that non-relative friends are more helpful than relatives, and that individuals benefit relatively more from their male friends.","Many workforce characteristics (such as wages) are determined outside formal market structures. For example, it is estimated that between 18% and 45% of jobs are found using personal contacts (Pellizzari, 2010; Topa, 2011). A significant portion of wage inequality between different groups, and the persistence of this inequality, may be due to differences in the composition of social networks (Ioannides and Soetevent, 2006; Fontaine, 2008). Understanding non-market forces governing employment and wages has been a preoccupation for economists, going back to Rees (1966), Granovetter, 1973, Granovetter, 1983 and Montgomery, 1991, Montgomery, 1992.====In this paper, we present a flexible structural framework for analyzing how non-market institutions (e.g. peer referrals) affect wages. In particular, we study how labour market transitions are affected by an individual's friendship network. We show that having more ==== friends leads to more job offers (particularly for women), but lower offered wages. We explore the heterogeneity of peer influence and find that non-relative friends are more helpful than relatives and that both men and women benefit more from their male (employed) friends than from their female (employed) friends.====A particular feature of our approach is that it allows for a systematic dependence between the individuals' wages. For example, the positive impact of an individual's peers on his wage is likely to grow with the proportion of his peers who are employed. This may result from the fact that employed individuals have better information on the state of the labour market, or from the fact that unemployed individuals may be more reluctant to share private information about jobs.====We build on the important contribution of Calvó-Armengol and Jackson, 2004, Calvó-Armengol and Jackson, 2007 by extending their model to include observed heterogeneity (e.g. gender) and unobserved heterogeneity (i.e. random-effect model). Besides, we show that a natural extension of their results holds outside the stationary distribution. This is empirically important since periods of interest often include short-term events such as recessions. In other words, we do not need to assume that the data is generated from the stationary distribution.====We find that the individuals' wages dynamic is ==== This implies that, conditional on the observables, the wages of any two individuals are positively correlated, across any point in time. We also show that, as time passes, this dependence is strict for any two socially connected individuals, and that the speed at which this dependence spreads can be expressed as a function of the social network. This allows us to describe the impact of a shock to an individual's wage on the overall distribution of wages, at any point in time.====We restrict our analysis to a time-invariant network. Although our theoretical model abstracts away from strategic network formation considerations, such as in Calvó-Armengol (2004) and Galeotti and Merlino (2014), our empirical analysis controls for the endogeneity of the network structure as in Qu and Lee (2015) and Hsieh and Lee (2014). Coherently with the literature (see Boucher and Fortin (2016)), we find little difference between the model allowing for an endogenous network and the model assuming that the network is exogenous.====Importantly, our theoretical framework allows to describe labour market transitions across potentially long periods of time. Wages are therefore correlated across individuals and time, allowing us to explore a rich variety of channels through which peer effects operate. Specifically, within a single coherent framework, we can separately identify the impact of having employed friends on the probability of receiving job offers, on the wage of such offers, as well as on the separation rate.====We present an empirical application using data from the BHPS from 2000 to 2006. We build on our theoretical framework and develop a non-linear dynamic panel model. An important feature of our model is that an individual's wage is not only dependent on his position in the network, but also on the employment status of the other individuals in the network. We model the dependence on the initial state using the Conditional Maximum Likelihood (CML) methodology of Wooldridge (2005) with correlated random effects.====We find that the number of employed friends an individual has at time ==== has a positive impact on the probability of receiving a job offer at time ====. We find that this effect is much stronger for women. Moreover, for women, having more employed friends tends to lower the separation rate. These results are in line with findings of stronger peer-effects for women (Dieye and Fortin, 2014; Neumark and Postlewaite, 1998). We also find that, conditional on receiving a job offer, having more employed friends leads to slightly lower offered wages for both men and women.====We interpret the negative effect of the number of employed friends on the distribution of job offers as evidence of mismatch. Although we do not identify the offers transmitted through the network from those received directly, we believe this to be the most credible explanation for our findings. Conditional on receiving job offers, and on the individuals current wage, individuals with more employed friends receive less attractive job offers (and are therefore less likely to accept them).====We also explore the heterogeneity of peer influence. We find that recent (employed) friends have a stronger impact on the probability of receiving a job offer. We also find a higher mismatch for (employed) female friends and relatives.====We now discuss our contributions in light of the literature.",Wage dynamics and peer referrals,https://www.sciencedirect.com/science/article/pii/S1094202518300115,January 2019,2019,Research Article,278.0
Olovsson Conny,"Sveriges Riksbank, SE-103 37 Stockholm, Sweden","Received 22 January 2018, Revised 5 November 2018, Available online 26 November 2018, Version of Record 28 November 2018.",https://doi.org/10.1016/j.red.2018.11.003,Cited by (6),"This paper analyzes the interaction between oil prices and ==== outcomes by incorporating oil as an input in production alongside a precautionary motive for holding oil in a ==== model. The driving forces are factor-specific technology shocks, oil supply shocks, and news shocks about future oil supply. Storage and the ==== on stored oil are crucial for the model to match observed business-cycle statistics, the relationship between oil price changes and recessions, and for generating state-dependent responses to shocks. Large oil-price increases are mainly driven by increasing precautionary/smoothing demand for oil. Most of the time, oil-related shocks are of limited importance for the business cycle, but when oil inventories are low, negative news about the future oil supply can drive the economy into a recession that is triggered by oil scarcity.",None,Oil prices in a general equilibrium model with precautionary demand for oil,https://www.sciencedirect.com/science/article/pii/S1094202518300218,April 2019,2019,Research Article,279.0
"Herkenhoff Kyle F.,Ohanian Lee E.","University of Minnesota, United States of America,UCLA & Federal Reserve Bank of Minneapolis, United States of America","Received 23 May 2017, Revised 30 October 2018, Available online 22 November 2018, Version of Record 27 December 2018.",https://doi.org/10.1016/j.red.2018.11.002,Cited by (11),This paper studies the impact of ,"There are two unique and synergistic features of the 2007–2009 recession and the subsequent recovery that made mortgage default an important option for unemployed homeowners. One is that the average time required to initiate and complete a home foreclosure in the United States rose from 9 months to 15 months. This increase in the time to foreclose reflects a combination of government policies designed to reduce and slow foreclosures, as well as congestion delays due to the enormous increase in the number of delinquent properties being processed by courts and lenders. Policies include both state and national foreclosure moratoria, as well as the National Mortgage Settlement, which is also known as the “robo-signing” settlement, and which permanently slowed foreclosures.==== We call this increase in time to foreclose ====.====The second feature is that the opportunity for unemployed homeowners to obtain cash-out home refinancing, which allows a homeowner to take out equity (cash) from their home, virtually stopped during the Great Recession. The sharp reduction in cash-out refinancing reflects both home price declines that substantially reduced equity, as well as regulatory changes in the mortgage market. This change in cash-out refinancing is important because it had been an important way for unemployed homeowners to smooth consumption. Hurst and Stafford (2004) describe how the unemployed were able to access home equity, drawing down on average $16,000 of equity to smooth consumption.====This paper analyzes the impact of foreclosure delay, in conjunction with the plunge in cash-out refinancing, on the U.S. labor market. We focus on how this delay has qualitatively and quantitatively affected the labor market decisions of unemployed mortgagors. By defaulting, homeowners open an unsolicited and risky implicit credit line with the servicing bank. This credit line has a stochastic and limited duration that crucially depends on the foreclosure timeline. Specifically, the credit line opens with a default and is extinguished with a foreclosure or a cure, in which borrowers keep their house by making up for missed payments. During the delinquency period, foreclosure delay provides credit that permits consumption smoothing and allows the mortgagor to search for a good job match. When foreclosure is imminent and the line of credit is expected to expire, the mortgagor has a strong incentive to find a job immediately. These economic forces are similar to those when unemployment benefits expire.====Two sets of statistics, which to our knowledge are new to the literature, motivate this study. The first is that the unemployment rate of persistently delinquent mortgagors is high, but this rate decreases considerably when the mortgagors are in foreclosure and thus are at risk of losing their homes. The second is that many of the mortgagors who persistently miss mortgage payments ultimately resolve their delinquency before foreclosure is completed. This resolution is accomplished by the mortgagor resolving previous missed payments and also paying late fees. This process of successfully exiting delinquency/foreclosure is known as curing. Although curing has been discussed in the literature (see Adelino et al., 2009), neither the frequency of curing, the transitions out of delinquency to curing, nor the macroeconomic implications of curing have been studied, to our knowledge.====We develop a search model of the labor market that includes homeownership, mortgages, and mortgage default, along with the possibility of curing. We use this model to assess the impact of foreclosure delay on the level of employment among mortgagors since the Great Recession. Unemployed households choose their search intensity, which affects the likelihood of receiving a job offer that pays a stochastic wage. The unemployed also choose a reservation wage. Additionally, households choose whether to default on their mortgage payment. By defaulting on their mortgage payment, mortgagors open an implicit credit line that has a limited duration that depends on the time it takes to complete foreclosure. The credit line is closed either with mortgagor curing or with a completed foreclosure. Foreclosure delay thus provides unemployed mortgagors with additional time to search for a high-paying job. As foreclosure becomes more likely, mortgagors' choices regarding search intensity and their reservation wage change as they try to cure and avoid eviction.====Foreclosure delay is somewhat similar to the impact of unemployment insurance. Both mechanisms allow unemployed mortgagors to smooth consumption while searching for a good match. Unemployment benefits and mortgage foreclosure delay have two key differences, however. One is that the delinquent mortgage payments ultimately must be resolved if the mortgagors are to stay in their homes. The second is that the incentives to find a job near foreclosure for a mortgagor may be stronger than in the case of the expiration of unemployment benefits because of the implications of foreclosure for (i) the cost of leaving the home and living elsewhere, (ii) the decline in credit access, and (iii) the potential deficiency judgment against a foreclosed mortgagor.====We conduct a quantitative experiment with three economies that are identical except for having different foreclosure timelines. Each economy experiences the analogue of the Great Recession with exogenously lower productivity, a higher rate of job destruction, and a lower rate of job creation. One economy has a 9-month timeline to foreclosure, which is the average prior to the Great Recession, another has a 15-month timeline, which is the U.S. average during and after the Great Recession, and another has a 24-month timeline, which is about the average of the 10 longest foreclosure timeline states.====Our main finding is that the increase in foreclosure time had a significant impact on mortgagor employment rates. Specifically, we find that the employment rate for mortgagors is about 1.3 percent points lower in the 24-month foreclosure economy and is about 0.75 percent points lower in the 15-month foreclosure economy compared with the 9-month economy. This impact of foreclosure delay on employment is roughly comparable to extending unemployment benefits by 6 months and 4 months, respectively. Foreclosure delays increase the stock of delinquent mortgagors by a factor of 2 but also allow more mortgagors to remain in their homes, as homeownership rates rise. Match quality improves in the delay economies as the average wage rises, reflecting the fact that unemployed mortgagors remain unemployed longer but on average find higher-paying jobs.====The remainder of the paper is organized as follows: Section 2 describes the details of the foreclosure process, Section 3 discusses key facts that are used to inform the model's mechanisms, Section 4 describes the model, Section 5 describes the calibration and steady state results, Section 6 includes the main foreclosure delay experiment, and Section 7 concludes.",The impact of foreclosure delay on U.S. employment,https://www.sciencedirect.com/science/article/pii/S1094202518302710,January 2019,2019,Research Article,280.0
"Alvarez Fernando,Lippi Francesco,Robatto Roberto","University of Chicago, United States,LUISS University and Einaudi Institute for Economic and Finance (EIEF), Italy,University of Wisconsin-Madison, United States","Received 24 February 2017, Revised 31 October 2018, Available online 8 November 2018, Version of Record 7 March 2019.",https://doi.org/10.1016/j.red.2018.11.001,Cited by (3), for money-in-the-utility function and for shopping-time models.,None,Cost of inflation in inventory theoretical models,https://www.sciencedirect.com/science/article/pii/S109420251830262X,April 2019,2019,Research Article,281.0
"Domínguez Begoña,Gomis-Porqueras Pedro","School of Economics, The University of Queensland, Colin Clark Building (39), St Lucia, Brisbane, Qld 4072, Australia,Deakin University, Department of Economics, Geelong, Australia","Received 19 February 2018, Revised 13 October 2018, Available online 25 October 2018, Version of Record 7 March 2019.",https://doi.org/10.1016/j.red.2018.10.004,Cited by (11),"We analyze how trading in secondary markets for public debt changes the inherent links between monetary and fiscal policy, by studying both ","When markets are complete and agents are homogeneous, rational and face lump sum taxes and no liquidity constraints, Barro (1974) shows that Ricardian equivalence holds. In such environments, the channels through which fiscal policy influences inflation are rather limited. This paper explores these channels when markets are incomplete and in the context of the Great Moderation. During this period, we witnessed several financial innovations that increased liquidity and reduced market incompleteness. Among such developments, we analyze the role of secondary markets for public debt in changing monetary and fiscal policy interactions. According to Power (1996), from 1986 to 1993, the U.S. trading volume in secondary markets for sovereign debt increased from $7 to $273 billion. These markets not only provide liquidity to households, they also change the inherent links between monetary and fiscal policies. This is the case as prices of the primary issuance of public debt also reflect the value associated with the potential future trading of these assets in secondary markets. Here we explore how trading in these markets changes monetary and fiscal policy interactions.====To do so, we consider simple monetary and fiscal policy rules in a frictional, stochastic and incomplete market framework with secondary markets for public debt. We find that inflation and bond dynamics crucially depend on whether agents participate in secondary markets or not. When there is no trade in these markets, we show that there exists a unique monetary steady state, where public debt does not affect inflation dynamics. However, when there is trade and bonds are scarce, public debt exhibits a liquidity premium. Agents are willing to buy additional bonds to increase their consumption possibilities in frictional goods markets. As a result, Ricardian equivalence breaks down. By issuing less bonds, the government can affect the premium and reduce the inflation rate. Thus, traditional active/passive monetary and fiscal policy prescriptions do not always deliver locally determinate equilibria in our environment.====When agents trade in secondary markets and bonds are scarce, the government is able to affect the real return on public debt through changes in the inflation rate as well as the issuance of public debt. As a result, there are different combinations of inflation and real public debt that satisfy the government budget constraint. Thus, self-fulfilling beliefs, that are consistent with the existence of multiple steady states, are possible. Regardless of how many steady states exist, we find that traditional active monetary policies decrease the steady state inflation, while passive monetary policies increase it. This novel result has not been emphasized by the previous literature and is, moreover, independent of how the premium is generated. In addition, we show that a modified Taylor principle, where interest rates respond more than two-to-one to changes in inflation, can rule out multiplicity of steady states.====In our numerical exercise, calibrated to the Pre Great Moderation period, we find that regardless of the fiscal policy stance, active monetary policies are more likely to deliver a unique monetary steady state. Whenever the steady state is unique, a passive monetary policy yields locally indeterminate equilibria regardless of the fiscal stance. Moreover, active monetary policies tend to deliver determinate equilibria. In contrast, passive monetary policy can lead to multiple steady states, one being stable and the other one unstable. These findings critically depend on the long-run inflation target. When it is high, two steady states may exist even under active monetary policies. However, when the central bank follows an active policy and has a low inflation target, then these policies are likely to deliver a unique and stable monetary equilibrium regardless of the fiscal stance. This result suggests that active monetary polices can be used as an equilibrium selection device. Lastly, we find that trading in secondary markets tends to reduce the stabilizing effect of monetary policy and, depending on the stance of monetary policy, they strengthen or weaken the stabilizing effect of fiscal policy.====The paper is organized as follows. Section 2 offers a literature review. Section 3 illustrates the mechanism by presenting a simple cashless model with an ad hoc bond premium. Section 4 describes the environment with an endogenous liquidity premium and characterizes the monetary equilibria. Section 5 presents the monetary equilibrium. In Section 6 we perform a numerical analysis. A conclusion then follows.",The effects of secondary markets for government bonds on inflation dynamics,https://www.sciencedirect.com/science/article/pii/S1094202518300644,April 2019,2019,Research Article,282.0
"Kudoh Noritaka,Miyamoto Hiroaki,Sasaki Masaru","Nagoya University, Japan,International Monetary Fund, United States of America,Osaka University, Japan","Received 9 May 2017, Revised 16 October 2018, Available online 25 October 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.10.002,Cited by (15),"This paper studies a large-firm search-matching model with variable hours of work to investigate how firms utilize the intensive and extensive margins of labor adjustment over the business cycle. The model replicates the observed ==== of the Japanese labor market, in which fluctuations in hours of work account for 79 percent of the variations in total labor input, well. Introduction of variable hours of work introduces the Frisch elasticity parameter into the analysis, and this is a key determinant of the magnitude of fluctuations in hours of work.","Firms adjust their labor inputs over the business cycle through the intensive margin (hours of work per employee) and the extensive margin (the number of employees). In their survey, Hall et al. (2000) found that 62 percent of firms consider overtime as the primary reaction to a demand boom. In a perfectly competitive labor market, firms do not need to utilize overtime because they can employ extra workers instantly at the going wage rate. This suggests the importance of ==== in understanding how firms utilize the intensive and extensive margins over the business cycle. However, little attention has been paid to the ==== of labor demand in frictional labor market models.====This paper studies the composition of labor demand over the business cycle in a search-matching model. To this end, we develop a large-firm search-matching model with the intensive margin. The large-firm search-matching model with intra-firm bargaining is now a standard framework for studying the labor market.==== However, its business cycle properties have not been fully explored in the literature.==== Our contribution is to explore this relatively uncharted area with special attention given to the intensive margin. Our model captures the fact that firms need to engage in time-consuming search-matching process when hiring new employees while changes in hours of work per employee are instantaneous.====A novel property of our model is that firms facing productivity shocks choose ==== vacancies and hours of work per employee. A caveat is that with variable hours of work, the hourly wage rate may depend on the level of hours of work. To allow for the possibility of nonlinear compensation scheme, we assume that each firm and its employees bargain over a state-contingent contract in the form of an earnings schedule which maps hours of work per employee into earnings (Cooper et al., 2007, Kudoh and Sasaki, 2011). The equilibrium earnings schedule turns out to be a convex function of hours of work, and this specifies the marginal hourly wage rate for the firm of choosing hours of work.====Our model replicates the observed cyclical behavior of the Japanese labor market well. In Japan, the intensive margin accounts for a large proportion of cyclical fluctuations in total labor input. Our empirical analysis reveals that the intensive margin accounts for 79 percent of the variations in total labor input, while the extensive margin accounts for 21 percent of the variations.==== Clearly, in understanding labor market fluctuations in Japan, it is a serious omission if one uses a model without the intensive margin of labor adjustment. Our model replicates much of the observed fluctuations in total labor input, hours of work per employee, employment, unemployment, and vacancies. In particular, variations in hours of work per employee account for 84 percent of the variations in the aggregate labor input, while variations in the number of employees account for 19 percent of the variations.====We find that the intensive margin magnifies fluctuations in total labor input and improves the model's ability to replicate the data. Specifically, in our basic model, total labor input fluctuates 25 times as much as that in the model ==== hours of work. Introduction of variable hours of work introduces the Frisch elasticity parameter into the analysis, and this is a key determinant of the magnitude of fluctuations in hours of work. In sharp contrast with the standard real business cycle model, our model can generate a realistic magnitude of hours fluctuations with Frisch elasticity much less than one.====Introduction of the intensive margin also magnifies labor market fluctuations along the ==== and helps resolve the unemployment volatility puzzle (Shimer, 2005). Our model's ability to generate realistic magnitudes of fluctuations in unemployment and vacancies comes partly from disutility from hours of work because it plays the same role as having a high unemployment benefit, generating a small surplus for the firm. This is essentially the mechanism often pointed out in the literature (Hagedorn and Manovskii, 2008). With variable hours of work, our model generates a small surplus for the firm under a set of plausible parameters. Through this channel, the Frisch elasticity also influences the magnitudes of fluctuations in unemployment and vacancies.====Although our model replicates the observed cyclical behavior of the Japanese labor market fairly well, the magnitude of fluctuations in employment is about a half of that in the data. This could partly be explained by the absence of the other extensive margin, namely, job creation by entrants. To assess the importance of this additional margin, we extend our model to include stochastic firm entry. We find that this extended model captures empirical observations strikingly well. While the textbook search-matching model makes no distinction between jobs created by incumbents and entrants, our large-firm model clearly distinguishes between them (Pissarides, 2000). Our exercise suggests the importance of exploring this line of research.====We intend our empirical analysis to be a contribution to the growing literature on re-examination of hours of work using new models and new datasets. This includes Rogerson (2006) and Ohanian and Raffo (2012), to name a few. Particularly relevant is Ohanian and Raffo (2012), who find that the importance of the intensive margin for labor adjustment has increased over time in 14 OECD countries they studied. Using the dataset we built for Japan, our empirical analysis confirms the importance of the intensive margin.====In terms of the structure of the model, particularly related to our study are Cooper et al. (2007) and Kudoh and Sasaki (2011), in which determination of hours of work is considered in the context of a search-matching model with large firms.==== Cooper et al. (2007) study both employment and hours of work over the business cycle using a model similar to ours. They emphasize the importance of nonlinear costs of posting vacancies, and as a result, wage determination is simplified by assuming a take-it-or-leave-it offer protocol. While we restrict our analysis to a linear vacancy cost, we employ a more general bargaining problem.====Our model is also closely related to the model developed by Fang and Rogerson (2009), who study the intensive and extensive margins using the framework of Merz (1995) and Andolfatto (1996). In this model, the production unit is a matched worker-job pair, and this rules out the issue of intra-firm bargaining.==== In addition, the Merz-Andolfatto paradigm has a utility-maximizing household, who optimally chooses the level of consumption. Fang and Rogerson (2009) show that, with a concave utility function, there is a rich interaction between employment and hours through consumption, and as a result, an increase in the vacancy cost decreases employment and increases hours of work. This rich interaction comes from the labor-supply side. In contrast, we focus on the labor demand and especially on its composition over the business cycle.====The remainder of the paper is organized as follows. In Section 2, we empirically examine how labor inputs are adjusted over the business cycle in Japan, and present some other cyclical characteristics of the labor market in Japan. Section 3 describes our basic model, followed by characterization of the model's equilibrium in Section 4. In Section 5, we calibrate the model parameters and present the business cycle properties of our model. Section 6 explores firm entry over the business cycle and Section 7 concludes. Proofs, additional results, and the log-linearized equilibrium conditions for the model with extra components are found in the Appendix.",Employment and hours over the business cycle in a model with search frictions,https://www.sciencedirect.com/science/article/pii/S1094202518305544,January 2019,2019,Research Article,283.0
"Di Pace Federico,Hertweck Matthias S.","Bank of England, Threadneedle Street, London EC2R 8AH, United Kingdom,Deutsche Bundesbank, Postfach 10 06 02, 60006 Frankfurt am Main, Germany","Received 13 March 2017, Revised 7 October 2018, Available online 25 October 2018, Version of Record 7 March 2019.",https://doi.org/10.1016/j.red.2018.10.003,Cited by (6),) labor search and matching frictions and ,"As demonstrated by Barsky et al. (2007), the two-sector New Keynesian model with flexible durable good prices and sticky non-durable good prices (consistent with Bils and Klenow 2004 and Klenow and Kryvtsov 2008) fails to generate sectoral comovement after a monetary contraction. Given that durable good prices fall steeply – but their shadow value is near-constant – the representative household has incentives to build up the stock of durable goods at low cost. With Walrasian labor markets, this scenario is indeed an equilibrium outcome – as the drop in non-durable consumption strongly reduces wages such that durable good producers are willing to expand production (Carlstrom and Fuerst, 2010). The opposing responses of sectoral outputs almost offset each other, which implies that monetary policy shocks are close to neutral for aggregate output. This result is clearly at odds with the observation that a monetary contraction causes an economy-wide downturn. Beyond that, the model suffers from a lack of internal propagation: the model-generated responses fall sharply on impact – whereas their empirical counterparts reach a trough with a delay of several quarters.====This paper argues that the labor market is key to understanding the “sectoral comovement puzzle”. We therefore extend the Barsky et al. (2007) model by adding labor search and matching frictions (Pissarides, 2000). This modification changes the monetary transmission mechanism in two ways. First, search and matching frictions introduce an extensive margin of labor adjustment, which breaks the near-constancy relationship. Hence, real marginal costs become less elastic and, consequently, durable good prices fall less sharply.==== In equilibrium, the limited response of durable good prices leads to a fall in new durable good purchases. We show that the strength of this channel depends on the value of the replacement rate. Second, with long-run employment relationships, shocks are propagated through changes in the stocks of sectoral employment (as in Hairault, 2002). This effect is reinforced by a modified hiring cost function whose response is much less elastic and more short-lived relative to the response under standard vacancy posting costs. For this reason, a larger fraction of a monetary policy shock is now absorbed by the associated quantity (employment), which also translates into long-lived responses in output.====In addition to search and matching frictions, we introduce standard DSGE features such as internal habit formation in non-durable consumption (Constantinides, 1990; Fuhrer, 2000). As a result, the durable goods sector absorbs a larger fraction of a monetary contraction. In this way, habit formation helps to appropriately distribute the impact of a given shock over the two sectors. Precisely for this reason, our model is able to generate sectoral comovement for ==== values of the replacement rate.====The main contribution of our paper is to show that the combination of two standard DSGE features – namely, labor search and matching frictions and internal habit formation – suffices not only to generate comovement across sectors, but also to closely replicate the amplitude and the curvature of the empirical impulse responses in both sectors. Among the growing literature on this topic, only a few exceptions were able to generate long-lived and u-shaped responses in sectoral outputs. The two studies most closely related to ours are DiCecio (2009) and Carlstrom and Fuerst (2010), who introduce sticky wages to dampen the responsiveness of real marginal costs in both sectors.==== However, for a standard calibration of sticky wages, the response in durable output would be too strong and too short-lived. For this reason, these studies introduce adjustment costs in investment and durable output, respectively, to “mechanically” stretch out the negative response over several years. By contrast, we show that modeling the extensive margin of labor adjustment explicitly is key to understanding the sectoral comovement puzzle.====Other supply-side approaches also require limiting the response of real marginal costs in the durable goods sector. Bouakez et al. (2011), Sudo (2012), Petrella and Santoro (2011) and Petrella et al. (2018) introduce an input–output structure with inter-sectoral linkages to establish a direct link between (sticky) non-durable good prices and real marginal costs in the durable goods sector. Tsai (2016) shows that a rise in the nominal interest rate increases the borrowing cost of working capital, which in turn dampens the fall in real marginal costs in both sectors. Kitamura and Takamura (2016) emphasize the role of sticky information, which slows down price adjustments in both sectors – thus mitigating the strong asymmetry in price flexibility. On the other hand, demand-side approaches either assess the role of alternative preferences (Levin and Yun, 2011; Katayama and Kim, 2013; Dey and Tsai, 2017) or question households' ability to use durable goods as an investment device to smooth their consumption profile, e.g. by introducing collateral constraints (Iacoviello, 2005; Monacelli, 2009; Iacoviello and Neri, 2010; Sterk, 2010; Chen and Liao, 2014).====We perform a number of checks to test the sensitivity of our results. We first examine the role of the key features of our baseline model (the replacement rate, the degree of habit formation, the price indexation parameter, and the hiring cost specification) in replicating the empirical regularities. We then estimate the model parameters using data from the post-Volcker period. We also present a model version where labor markets are sector-specific, i.e. workers are immobile across sectors. This setting allows us to study the impact of steady-state wage differentials across sectors. Finally, we assess the role of physical capital and endogenous job separations in the sectoral comovement puzzle.====The remainder of this paper is organized as follows. Section 2 presents a two-sector New Keynesian model with search frictions and habit formation. Section 3 discusses the estimation strategy. Section 4 presents the results. Section 5 inspects the model mechanism. Section 6 evaluates alternative model specifications. Section 7 concludes.","Labor market frictions, monetary policy and durable goods",https://www.sciencedirect.com/science/article/pii/S1094202518302667,April 2019,2019,Research Article,284.0
Biolsi Christopher,"Department of Economics, Western Kentucky University, Bowling Green, KY 42101, USA","Received 29 March 2018, Revised 17 October 2018, Available online 25 October 2018, Version of Record 7 March 2019.",https://doi.org/10.1016/j.red.2018.10.005,Cited by (3),"Using novel county-level data on shipbuilding contracts awarded during the Great Depression by the federal government, I estimate a local government spending multiplier. Manufacturing output, value added, employment, and average earnings all rise significantly in counties receiving naval spending. Contracts worth 12 percent of lagged output generate an extra 1.9 percentage points of manufacturing output growth over the following two years. The effects grow over time, and ==== are estimated to be positive. Household survey data suggests that consumption rises at the household level.","What are the effects of government purchases on local economies, especially when the aggregate economy is in a state of weakness?==== Normally, these effects are summarized in terms of a “multiplier,” defined as the amount of extra output generated by an additional dollar of government purchases. This paper exploits cross-sectional variation in federal spending during the Great Depression (a period of extreme economic slack, with unemployment rates rising above 25 percent) to calculate a relative government spending multiplier. Specifically, I examine a previously understudied naval expansion during the 1930s that was motivated by geopolitical concerns about the increasing aggression of the Japanese Imperial Navy. Bringing historical county-level data on manufacturing to bear on the analysis in combination with novel data on naval procurement contracts awarded during this period, this paper addresses something of a gap in the literature by analyzing the local area effects of a direct increase in federal purchases that took place during a time of severe economic weakness. Previous papers, such as Suárez-Serrato et al. (2016) and Chodorow-Reich et al. (2012), focus more on federal policies that increase transfers, either to individuals or to subnational governments. Other papers, such as Clemens and Miran (2012) and Zou (2016), consider the effects of cuts to government spending on local economies. Studies that do explicitly analyze increases in purchases, like Nakamura and Steinsson (2014), Hooker and Knetter (1997), Dupor (2015), and Dupor and Guerrero (2017) do not singularly focus on modern episodes when unemployment rates rise well above their natural rates or output falls well below potential.====The main result of this paper is that a U.S. county receiving naval procurement contracts valued at around 12 percent of pre-recession manufacturing output grew about 1.9 percentage points faster over a two year period. The cumulative manufacturing output multiplier on a dollar of naval procurement is around 0.77 at a six-year horizon, while the value added multiplier is around 0.20. These results are robust to controlling for spending associated with the New Deal and to controlling for possibly endogenous contract allocations. Spillovers are estimated to be positive and significant. In addition, a household living near the areas experiencing increased federal government demand for shipbuilding spends between 8 and 12 percent more on consumption than a household less exposed to the spending shock.====Up until fairly recently, the conventional approach to estimating government spending multipliers in the literature has relied on aggregate time series data. See, for example, Ramey (2011), Barro and Redlick (2011), and Auerbach and Gorodnichenko (2012), among many others. The most obvious merit to this approach is that, conditional on being confident in the identification of the shock to government spending and the statistical power of the econometric technique, the resulting estimated government spending multiplier is easily interpreted as the additional national economic output that is caused by an extra dollar of government spending, a statistic that federal policy makers may have the most interest in. This approach does, however, have a number of drawbacks, among these being that identification of shocks is not straightforward (although the narrative series of defense shocks introduced by Ramey and Shapiro, 1998 and extended by Ramey, 2011 and Ramey and Zubairy, 2018 is arguably the most compelling introduced so far). Still, even conditional on precise identification of a shock, identification of its effects is confounded by endogenous monetary policy or tax policy responses. In addition, especially if a researcher is relying on post-war U.S. data, the relatively small number of observations, especially during downturns, reduces statistical power, and there is little consensus as to how best to set up the econometric specification.====Estimation of the government spending multiplier at the local level helps to circumvent these difficulties. Because the naval expansion was a result of concerns about Japanese actions in the Asia-Pacific, it is a plausible assumption that local economic conditions did not drive the issuance of naval contracts. The use of time fixed effects allows me to difference out monetary policy responses and national tax policy, and, because this study examines outcomes at a very low level of geographic aggregation (the county), there is a larger number of observations with which to estimate the effects of the government policy. Unfortunately, the multiplier estimated here is not “the” government spending multiplier, but rather what Nakamura and Steinsson (2014) term the “Open Economy Relative Multiplier,” which describes the differential output response in a county receiving an extra dollar of federal spending compared with one that does not. Beraja et al. (2016) demonstrate that local elasticities with respect to a given shock need not necessarily be informative of the elasticity for the economy as a whole. In particular, spillovers from spending can be positive or negative, and the response of monetary authorities, though washed out in the cross-section to allow identification of relative effects across regions, is not washed out in the aggregate. Thus, mapping this local multiplier to the aggregate multiplier requires some reliance on theory, such as that provided by Nakamura and Steinsson (2014), Chodorow-Reich (2017), or Dupor et al. (2018), but such an analysis is beyond the scope of this paper.====The rest of the paper proceeds as follows. Section 2 discusses the empirical methodology and data, devoting special attention to describing the historical episode that provides the setting for the analysis, and especially the contracts data that I employ, which is, to the best of my knowledge, a novel contribution to the literature. Specification of the regression equations and results follow in Section 3 (for county-level analysis) and Section 4 (household-level analysis). Section 5 concludes.",Local effects of a military spending shock: Evidence from shipbuilding in the 1930s,https://www.sciencedirect.com/science/article/pii/S1094202518301406,April 2019,2019,Research Article,285.0
Lalé Etienne,"Université du Québec à Montréal, Canada,CIRANO, Canada,IZA, Germany","Received 10 August 2016, Revised 2 October 2018, Available online 19 October 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.10.001,Cited by (5),We analyze the effects of government-mandated ,"The effects of government-mandated severance payments on equilibrium allocations and welfare is a topic of keen interest in macro and labor economics.==== Two approaches dominate the literature. In the first one, bargaining between firms and workers plays a key role. The insight is that, in tune with Lazear, 1988, Lazear, 1990's bonding critique, the outcomes of the bargaining process determine whether or not severance payments have any effect on equilibrium allocations.==== Most papers in this vein of the literature, however, assume risk-neutral preferences, making it hard to rely on welfare considerations to explain why severance payments should be introduced in the first place. In the other approach, agents are risk averse and severance payments have non-trivial welfare implications. A strand of the literature following this approach, and which we discuss below, proceeds using incomplete market models. Meanwhile, due to the complexity of this class of models, this research typically rules out worker-firm bargaining. As a result, not much is known about the equilibrium and welfare effects of severance payments when workers care about consumption smoothing and bargain with firms.====This paper contributes to filling this gap in the literature. We revisit the classic question of how severance payments affect equilibrium labor market allocations and workers' welfare, using a model that combines the strengths of the two approaches described above.====We bring together mainly three ingredients to study the effects of severance payments. First, we use the search-matching model with incomplete markets developed by Krusell et al. (2010), in which we introduce endogenous job separations as in Bils et al. (2011). Our motivation for including this feature is that one of the purposes of severance payments is to improve job security by deterring firms from laying off workers. Second, we consider two-tier employment relationships between firms and workers along the lines of Mortensen and Pissarides (1999)'s textbook model. So doing, we aim to capture the fact that severance payments affect new hires and incumbent workers differently. Third, given the strong age components of worker productivity, job separations and asset holding decisions, and their importance for understanding the effects of severance payments, we cast the model in a life-cycle setting. Our framework therefore also captures the implications of a finite working life-time on labor market trajectories (Chéron et al., 2011, Chéron et al., 2013). We use data moments and policies for the United States (U.S.) to inform the model and proceed with a quantitative analysis.====The main results of the paper are as follows. We emphasize that severance payments lead to a steepening of the wage profile, as the wages paid to newly-hired workers are discounted to (partially) offset future severance payments. This outcome is a reflection of Lazear, 1988, Lazear, 1990's bonding critique, although severance payments are not fully neutralized in our quantitative exercise due to the sources of incompleteness embedded in the model. The key observation is that this tilted wage profile runs counter to having a smooth consumption path. Consequently, we find that severance payments produce mostly negative welfare effects for workers. In the baseline experiment, introducing the severance pay rates that prevail in Southern European countries reduces welfare in a way that is equivalent to reducing consumption at every stage of the life cycle by more than 1 percent. We also tabulate that no less than 40 percent and up to 80 percent of the figure is driven by the wage-shifting response of severance payments. Although these figures include wage changes that are coming from general equilibrium responses of the economy, they give a sense of the importance of the wage-shifting effect highlighted by our model. In particular, faced with high government-mandated severance payments, firms open up fewer vacant jobs.==== Employment deteriorates because severance payments reduce its inflows (the probability to find a job), while at the same time they have little impact on employment outflows (the probability to lose a job).====We provide several analyses to complement these results. We show that life-cycle factors – namely, the hump-shaped profile of worker productivity and the higher risk of job separation among young workers – must be accounted for in order to capture the full welfare cost of severance payments. We also show that pension benefits mitigate the welfare cost somewhat, while a tighter borrowing limit turns out to have little impact on the results. In these variants of the model, the wage-shifting effects remain the key mechanism to understand the welfare consequences of severance payments.","Labor-market frictions, incomplete insurance and severance payments",https://www.sciencedirect.com/science/article/pii/S1094202518305428,January 2019,2019,Research Article,286.0
"Mele Antonio,Stefanski Radoslaw","University of Surrey, United Kingdom,University of St Andrews and University of Oxford (OxCarre), United Kingdom","Received 28 July 2016, Revised 20 August 2018, Available online 1 October 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.09.004,Cited by (3),"Monetary velocity declines as economies grow. We demonstrate that this is due to the process of structural transformation – the shift of workers from agricultural to non-agricultural production associated with rising income. A calibrated, two-sector model of structural transformation with monetary and non-monetary trade accurately generates the long run monetary velocity of the US between 1869 and 2013 as well as the velocity of a panel of 102 countries between 1980 and 2010. Three lessons arise from our analysis: 1) Developments in ","How does a country's long-run money demand change with its economic development? An extensive literature==== finds that for broad enough measures of the money stock and over long periods of time, increases in income per capita tend to be associated with increases in the money-to-GDP ratio (or, equivalently, a falling monetary velocity).==== The possible sources of this stylized fact have been widely debated and include institutional changes, financial innovations, improvements in communication and information-gathering technologies as well as changes in the composition of output.==== Perhaps surprisingly, this research has been almost entirely empirical in nature==== which has made it challenging to quantify the role played by individual channels due to associated issues with endogeneity and causality (Bosworth and Collins, 2003).====In this paper we quantify the role of a single mechanism driving the income-velocity relationship: ====. This process, also known as industrialization, is the systematic change in the composition of an economy's employment and output, from agriculture towards non-agriculture, associated with economic growth. Although structural transformation is certainly known to influence money demand (e.g. Jonung (1983), Friedman (1959), Chandavarkar (1977)), no theoretical models of the process exist, and the quantitative importance of this channel is unclear. Rather than taking a purely empirical or accounting approach, we construct and calibrate a multi-sector model of long-run monetary demand and compare our model's predictions to those of a standard one-sector model of money demand. This allows us to isolate and quantify the role of structural transformation on the money-share whilst avoiding most of the issues of endogeneity and causality usually encountered in the empirical work.====Our model is motivated by two facts found in the data. First, agriculture – especially traditional agriculture – tends to be largely non-monetary due to the dominance of compensation in kind, home production and barter. Non-agriculture on the other hand, is more likely to require money to enable exchange due to the greater variety of goods within that sector (Chandavarkar, 1977). Second, as an economy grows, the relative size of the agricultural sector (in terms of employment and output) tends to shrink whilst that of the non-agricultural sector tends to rise. Together, these empirical facts suggest that the changing composition of economies associated with growth will contribute to a rising demand for money, a rising money-to-GDP ratio and consequently a falling velocity.====To capture these regularities, we construct a model with two sectors: agriculture, which we assume produces goods traded purely without money, and non-agriculture, in which an endogenous share of goods is exchanged using money.==== The demand for money is introduced through a cash-in-advance constraint on non-agricultural consumption goods following Cole and Kocherlakota (1998). Structural transformation is generated by introducing non-homothetic preferences: consumers are assumed to have a subsistence level of agricultural consumption.==== As agricultural productivity increases, fewer workers are needed to satisfy subsistence consumption, prompting workers to move into the non-agricultural sector and thus increasing that sector's share in total employment and output. Given that a part of non-agricultural goods are traded with money, the shift in the composition of the economy towards the non-agricultural sector will result in the model predicting an increase in monetary transactions, an increase in the money-to-GDP ratio and hence in lower velocity.====By contrast, in a standard one-sector model, money demand is only affected by the nominal interest rate. Such a model is thus unable to reproduce the trend in money-share and exhibits a stable velocity with respect to income. We compare our multi-sector model – which includes both the interest-rate money-demand mechanism as well as the additional compositional money-demand mechanism – with a standard one-sector model – which only contains the interest-rate money-demand mechanism. This comparison allows us 1) to disentangle the effects of structural change on money shares from other potential drivers, such as different monetary policies (i.e. different nominal interest rates), different productivity levels and growth rates, and differences in capital accumulation and 2) to quantify how much of the trend in the money share is explicitly explained by structural change. Finally, since we explicitly model the sources of structural transformation and money-demand, our model-based approach avoids endogeneity and causality pitfalls associated with purely empirical attempts to isolate the role of structural transformation in driving money demand.====We calibrate our multi-sector model to the 1869–2007 patterns of US growth, structural transformation, and monetary supply. This simple framework successfully replicates several features of the long run data including agricultural labor shares, GDP per worker, sectoral prices, nominal interest rates, and aggregate inflation. Most importantly, the model reproduces the evolution of the US long run money-to-GDP ratio over 140 years, capturing 75.2% of the increase in the data. A traditional, similarly calibrated one-sector model fails to replicate observed money-share, as it is unable to match the observed price dynamics – and in particular the so-called ‘Great-Deflation’ of the late 19th century. Our model also accurately predicts the variability in money-shares ==== countries. Keeping preference parameters of the US, we recalibrate the model to a panel of 102 countries between 1980 and 2010. The model accurately captures cross-country differences in incomes, employment shares, interest rates, and inflation rates. The baseline model captures 83% of the increase in money-share between the top and bottom deciles of cross-country data and does exceptionally well in replicating the income-velocity relationship,==== whereas a one-sector model fails to capture any part of the relationship.====Finally, we examine how the costs of suboptimal monetary policies vary with income. Inflation is more costly in richer countries than in poorer countries – where the monetary part of the economy is smaller and therefore distortions from the inflation tax are less damaging. For example, a hyperinflation of approximately 400% a year in a poor country like Zimbabwe (where GDP per worker is 2% of that of the US) will have negligible welfare costs. By contrast, the same hyperinflation in a country like Argentina (30% of US GDP per worker) will have very large welfare costs. Argentinean incomes would have to rise by approximately 29% to deliver the same expected flow utility as without the hyperinflation. The low cost of inflation in poorer countries may help explain why they tend to have much higher inflation rates than richer countries.====There are three lessons from our work. First, monetary velocity is not constant over the development process but falls with income. Most existing explanations of this observation are rooted in the non-agricultural sector – and focus on factors such as financial innovation or the expansion of the banking sector. The surprising finding of this paper is that the evolution of monetary velocity is driven largely by developments in the ==== sector. It is the variation in agricultural productivity that influences the size of the non-agricultural sector and in turn influences monetary demand and hence velocity. Second, the cost of inefficient monetary policy varies with income and is higher in richer countries than in poorer countries. An inflation tax offers a relatively cheap source of income to poor-country governments, and its distortive effects are relatively small in economies that are dominated by large, non-monetary, agricultural sectors. This may help explain why we observe persistently higher inflation in poorer countries than in richer countries – despite recommendations of strong anti-inflationary policies by international financial institutions such as the International Monetary Fund (IMF).==== Finally, the third lesson is that, since velocity depends systematically on the composition of output, a country's price levels and inflation rates may not ‘always and everywhere be a monetary phenomenon’, as suggested for example by the work of Friedman and Schwartz (1963). The price level in an economy, ====, is defined as ==== where ==== is the money stock, ==== is output and ==== is velocity. Two countries, with identical money stocks and identical output ==== – but with different output ==== – will have entirely different velocities, ====, and hence different price levels. The message to researchers from these findings is that a one-sector model cannot be successfully used to understand the long run dynamics of monetary velocity and hence the evolution of price levels or inflation rates. These findings should also be of interest to policymakers in developing countries who may have overlooked the importance of the agricultural sector in their monetary policy decisions.====In the next section we document the main facts regarding structural transformation, monetary velocity, and the extent of non-monetary production in agriculture. In section 3, we construct and solve our simple baseline model. In section 4 we calibrate the model to the experience of the United States and in section 5 we show the results for the US. Section 6 carries out the cross-country analysis by examining how the model performs in international, cross-country data and by running a number of counterfactuals to quantify the importance of the different mechanisms of the model. Section 7 performs a number of robustness checks and extensions. Section 8 examines the different costs of inflation in rich and poor countries. Finally, section 9 offers some concluding remarks on the importance of our findings to researchers and policy makers.",Velocity in the long run: Money and structural transformation,https://www.sciencedirect.com/science/article/pii/S1094202518302485,January 2019,2019,Research Article,287.0
"Eggertsson Gauti B.,Garga Vaishali","Department of Economics, Brown University, United States of America","Received 27 September 2017, Revised 13 September 2018, Available online 25 September 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.09.003,Cited by (7),". Similarly, well known paradoxes, e.g., the paradox of toil and the paradox of flexibility become more severe under sticky information. For the case of sticky information it is important to assume that the fiscal policy intervention coincides with the duration of zero ====, while such a distinction is less important for ====. We unify and clarify results that may appear to contradict each other in the literature.","A number of papers have documented that fiscal policy is extremely effective to increase demand at the ZLB. In particular the classic government spending multiplier is greater than one, while under normal circumstances it is not, for then monetary policy can do the job via interest rate cuts. Examples include Eggertsson (2011), Christiano et al. (2009), Woodford (2011). The literature has also uncovered peculiar paradoxes, such as the paradox of toil (that is, distortionary labor tax cuts are contractionary, see Eggertsson, 2010) and the paradox of flexibility (that is, for a given demand shock, greater price flexibility is more contractionary, see Bhattarai et al., 2014 for an overview of this literature and some general results). These results can be interpreted either as a serious challenge to the conventional wisdom or reflect some fundamental flaws of the New Keynesian framework. These results are, however, derived under the assumption that prices are sticky, as in Calvo (1983).====It has long been recognized that the Calvo model of price setting has many peculiar features. This led researchers to explore alternatives, such as information frictions. One of the most prominent proposal to replace the New Keynesian Phillips curve based upon Calvo prices is the assumption of sticky information, proposed by Mankiw amd Reis (2001). According to their hypothesis, firms adjust their prices slowly because they do not continuously update their information set. Mankiw and Reis argued that this alternative assumption helps explain the data better along certain dimensions. A very natural question, in the light of the radical findings documented in the Calvo model at the ZLB, is if these results carry over to a setting where information rigidities are assumed instead of sticky prices. The main conclusion of this paper is that under a strict inflation targeting rule, the answer is yes.====In an important and intriguing recent paper, Kiley (2016), documents experiments in which the fiscal policy results of the Calvo model are overturned upon assuming informational frictions as in Mankiw and Reis. The thought experiment Kiley conducts is as follows: Suppose the Central Bank follows an interest rate peg for 100 periods. What happens if the government increases spending for ==== upto 25 periods? What happens if taxes are increased for 1 to 25 periods? Kiley documents that while under Calvo prices the predictions are in line with the existing literature at the ZLB, the predictions are different under informational frictions. In particular, the government spending multiplier is small, the tax multiplier changes sign and the paradox of flexibility disappears. Kiley's experiment is referred to in this paper as an interest rate peg experiment (PEG-EX). Kiley interprets these findings as suggesting that the sticky-information model is free from policy paradoxes, and thus to be favored over the Calvo model. Here, instead, we argue that these findings are an artifact of the thought experiment considered. The paradoxes in the sticky information framework in fact get even stronger in policy experiments that correspond more closely to those considered in the existing literature.====This paper compares sticky prices and sticky information doing a different experiment from the PEG-EX. This experiment is identical to the one conducted in Eggertsson (2011), Christiano et al. (2009), Eggertsson (2010), Woodford (2011). The ZLB is binding due to exogenous fundamental shocks. Once the shocks are over, the policy is given by a strict inflation target (which is missed for the duration of the shocks, due to the ZLB). This experiment is referred to as ZLB experiment (ZLB-EX). The paper documents that the results derived in the literature under sticky prices in the ZLB-EX are even more extreme if sticky prices are replaced with sticky information, which is the opposite of Kiley's result. The government spending multiplier becomes larger, and the paradox of toil and flexibility become more pronounced.====While this may seem to contradict Kiley's findings, it does not. Instead it clarifies that Kiley's PEG-EX is a fundamentally different experiment than done in the existing literature. What is particularly subtle – and interesting – about the comparison, and likely to trigger confusion, is that under sticky prices the ZLB-EX and PEG-EX lead to exactly the same result. It is only when assuming sticky information that the results of the ZLB-EX and the PEG-EX are different. This does not have anything to do with the nature of the nominal frictions. Instead, it is a consequence of the fact that the sticky-information model has infinite number of endogenous state variables. Meanwhile the Calvo model is purely forward looking. The presence of endogenous state variables in the sticky-information model implies that comparing the reaction of an economy assuming an exogenous interest rate peg, versus the reaction of the economy if the central bank's interest rate policy is bounded by zero due to fundamental shocks and fiscal policy is in direct response to this constraint, leads to very different results. The same does not apply for perfectly forward looking systems like the Calvo model of price stickiness.====This paper first shows analytical examples that clarify the intuition behind these findings. It then moves to numerical examples that replicate Kiley's results. These examples confirm that Kiley's results are driven by the difference in experiments being conducted rather than anything fundamental about the assumption of price stickiness. In a calibration of the two models chosen produce 10% drop in output and 2% annual deflation on impact, government spending is nearly thrice as expansionary and tax cuts are six times as contractionary under SI than SP in the ZLB-EX with a strict inflation target.====Finally, this paper shows that the distinction between PEG-EX and ZLB-EX is important even under non-strict inflation targeting regimes, as the two remain fundamentally different experiments, and one cannot be treated as a sub-case of the other. Under a general targeting rule, where the central bank stabilizes inflation as well as output deviations from target, the fiscal multipliers in the SI model under ZLB-EX depend on the relative weight on output stabilization (the larger the weight on output stabilization, the smaller the multipliers), while those under PEG-EX are independent of the relative weight on output stabilization.====Arguably, the ZLB-EX is more economically relevant than PEG-EX. It seems of more limited economic interest – at least in the context of the crisis that started in 2008 – to explore the behavior of New Keynesian models if the short-term interest rate is temporarily pegged for no apparent reasons. Instead, the most economically interesting experiment appears to be when the interest rate is pegged due to the fact that the ZLB is binding on account of a fundamental recessionary shock that prevents the central bank from achieving its objective of stabilizing inflation and output.====As a final note, let us observe that the reason fiscal policy has such a large effect in our experiments, and also at heart of the policy paradoxes, is that monetary policy is set in a sub-optimal way. For example, if monetary policy is conducted as in Eggertsson and Woodford (2003), output and inflation are almost entirely stabilized due to history dependent monetary policy (i.e. committing to lower future real interest rate). This, then, leaves much less room for fiscal policy to have a large effect on output.",Sticky prices versus sticky information: Does it matter for policy paradoxes?,https://www.sciencedirect.com/science/article/pii/S1094202518302886,January 2019,2019,Research Article,288.0
Saijo Hikaru,"University of California, Santa Cruz, United States of America","Received 1 July 2017, Revised 21 May 2018, Available online 13 September 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.09.002,Cited by (1),"I exploit heterogeneous impulse responses at the household level due to limited stock market participation to provide novel evidence on the degree of nominal rigidities. A number of studies show that positive technology shocks reduce aggregate hours. The finding is often interpreted as evidence in favor of ====. Using the Consumer Expenditure Survey, I show that, while non-stockholders reduce hours in response to a positive technology shock, stockholders increase them. Aggregate hours fall because most households are non-stockholders. This finding is inconsistent with models featuring a high degree of nominal rigidities.","One of the fundamental questions in macroeconomics is to what extent nominal rigidities matter for the transmission of aggregate shocks. For example, the extent to which the central bank can influence the real interest rate and hence economic activity depends on how prices adjust to changes in the nominal rate. Similarly, government spending is likely to have greater effects in stimulating aggregate demand when prices and wages are rigid. In this paper, I exploit heterogeneous impulse responses at the household level due to limited stock market participation and provide novel evidence on the degree of nominal rigidities.====In an influential paper, Galí (1999) finds that positive technology shocks, identified from a long-run restriction in a structural vector auto-regression (VAR), reduce aggregate hours worked. Many researchers view the finding as important because it provides evidence against standard real business cycle (RBC) models in favor of New Keynesian models with weak monetary accommodation to technology shocks. However, it is also well known that if the income effect of labor supply is stronger than the substitution effect, then hours could fall in response to positive technology shocks even in RBC models.==== That both New Keynesian and RBC models can explain the empirical finding by Galí (1999) suggests that macro data do not provide enough restrictions to differentiate among competing theories that differ in their degree of nominal rigidities.====This leads me to exploit cross-sectional heterogeneities at the household level. Intertemporal substitution of labor supply through capital accumulation is central to modern business cycle theories. For example, in an RBC model, households increase their labor supply in response to a permanent increase in technology in order to reap the benefit of a higher return on investment. In principle, the strength of this intertemporal substitution effect critically depends on households' stock market participation status and hence, differences in conditional movements of hours worked to technology shocks may arise among households that participate in the stock market and those that do not. Because most U.S. households do not participate in the stock market,==== conclusions based on aggregate hours could thus be misleading. Indeed, when limited stock market participation is taken into account, aggregate hours could fall in response to an improvement in technology in both flexible-price and sticky-price models. However, household-level data are useful in discriminating among alternative models even when aggregate data are not. In flexible-price models, stockholders increase their labor supply because of the standard intertemporal substitution effect, while non-stockholders reduce their labor supply due to the income effect. In contrast, both stockholders and non-stockholders reduce their labor supply in sticky-price models.====To empirically determine the impact of limited stock market participation, I use micro data from the Consumer Expenditure Survey (CEX) to estimate the responses of hours worked by stockholders and non-stockholders to a technology shock identified by a long-run restriction as in Galí (1999). I find that, in response to a positive technology shock, stockholders increase their hours but non-stockholders reduce them. Since most households are non-stockholders, hours worked decline in the aggregate. Thus, the micro data is consistent with the hypothesis that, due to limited stock market participation, the degree of intertemporal substitution of labor supply varies across households, and that the aggregate data masks that heterogeneity. The empirical finding is robust to an alternative specification of using the utilization-adjusted total-factor productivity (TFP) series constructed in Fernald (2014), which controls for heterogeneity across different types of inputs and variations in factor utilization such as labor effort and the workweek of capital.====Importantly, I show that this heterogeneity in impulse responses does not arise among households who hold other assets such as bonds or savings and those who do not. The holding statuses of other assets do not matter because interest rates on these assets do not move following a technology shock. In contrast, the return on stocks increases by 5 annual percentage points in response to a technology improvement. In addition, splits based on the amount of total wealth do not generate the heterogeneous impulse responses I find from the classification based on the households' stock-holding status. The evidence thus supports the idea that stockholders increase their hours in order to reap the benefit of the higher return on investment.====In the second half of the paper, I interpret my findings through the lens of a parsimonious, two-agent dynamic stochastic general equilibrium (DSGE) model with limited stock market participation based on Galí et al. (2007) and Bilbiie (2008). The framework is attractive because it nests a standard representative-agent DSGE model widely used in applied research as a special case. The model features nominal rigidities and thus also nests basic RBC and New Keynesian models as special cases. As in Altig et al. (2011), Dupor et al. (2009), and Liu and Phaneuf (2007), the structural parameters are estimated by matching the empirical impulse responses to technology shocks with the model counterpart. The key difference is that, in addition to standard macro variables, I also match the labor supply responses at the household level estimated from the CEX. I find that the limited stock market participation model is able to replicate the heterogeneous impulse responses and the estimated price and wage rigidities are much smaller than are typically found. In particular, the estimates imply that both firms and households adjust their prices and wages roughly every quarter. In contrast, when the model is estimated under the conventional assumption of full stock market participation using aggregate data only, the estimated frequencies of price and wage adjustments are 4 and 2 quarters, respectively. The exercise thus provides evidence in favor of the transmission mechanism of the limited stock market participation model and underscores its impact on the inference for competing business cycle theories that differ in their degrees of nominal rigidities.====The rest of the paper is organized as follows. After reviewing the relevant literature, in Section 2, which is the main part of this paper, I estimate the household-level labor supply responses to technology shocks using micro data from the CEX. In Section 3, I explore the quantitative implications of my empirical findings by estimating DSGE models with limited stock market participation. Finally, Section 4 concludes with some directions for future research.",Technology shocks and hours revisited: Evidence from household data,https://www.sciencedirect.com/science/article/pii/S109420251830276X,January 2019,2019,Research Article,289.0
Kang Kee-Youn,"School of Business, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, South Korea","Received 29 August 2017, Revised 30 August 2018, Available online 13 September 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.09.001,Cited by (5),"We develop a model of asset exchange and ==== consumption. Banks use ==== and government liabilities as collateral to secure deposit contracts, but they have an incentive to falsify the quality of mortgages at a cost. Quantitative easing in the form of central bank purchases of mortgages from private banks has an effect on the composition of assets in the economy and on the incentive structure of the private sector. When the incentive problem is severe, private banks hold capital with mortgage retention to mitigate the incentive problem, and the central bank can unambiguously improve welfare by purchasing mortgages. However, when this problem is not severe, the central bank's mortgage purchases cause a housing construction boom and can sometimes decrease an exchange in the economy, thereby reducing welfare.","In response to the Great Recession and its aftermath, the Federal Reserve embarked on an unconventional monetary policy in the form of large-scale asset purchases, also known as quantitative easing (QE). Through successive rounds of QE, the Federal Reserve purchased long term government bonds, agency debt, and mortgage backed securities (MBS) over time, dramatically increasing the size of the Federal Reserve's balance sheet.==== These unconventional policy actions have generated a substantial debate in the economics profession about the effects of QE on market interest rates and the real economy. However, the precise mechanism through which QE has affected economic activities is still not well understood.==== The following questions still need to be addressed: How does an unconventional monetary policy affect market interest rates, the incentive structure of the private sector, and the real economy? What is the relationship between a conventional and unconventional monetary policy? Can an unconventional monetary policy substitute a conventional monetary policy? Is there any risk of implementing QE, and under what conditions does it improve or diminish welfare?====Our objective is to construct a New Monetarist model to study the effects of QE in the form of the central bank's purchases of private assets, such as MBS, and to address the above questions.==== For this purpose, we incorporate housing construction and asymmetric information concerning the quality of private financial assets in the Williamson (2012) framework. In the model, there is a fundamental role in exchange using currency and exchange with secured credit in a decentralized market as a result of limited commitment and a lack of record keeping. Then, in equilibrium, financial intermediation is a type of insurance arrangement in which banks efficiently allocate liquid assets in the form of currency and deposit claim on a bank for appropriate transactions.====However, banks are inherently untrustworthy. Limited commitment implies that banks' deposit liabilities must be secured by other financial assets. Primitive assets in the model are government liabilities (currency, reserves, and nominal government bonds) and houses constructed by private agents. Although houses cannot be directly used as collateral by banks, they are indirectly useful in an exchange in the decentralized market. Homeowners can take out mortgages with banks using residential properties as collateral, and banks can then pledge mortgages as collateral to secure their deposit claims.====A key source of friction in the model economy is that the usefulness of mortgages as collateral is limited by the threat of fraud. More precisely, banks can produce fraudulent mortgages and pledge them as collateral at a cost similar to Williamson (2016b). For example, banks can misrepresent sub-prime mortgages as prime mortgages by producing false documentation about the value of mortgages. Certainly, this type of security fraud requires bad assets as raw material and a professional agent who can manipulate the assets' quality in a sophisticated manner. The hiring cost of such an agent and the cost of losing reputation from depositors for fraud are not necessarily proportional to the scale of the fraud. Thus, we introduce a fixed fraud cost in addition to the proportional cost as in Li et al. (2012), which is a key difference from Williamson (2016b) that only considers the proportional cost of cheating. Simultaneously, although we adopt the structure of the incentive problem from Li et al. (2012), we add housing construction and explicitly include the banking sector and central bank's balance sheet as part of the model to answer the questions that we are interested in. By incorporating both the fixed and proportional costs, and endogenizing housing construction, we provide new insights about the central bank's private asset purchase program: In our model economy, the central bank's mortgage purchase can affect the real economy, increasing or decreasing welfare, in contrast to Williamson (2016b), which concludes that the mortgage purchase program has no effect on the economy despite using a similar model framework.====In the model, the cost of fraud and the aggregate quantity of government liabilities – assets safe from fraud – matter for the incentive problem. This is because a scarcity of government liabilities tends to make the mortgage price high through its effects on the liquidity premium on assets, which makes misrepresentation of mortgages as being more profitable. In particular, if an efficient amount of trade can be achieved with sufficient government liabilities, and thus no mortgages (or few mortgages) need to be used as collateral, there is no incentive for misrepresentation. On the other hand, when the cost of fraud and the quantity of government liabilities are sufficiently low, the incentive problem becomes so severe that banks retain some fraction of mortgages in their books, combined with bank capital, to circumvent the incentive problem instead of using them as collateral. This is owing to the fact that the mortgage retention helps banks signal the quality of mortgages on their balance sheet and hence mitigates the informational asymmetries. Williamson (2016b) also derives asset retention when the incentive constraint binds. The difference is that our model admits another equilibrium in which the incentive problem matters, but there is no mortgage retention, and the existence of this type of equilibrium allows us to understand the greater implications for the central bank's private asset purchases.====We first use the model to study the effects of conventional monetary policy that determines a nominal interest rate of short-maturity government bonds on equilibrium quantities, prices, bank capital with asset retention, and welfare. An accommodative policy in the form of lower nominal interest rates on government bonds make collateralizable assets scarce, which increases the demand for mortgages, lowers the mortgage interest rates, and raises demand for houses, and through market clearing, increases housing construction and price in general. Thus, an accommodative policy makes the level of housing construction inefficiently high, although it may improve the exchange process. However, when the incentive constraint binds, but banks post all mortgages in their books as collateral, a conventional monetary policy does not affect the mortgage and housing markets. In this case, a zero nominal interest rate is optimal, at least locally, due to the shut-down of a channel through which a conventional monetary policy affects the housing market.====Next, we examine the effects of QE as an unconventional monetary policy. In the model, the central bank purchases mortgage loans from banks at the market price.==== As private banks can sell fraudulent mortgages to the central bank at a cost, the quantity of mortgages that the central bank can purchase from private banks is limited by the threat of fraud. Furthermore, unlike conventional monetary policy, QE has an effect on the economy only if the bank's incentive constraint binds. As the central bank purchases mortgages in exchange for reserves, fewer mortgages are pledged as collateral. Thus, the incentive of misrepresenting mortgages diminishes, given the existence of the fixed fraud cost. In particular, the central bank can make the incentive constraint slack by purchasing a sufficient quantity of mortgages. However, the effects of the central bank's mortgage purchases on real allocations and welfare depend on whether banks retain some fraction of mortgages in their books or not, which, in turn, hinges on the cost of fraud.====First, if the cost of fraud is below a certain threshold level, banks retain some fraction of mortgages on their book to circumvent the incentive problem. Under QE, the central bank purchases these illiquid mortgages in exchange for reserves relaxing the binding incentive constraint, and the quantity of trading in the decentralized market increases as a result. Because the central bank purchases illiquid mortgages, mortgage purchases do not directly affect the mortgage price at the margin. Instead, a liquidity premium on the mortgage price falls as the quantity of collateralizable assets increases, which reduces the housing demand and construction. However, in this economy, whenever the housing price exhibits a liquidity premium, the level of housing construction is inefficiently high. Thus, a decrease of housing construction contributes to the improvement of welfare, and in this equilibrium, the central bank's mortgage purchases unambiguously improve welfare.====Second, when the cost of fraud is above the threshold level, banks pledge all the mortgages as collateral, although the incentive constraint binds. In this case, the real rate of return on reserves is lower than the real rate of return on mortgages to make fraud unprofitable, although both the assets contribute equally to exchanges in a decentralized market at the margin. Thus, the mortgage purchase can absorb more collateralizable assets, which pushes down the quantity of exchanges. However, the central bank's mortgage purchases always relax the binding incentive constraint, which pushes up the quantity of exchanges. In addition, an increase in mortgage purchases boosts housing construction and the mortgage supply, contributing to an increase in trade. In terms of net, it is not clear whether trades in the decentralized market increase or decrease with the implementation of the mortgage purchase program. The effects on welfare are also ambiguous. The central bank's mortgage purchases can lower welfare even when the quantity of exchanges increases in the decentralized market because mortgage purchases cause excessive investment in housing construction. Under certain circumstances, it is optimal for the central bank to not implement QE, even though the incentive problem in the banking sector prevents efficient financial intermediation and the mortgage purchase program can eliminate the friction.====  A number of papers have analyzed an unconventional monetary policy known as QE. Cúrdia and Woodford (2011), Gertler and Karadi, 2011, Gertler and Karadi, 2013, Gertler et al. (2012), and Del Negro et al. (2017) quantitatively studied the effects of QE by extending a standard New Keynesian model to include financial frictions. In their model, the central bank directly invests in private assets under “QE” that looks more like a credit market intervention by the fiscal authority. Boel and Waller (2015) and Williamson, 2016b, Williamson, 2016a studied QE as asset purchases by the central bank similar to the Fed's policy intervention in the United States. In their model, QE has an effect on the economy by changing the composition of exchangeable assets in the private sector. These previous studies focused on the channel under which QE spurs economic activity without consideration of its potential risk and welfare costs.==== Gu and Haslag (2014) constructed an overlapping generations model and examined the effects of the central bank's private debt purchase when the verifiability of private debt and a timing mismatch in debt settlements lead to a liquidity problem. They were interested in related issues, but approached the problem in a different way.====Our study is also related to research on the housing market and financial frictions. He et al. (2015) and Branch et al. (2016) studied the implications of financial innovation that improves households' ability to borrow against their home equity. He et al. (2015) studied how continuing financial innovation can generate a housing price boom and bust, and showed that the changing nominal interest rate has non-monotone effects on housing prices. Branch et al. (2016) extended the work of He et al. (2015) to study the effects of financial innovation on unemployment and sectoral reallocations in labor markets, in addition to its impact on the housing market. Our model differs from these studies; in our study, there is no friction when households borrow from banks against their houses. Instead, friction exists when banks pledge mortgages as collateral. Furthermore, while these studies used an exogenous pledgeability as a key parameter of financial friction, we explicitly specify the informational friction that prevents an efficient financial intermediation, which allows us to study how an (un)conventional monetary policy affects the degree of friction itself.====The fact that banks pledge mortgages that are secured by houses as collateral to secure their deposit claims is echoed in related literature on rehypothecation, such as Muley (2016), Andolfatto et al. (2017), Gottardi et al. (2017), Maurin (2017), and Park and Kahn (2018). Muley (2016) and Andolfatto et al. (2017) studied how monetary policy interacts with the practice of rehypothecation. Gottardi et al. (2017) showed that rehypothecation increases borrowing in the economy and can explain intermediation whereby creditworthy investors borrow on behalf of riskier counterparties. Maurin (2017) and Park and Kahn (2018) studied the trade-off of rehypothecation between providing more liquidity to the economy and the re-use risk that the lender fails to return collateral assets. Unlike these papers, our model is closer to pyramiding – using a collateralized debt contract itself as a collateral –, and, thus, there is no re-use risk of collateral. Instead, we focus on the informational asymmetry of the value of mortgages when banks use them as collateral, and study the effects of (un)conventional monetary policy on this informational friction.====The rest of the paper is organized as follows. Section 2 presents the environment of the model, section 3 solves economic agents' problems, and section 4 characterizes the equilibrium. In section 5, we study the effects of monetary policy. Section 6 is the conclusion. Proofs are relegated to Appendix A.",Central Bank purchases of private assets: An evaluation,https://www.sciencedirect.com/science/article/pii/S1094202518302850,January 2019,2019,Research Article,290.0
"Bonfiglioli Alessandra,Gancia Gino","Queen Mary University of London, Mile End Road, E1 4NS London, UK","Received 25 November 2016, Revised 23 July 2018, Available online 31 August 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.08.002,Cited by (5),"We propose a model in which differences in socioeconomic and labor market outcomes between ex-ante identical countries can be generated as multiple equilibria sustained by different beliefs on the value of effort for finding jobs. To do so, we study the incentive to improve ability in a model where heterogeneous firms and workers interact in a labor market characterized by matching frictions and costly screening. When effort in improving ability raises both the mean and the variance of the resulting ability distribution, a complementarity between workers' choices and firms' hiring strategies can give rise to multiple equilibria. In the high-effort equilibrium, heterogeneity in ability is larger and induces firms to screen more intensively workers, thereby confirming the belief that effort is important for finding good jobs. In the low-effort equilibrium, ability is less dispersed and firms screen less intensively, which confirms the belief that effort is not so important. The model has novel implications for wage inequality, the distribution of firm characteristics, productivity, sorting patterns between firms and workers, and unemployment rates that can help explain observed differences across countries.","Countries at similar stages of development differ markedly in a number of socioeconomic indicators. For instance, wage inequality, labor productivity, school attainment and employment rates are all higher in the United States than in Southern Europe. The population of active firms differs too, with a relatively larger number of small and less productive firms in the latter group of countries. While understanding these differences is important both from a positive and a normative standpoint, their origin remains largely an open question. One strand of literature attributes them to distortions, but typically does not explain how they arose in the first place.==== Another strand of literature emphasizes the role of cultural values.==== Yet, the mechanisms through which values and beliefs translate into different economic outcomes, even in places that started with similar conditions, are still poorly understood.====The objective of this paper is to show that significant differences in socioeconomic and labor market outcomes can emerge as alternative equilibria sustained by different, and yet rational, beliefs on the role played by ability and effort in determining individual economic success. We will argue that the mechanism we identify has implications for wage inequality, the distribution of firm characteristics, sorting patterns between firms and workers, and unemployment rates that can help to explain the cross-country variation observed in the data.====To this end, we study the incentives to invest in ability in a model where heterogeneous firms and workers interact in a labor market with matching frictions. Ability is unobservable, but firms can use a screening technology to select the best workers. As in Helpman et al. (2010), the combination of these features yields realistic distributions of firms and wages. We then allow workers to invest costly effort to improve their ability under the realistic assumption that effort and exogenous talent are complementary.==== The latter feature implies that exerting effort increases average ability, but also its dispersion in the population, and introduces a novel complementarity between firms' and workers' strategies.==== On the one hand, the returns to screening are higher when ability is more dispersed, i.e., when effort is high. On the other hand, investing effort pays out more when firms screen workers more intensively.====The main result of the paper is to show that this complementarity can give rise to two equilibria. In the high-effort equilibrium, heterogeneity in ability is higher and this induces firms to be more selective when hiring workers. In turn, this makes ability, and hence effort, more important for finding good jobs, thereby confirming the initial belief. In the low-effort equilibrium, instead, since ability is less dispersed, firms screen less intensively and hence the probability of finding jobs depends more on luck rather than merit, which confirms the initial belief on the low value of effort. Relative to the alternative scenario, in the high-effort equilibrium ability is higher and more dispersed, firms are more productive, and a stronger sorting pattern between firms and workers generates more inequality among firms. Wage inequality is also typically higher. Our aim is to show that this mechanism can replicate several salient differences observed between countries such as the United States, Italy and Spain.====First, regarding perceptions, the existing evidence suggests that Americans believe in individual merit, work ethic and competition more than Southern Europeans. For instance, according to the 1981–2000 World Values Survey, 26.4% of Americans strongly agree with the statement that “hard work brings success”, against a share of 14.6% in Italy and 12.2% in Spain. Those who instead strongly believe that success “is a matter of luck and connections” represent 2.3%, 8.9% and 7.8% of respondents in the three countries, respectively. Similarly, 43.3% of Americans think that “hard work is an important quality that a child should learn”, against 26.8% in Italy. More broadly, 29.6% of Americans strongly believe that “competition is good”, as opposed to 19.2% of Italians and 15.6% of Spaniards.====Second, these beliefs come together with significant differences in investment in education. Available data on the quality and quantity of schooling indicate that Americans attach a higher value to education than people from Southern Europe. For instance, in 2010 the working-age population with tertiary schooling was 41% in the United States against 15% in Italy and 32% in Spain (OECD, 2013).==== Investment in education, both private and total, is also higher in the United States. For instance, total expenditure on tertiary education as a percentage of GDP is 2.8%, 1% and 1.3% in the three countries respectively. Regarding outcomes, U.S. students outperform those from Italy and Spain in all major international comparisons, but also exhibit more dispersion in the results. For example, the standard deviation of IALS test scores is about 22% higher in the United Sates than in Italy (Cebreros, 2014).==== Finally, the United States also score higher than Souther European countries in reported measures of discipline at school, which may be a proxy for effort (OECD, 2010a). However, effort in acquiring human capital is notoriously difficult to observe. For instance, Hamermesh and Donald (2008) show that college GPAs have small and mostly non-significant effects on earnings. The high value attached to education in the United States may also be reflected in the fierce competition for admission and the high tuition fees of top schools. Yet, the quality of the long tail of non-top institutions is hard to assess for employers without investing resources.====Third, the differential value attached to education and effort is also reflected in measures of wage inequality and other labor market outcomes. In particular, the college premium relative to the earnings of workers with secondary education is higher than 1.7 in the United States against 1.5 in Italy and 1.4 in Spain (OECD, 2013). Broader measures of wage inequality display similar patterns. For instance, the variance of the logarithm of hourly wages in 2006 is 0.38 in the United States against 0.17 in the other two countries.==== Even after controlling for workers' characteristics, the variance of the logarithms of residual wages is 0.26 in the United States and 0.12 in Italy.==== Unemployment is also lower in the United States, especially for skilled workers. For example, the unemployment rate of U.S. college graduate is about half of that of the total workforce, while in Italy it is about 70% of the mean (OECD, 2013). As a result, the different composition of the workforce alone contributes to generate a significantly lower unemployment rate in the former country.====Fourth, there are also large cross-country differences in firm-level outcomes. Available data suggest U.S. firms to be on average bigger and more productive, and their size distribution to be more dispersed than their European counterparts. For example, the standard deviation of log sales among manufacturing firms is 0.66 in the United States, 0.53 in Italy and 0.49 in Spain.==== Interestingly, there is also evidence that American markets are more selective: for example, the survival rate for new firms is about 10% lower in the United States than in Italy (Bartelsman et al., 2009). Regarding the covariance between size and productivity, Bartelsman et al. (2013) find that, within the typical U.S. manufacturing industry, labor productivity is almost 50% higher than it would be if employment was allocated randomly and that this measure of allocative efficiency is much lower on average in European countries. Finally, U.S. labor productivity, measured as GDP per hour worked in 2006, is 21% higher than in Italy and 38% higher than in Spain (OECD Data).====Fifth, existing data suggest that American firms value selecting talent more. From their survey on managerial practices around the world, Bloom and Van Reenen (2010) build a synthetic measure of how strongly firms value selection, based on the answers to questions on the importance of attracting and keeping talented people to the company. In their sample of 17 countries, U.S. firms have the highest average score, while Italian firms have the lowest one.==== Moreover, consistently with our hypothesis on differences in hiring strategies, only 13% of U.S. workers claim to have found their job through personal contacts against 25.5% in Italy and 45% in Spain (Pellizzari, 2010).====To our knowledge, our theory is the first to be able to match all these observations without referring to exogenous differences in preferences and/or institutions. Despite this being a remarkable result, it is important to stress that we do not believe the multiplicity of equilibria identified in this paper to be the only or even the most important source of these socioeconomic differences. Rather, our theory illustrates a simple and yet powerful mechanism through which large differences in economic outcomes can arise even when countries have access to the same technologies and share similar market and political institutions. The success at replicating some of the salient differences between the two sides of the Atlantic makes us more confident that the model is capturing real-world phenomena. In particular, we present numerical exercises suggesting that multiple equilibria can account for a significant part of the observed differences between Italy and the United States. Moreover, given that labor markets are often segmented regionally, we believe that our model can be useful for understanding disparities in firm- and labor-market outcomes between regions of the same country, such as the North and South of Italy, which share the same broad institutions and policies.====Our paper is related to several lines of research. First, it contributes to a set of papers that study the role of social beliefs in explaining the main differences in economic performance and inequality observed between the United States, Europe and other countries. Several important contributions show how alternative sets of beliefs can sustain equilibria with high and low levels of inequality. In Benabou (2000), Alesina and Angeletos (2005), Hassler et al. (2005) and Benabou and Tirole (2006), this happens through the endogenous determination of the political support for redistributive policies; in Piketty (1998) through a status motive. In other papers multiple equilibria arise through endogenous preference formation (e.g., Francois and Zabojnik, 2005, Doepke and Zilibotti, 2014). Differently from these works, we focus on a complementarity between workers' effort decisions and the hiring strategies of heterogeneous firms. This approach seems well-suited for our aim of studying especially differences in the distribution of wages, workers and firms.====The paper is also related to the large literature on the role of human capital, broadly defined, for economic development. Several contributions have shown how multiple equilibria and poverty traps can arise in the presence of increasing returns due to human capital externalities (e.g., Azariadis and Drazen, 1990), non-convexities coupled with credit frictions (e.g., Galor and Zeira, 1993), or a complementarity between talent and technological change (e.g., Hassler and Rodriguez Mora, 2000). Differently from these works, technological increasing returns to human capital or credit frictions are not needed in our approach to generate multiple equilibria. Moreover, none of the above mentioned papers examines the interaction between workers and firm heterogeneity.====Closer to our spirit, Acemoglu (1996) and Burdett and Smith (2002) show that human capital externalities may arise naturally when labor markets are characterized by search frictions. Similarly to our model, agents choose human capital depending on their job prospects and firms choose jobs depending on the average human capital of the workforce.==== Differently from our framework, however, these papers abstract from firm heterogeneity and selection through screening. The importance of the allocation of talent is stressed by many papers, including Acemoglu (1995), Hsieh et al. (2013) and Bonfiglioli and Gancia (2014a).==== None of them, however, studies its interplay with the hiring strategies of heterogeneous firms which is at the core of our theory.====Finally, the paper builds on the literature on wage inequality in models with imperfect labor markets and firm heterogeneity. Acemoglu (1997) shows how search frictions ==== Mortensen and Pissarides (1994) can generate and shape wage inequality.==== Lagos (2006) and Marimon and Zilibotti (1999) study how different shocks and policies may affect aggregate outcomes and wage inequality in labor markets with matching frictions. Helpman et al., 2008, Helpman et al., 2010 combine search frictions, firm heterogeneity (as in Melitz, 2003) and worker heterogeneity to study wage dispersion, wage-size premia and unemployment in open and closed economy. Our model builds on these frameworks by adding an endogenous ability distribution and by exploring how the novel equilibrium multiplicity that arises can help explain some of the observed cross-county differences in the distribution of wages, firm characteristics and unemployment rates.====The rest of the paper is organized as follows. In Section 2 we lay down the model and derive the conditions for equilibrium multiplicity. In Section 3 we compare labor market outcomes, firms and welfare across equilibria. In Section 4 we explore the quantitative implications of the model by comparing numerical simulations to data for the United States and Italy. Section 5 concludes.","Heterogeneity, selection and labor market disparities",https://www.sciencedirect.com/science/article/pii/S1094202518302539,January 2019,2019,Research Article,291.0
,"Graduate School and Faculty of Economics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan","Received 17 December 2015, Revised 31 July 2018, Available online 24 August 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.08.001,Cited by (16),"The current U.S. Social Security program redistributes resources from high-wage workers to low-wage workers through its progressive benefits schedule and from two-earner married couples and single workers to one-earner married couples through the program's spousal and survivors benefits. This paper extends a standard general-equilibrium overlapping-generations model with uninsurable wage shocks to analyze the effect of spousal and survivors benefits on the labor supply of married households and the overall economy. The heterogeneous-agent model calibrated to the U.S. economy predicts that removing spousal and survivors benefits would increase the female labor participation rate by 1.5–1.6%, women's total work hours by 1.7–1.8%, and the total output of the economy by 0.5–0.6% in the long run. A phased-in cohort-by-cohort removal of these benefits would make all age cohorts, on average, better off under the balanced-budget assumption, although the policy change would make the majority of young married households worse off in the short run.","The current Old-Age and Survivors Insurance (OASI) program of the U.S. Social Security system redistributes resources from high-wage workers to low-wage workers through its progressive benefits schedule and from two-earner couples and single workers to one-earner couples through the program's spousal and survivors benefits. Yet, to avoid complex models, most previous literature on dynamic general-equilibrium analysis of Social Security does not explicitly consider the redistribution between one-earner households and two-earner households.==== However, excluding these benefits from the model economy could cause the model to underestimate the labor supply distortion of the OASI payroll tax, as the effective payroll tax rate of the secondary earner is higher in an economy that provides spousal and survivors benefits.====This paper extends a standard dynamic general-equilibrium overlapping-generations (OLG) model with uninsurable wage shocks by implementing the joint labor supply decision of married couples. Assuming the spousal and survivors benefits under current law, this paper calibrates the heterogeneous-agent model to the U.S. economy in recent years. Then, the paper analyzes to what extent spousal and survivors benefits distort the labor supply decision of married households and examines whether the government can improve social welfare without significantly reducing the insurance aspect of the current Social Security OASI program.====In the model economy, households are heterogeneous with respect to their marital status, age, wealth, the husband's wage rate, the wife's wage rate, the husband's average historical earnings, and the wife's average historical earnings. In each period, which is a year in the model economy, a working-age household receives idiosyncratic wage shocks—one for the husband and one for the wife if the household includes a married couple—and jointly determines its consumption, work hours, and end-of-period wealth to maximize their rest-of-the-lifetime utility, taking factor prices and the government policy schedule as given.====This paper first constructs a baseline economy, which is on a balanced growth path, with the current OASI program that includes spousal and survivors benefits. Then, the paper assumes that the government removes the spousal and survivors benefits in a cohort-by-cohort phased-in manner, and this paper solves the model for equilibrium transition paths under two alternative financing assumptions of the government. Regarding the Social Security budget, this paper assumes that it is separate from the rest of the government budget and that workers' own benefits (OA benefits) are increased proportionally to balance the budget, keeping the payroll tax rate unchanged. Regarding the rest of the government budget, this paper assumes the government either increases lump-sum transfer payments or reduces marginal income tax rates to balance the budget after the policy change.====The main findings of this paper are as follows: If the spousal and survivors benefits were removed, in the long run, the female labor participation rate would increase by 1.5–1.6%, depending on the government financing assumption, the average work hours of female workers would increase by 0.2–0.3%, and women's total work hours would increase by 1.7–1.8%. Men's total work hours would decrease by 0.3%, however, because of the intrafamilial substitution of work hours. The total labor supply in efficiency units would increase by 0.3–0.4%, the capital stock would increase by 0.8–1.1%, and the gross domestic product (GDP) would increase by 0.5–0.6%. (The macroeconomic effect would be larger if the government cut marginal income tax rates to balance the rest of the government budget rather than increased lump-sum transfers.)====Because of the higher economic activity and the lower distortion from the Social Security benefit schedule, future newborn (age 21) households would be, on average, better off by 0.47–0.51% in the long run with the consumption equivalence measure. (The welfare effect of this policy change would be a little better if the government balanced the rest of its budget by increasing lump-sum transfer payments.) Moreover, the phased-in removal of spousal and survivors benefits would make all age cohorts over the transition path, on average, better off under both financing assumptions, although the policy change would make the majority of young married households worse off in the short run.====The Social Security Old-Age Insurance (OAI) program was created by the Social Security Act of 1935. Under this program, individuals age 65 and older were insured by the program if they had worked at least 5 years in jobs covered by the program and earned at least $2000 in total. Although coverage was linked to the recipients' work history, the program was considered social insurance aimed to lower the poverty rates among elderly individuals during the Great Depression. Then, the 1939 amendments extended the OAI program to provide benefits for dependents of insured workers. The dependents included wives and widows age 65 and older and children younger than age 16 or 18 (if attending school). These spousal and survivors benefits were also implemented as social insurance to protect elderly families from the sudden decrease in family income.====Although the welfare implication of this paper might be surprising at first, spousal and survivors benefits are welfare-reducing for the following reasons: First, these benefits distort the labor supply decision of married households. Spousal and survivors benefits increase the effective payroll tax rate of the secondary earner of a married household, although these benefits decrease that of the primary earner. As, under the current U.S. tax system, the marginal labor income tax rate of the secondary earner is already higher than that of the primary earner and a single household, the total size of the tax distortion could be significantly large. Second, these benefits are regressive; that is, spousal and survivors benefits redistribute wealth from relatively low-income households to high-income households. Households with a high-wage husband and a low-wage wife tend to be one-earner households and benefit most from spousal and survivors benefits, although these households are relatively affluent; whereas households with a low-wage husband and a low-wage wife tend to be two-earner households, and these households are much less likely to benefit from spousal and survivors benefits.====To the best of my knowledge, few studies have used a large-scale dynamic general-equilibrium OLG model to analyze the effect of spousal and survivors benefits on the labor supply of married households, the macroeconomy, and social welfare. Kaygusuz (2015) is probably the first study that constructs a heterogeneous-agent OLG model to explicitly analyze the effect of spousal and survivors benefits. The present paper, however, is different from Kaygusuz's in two main aspects: First, the present paper assumes uninsurable idiosyncratic wage shocks in the model economy. This is important because if a husband and a wife were not certain about their future wage rates and thus, their own old-age benefits, a possible labor supply distortion caused by spousal and survivors benefits would be attenuated, especially when the couple were young.==== Second, the present paper solves the model for an equilibrium transition path to check whether the removal of these benefits are welfare improving even in the short run. To evaluate the welfare effect of a policy change, it is necessary to solve the model for a transition path in addition to a long-run equilibrium. It is well-known, for example, that partial privatization of Social Security pensions would hurt current generations significantly through the transition costs, although future generations would gain from the policy reform in the long run.====Kaygusuz (2010) and Guner et al., 2012b, Guner et al., 2012a construct a deterministic general-equilibrium OLG model with heterogeneous married and single households, similar to Kaygusuz (2015), and these papers instead analyze the effect of the U.S. income tax system on the female labor supply. Hong and Ríos-Rull (2007) also construct a heterogeneous-agent OLG model of married and single households to analyze the welfare effect of Social Security in the presence/absence of life insurance and annuity markets. However, in their model, the household's labor supply is inelastic, the age-earning profiles of the workers are deterministic, and Social Security benefits are uniform and independent of the household's earning histories. Thus, the present paper contributes to the current literature by demonstrating how married couples react to a future Social Security benefit schedule by choosing their optimal labor supply and saving.====The present paper is also related to recent studies on the female labor supply with a partial-equilibrium life cycle model. Olivetti (2006) constructs a life cycle model of married couples that includes the home production of childcare and the learning-by-doing type of human capital accumulation, and she analyzes the importance of these factors for the increase in female market work hours. Attanasio et al. (2008) also construct a life cycle model of female labor participation (the male labor supply is assumed to be inelastic). They explain how the female labor supply has been changed by the declining cost of raising children and labor participation. They assume that the earnings of the husband and wife are subject to positively correlated permanent shocks. Similar to these studies, the present paper assumes unitary households—perfectly altruistic married couples—but focuses more on the household's reaction to the current and future OASI policy. More recently, Sánchez-Marcos and Bethencourt (2018) construct a partial-equilibrium life cycle model with married and widowed households, and they analyze the effect of the U.S. Social Security system, including survivors benefits, on female labor participation rates.====The rest of this paper is presented as follows: Section 2 describes the heterogeneous-agent OLG model with the joint decision making of married couples, Section 3 shows the calibration of the baseline economy to the U.S. economy, Section 4 explains the effects of removing spousal and survivors benefits in equilibrium transition paths, Section 5 examines the robustness of the model, and Section 6 concludes the paper. The computational algorithm used to solve the household's optimization problem is described in Appendix A.",The joint labor supply decision of married couples and the U.S. Social Security pension system,https://www.sciencedirect.com/science/article/pii/S1094202518302424,January 2019,2019,Research Article,292.0
"Donovan Kevin,Herrington Christopher","Yale University, United States of America,Virginia Commonwealth University, United States of America","Received 11 January 2017, Revised 6 July 2018, Available online 25 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.07.003,Cited by (8),"We develop a dynamic lifecycle model to study long-run changes in college completion and the relative ability of college versus non-college students in the early twentieth century. The model is disciplined in part by constructing a historical time series on real college costs from printed government documents dating to 1916. The model captures nearly all of the increase in attainment and ability sorting between college and non-college individuals between the 1900 to 1950 birth cohorts. Time variation in college costs, the college earnings premium, and the precision of ability signals all play a critical role for explaining different data moments and time periods, primarily through their interaction with binding borrowing constraints. Our quantitative results imply that attainment is broadly driven by the interaction of changing real college costs and the rising earnings premium, while ability sorting is driven by the earnings premium and increasing precision of ability signals.","During the twentieth century, higher education expanded dramatically in the United States. As shown in Fig. 1a, bachelor's degree completion as a share of the 23-year-old population increased from less than four percent to more than 30 percent between the 1900 and 1972 birth cohorts. Concurrent with the increase in college attainment, the gap in measured cognitive ability widened substantially between individuals who completed at least some college and those whose formal education ended with high school (i.e., “non-college” individuals). This pattern is shown in Fig. 1b, which plots the difference between the average ability percentile of college and non-college individuals.==== The average college student born around 1900 had measured cognitive ability about 10 percentage points above the average non-college individual, and this gap more than doubled by the 1940s birth cohorts.====We think it important that theories of educational attainment be able to jointly confront both trends for two reasons. First, both result from the decisions of high school graduates making college attendance decisions. Thus, underlying economic factors affecting one margin potentially impact the other. Second, the ability of students potentially affects college completion through its impact on dropout risk. While much research has focused on post-World War II policy in accounting for these trends, less is known about the factors driving attainment and sorting among pre-World War II cohorts.==== This is in spite of the fact that these cohorts account for the vast majority of attainment growth and ability sorting during the 1900s (Fig. 1).====The goal of this paper is to evaluate the causes of these empirical trends in a historical context and study what – if any – relationship there is between the factors driving college attainment and selection by ability. This task is complicated by the various changes in the aggregate economy and education sector over the early 20th century. We combat this issue by developing a quantitative theory of educational attainment disciplined using newly digitized historical data. Specifically, we develop a lifecycle model populated by an exogenous number of male and female high school graduates in each birth cohort. Endowed with financial assets and an ability to complete college – both of which are heterogeneous – these individuals immediately decide whether to enter college or go to work. Moreover, ability is not perfectly known. Consistent with evidence in Cunha et al. (2005), individuals observe only a noisy signal about true ability, which introduces a measure of uncertainty when combined with the limited borrowing opportunities for college. These features allow us to quantitatively compare a number of potential explanations within the framework of the model.====Naturally, our model predictions rely heavily on the costs and benefits associated with attending college, and we therefore take care to measure them properly. On the cost side, we construct out-of-pocket costs for tuition and fees by combining data from a series of printed government documents dating back to 1916. This data work allows us to feed in realistic time series data for college costs, instead of relying on assumptions such as a constant tuition growth rate. Furthermore, we construct the opportunity cost of attending college, along with the benefit of attaining a degree, by computing detailed wage profiles and dropout risk by ability. Wage profiles are estimated from the 1940 to 2000 U.S. Censuses and 2006–2010 American Community Surveys. The wage profiles are allowed to depend on sex, age, and education to accurately capture the changing education earnings premia in the United States. These time series are exogenously fed into the model to capture the relevant tradeoffs faced by high school graduates when choosing to enter college.====In our framework students can pay for college using a combination of endowed financial assets (i.e., parental transfers) and borrowing. Yet despite the historical data we do have, estimating how these features vary over time is beyond what we have available. We therefore set parameters for these as follows. First, we use more recent and reliable micro data to estimate a flexible joint distribution of ability and parental transfers by combining data from the 1979 National Longitudinal Survey of Youth (NLSY) and High School & Beyond Survey (HSB).==== Second, we assume that the borrowing constraint is a constant fraction of expected lifetime income. We choose this fraction to match attainment in the initial 1900 birth cohort and hold it fixed across all cohorts; however, the amount of borrowing available changes with both the level of wages and the premia afforded to those who complete more than high school education. Moreover, because ability determines the likelihood of completing college and thus expected earnings, individuals of different ability will have different borrowing limits as well. In addition, we model the introduction of Federal student loans for later cohorts, which can offer expanded borrowing opportunities for some individuals.====With the calibrated model in hand, we assess its capacity to replicate college attainment and ability changes across cohorts. Our benchmark quantitative results show that it can. Between the 1900 and 1950 birth cohorts, the model predicts annualized growth in college attainment of 3.77 percent, compared to 3.98 percent in the data. Similar results hold when we consider other sub-periods. In terms of ability sorting, the model predicts that the average college attendee in the 1900 birth cohort has IQ 10 percentage points higher than the average non-college individual. This gap more than doubles by the 1950 cohort, just as in the data. Thus, the model predicts almost the entire increase in college attainment from the 1900 to 1950 cohorts while also matching the increased ability sorting over the same period. When we consider post-1950 cohorts, we find that it matches the overall trend between cohorts born 1950 to 1972, predicting an annualized growth rate of 1.2 percent, compared to 0.80 percent in the data. Indeed, over the entire 1900 to 1972 birth cohort, the model matches the annual growth in college attainment almost exactly, predicting a 2.98 percent annual growth rate, compared to 3.00 in the data, even though the calibration only targets college attainment for the initial cohort. We note, however, that as in much of the literature, the model struggles to match the time series variation within the 1950 to 1972 birth cohorts (Card and Lemieux, 2001a), so these post-1950 results should be interpreted with more caution.====Since the model matches the aggregate trends well, we next investigate the underlying causes of our results. We find that time series variation in earnings premia, college costs, and ability signal variance interact critically with binding borrowing constraints to generate our results. To start, note that purely from a lifetime income perspective, college costs are sufficiently low and earnings premia sufficiently high that ==== high school graduates would like to attend college. Put differently, borrowing constraints are critical for keeping some high school graduates out of college.====These binding constraints then interact with time varying college costs and earnings premia to generate our results. First, the low earnings premium we observe for pre-1930 cohorts endogenously tightens the borrowing constraint through its effect on expected income, implying fewer individuals can fund college education. The rising premium for post-1930 cohorts begins to loosen borrowing restrictions, thus increasing college attainment. A similar result emerges for costs – relative to income, college costs spike for 1910 to 1920 cohorts, drop sharply between the 1920 and 1930 cohorts, then rise steadily after that. Declining costs allow more people to fund college education and help generate the large increase in attainment among the 1920 to 1930 cohorts. The combination of the two features are critical. Without these cost fluctuations, we would first overshoot attainment for 1910–1920 cohorts and then undershoot post-1920. Without the rise in earnings premia we would undershoot attainment for the 1930–1950 cohorts. The combination of rising costs and rising earnings premia jointly generate the steady increase in attainment seen in the data for post-1930 cohorts.====Interestingly, however, costs play relatively little role in understanding the rising ability gap over time. To see why, consider the type of student who changes her attendance decision in response to a decline in college costs. Intuitively, only marginal ability individuals switch because high ability students can already borrow sufficiently against their own future income to fund college. That is, they borrow against the fact that they are more likely to graduate and reap higher wages. The marginal student is below average compared to the existing set of college students, but above average compared to the set of non-college individuals. When these marginal students switch from non-college to college, the ability gap is little affected because the average ability of both college and non-college individuals decreases. The same logic applies if ability is imperfectly observed, as in our model, but additional noise in the college entry decision further tempers the quantitative impact of costs on ability sorting.====The college earnings premium, on the other hand, plays an important quantitative role in shrinking the ability gap among the earliest cohorts. Our counterfactual exercise holding the earnings premium constant across cohorts increases the ability gap by 65 percent for the 1900 cohort and 33 percent for the 1920 cohort, when the actual earnings premia were substantially lower. Because the low earnings premia generate tight borrowing constraints, a number of high ability students with low assets do not attend college. As the earnings premia rise, a larger fraction of all individuals attend college, implying that the group of non-college individuals becomes increasingly comprised of first-year college dropouts. These dropouts are negatively selected on ability, thus inducing a larger ability gap as the returns to college increase.====Finally, the increasing precision of ability signals over time plays a critical role in understanding sorting. We attribute this to rapid growth of standardized testing during the first half of the century, which gave students more precise information about their own ability relative to peers in their cohort. If early cohorts had ability signals as precise as later cohorts, our model would instead generate a decline in the ability gap over time. Again, the result rests critically on individuals' ability to finance college. With high signal variance, the set of people with perceived high ability are not necessarily those with truly high ability. These misinformed individuals internalize their expected loose borrowing constraints due to (perceived) high ability, and thus choose to enter college. As the signal gets more precise, true ability becomes more aligned with perceived ability, and the truly high ability students begin to sort into college. Thus, the precision of the signal shifts only the ==== of people who go to college, with little change in the overall number.====Taken together, our results show the intricate balance of several times varying forces are required to match the college attainment and ability sorting of cohorts in the early twentieth century.",Factors affecting college attainment and student ability in the U.S. since 1900,https://www.sciencedirect.com/science/article/pii/S1094202518303892,January 2019,2019,Research Article,293.0
"Iftikhar Zainab,Zaharieva Anna","IRES, Universite Catholique de Louvain, Department of Economics, Louvain-la-Neuve, Belgium,Center for Mathematical Economics and Faculty of Business Administration and Economics, Bielefeld University, Bielefeld, Germany","Received 22 September 2016, Revised 19 June 2018, Available online 23 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.07.004,Cited by (18),"In this study we develop and calibrate a search and matching model of the German labour market and analyse the impact of a 25% increase in immigration observed in the period 2012–2016. Our model has two production sectors (tradable manufacturing and non-tradable services), two skill groups and two ethnic groups of workers (natives and immigrants). Moreover, we allow for the possibility of vertical skill mismatch of high skill workers, endogenous price setting in the non-tradable sector and fiscal redistribution policy. We find that immigrant workers are underrepresented in services compared to manufacturing, so there is only a moderate output increase in the production of services in response to immigration. This output increase is insufficient to cover the associated higher demand for services, which generates a higher price in this sector and stimulates ","This study investigates the effect of immigration on the German labour market in a search and matching framework. We develop a detailed model of the German labour market with heterogeneous worker groups and calibrate it to match group-specific moments of empirical data in the year 2011. Based on this model we perform a theoretical experiment by increasing the stock of immigrants, adequate in size to the observed increase in the years 2012–2016, and quantify the economic impact on native workers and incumbent immigrants.====Existing literature is focused on estimating wage effects of immigration and is mostly based on the assumption of a competitive labour market with the marginal pricing of labour.==== This approach doesn't take into account the bargaining nature of wage-setting in Germany and high propensity of vertical skill mismatch, implying that labour market frictions may be an important transmission channel when analysing the effect of immigration, which is not captured by the neo-classical competitive market framework. Moreover, there remain open questions concerning the general equilibrium effects and welfare changes in response to immigration. Our study addresses these issues by explicitly distinguishing between the tradable manufactured goods and non-tradable services. Hence we are able to decompose the total effect of immigration into the separate contributions of a rise in labour supply, endogenous response in the creation of vacancies, price-setting in the non-tradable sector and the redistributive fiscal policy. To the best of our knowledge there is no other study for Germany that estimates these effects in a unified labour market framework with search frictions. To calibrate the model we use data from the German Socio-Economic Panel and aggregate macroeconomic indicators.====More specifically, we consider a labour market with two production sectors: tradable (manufacturing) and non-tradable (services). In both sectors high and low skill labour is employed in the production of output and combined together by means of a CES production function.==== Thus there are four separate submarkets in our model depending on the sector and the skill level of workers. We use a search and matching framework to model endogenous job creation (vacancies) in each submarket following the approach of Pissarides (2000). Every worker can be a native or an immigrant, so there are eight heterogeneous worker groups in our model depending on the ethnic origin, skill level of the worker and the production sector. Native and immigrant workers apply for the same vacancies in each of the four submarkets, however, their employment chances, productivities and wages can be different. Wages are set by means of Nash-bargaining and reflect differences in productivities and outside opportunities of workers.====Low skill workers can be employed and producing output or unemployed and searching for low skill jobs. Unemployed high skill workers are simultaneously applying to high and low skill jobs and can be vertically mismatched in the sense of overeducation. We assume that mismatched high skill workers performing low skill jobs continue searching on-the-job and applying to vacancies with a high skill requirement. Our motivation to account for the occupational mismatch of high skill workers follows the argument of Dustmann and Preston (2012) and Dustmann et al. (2013) that high skill immigrant workers from developing countries are often downgraded when they enter the labour market of a developed receiving country. Moreover, immigrant workers experience higher rates of overeducation in Germany compared to native workers (see Salikutluk et al., 2016). Accounting for the higher propensity of skill mismatch among immigrants allows us to capture the asymmetric effects of the recent low skill immigration on high skill native and incumbent immigrant workers.====Further, workers have homothetic preferences with a CES utility function and decide how to spend their income (wages or unemployment benefits) between consumption of goods and services. Given that Germany is an export oriented economy we assume that manufactured goods are traded on the international market and their price is exogenous to the model. In contrast, services are non-tradable, so their price is determined endogenously by equating the supply and demand. Thus we explicitly take into account that the inflow of new immigrants changes both the supply and demand for the non-tradable good (services) thereby influencing its price. Here we follow an approach of Altonji and Card (1991) who emphasise that “the arrival of new immigrants shifts the demand for city output and hence the demand functions for skilled and unskilled labour” (p. 203). Finally, we assume a balanced budget of the social planner, who collects income taxes, pays out unemployment benefits and distributes the remaining surplus as a lump-sum transfer to all workers.====We use data from the German-Socio Economic Panel, wave 2011, to calibrate group-specific wages, unemployment rates and mismatch rates. The data reveals that native workers are facing about 1.5% lower probability of unemployment and about 9% lower probability of overqualification than immigrant workers. Further, high skill workers earn about 50% more than low skill workers, however, most of this return (43%) is lost if high skill workers are mismatched and perform low skill jobs. Considering the sectoral perspective, we find that low skill immigrant workers are overrepresented in manufacturing jobs (12.6%) and underrepresented in service jobs (8.8%). One reason for this is that manufacturing is a low skill intensive sector and there is a higher fraction of the low skilled among immigrants compared to natives. Another reason could be a language disadvantage of immigrant workers in providing communication intensive services. In order to calibrate the matching function for Germany we used aggregate data of the Federal Employment Agency (BA) for the period 2000–2017. We find that the average vacancy duration has almost tripled over this period from 35 to 100 days, reflecting a rising shortage of the workforce. Our estimate of the elasticity of the matching function (job-filling rate) is equal to 0.46 and is close to 0.5, which is often assumed in the search and matching literature.====Having performed the calibration we analysed the implications of a 25% increase in immigration observed in Germany in the period 2012–2016. Due to the data limitation we consider refugees and economic immigrants as one group. Even though reliable data on the skill composition of recent migrants is not available yet, a large fraction of refugees from low income countries indicates that the inflow of recent migrants is low-skill biased, so we focus on the case of low skill immigration. In this case we find that immigration leads to 1.2% higher aggregate welfare of incumbent workers in Germany. The primary reason for a positive effect is a higher internal demand for the non-tradable good (services). Given that immigrant workers are overrepresented in manufacturing jobs, a larger fraction of new immigrants finds jobs in manufacturing compared to services, so the relative increase in the supply of services is insufficient to match the relative increase in the demand. This leads to a higher price of services and a higher labour demand in this sector inducing higher wages and lower unemployment of both skill groups.====Further, we find that immigration raises a positive surplus of the public budget. We decompose this change into a direct and an indirect effect. First, low skill immigrant workers have low wages and high unemployment rates, so the direct effect of adding more low skill immigrants to the market reduces the positive surplus of the budget. Hence the direct effect of low skill immigration is negative. However, there is also an indirect general equilibrium effect: native and incumbent immigrant workers in the non-tradable sector (services) receive higher wages and their unemployment rate is lower. This leads to the fact that their tax contributions are higher and budget expenses on their unemployment benefits are lower. So the indirect effect of immigration on the budget surplus is positive and dominates the direct negative effect.==== Allowing for higher lump-sum transfers to restore the balance of the budget we report a total increase in welfare equal to 3%. This corresponds to the elasticity of immigration equal to ==== and means that one percent increase in immigration leads 0.12% increase in average welfare. This estimate is somewhat larger than estimates provided by Battisti et al. (2017) for Germany due to the fact that we explicitly account for price adjustment in the non-tradable sector. Moreover, the positive effect of immigration on public finances in Germany is not completely unexpected. For example, in his empirical research Bonin (2014) reports that a netto contribution of an average immigrant living in Germany in 2012 was about €3300 with the total contribution of all immigrants equal to €22 Bln. Dustmann and Frattini (2014) report similar results for the UK.====Considering the effect on different worker groups, we find that productivity of low skill workers falls, whereas the productivity of high skill workers increases in response to low skill immigration in line with the neoclassical tradition in Borjas (1999). The new result is that immigration puts an upward pressure on the price of services benefiting workers employed in this sector. However, a higher price index is harmful for workers employed in manufacturing since their real income is lower. So there is an asymmetric effect on the two sectors. Whereas manufacturing workers are worse off due to the fact that services become more expensive, workers employed in services are gaining since higher prices stimulate job creation in their sector. Overall, low skill workers employed in manufacturing face lower wages and higher unemployment. Combined with a higher price of services this leads to the change in welfare of these workers equal to −3.3% prior to the budget adjustment and −1.2% afterwards. High skill workers employed in manufacturing gain in terms of productivity, which leads to higher wages and lower unemployment, but this positive effect is mitigated by higher prices, so the change in their welfare is equal to −0.3% prior to the budget adjustment and +1.3% afterwards. Low skill workers employed in services gain higher wages and lower unemployment, which is due to the positive price effect. So the change in their welfare is equal to +2.9% prior to the budget adjustment and +5.0% afterwards. Finally, high skill workers employed in services gain most from immigration. Not only their productivity increases, but also the price for their services is higher. This is associated with a change in welfare equal to +4.7% prior to the budget adjustment and +6.3% afterwards. Note that adverse immigration effects contribute to larger income inequality of incumbent workers.====We also perform a number of alternative calibrations and robustness checks. In particular, we compare our benchmark model with a competitive labour market framework. In both models immigration leads to higher demand for services which is not fully covered by the additional production of services, so the price of services is increasing. However, in the benchmark model with frictions higher price leads to more intensive job creation and accelerated reallocation of incumbent workers from unemployment into employment which is not possible in a competitive setting with full employment. Higher labour demand benefits workers in service occupations and mitigates the price increase in this sector, so the average increase in welfare is positive in the benchmark model with frictions but it is close to zero in a competitive setting.====Using a search and matching framework to address the impact of immigration is a relatively recent tendency in the literature. For example, Chassamboulli and Palivos (2014) have incorporated a nested CES production function with capital, skilled and unskilled labour into the frictional labour market and calibrated this model to the U.S. economy, which experienced a high-skill biased immigration in the period 2000–2009. They conclude that the overall effect of immigration in the U.S. in the considered period should be estimated as positive. Second closely related study is by Battisti et al. (2017). These authors calibrated their search and matching model to 20 different OECD countries and arrived at a general conclusion that immigration in the period 2000–2011 has increased the welfare of native workers in almost all these countries. Our model has a number of similarities with Battisti et al. (2017) e.g. the wage gap between natives and immigrants, redistributive fiscal policy and perfect substitution between these groups in the CES production function. However, there are also notable deviations, for example, in our model we consider two production sectors – manufacturing and services – with an endogenous price setting in services and vertical mismatch of native and immigrant workers. We find that the model with two production sectors is better in capturing the asymmetric effects of immigration on high and low skill workers, who are not equally distributed between these sectors. In particular, our model takes into account that labour market outcomes of low skill workers, who are overrepresented in manufacturing, are less sensitive to changes in the domestic output demand since manufactured goods are largely traded on the international market.====Another study by Ortega (2000) also deals with the issue of immigration in a frictional labour market. He considers a two–country model and shows that both equilibria with and without immigration between the two countries are possible, but the equilibrium with immigration Pareto dominates the other one. Finally, there are two other studies combining immigration and search frictions by Moreno-Galbis and Tritah (2016) and Nanos and Schluter (2014). In the former paper immigrant workers earn lower wages which leads to higher profits of firms. So they show theoretically and empirically that an inflow of immigrant workers intensifies job creation due to the higher expected profits of firms, which benefits natives and incumbent immigrants. This effect is also present in our study, however, in our model this positive effect is reduced due to the fact that not all matches between firms and immigrant workers lead to employment. The study by Nanos and Schluter (2014) constructs a theoretical search and matching model and calibrates it to the German data. One important advantage of Nanos and Schluter (2014) over our work is that they explicitly allow for on-the-job search of all workers in their model whereas in our model only mismatched high skill workers continue searching on-the-job. However, their model builds on the assumption that labour markets of immigrant workers are completely separated from those of natives, which is different in our model and allows us to quantify the general equilibrium effects of migration on native workers.====Next we turn to the characterisation of the large literature on the effect of immigration in a competitive labour market framework without frictions. Two early studies by Borjas (1999) and Ben-Gad (2004) consider a setting with homogeneous workers and show that the response of wages strongly depends on the mobility of capital stock. Schmidt et al. (1994) and Ben-Gad (2008) extend this framework to the case of heterogeneous skill groups. Even though both skilled and unskilled wages are generally expected to fall with a skill-balanced immigration, the elasticity of labour demand and the skill direction of immigration are found to be crucial in this setting. Empirical evidence suggests that the demand for skilled labour is less elastic than the demand for unskilled labour. Hence according to these studies the immigration surplus is maximised when the inflow of immigrants is exclusively skilled. To the best of our knowledge Schmidt et al. (1994) is the only immigration study emphasising the importance of incorporating labour unions and wage bargaining into the model, so they consider a right-to-manage model with a monopoly union, however, it is not calibrated to any specific economy.====Altonji and Card (1991) identify one of the positive general equilibrium effects of immigration. Specifically, in their model immigrants are likely to increase the overall demand for goods and services in the receiving country. This demand effect will tend to increase the overall demand for native labour, thus raising native wages and employment. Among more recent studies this idea is emphasised by Moretti, 2010a, Moretti, 2010b, who writes that “Every time a local economy generates a new job..., additional jobs might also be created, mainly through increased demand for local goods and services” (multiplier effect). Even though Moretti, 2010a, Moretti, 2010b doesn't consider immigration he finds that for each additional job in manufacturing, 1.6 jobs are created in the non-tradable sector. Moreover, one additional skilled job in the tradable sector generates 2.5 job in local goods and services. It is a similar multiplier effect in response to immigration that we quantify in our study for Germany.====Recent studies investigating the effect of immigration started with the seminal approach by Borjas (2003) who observed that the rise in the U.S. immigration was not uniformly distributed across education/experience cells. This novel approach was further extended in the research of Manacorda et al. (2012) for the United Kingdom and D'Amuri et al. (2010) for Germany. These studies allow for the imperfect substitution of native and immigrant workers and show that the additional inflow of foreign workers to UK and Germany has a large negative effect on wages of the existing immigrants in the country leaving native wages virtually unaffected. This conclusion is supported by Felbermayr et al. (2010) who include the possibility of unemployment into the model and show that a new immigration inflow of workers from the east-ward enlargement of the European Union had a strong positive effect on the unemployment of incumbent foreign workers in Germany. On the contrary, the effect on native employment is very modest.====Dustmann and Preston (2012) have criticised the education-experience cell approach by observing that immigrant workers downgrade at arrival, which implies accepting jobs with a lower education/experience requirement than comparable native workers. Hence immigrants may compete with native workers at parts of the skill distribution which is different to where they should be assigned based on their observable characteristics. In response to this critique Dustmann et al. (2013) develop a new estimation strategy and study a continuous effect of immigration on workers situated at different quantiles of the earnings distribution. Their findings suggest that immigration reduced wages of natives below the 20th percentile of the wage distribution in the UK, but slightly increased wages in the upper part of the distribution.====Overall, all of these studies are focused on the effect of immigration on wages rather than (un)employment. Only D'Amuri et al. (2010) and Felbermayr et al. (2010) follow a reduced form approach and assume a negative relation between employment and wages, however, this assumption is not micro-founded and there is no economic mechanism in their model leading to this relationship. This is the primary difference between these studies and our approach.====The outline of the paper is as follows. Section 2 presents notation and the economic environment. Section 3 describes the model, while section 4 presents the calibration of the model to the German labour market. Section 5 contains our main results for the benchmark model. Section 6 includes extensions, robustness checks and some alternative calibrations. Finally, section 7 concludes the paper.",General equilibrium effects of immigration in Germany: Search and matching approach,https://www.sciencedirect.com/science/article/pii/S1094202518302503,January 2019,2019,Research Article,294.0
Steinberg Joseph B.,"University of Toronto, Department of Economics, 150 St. George Street, Toronto, M5S 3G7, Canada","Received 29 June 2016, Revised 3 July 2018, Available online 21 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.07.002,Cited by (9),"Are U.S. trade deficits caused by high foreign saving—a global saving glut—or low domestic saving—a domestic saving drought? To answer this question, I conduct a wedge accounting analysis of U.S. trade balance dynamics during 1995–2011 using a dynamic general equilibrium model. I find that a global saving glut explains 96 percent of U.S. trade deficits in excess of those that would have occurred naturally as a result of productivity growth and demographic change. Contrary to widespread belief, however, investment distortions, not a global saving glut, account for much of the decline in real interest rates that has accompanied U.S. trade deficits.","The United States has run trade deficits for more than two decades. In a well-known speech, Bernanke (2005) argued that a high supply of saving in the rest of the world—a global saving glut—is responsible for U.S. trade deficits. More recently, however, Chinn and Ito, 2007, Chinn and Ito, 2008, Laibson and Mollerstrom (2010), and other researchers have argued that a low domestic supply of saving—a domestic saving drought—is the culprit. In this paper I use a dynamic general equilibrium model to quantify the contributions of domestic and foreign forces to U.S. trade balance dynamics between 1995–2011. I find that Bernanke (2005) was right: a global saving glut is, in fact, the primary cause of U.S. trade deficits.====Fig. 1 depicts the key facts that motivate this study. As the U.S. trade deficit, shown in panel (a), grew, the U.S. real interest rate, shown in panel (b), fell dramatically. Bernanke (2005) argued that this decline is evidence that foreign forces, rather than domestic ones, caused U.S. trade deficits. By contrast, Kehoe et al. (2018), henceforth KRS, find that while an increased supply of saving in the rest of the world accounts for U.S. trade deficits, it does not explain the dynamics of the real interest rate; instead, increased foreign saving caused the U.S. real exchange rate to appreciate as shown in panel (c). Panel (d) shows that the U.S. investment rate rose during the 1990s as the U.S. trade deficit began to grow, but declined during the recessions of 2000–2001 and 2008–2009. Laibson and Mollerstrom (2010) argue that U.S. investment rate dynamics suggest that domestic forces had caused U.S. trade deficits; if foreign forces had caused these deficits, they argue, the U.S. investment rate should have continued to rise during the mid-2000s. The objective of this paper is to use the data shown in Fig. 1 in a wedge accounting analysis to identify the causes of U.S. trade deficits.====In section 2, I use a series of two-period models to illustrate analytically the economic principles that underpin my analysis. First, I use a simple endowment economy to express Bernanke (2005)'s argument as a wedge accounting exercise. This model features two wedges: saving wedges in the United States and the rest of the world that distort households' incentives to borrow or lend as in Gourinchas and Jeanne (2013) and KRS. I show that a U.S. trade deficit could be the result of a high foreign saving wedge—a global saving glut—or a low U.S. saving wedge—a domestic saving drought—but that only the former would cause the real interest rate to fall. Next, I explain why investment data, and thus investment wedges, are also needed to identify the causes of U.S. trade deficits, and I add these ingredients to the model to describe the full wedge identification procedure. Last, I show that when domestic and foreign products are imperfect substitutes, a global saving glut has less impact on domestic real interest rates because it causes the real exchange rate to appreciate instead, particularly in countries like the United States in which domestic products account for the bulk of aggregate consumption expenditures.====In my quantitative analysis, I use a dynamic general equilibrium model to determine the causes of U.S. trade deficits. Households in the United States and the rest of the world work, consume, and borrow or lend by trading bonds. Each country has three production sectors: goods, services, and construction. Firms in each sector use capital, labor, and domestic and foreign intermediate inputs to produce their output. I calibrate the parameters that govern the model's production and demand structure so that it replicates an input–output matrix from the World Input Output Database (Timmer et al., 2015). When calibrated, this multi-sector, input–output structure captures several important facts about the U.S. economy and its relationship to the rest of the world: trade deficits are driven by deficits in goods trade, particularly intermediate goods, as shown by Fig. 2; construction, which is not traded, is the largest input to the production of U.S. investment; and U.S. consumption consists mostly of domestic services.====The model has several exogenous forces that affect trade balance dynamics. Two of these forces, labor productivity growth and demographic change, are taken from external data sources. Productivity in the rest of the world grows faster than U.S. productivity, creating a permanent-income motive for the United States to lend. At the same time, the U.S. population becomes younger than the rest of the world's population, creating a motive for the United States to borrow. The former motive is stronger than the latter, however; together, productivity growth and demographic change lead the United States to run a large trade surplus in equilibrium. The remaining drivers of trade balance dynamics are saving, investment, and trade wedges. The saving wedges distort households' Euler equations, the investment wedges distort the arbitrage conditions that relate returns on investment to returns on bonds, and the trade wedges distort marginal rates of substitution between domestic and foreign goods. In my wedge accounting exercise, I calibrate these wedges so that the model matches the U.S. trade balance, real interest rate, and real exchange rate, and the investment rates in the United States and the rest of the world during 1995–2011. To isolate the effects of each of these wedges, I construct a set of counterfactual equilibria in which I hold all but one of these wedges constant. I find that the rest of the world's saving wedge explains 96 percent of the cumulative difference between the observed trade deficit and the trade surplus that would have occurred naturally as a result of productivity growth and demographic change. Hence, a global saving glut is the primary cause of U.S. trade deficits.====Like KRS, I find that while a global saving glut also explains U.S. real exchange rate dynamics, it is not the primary cause of the decline in U.S. real interest rates. My results indicate that while saving distortions in the rest of the world do play a role in this decline, waning investment distortions in the United States and the rest of the world are also important factors as argued by Yi and Zhang (2016). Although this finding contradicts widespread belief that U.S. trade deficits and low interest rates are closely related, it is consistent with theory (see section 2) and with empirical estimates of the impact of foreign lending on U.S. interest rates (Krishnamurthy and Vissing-Jorgensen, 2007; Warnock and Warnock, 2009).====U.S. investment dynamics are driven both by domestic investment distortions and a global saving glut. Thus, Laibson and Mollerstrom (2010) are partly right; in isolation, a global saving glut would have caused U.S. investment to rise throughout the 2000s, but a drop in the U.S. investment wedge counteracted this effect. The trade wedge helps the model to capture the J-curve dynamics (Backus et al., 1994; Alessandria and Choi, 2018) of the U.S. trade balance and real exchange rate in the short run, particularly between 2002 and 2006, but plays little role in explaining cumulative U.S. trade deficits or the long-term real exchange rate appreciation that accompanied them.====My paper makes several contributions to the extensive literature on U.S. trade deficits, and, more broadly, global trade imbalances. Explanations for global imbalances abound: mercantilist government policy in emerging economies (Dooley et al., 2003, Dooley et al., 2007); emerging economies' poor financial systems (Caballero et al., 2008; Mendoza et al., 2009); U.S. government fiscal policy (Chinn and Prasad, 2003; Chinn and Ito, 2007, Chinn and Ito, 2008); asset price bubbles (Kraay and Ventura, 2007; Laibson and Mollerstrom, 2010; Gete, 2015); and cross-country differences in growth rates (Engel and Rogers, 2006; Choi et al., 2008; Backus et al., 2009), demographic trends (Sposi, 2017), or macroeconomic volatility (Fogli and Perri, 2015). These explanations can, broadly speaking, be split into two groups: foreign forces like mercantilism and poor financial development that boost saving (or depress investment) in the rest of the world; and domestic forces like asset bubbles and fiscal policy that depress saving (or boost investment) in the United States. My paper is the first to quantify the contributions of foreign and domestic forces to U.S. trade deficits. Additionally, I provide empirical evidence that a global saving glut is the product of capital controls and domestic financial frictions in the rest of the world; I find that saving distortions in the rest of the world are highly correlated with measures of capital account openness and domestic financial development in the rest of the world relative to the United States.====Several other papers use wedge accounting, which was originally developed by Cole and Ohanian (2002) and Chari et al. (2007) to study closed-economy business cycles, to analyze international capital flows. In a seminal paper, Gourinchas and Jeanne (2013) document that fast-growing countries save, rather than borrowing as neoclassical theory predicts, and show that saving wedges account for this pattern. Their small-open-economy analysis, however, cannot separately identify the roles of domestic and foreign distortions. Similarly, Choi et al. (2008), Gete (2015), and KRS simply assume that trade imbalances are driven by foreign saving wedges. Ohanian et al. (2015) and Sposi (2017) conduct wedge-accounting analyses of global imbalances using multi-country general equilibrium models, but their one-good environments abstract from the important features of U.S. trade and investment highlighted above and cannot explain exchange rate or interest rate dynamics. My paper is the first to conduct a wedge-accounting analysis of U.S. trade deficits using a general equilibrium model with a realistic input–output production structure, and the first to identify the contributions of domestic and foreign forces to U.S. real exchange rate and real interest rate dynamics.====Finally, my paper builds on work by KRS, who use a similar model to analyze the contribution of trade deficits to declining goods-sector employment in the United States. They assume that U.S. trade deficits are driven by a global saving glut and show that this assumption is consistent with the behavior of the U.S. real exchange, investment, and other macroeconomic variables. My quantitative analysis fills a number of gaps left by KRS: my model incorporates capital formation and a detailed input–output structure in the rest of the world; my wedge accounting procedure quantifies the contributions of saving distortions in both the United States and the rest of the world to U.S. trade deficits as well as the contributions of investment and trade distortions; and my approach to measuring demographic change and productivity growth in the rest of the world highlights the importance of fast-growing emerging economies like China for the behavior of the U.S. trade balance. Additionally, my findings about the relationship between foreign saving distortions and interest rates resolve a puzzle identified by KRS, who cannot explain why a global saving glut does not significantly affect U.S. real interest rates in their model, and I present evidence on the underlying causes of a global saving glut.",On the source of U.S. trade deficits: Global saving glut or domestic saving drought?,https://www.sciencedirect.com/science/article/pii/S1094202518303843,January 2019,2019,Research Article,295.0
"Nakata Taisuke,Schmidt Sebastian","Federal Reserve Board, United States of America,European Central Bank, Germany","Received 19 July 2017, Revised 27 June 2018, Available online 17 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.07.001,Cited by (29)," and output when the lower bound constraint is binding. In a calibrated quantitative model, we find that the introduction of an interest-rate smoothing objective can reduce the welfare costs associated with the lower bound constraint by about one-half.","Gradual adjustment in the federal funds rate has been a key feature of monetary policy in the United States. Over the two decades prior to December 2008—the beginning of the most recent lower-bound episode—the Federal Open Market Committee (FOMC) changed its target for the federal funds rate at 89 out of 191 meetings. At these 89 meetings, the FOMC adjusted the federal funds target rate, on average, just 33 basis points in absolute terms. More recently, when announcing the first increase in its target range for the federal funds rate in December 2015 after seven years of zero-interest rate policy, the FOMC emphasized that it expected the policy rate to increase only gradually (Federal Open Market Committee, 2015). Indeed, as of February 2018, the federal funds target range has been raised only five times, in steps of 25 basis points, since December 2015.====While there are likely myriad factors behind this gradual adjustment in the policy rate, some evidence suggests that the observed inertia in the policy rate reflects the central bank's deliberate desire to smooth the interest rate path beyond what the intrinsic inertia in economic conditions calls for (Coibion and Gorodnichenko, 2012; Givens, 2012). As we will review, several studies suggest that interest-rate smoothing can improve society's welfare in various environments.====In this paper, we revisit the desirability of interest-rate smoothing in an economy in which large contractionary shocks occasionally force the central bank to lower the policy rate to the zero lower bound (ZLB). We conduct our analysis in the framework of policy delegation in which society designs the central bank's objective function and the central bank, in turn, acts under discretion and sets the policy rate in accordance with the objective.==== In so doing, we stick to the optimal delegation literature's focus on simple non-state-contingent objective functions that involve only a small number of target variables.==== Using a stochastic New Keynesian model, we ask how modifying the central bank's objective function to include an interest-rate smoothing (IRS) objective affects stabilization policy and society's welfare, as measured by the expected lifetime utility of the representative household. We first use a stylized version of the model to transparently describe the key trade-off involved in adopting a gradualist policy. We then move on to the analysis of a quantitative model to understand the quantitative relevance of gradualism.====Our main finding is that adding an IRS objective to central banks' standard inflation and output gap stabilization objectives can go a long way in mitigating the adverse consequences of the ZLB constraint. In the aftermath of a deep recession involving a binding ZLB constraint, a gradualist central bank increases the policy rate more slowly than a central bank with the standard objective. Such a slow increase of the policy rate generates a temporary overheating of the economy, which mitigates the declines in inflation and output while the ZLB constraint is binding, by raising expectations of future inflation and real activity. A smaller contraction at the ZLB, in turn, alleviates the deflationary bias—the systematic undershooting of the inflation target—away from the ZLB via expectations. In equilibrium, interest-rate smoothing increases society's welfare by improving stabilization outcomes not only when the policy rate is at the ZLB but also when the policy rate is away from it.====Interest-rate smoothing, however, does not provide a free lunch. In particular, interest-rate smoothing prevents the central bank from responding sufficiently to less severe shocks that could be neutralized by an appropriate policy rate adjustment without hitting the ZLB. From a normative perspective, when the policy rate is away from the ZLB, the central bank should reduce the policy rate one-for-one to a downward shift in the natural real rate of interest—the real interest rate prevailing in an economy with flexible prices—to offset completely the effect of the shock to the natural real rate. A gradualist central bank will reduce the policy rate by less on impact, thus failing to keep inflation and the output gap fully stabilized.==== The optimal degree of interest-rate gradualism balances this cost against the aforementioned benefits. We find that the welfare gains from interest-rate smoothing are quantitatively important. In our quantitative model calibrated to match key features of the U.S. economy, a central bank with an optimized weight on its IRS objective improves society's welfare by about one-half relative to a discretionary central bank that has the same objective function as society.====We also explore a refinement to our baseline IRS objective function that enhances the welfare gains from interest-rate gradualism. Instead of a smoothing objective for the actual policy rate, the refinement requires the central bank to be concerned with smoothing of the shadow policy rate—the policy rate that it would like to set given the current state of the economy if the ZLB were not a constraint for nominal interest rates. If the policymaker aims to smooth the shadow rate, the lagged shadow rate becomes an endogenous state variable that remembers the history of inflation rates and output gaps. In particular, the larger the economic downturn in a liquidity trap, the lower the shadow rate and the longer the actual policy rate remains low. The resulting history dependence is akin to that observed under the optimal commitment policy, and increases the welfare gains from interest-rate smoothing.====The empirical evidence in favor of interest-rate gradualism is usually based on estimates of interest-rate feedback rules (e.g. Coibion and Gorodnichenko, 2012). To relate our results to the empirical literature, we also explore how the ZLB constraint affects the optimal degree of interest-rate smoothing in a Taylor-type interest-rate rule. We find that the ZLB constraint can help to rationalize the degree of interest-rate smoothing observed in practice.====Our paper is related to a body of work that has examined various motives for gradualist monetary policy.==== The strand of the literature closest to our paper emphasizes the benefits of interest-rate smoothing arising from its ability to steer private-sector expectations by inducing history dependence in the policy rate (Woodford, 2003b; Giannoni and Woodford, 2003).==== Another strand of the literature emphasizes the benefit of interest-rate smoothing arising from its ability to better manage uncertainties about data, parameter values, or the structure of the economy facing the central bank (Sack, 1998; Orphanides and Williams, 2002; Levin et al., 2003; Orphanides and Williams, 2007). Some studies emphasize the costs and benefits of interest-rate smoothing arising from its effects on financial stability (Cukierman, 1991; Stein and Sunderam, 2015). None of these studies, however, accounts for the ZLB on nominal interest rates. Our contribution is to show that the presence of the ZLB provides a novel rationale for guiding monetary policy by gradualist principles.====Our work is also closely related to a set of papers that explores ways to mitigate the adverse consequences of the ZLB constraint while preserving time consistency. In particular, several approaches try to mimic the prescription of the optimal commitment policy for liquidity traps to keep the policy rate low for long, thus generating a temporary overheating of the economy. Eggertsson (2006) and Burgert and Schmidt (2014) show that in models with non-Ricardian fiscal policy and nominal government debt, discretionary policymakers can provide incentives to future policymakers to keep policy rates low for long periods of time by means of expansionary fiscal policy that raises the nominal level of government debt. Jeanne and Svensson (2007), Berriel and Mendes (2015), and Bhattarai et al. (2015) find that central banks' balance sheet policies can, under certain conditions, operate as a commitment device for discretionary policymakers that facilitates the use of “low-for-long” policies. Finally, Billi (2017) explores policy delegation schemes in which the discretionary central bank's standard inflation and output gap stabilization objectives are replaced by either a price-level or a nominal-income stabilization objective. He finds that these delegation schemes can generate low-for-long policies and thereby improve welfare.==== Compared with these approaches, the relative appeal of our approach is that it neither requires an additional policy instrument nor does it represent a fundamental departure from the inflation-targeting framework currently embraced by many central banks.====The paper is organized as follows. Section 2 describes the baseline model. Section 3 presents the main results on the effect of interest-rate smoothing in the baseline model. Section 4 presents additional results for the baseline model. The first part considers a refinement of the interest-rate smoothing objective that helps to further mitigate the welfare costs associated with the ZLB. The second part explores the role of cost-push shocks for the welfare results. The third part analyzes the optimal degree of gradualism in an interest-rate feedback rule. Section 5 extends the analysis to a more elaborate quantitative model of the U.S. economy. A final section concludes.",Gradualism and liquidity traps,https://www.sciencedirect.com/science/article/pii/S1094202518303806,January 2019,2019,Research Article,296.0
"Pintus Patrick A.,Suda Jacek","CNRS-InSHS and Aix-Marseille University, 3 rue Michel-Ange, 75794 Paris cedex 16, France,Narodowy Bank Polski, Warsaw School of Economics, and FAME|GRAPE, Swietokrzyska 11/21, 00-919 Warsaw, Poland","Received 13 July 2015, Revised 5 June 2018, Available online 10 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.06.002,Cited by (8),"This paper develops a simple business-cycle model in which financial shocks have large macroeconomic effects when private agents are gradually learning the uncertain environment. Agents update their beliefs about the reduced-form structure of the economy. Because the persistence of leverage is overestimated by adaptive learners, the responses of output, investment, and other aggregates under adaptive learning are significantly larger than under rational expectations. In our benchmark case calibrated using US data on leverage, debt-to-GDP and land value-to-GDP ratios for 1996Q1–2008Q4, learning amplifies leverage shocks by a factor of about three, relative to rational expectations. When fed with actual leverage innovations observed over that period, the learning model predicts that the persistence of leverage shocks is increasingly overestimated after 2002 and that a sizeable recession occurs in 2008–2010, while its rational expectations counterpart predicts a counter-factual expansion. In addition, we show that procyclical leverage reinforces the amplification due to learning and, accordingly, that macro-prudential policies that enforce countercyclical leverage dampen the effects of leverage shocks.","Whether or not banks and other financial institutions, policy-makers, households and firms relied on a decent approximation of the “true” probability distribution prior to the 2007–2008 financial collapse is a key question to address if one is to understand the Great Recession. On the theoretical side, answering such a question requires relaxing the assumption that the data-generating process is known when agents make decisions in an economy that is subject to random disturbances (see Woodford, 2013 for a recent survey). New tractable approaches to tackle parameter uncertainty have recently been proposed. Fuster et al. (2012a) show that asset price booms and busts are more satisfactorily explained when forecasters are assumed to use simple models that typically underestimate mean-reversion and overestimate the persistence of the impact of shocks. Ilut and Schneider (2014) show that shocks driving the unknown mean level of productivity contribute significantly, under ambiguity aversion, to business cycles. In both contributions, the key assumption is that there is uncertainty about the “true” parameters (e.g. the mean and the autocorrelation) governing the random shocks that affect the economy.====Our distinctive contribution to this strand of literature is the introduction of statistical learning in a setting where consumption, investment and labor supply decisions depend on aggregate credit availability, which itself varies over time in a stochastic fashion. We focus on how decision-makers set and revise their beliefs about the parameters of the stochastic process governing financial shocks as new observed data arrive, following Marcet and Sargent (1989) and Evans and Honkapohja (2001) (see also the related discussion in Evans, 2012). Although our analysis is similar in spirit to Fuster et al. (2012a), Ilut and Schneider (2014), since agents do not know these parameters, the key dimension we add is that agents learn their economic environment by estimating the unknown parameters driving aggregate credit conditions and by updating each period such estimates. In turn, those beliefs about the shock process are used to make forecasts that affect decisions and hence determine economic outcomes.====In a simple business-cycle model with collateral constraints and stochastic leverage, we show that dynamics under learning can differ significantly from the dynamics under rational expectations. More precisely, we compare two settings: (====) the model with full information (rational expectations), in which agents know the parameters governing the VAR(1) process governing the behavior of the economy; (====) the model of incomplete information with learning, in which agents do not know the “true” parameters of the VAR(1) model and update their estimates as new data arrive. We show that when agents update their beliefs about the parameters that govern both the dynamics of endogenous variables as well as the unobserved process driving shocks to the leverage ratio, the responses of output and other aggregates under adaptive learning can differ significantly from those under rational expectations. We find that the amplification of financial shocks is particularly large when agents ==== either the persistence of financial shocks or the long-run level of credit conditions. When we simulate the stylized model using actual financial innovations we find that our learning model delivers a sizeable recession in 2008–2010, in contrast to the full information rational expectations version that predicts a counterfactual ==== when subjected to the same financial shocks. The key random variable in our analysis is the leverage ratio defined by how much households can borrow out of the land market value.====Our results can be anticipated by looking at the panels in Fig. 1. Panel a) of Fig. 1 plots the US quarterly households' leverage data (provided by Boz and Mendoza, 2014) over the period 1996Q1–2010Q1 that covers the latest boom-bust behavior in the housing market.==== Panel b) of Fig. 1 reports the autocorrelation coefficient of the exogenous leverage component that is estimated under learning. The autocorrelation coefficient graphed in panel b) is obtained through constant-gain recursive estimation in real time, as new data is collected. Panel b) shows that when confronted with the data in panel a), learning agents think of the AR(1) leverage process as moving towards unit root at the end of the period. Essentially, the level of leverage trends up and accelerates in 2007, when the land price stops expanding and starts falling while borrowing is sticky – panel a) – and this translates into an increasing estimate of autocorrelation by learning agents, which ends up being very close to one in the last two quarters of 2008. As a result, both the observed level of leverage and its estimated persistence peak at the same time, in 2008Q4. On the other hand, the rational expectations estimate, obtained by ordinary least squares over the whole sample period, is lower than its learning counterpart over the period shown in panel b) and it is around 0.976. The learning model generates the estimate shown in panel b) when fed with the actual leverage innovations and predicts that the impact of the negative shock to leverage observed in 2008Q4 is about three times bigger than under full information. When believed under learning to be close to permanent, shocks to credit conditions have a larger effect on the economy compared to rational expectations. As a consequence, the learning model generates, under a negative leverage shock, a contraction that is similar in magnitude to the Great Recession.====Our focus is on financial shocks that drive up and down the leverage ratio. We derive our set of quantitative results about the model-generated recession for 2008–2010. In line with the literature (see Kiyotaki et al., 2011, Liu et al., 2013, Justiniano et al., 2015, Kaas et al., 2016, among others), we show that replicating the observed boom-bust pattern of land prices over the 2000s requires another source of shocks in addition to leverage shocks. We introduce a land price shock that we calibrate to ensure that the behavior of the endogenous land price matches its observed counterpart.==== We also feed the model with the actual innovations to leverage and show that the model predicts a sizeable fall in output, of similar magnitude to that observed during the Great Recession. More precisely, we do that in a setting where agents do not know the steady state and the autocorrelation matrix in the VAR representation of the economy that they have to estimate using constant-gain learning. Agents observe a sequence of positive innovations to leverage in the run-up to the crisis. When agents revise their estimates in reaction to these leverage shocks, the learning model predicts a boom that is followed by a sizeable recession in 2008–2010 (see Fig. 8): output falls by about 5%, a close match to the actual data. In sharp contrast, in the 2000s the rational expectations model predicts a long recession that is followed by an expansion, which are both at odds with the data.====In order to dissect such an amplification mechanism, we perform two theoretical experiments. The first one assumes that agents know the economy's steady state and, in particular, the mean level of leverage but not its autocorrelation, which is allowed to be time-varying. We calibrate the model using data on leverage, debt-to-GDP and land value-to-GDP ratios for the period 1996Q1–2008Q4 and we subject the economy to the large negative shock to leverage that was observed in 2008Q4 (see panel a) in Fig. 1). We compare the responses of the linearized economy under adaptive learning and under rational expectations. Learning agents overestimate the autocorrelation of the leverage shock, which is believed to be close to unity according to panel b) in Fig. 1. Our typical sample of results shows that learning amplifies leverage shocks by a factor of about 2.5 (see Fig. 2), compared to rational expectations. Consumption and investment go down by a significantly larger margin under learning because de-leveraging is more severe: land price and debt are much more depressed after the negative leverage shock hits when its persistence is overestimated by agents who are constantly learning their environment and, because of recent past data, temporarily pessimistic.====We next show that the magnitude of the consequent recession may in part be attributed to the high ==== of leverage (and the correspondingly high level of the debt-to-GDP ratio) observed in 2008Q4. When the same negative leverage shock occurs in the model calibrated using 1996Q1 data, when leverage was much lower, the impact on output's response is reduced by about two thirds (see Fig. 3). In this sense, our model points at the obvious fact that financial shocks to leverage originate larger aggregate volatility in economies that are more levered. In addition, we ask whether procyclical leverage may act as an aggravating factor and our answer is positive. The assumption that households' leverage responds to land price is motivated by the recent evidence provided by Mian and Sufi (2011). The counter-factual experiment with countercyclical leverage shows dampened effects of leverage shocks, with responses of aggregate variables under learning that are close to their rational expectations counterpart (see Fig. 3). One possible interpretation of this finding is that macro-prudential policies enforcing countercyclical leverage have potential stabilizing effects on the economy in the face of financial shocks, at small cost provided that non-distortionary policies are implemented (e.g. through regulation).====Our second theoretical experiment is carried out under the assumption that learning agents do not know the steady state of the economy and, in particular, that they do not know the long-run level of leverage. This is arguably a more realistic description of the difficulties that forecasting agents/econometricians face when trying to figure out the parameters governing the data generating process. In such a setting, we again feed the model with the negative leverage shock of about ==== observed in 2008Q4 and we show that the responses of the economy are further amplified under learning when agents' belief about the mean level of leverage is overestimated (see Fig. 4). Summing up the results from our two model experiments, our main conclusion is that in a world where agents overestimate the persistence of financial shocks and/or the mean level of leverage, learning amplifies the disturbances to borrowing capacity.==== Our paper connects to several strands of the literature. The macroeconomic importance of financial shocks has recently been emphasized by Jermann and Quadrini (2012), among others, and our paper contributes to this literature about credit shocks by showing how learning under parameter uncertainty matters. Closest to ours are the papers by Adam et al. (2012), who focus on exogenous interest rate changes, and by Boz and Mendoza (2014), who show how changes in the leverage ratio have large macroeconomic effects under Bayesian learning and Markov regime switching.==== As in Boz and Mendoza (2014), we focus on leverage shocks but our setting is different. First, our model with adaptive learning is easily amenable to simulations and we solve it using the usual linearization techniques. Because we assume that agents are adaptively learning through VAR estimation, it is possible to enrich the model by adding capital accumulation and endogenous production. Most importantly, our model predicts large output drops when the economy is hit by negative leverage shocks. In sharp contrast, absent TFP shocks, output remains constant after a financial regime switch in Boz and Mendoza (2014). In addition, we show that our results are robust to the introduction of heterogeneous agents and endogenous interest rate. Since in such setting the interest rate is endogenously procyclical, it could completely defeat the effect of an increase in credit supply even under learning. Our robustness analysis makes clear that this is not the case and that amplification due to learning does not rely upon the small-open economy assumption, an issue that is addressed neither in Adam et al. (2012) nor in Boz and Mendoza (2014).==== To sum up, this paper follows the literature by emphasizing how financial shocks affect asset prices, but it also differs by measuring to what extent financial shocks help explain the fall in output and investment observed over the Great Recession period.====Our paper also relates to some of the insights in Howitt (2001) and Fuster et al., 2012a, Fuster et al., 2012b. Contrary to Fuster et al., 2012a, Fuster et al., 2012b who assume that agents use a misspecified model, in our case the overestimated persistence of shocks arises endogenously under adaptive learning when agents face the sequence of financial innovations that was observed in the run-up to the crisis.==== In addition, our paper stresses that endogenous changes in the beliefs about the long-run level of leverage may also matter for explaining why shocks get amplified under adaptive learning. This is also where our paper departs from Ilut and Schneider (2014), who do not consider learning in their setting with exogenously driven ambiguity about TFP shocks. Although, in theory, endogenous persistence could arise under i.i.d. shocks when agents learn the steady-state leverage level, this effect turns out not to be quantitatively important in our setting. In contrast, persistent slumps are shown by Kozlowski et al. (2015) to arise under reasonable calibrations when learning is about the tails of the real shocks distribution. Close to our macro perspective is Pancrazi and Pietrunti (2015), who use survey evidence to show that financial experts overestimated long-run prices and did not forecast a mean reversion in long-run housing price dynamics, implying the overestimation of the persistence of housing prices. In a similar vein, Piazzesi and Schneider (2009) identify momentum traders, using Michigan Survey of Consumers data, and show that their size (and optimism) increased during the housing price boom.====In the literature, the idea that procyclical leverage has adverse consequences on the macroeconomy is developed in Geanakoplos (2010) (see also Cao, 2017, and Geerolf, 2015 for a tractable model of endogenous leverage distribution). Although our formulation of elastic leverage is derived in an admittedly simple setup, it allows us to examine its effect in a full-fledged macroeconomic setting. Lastly, the notion that learning is important in business-cycle models when some change in the shock process occurs has been discussed by, e.g., Bullard and Duffy (2004) and Williams (2003). More recently, Eusepi and Preston (2011) have shown that learning matters in a standard RBC model when the economy is hit by shocks to productivity growth. Our paper adds to this literature by focusing on financial shocks under collateral constraints.====The paper is organized as follows. Section 2 presents the model and derives its rational expectations equilibrium. Section 3 relaxes the assumption that agents form rational expectations in the short run and it shows how financial shocks are amplified under learning when agents update their estimates about the parameters of the stochastic process driving financial shocks. Section 4 shows that the model with learning predicts a sizeable recession in 2008–2010 while its rational expectations counterpart does not. Section 5 gathers concluding remarks and all proofs are presented in the appendices.",Learning financial shocks and the Great Recession,https://www.sciencedirect.com/science/article/pii/S1094202518302400,January 2019,2019,Research Article,297.0
Siassi Nawid,"University of Konstanz, Germany","Received 21 June 2018, Available online 9 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.06.004,Cited by (2),"Marriage is one of the most important determinants of economic prosperity, yet most existing theories of ==== ignore the role of the family. This paper documents that the distributions of earnings and ==== are highly concentrated, even when disaggregated into single and married households. At the same time, there is a large marriage gap: married people earn on average 26 percent more income, and they hold 35 percent more net worth. To account for these facts, I develop a ","Marriage is one of the most important determinants of economic prosperity. Yet, perhaps surprisingly, most existing theories of inequality abstract from the role of the family: the standard framework for studying inequality treats all households as being comprised of a single decision-maker, without making the role of marital status explicit. The main contribution of this paper is to fill this void and present a theory that can account for the observed inequality between single and married households.====The cross-sectional distributions of earnings, income and wealth in the United States display a large degree of concentration.==== When disaggregated into married and single households, economic prosperity remains very unequally distributed ==== both subgroups. At the same time, there is a striking divergence ==== both subgroups: on average, married people have 32.3 percent higher labor earnings, they earn 25.5 percent more income, and they are 34.9 percent richer than singles. This disparity, the marriage gap, is not driven by very rich households as the corresponding ratios of medians look similar, and it is robust to controlling for age and other potentially confounding factors. In light of the empirical relevance of the family – in the year 2013, half of the adult population in the United States was married – reconciling the strong association between marital status and economic outcomes is a challenge that models of inequality must face.====To account for these stylized facts, I develop a dynamic general equilibrium model where females and males transit stochastically through a life cycle that consists of a working age and a retirement phase. Throughout working age, they face uninsurable, idiosyncratic labor productivity risk, and they make decisions on consumption, labor supply and savings. Single individuals further participate in a marriage market where they randomly meet individuals of opposite gender and decide whether to get married. Marriage formation decisions in the model are bilateral, i.e. both partners have to be better off entering marriage, and they depend on the productive characteristics – permanent and time-varying labor productivities, individual assets – of both persons. Married households pool their income and savings, and they commit to a Pareto-efficient allocation subject to exogenous divorce risk. Once retired, households make consumption-savings decisions taking into account their bequest motives. Finally, there is a firms sector producing a homogeneous good with capital and labor services, and there is a government that taxes income and pays pension benefits to retirees.====A calibrated version of the model is largely successful in accounting for the facts from the data. The model generates substantial inequality within the subgroups of single and married households, and it predicts a positive marriage gap for earnings, income and wealth. Three factors are key for generating the marriage gap. First, the model creates endogenously strong selection effects into marriage: more productive and asset-rich individuals are also more likely to find a spouse on the marriage market. With persistent productivity levels, this force shapes the composition of the population of single and married households and contributes crucially to generating the marriage gap. Second, one of the novel features I propose in this paper is the notion of stronger dynastic ties in households with descendants. The benchmark models embeds this idea by allowing bequest motives to depend on the presence of descendants. Since married households tend to have more descendants, a dynastic saving motive adds to explaining the marriage gap in wealth. Third, the model captures the differential tax treatment of single and married households as implied by the U.S. tax code. I show that married couples often face lower effective income taxes by filing their taxes jointly, which leads them to work longer hours and raises their permanent disposable income. Since precautionary saving is tightly associated with a target wealth-to-permanent-income ratio, married couples are also led to save more. In setting up a series of counterfactuals, I show that all three factors contribute significantly to generating the marriage gap, with the largest contribution coming from selection into marriage.====In further experiments, I explore the behavior of single and married households along the wealth distribution. A key finding emerging from the analysis is that marriage plays a relatively larger role for poor and middle-class households. For instance, I show that divorce risk has a disproportionally larger effect on savings for asset-poor couples. One reason is that the precautionary motive leads these couples to insure against the risk of losing access to intrahousehold insurance. A second reason is that the possibility of potentially returning to the marriage market in the future provides an additional incentive to accumulate assets. In fact, I show that marriage rates are strongly responsive to the amount of initial wealth brought into the marriage market, in particular for asset-poor individuals. As a final experiment, I conduct a hypothetical policy reform that abolishes the possibility of joint tax filing for married couples. I find that such a reform would lead to substantial output gains and a rise in hours worked driven by increasing labor supply of secondary earners in married couples. My results further suggest a strong rise in assortative mating as joint filing is relatively more advantageous for couples with very different incomes.====This paper relates to two strands of literature. First, it builds upon a host of studies addressing income and wealth inequality in general equilibrium frameworks, e.g. Aiyagari (1994), Huggett (1996), Krusell and Smith (1998), Castañeda et al. (2003) and De Nardi (2004). All of these studies, however, abstract from modeling the marital status of a household. A second body of literature makes this distinction explicit by considering single and married households separately; some examples include Aiyagari et al. (2000), Greenwood et al. (2003), Regalia and Ríos-Rull (2001), Hong and Ríos-Rull (2007), Heathcote et al. (2017), and Guvenen and Rendall (2015). To the best of my knowledge, there is little theoretical work on the role of marital status and cross-sectional inequality in a joint context. The study most closely related is Guner and Knowles (2004) who investigate the link between marriage and wealth in an OLG setting. In their model, single agents and married couples make decisions on consumption, hours and savings, and they decide whom to marry and when to divorce. Their model can generate a positive wealth gap. The mechanism is based on their modeling of consumption within married households as a public good and calibrating it using estimates for adult equivalence scales. Since their setup only consists of three periods, it neglects intrahousehold insurance effects on savings and may perform poorly when tested along the cross-sectional dimension. Mustre-del-Río (2015) develops a model that matches the wealth distribution of married households; however, he does not include singles in the analysis. Greenwood et al. (2016) construct a framework of marriage, divorce, educational attainment and married female labor force participation. Their analysis focuses on the impact on income inequality without looking explicitly at wealth inequality. Several other studies examine the relationship between marital sorting and income inequality in a static context: Fernández and Rogerson (2001), Choo and Siow (2006) and Greenwood et al. (2014) are some examples from this literature.====This paper contributes to a growing literature employing dynamic life-cycle models with equilibrium marriage markets. Caucutt et al. (2002) explore the link between wage inequality and marriage decisions of young women. However, like Greenwood et al. (2003) and Greenwood et al. (2016), they assume that agents are unable to borrow or save. Other studies allow for savings in dynamic life-cycle models, but they treat marital transitions as exogenous shocks. For instance, Cubeddu and Ríos-Rull (2003) study the implications of marital turnover for macroeconomic aggregates, and Fernández and Wong (2014) investigate the link between marital instability and married women's labor force participation. So far only a handful of papers have embedded savings into a dynamic model of equilibrium household formation/destruction. Mazzocco et al. (2007) propose a collective household model of labor supply, savings and marriage decisions. Their analysis is centered on individual household behavior, while my paper focuses on cross-sectional inequality and the marriage gap. Voena (2015) constructs a collective model of household decision making to explore how divorce laws affect couples' intertemporal behavior. Her focus is on studying the role of limited commitment in different divorce regimes. She models remarriages as exogenous events, while in my paper household formation is modeled explicitly and divorces are exogenous. Santos and Weiss (2016) assess to what extent the rise in labor income volatility over the last decades can explain the decline and delay in first-time marriages. While they also consider a unitary model of the household, a key difference to my framework is that they do not allow for divorces and remarriages.====The divergence in effective taxation between single and married households and the role of joint tax filing is studied by Guner et al. (2012). These authors construct a life-cycle economy populated by single and married workers who differ according to their labor efficiency and age. At the heart of their analysis lies an exogenous utility cost of participating in the labor market which allows them to focus on the extensive margin of married female labor supply. The authors use their model to evaluate various tax reforms, ==== the abolition of joint tax filing. Their results associate substantial output gains with such a reform and, thus, share a commonality with my own findings. However, their framework does not allow for endogenous adjustments along the household formation margin which is an important model ingredient put forward in this paper.====The literature has identified bequest motives to generate a lifetime saving profile consistent with the data. De Nardi (2004) shows that intentional bequests can explain the emergence of very large estates at the upper tail of the wealth distribution. Fuster et al. (2008) study the significance of intergenerational links for the impact of various tax reform proposals. They find that tax reforms can have very different implications depending on whether individuals derive utility from bequeathing to their descendants or not. Laitner (2001) introduces the existence of intentional and accidental bequests in a common framework. In his model, a constant fraction ==== of households cares about their heirs; the remaining households care only about their own utility. In comparison to his approach, my framework relates the existence of a bequest motive explicitly to the presence of a descendant.====The remainder of the paper is organized as follows. Section 2 documents the empirical facts. In Section 3, I present my benchmark model and define a stationary equilibrium. Section 4 describes the calibration strategy, and Section 5 contains my results. Section 6 assesses the implications of moving from joint tax filing to separate filing. Concluding remarks are offered in Section 6.",Inequality and the marriage gap,https://www.sciencedirect.com/science/article/pii/S1094202518303521,January 2019,2019,Research Article,298.0
Eden Maya,"Economics Department, Sachar International Center Room 130D, Brandeis University, 415 South Street, Waltham, MA 02453, United States of America","Received 18 September 2017, Revised 19 June 2018, Available online 5 July 2018, Version of Record 29 January 2019.",https://doi.org/10.1016/j.red.2018.06.003,Cited by (0),This paper proposes a simple two-country model for studying the ,"A common view of the recent financial crisis is that it was triggered by a shock to the perceived safety of capital-backed securities (CBS), such as ABS and MBS.==== A large literature that followed has focused on the economic roles of safe and liquid assets, and the recessionary dynamics associated with a contraction in their stocks.====Against this backdrop, this paper studies a simple neoclassical framework in which liquidity is valued as an input of production. In addition, a liquidity premium on CBS acts as a subsidy for investment: since investors can issue liquid claims backed by capital, there is a heightened incentive to invest when the liquidity premium is high. The goal of this paper is to study the recessionary dynamics associated with a negative shock to the ability to issue liquid CBS, and to gauge at the extent to which such a model can match the evolution of various macroeconomic aggregates during the 2008-9 recession.====The model features a production framework in the spirit of money-in-the-production-function models (see Levhari and Patinkin, 1968 and Fischer, 1974). The key departure from this earlier literature is that, in addition to money and other government-issued liquid claims, liquidity services can be derived from liquid claims that are backed by private capital. Recent literature has emphasized the role of privately-issued liquid claims in providing liquidity services (see, for example, Benk et al., 2005, Gorton and Metrick, 2010, Kiyotaki and Moore, 2012 or Del Negro et al., 2017). I estimate that, in 2008, the value of liquidity services from capital was roughly half of the value of liquidity services from government-issued liquid claims (see Fig. 1). After the crisis, the value of liquidity services from capital drops to 0.====The crisis is modeled as a shock to the transformation rate between capital and liquid claims, reducing the amount of liquid claims that can be issued against a unit of capital. The recessionary dynamics that follow can be understood purely in neoclassical terms: the contraction in liquidity inputs triggers an instantaneous drop in output. In addition, lower expected liquidity benefits associated with capital lead to a contraction in investment and subsequent declines in output and consumption. A calibration suggests that the recessionary dynamics generated by this model are roughly consistent with the magnitudes observed in the data.====In the model, capital-backed liquid claims and government-issued liquid claims are imperfect substitutes. As the US economy is an exporter of liquidity services (see, for example, Gourinchas and Rey, 2007), the value of its liquidity exports may either increase or decrease following the shock: on the one hand, the economy loses revenue from issuing privately-backed liquid claims. On the other hand, the shock leads to an increase in the demand for government-issued liquid claims, which are somewhat substitutable with privately-issued liquid claims. The calibration suggests that while the former effect dominated during the recent crisis, liquidity provision by the government mostly offset the loss in revenue from exporting liquidity services.====This framework is also useful for evaluating the benefits of quantitative easing (QE) policies during the crisis. QE is modeled as an increase in the supply of government-issued liquid claims. The model suggests that QE policies were welfare-improving, and produced a welfare gain equivalent to a permanent increase in consumption of 0.3%.====It is useful to clarify how this model relates to other models of the recent crisis. Broadly, theories attempting to explain the behavior of macroeconomic variables during the crisis fall into one of two categories: the “credit shock” view (e.g., Jermann and Quadrini, 2012) and the “demand shock” view (e.g., Mian and Sufi, 2014 or Heathcote and Perri, 2018). The key difference between them is in the implications for the investment wedge. According to the “credit shock” view, the tightening of a financial constraint creates an investment wedge that prevents firms from taking advantage of high-return investment opportunities. An appealing feature of this view is its focus on a shock to the financial system, in line with the clear financial origins of the crisis.====In contrast, according to the “demand shock” view, the contraction in investment is reflective of lower returns to capital due to depressed demand. However, a potential source of discomfort with this view is its apparent disconnect from any financial culprit.====The mechanism explored in this paper features both a financial culprit for the crisis and a decline in investment demand. Rather than focusing on the financial system's role in credit provision, this paper highlights its role in liquidity creation. The impetus for the crisis is a shock to the ability to issue liquid claims backed by physical capital, which can be interpreted as a negative productivity shock to the “liquidity creating” technology.==== The shock reduces the demand for investment, as capital is no longer valued for its liquidity-creating properties.====This mechanism appears also in Kiyotaki and Moore (2012) and Del Negro et al. (2017), who also study the macroeconomic implications of a shock to the liquidity of capital. These papers consider a richer environment in which the liquidity of capital interacts with a borrowing constraint: because firms are credit constrained, they must sell some of their equity in order to finance new investment opportunities. In their models, the liquidity of capital is important because it helps relax financing constraints. Other papers emphasize other ways in which liquid assets help improve market efficiency (see, for example, in Guerrieri and Lorenzoni, 2009, Gorton, 2010, Lucas and Stokey, 2011, Eden and Kay, 2015, Caballero and Farhi, 2017 and Quadrini, 2017). The goal of this paper is to provide a unifying framework for assessing the macroeconomic implications of a shock to private liquidity provision, without taking a strong stance on the microeconomic details of the interactions between liquidity and market efficiency.",International liquidity rents,https://www.sciencedirect.com/science/article/pii/S1094202518302278,January 2019,2019,Research Article,299.0
"Lester Benjamin,Weill Pierre-Olivier,Zetlin-Jones Ariel","Federal Reserve Bank of Philadelphia, United States of America,University of California, Los Angeles, and NBER, United States of America,Carnegie Mellon University, United States of America","Available online 21 June 2019, Version of Record 21 June 2019.",https://doi.org/10.1016/j.red.2019.05.006,Cited by (0),None,None,RED Special Issue on Fragmented Financial Markets: An Introduction,https://www.sciencedirect.com/science/article/pii/S1094202519302583,21 June 2019,2019,Research Article,301.0
